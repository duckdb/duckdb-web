---
warning: DO NOT CHANGE THIS MANUALLY, THIS IS GENERATED BY https://github/duckdb/community-extensions repository, check README there
title: crawler
excerpt: |
  DuckDB Community Extensions
  SQL-native web crawler with HTML extraction and MERGE support

extension:
  name: crawler
  description: SQL-native web crawler with HTML extraction and MERGE support
  version: '2026020501'
  language: C++
  build: cmake
  license: MIT
  maintainers:
    - onnimonni

  requires_toolchains: rust
  excluded_platforms: "windows_amd64_mingw;wasm_mvp;wasm_eh;wasm_threads"

repo:
  github: midwork-finds-jobs/duckdb-crawler
  ref: f0aad857435a2ce138c567e8af11f9d4f8ae0c25
  ref_next: 7725ede99eff1657a2cee770048be33328ae9f02

docs:
  hello_world: |
    SELECT url, jq(html.document, 'h1').text as title
    FROM crawl(['https://example.com']);
  extended_description: |
    The crawler extension provides SQL-native web crawling capabilities for DuckDB.

    Features:
    - `crawl()` table function with automatic rate limiting and robots.txt compliance
    - `crawl_url()` for LATERAL joins
    - `sitemap()` for XML sitemap parsing
    - `read_html()` for Google Sheets-style IMPORTHTML (tables, lists, JS variables)
    - `jq()` and `htmlpath()` functions for CSS selector-based extraction
    - `html.readability` for article extraction
    - `html.schema` for JSON-LD/microdata parsing
    - `CRAWLING MERGE INTO` syntax for upsert operations

    Example with read_html (like Google Sheets =IMPORTHTML):
    ```sql
    SELECT * FROM read_html('https://en.wikipedia.org/wiki/...', 'table.wikitable', 1);
    SELECT * FROM read_html('https://example.com/page', 'js=jobs');
    ```

    Example with extraction:
    ```sql
    SELECT
        url,
        jq(html.document, '.price', 'data-amount') as price,
        html.readability.title as article_title
    FROM crawl(['https://example.com/products']);
    ```

    Example with MERGE:
    ```sql
    CRAWLING MERGE INTO pages
    USING crawl(['https://example.com']) AS src
    ON (src.url = pages.url)
    WHEN MATCHED THEN UPDATE BY NAME
    WHEN NOT MATCHED THEN INSERT BY NAME;
    ```

    For full documentation see: https://github.com/midwork-finds-jobs/duckdb-crawler

extension_star_count: 4
extension_star_count_pretty: 4
extension_download_count: 416
extension_download_count_pretty: 416
image: '/images/community_extensions/social_preview/preview_community_extension_crawler.png'
layout: community_extension_doc
---

### Installing and Loading
```sql
INSTALL {{ page.extension.name }} FROM community;
LOAD {{ page.extension.name }};
```

{% if page.docs.hello_world %}
### Example
```sql
{{ page.docs.hello_world }}```
{% endif %}

{% if page.docs.extended_description %}
### About {{ page.extension.name }}
{{ page.docs.extended_description }}
{% endif %}

### Added Functions

<div class="extension_functions_table"></div>

|     function_name     | function_type | description | comment | examples |
|-----------------------|---------------|-------------|---------|----------|
| crawl                 | table         | NULL        | NULL    |          |
| crawl_stream          | table         | NULL        | NULL    |          |
| crawl_url             | table         | NULL        | NULL    |          |
| css_select            | scalar        | NULL        | NULL    |          |
| discover              | scalar        | NULL        | NULL    |          |
| htmlpath              | scalar        | NULL        | NULL    |          |
| jq                    | scalar        | NULL        | NULL    |          |
| read_html             | table         | NULL        | NULL    |          |
| sitemap               | table         | NULL        | NULL    |          |
| stream_merge_internal | table         | NULL        | NULL    |          |

### Added Settings

<div class="extension_settings_table"></div>

|            name            |                     description                     | input_type | scope  | aliases |
|----------------------------|-----------------------------------------------------|------------|--------|---------|
| crawler_default_delay      | Default crawl delay in seconds if not in robots.txt | DOUBLE     | GLOBAL | []      |
| crawler_max_response_bytes | Maximum response body size in bytes (0 = unlimited) | BIGINT     | GLOBAL | []      |
| crawler_respect_robots     | Whether to respect robots.txt directives            | BOOLEAN    | GLOBAL | []      |
| crawler_timeout_ms         | HTTP request timeout in milliseconds                | BIGINT     | GLOBAL | []      |
| crawler_user_agent         | User agent string for crawler HTTP requests         | VARCHAR    | GLOBAL | []      |


