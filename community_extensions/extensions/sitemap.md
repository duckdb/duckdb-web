---
warning: DO NOT CHANGE THIS MANUALLY, THIS IS GENERATED BY https://github/duckdb/community-extensions repository, check README there
title: sitemap
excerpt: |
  DuckDB Community Extensions
  Parse XML sitemaps from websites with automatic discovery via robots.txt

extension:
  name: sitemap
  description: Parse XML sitemaps from websites with automatic discovery via robots.txt
  version: 0.1.0
  language: C++
  build: cmake
  license: MIT
  excluded_platforms: "windows_amd64_mingw"
  requires_toolchains: "libxml2;zlib"
  maintainers:
    - onnimonni

repo:
  github: midwork-finds-jobs/duckdb-sitemap
  ref: 6cd72d65fa3ee544cc25bac574094a8c4faeeb4d

docs:
  hello_world: |
    -- Get all URLs from a sitemap (auto-prepends https://)
    SELECT * FROM sitemap_urls('example.com');

    -- Filter specific URLs
    SELECT * FROM sitemap_urls('example.com')
    WHERE url LIKE '%/blog/%';

    -- Count URLs by type
    SELECT
        CASE
            WHEN url LIKE '%/product/%' THEN 'product'
            WHEN url LIKE '%/blog/%' THEN 'blog'
            ELSE 'other'
        END as type,
        count(*) as count
    FROM sitemap_urls('https://example.com')
    GROUP BY type;

  extended_description: |
    The sitemap extension provides a table function for parsing XML sitemaps from websites.
    It automatically discovers sitemaps via robots.txt and supports recursive sitemap index traversal.

    Features:
    - Automatic sitemap discovery from /robots.txt
    - Sitemap index support (nested sitemaps)
    - Retry logic with exponential backoff
    - Respects Retry-After header on 429 responses
    - Gzip decompression for .xml.gz files
    - Multiple namespace support (standard + Google schemas)
    - SQL filtering with WHERE clauses

    The function returns a table with columns:
    - url: Page URL (VARCHAR)
    - lastmod: Last modification date (VARCHAR, optional)
    - changefreq: Change frequency hint (VARCHAR, optional)
    - priority: Priority hint 0.0-1.0 (VARCHAR, optional)

    Options:
    - follow_robots (BOOLEAN): Parse robots.txt first (default: true)
    - max_depth (INTEGER): Max sitemap index nesting (default: 3)
    - max_retries (INTEGER): Max retry attempts (default: 5)
    - backoff_ms (INTEGER): Initial backoff in ms (default: 100)
    - max_backoff_ms (INTEGER): Max backoff cap in ms (default: 30000)

    Example with options:
    ```sql
    SELECT * FROM sitemap_urls(
        'https://example.com',
        follow_robots := true,
        max_depth := 5,
        max_retries := 10
    ) WHERE url LIKE '%/product/%';
    ```

    Compose with http_request to fetch page content:
    ```sql
    SELECT s.url, h.body
    FROM sitemap_urls('https://example.com') s
    JOIN LATERAL (SELECT * FROM http_get(s.url)) h ON true
    WHERE s.url LIKE '%/product/%'
    LIMIT 10;
    ```

extension_star_count: 1
extension_star_count_pretty: 1
extension_download_count: 19
extension_download_count_pretty: 19
image: '/images/community_extensions/social_preview/preview_community_extension_sitemap.png'
layout: community_extension_doc
---

### Installing and Loading
```sql
INSTALL {{ page.extension.name }} FROM community;
LOAD {{ page.extension.name }};
```

{% if page.docs.hello_world %}
### Example
```sql
{{ page.docs.hello_world }}```
{% endif %}

{% if page.docs.extended_description %}
### About {{ page.extension.name }}
{{ page.docs.extended_description }}
{% endif %}


