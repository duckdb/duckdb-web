{
	"data": [
		{
			"title": "Frequently Asked Questions",
			"text": "who makes duckdb duckdb is maintained by dr mark raasveldt dr hannes mühleisen along with many other contributors from all over the world mark hannes and some other contributors are employees at the database architectures group at the centrum wiskunde informatica cwi in amsterdam the netherlands cwi is the national research institute for mathematics and computer science in the netherlands why call it duckdb ducks are amazing animals they can fly walk and swim they can also live off pretty much everything they are quite resilient to environmental challenges a duck s song will bring people back from the dead and inspires database research they are thus the perfect mascot for a versatile and resilient data management system also the logo designs itself where do i find the duckdb logo you can download the duckdb logo here br web png jpg br print svg pdf br br the duckdb logo website were designed by jonathan auch max wohlleber how can i expand the duckdb website the duckdb website is hosted by github pages its repository is here pull requests to fix issues or generally expand the documentation section are very welcome i benchmarked duckdb and its slower than some other system in a departure from traditional academic systems research practise we have at first focused our attention on correctness not raw performance so it is entirely possible duckdb is slower than some other more mature system at this point that being said we are now confident duckdb produces correct query results and are actively working to make it fast too so publishing benchmark numbers from the current preview releases is certainly interesting but should not be taken as the definitive results on what the duckdb architecture can or cannot do does duckdb use simd duckdb does not use explicit simd instructions because they greatly complicate portability and compilation instead duckdb uses implicit simd where we go to great lengths to write our c code in such a way that the compiler can auto-generate simd instructions for the specific hardware as an example why this is a good idea porting duckdb to the new apple m1 architecture took 10 minutes",
			"category": "docs",
			"url": "../docs/faq",
			"blurb": "Who makes DuckDB?  DuckDB is maintained by   Dr. Mark Raasveldt  &   Dr. Hannes Mühleisen  along with   many other..."
		},
		{
			"title": "CSV Import/Export",
			"text": "copy moves data between duckdb tables and external comma separated value csv files csv import copy from imports data into duckdb from an external csv file into an existing table the data is appended to whatever data is in the table already the amount of columns inside the file must match the amount of columns in the table table_name and the contents of the columns must be convertible to the column types of the table in case this is not possible an error will be thrown if a list of columns is specified copy will only copy the data in the specified columns from the file if there are any columns in the table that are not in the column list copy from will insert the default values for those columns -- copy the contents of a comma-separated file test csv without a header into the table test copy test from test csv -- copy the contents of a comma-separated file with a header into the category table copy category from categories csv header -- copy the contents of lineitem tbl into the lineitem table where the contents are delimited by a pipe character copy lineitem from lineitem tbl delimiter -- read the contents of a comma-separated file names csv into the name column of the category table any other columns of this table are filled with their default value copy category name from names csv syntax csv export copy to exports data from duckdb to an external csv file it has mostly the same set of options as copy from however in the case of copy to the options specify how the csv file should be written to disk any csv file created by copy to can be copied back into the database by using copy from with the same set of options the copy to function can be called specifying either a table name or a query when a table name is specified the contents of the entire table will be written into the resulting csv file when a query is specified the query is executed and the result of the query is written to the resulting file -- copy the contents of the lineitem table to the file lineitem tbl where the columns are delimited by a pipe character including a header line copy lineitem to lineitem tbl delimiter header -- copy the l_orderkey column of the lineitem table to the file orderkey tbl copy lineitem l_orderkey to orderkey tbl delimiter -- copy the result of a query to the file query csv including a header with column names copy select 42 as a hello as b to query csv with header 1 delimiter syntax parameters name description --- --- table_name the name optionally schema-qualified of an existing table column_name an optional list of columns to be copied if no column list is specified all columns of the table will be copied filename the path and name of the input or output file boolean specifies whether the selected option should be turned on or off you can write true on or 1 to enable the option and false off or 0 to disable it the boolean value can also be omitted in which case true is assumed format if this option is used its value must be csv with any other format an error will be thrown delimiter specifies the string that separates columns within each row line of the file the default value is a comma null specifies the string that represents a null value the default is an empty string please note that in copy from both an unquoted empty string and a quoted empty string represent a null value if any other null string is specified again both its quoted and its unquoted appearance represent a null value copy to does not quote null values on output even if force_quote is true header specifies that the file contains a header line with the names of each column in the file thus copy from ignores the first line when importing data whereas on output copy to the first line of the file contains the column names of the exported columns quote specifies the quoting string to be used when a data value is quoted the default is double-quote escape specifies the string that should appear before a data character sequence that matches the quote value the default is the same as the quote value so that the quoting string is doubled if it appears in the data force_quote forces quoting to be used for all non-null values in each specified column null output is never quoted if is specified non-null values will be quoted in all columns this option is allowed only in copy to force_not_null do not match the specified columns values against the null string in the default case where the null string is empty this means that empty values will be read as zero-length strings rather than nulls this option is allowed only in copy from encoding if this option is used its value must be utf8 with any other encoding an error will be thrown notes it is recommended that the file name used in copy always be specified as an absolute path the values in each record are separated by the delimiter string if the value contains the delimiter string the quote string the null string a carriage return or line feed character then the whole value is prefixed and suffixed by the quote string and any occurrence within the value of a quote string or the escape string is preceded by the escape string you can also use force_quote to force quotes when outputting non-null values in specific columns the csv format has no standard way to distinguish a null value from an empty string you can use force_not_null to prevent null input comparisons for specific columns in csv format all characters are significant a quoted value surrounded by white space or any characters other than delimiter will include those characters this can cause errors if you import data from a system that pads csv lines with white space out to some fixed width if such a situation arises you might need to preprocess the csv file to remove the trailing white space before importing the data into duckdb",
			"category": "docs",
			"url": "../docs/csv_import",
			"blurb": "COPY  moves data between DuckDB tables and external Comma Separated Value (CSV) files.   CSV Import  COPY ... FROM ..."
		},
		{
			"title": "Sitemap",
			"text": "",
			"category": "docs",
			"url": "../docs/index",
			"blurb": ""
		},
		{
			"title": "Python API",
			"text": "installation the duckdb python api can be installed using pip pip install duckdb please see the installation page for details it is also possible to install duckdb using conda conda install python-duckdb -c conda-forge basic api usage the standard duckdb python api provides a sql interface compliant with the db-api 2 0 specification described by pep 249 similar to the sqlite python api startup shutdown to use the module you must first create a connection object that represents the database the connection object takes as parameter the database file to read and write from the special value memory the default can be used to create an in-memory database note that for an in-memory database no data is persisted to disk i e all data is lost when you exit the python process if you would like to connect to an existing database in read-only mode you can set the read_only flag to true read-only mode is required if multiple python processes want to access the same database file at the same time import duckdb to start an in-memory database con duckdb connect database memory to use a database file not shared between processes con duckdb connect database my-db duckdb read_only false to use a database file shared between processes con duckdb connect database my-db duckdb read_only true if you want to create a second connection to an existing database you can use the cursor method this might be useful for example to allow parallel threads running queries independently a single connection is thread-safe but is locked for the duration of the queries effectively serializing database access in this case connections are closed implicitly when they go out of scope or if they are explicitly closed using close once the last connection to a database instance is closed the database instance is closed as well querying sql queries can be sent to duckdb using the execute method of connections once a query has been executed results can be retrieved using the fetchone and fetchall methods on the connection below is a short example create a table con execute create table items item varchar value decimal 10 2 count integer insert two items into the table con execute insert into items values jeans 20 0 1 hammer 42 2 2 retrieve the items again con execute select from items print con fetchall jeans 20 0 1 hammer 42 2 2 the description property of the connection object contains the column names as per the standard duckdb also supports prepared statements in the api with the execute and executemany methods in this case the parameters are passed as an additional parameter after a query that contains placeholders here is an example insert a row using prepared statements con execute insert into items values laptop 2000 1 insert several rows using prepared statements con executemany insert into items values chainsaw 500 10 iphone 300 2 query the database using a prepared statement con execute select item from items where value 400 print con fetchall laptop chainsaw do not use executemany to insert large amounts of data into duckdb see below for better options efficient transfer transferring large datasets to and from duckdb uses a separate api built around numpy and pandas this api works with entire columns of data instead of scalar values and is therefore far more efficient duckdb supports registering a pandas data frame as a virtual table comparable to a sql view below is an example import pandas as pd test_df pd dataframe from_dict i 1 2 3 4 j one two three four con register test_df_view test_df con execute select from test_df_view con fetchall 1 one 2 two 3 three 4 four you can now use the registered view to create a persistent table in duckdb con execute create table test_df_table as select from test_df_view duckdb keeps a reference to the pandas data frame after registration this prevents the data frame from being garbage-collected by the python runtime the reference is cleared when the connection is closed but can also be cleared manually using the unregister method when retrieving the data from duckdb back into python the standard method of calling fetchall is inefficient as individual python objects need to be created for every value in the result set when retrieving a lot of data this can become very slow in duckdb there are two additional methods that can be used to efficiently retrieve data fetchnumpy and fetchdf fetchnumpy fetches the data as a dictionary of numpy arrays fetchdf fetches the data as a pandas dataframe below is an example of using this functionality fetch as pandas data frame df con execute select from items fetchdf print df item value count 0 jeans 20 0 1 1 hammer 42 2 2 2 laptop 2000 0 1 3 chainsaw 500 0 10 4 iphone 300 0 2 fetch as dictionary of numpy arrays arr con execute select from items fetchnumpy print arr item masked_array data jeans hammer laptop chainsaw iphone mask false false false false false fill_value dtype object value masked_array data 20 0 42 2 2000 0 500 0 300 0 mask false false false false false fill_value 1e 20 count masked_array data 1 2 1 10 2 mask false false false false false fill_value 999999 dtype int32 also refer to the data import documentation for more options of efficiently importing data relational api in addition to the sql api duckdb supports a programmatic method to construct queries see https github com duckdb duckdb blob master examples python duckdb-python py for an example",
			"category": "docs",
			"url": "../docs/api/python",
			"blurb": "Installation  The DuckDB Python API can be installed using   pip :  pip install duckdb . Please see the   installation..."
		},
		{
			"title": "Client APIs Overview",
			"text": "there are various client apis for duckdb duckdb s native api is c with official wrappers available for c python r and java there are also contributed third-party duckdb wrappers for ruby go c and rust pages in this section",
			"category": "docs",
			"url": "../docs/api/overview",
			"blurb": "There are various client APIs for DuckDB. DuckDB's native API is   C++ , with official wrappers available for   C ,  ..."
		},
		{
			"title": "CLI API",
			"text": "coming",
			"category": "docs",
			"url": "../docs/api/cli",
			"blurb": ""
		},
		{
			"title": "Rust API",
			"text": "installation the duckdb rust api can be installed from crate io please see the docs rs for details basic api usage duckdb-rs is an ergonomic wrapper based on the duckdb c api please refer to the readme for details startup shutdown to use duckdb you must first initialize a connection handle using connection open connection open takes as parameter the database file to read and write from you can also use connection open_in_memory to create an in-memory database note that for an in-memory database no data is persisted to disk i e all data is lost when you exit the process use duckdb params connection result let conn connection open_in_memory you can conn close the connection manually or just leave it out of scope we had implement the drop trait which will automatically close the underlining db connection for you querying sql queries can be sent to duckdb using the execute method of connections or we can also prepare the statement and then query on that conn execute insert into person name data values params me name me data let mut stmt conn prepare select id name data from person let person_iter stmt query_map row ok person id row get 0 name row get 1 data row get 2 for person in person_iter println found person person unwrap",
			"category": "docs",
			"url": "../docs/api/rust",
			"blurb": "Installation  The DuckDB Rust API can be installed from   crate.io . Please see the   docs.rs  for details.   Basic API..."
		},
		{
			"title": "R API",
			"text": "installation the duckdb r api can be installed using install packages please see the installation page for details basic api usage the standard duckdb r api implements the dbi interface for r if you are not familiar with dbi yet see here for an introduction startup shutdown to use duckdb you must first create a connection object that represents the database the connection object takes as parameter the database file to read and write from the special value memory the default can be used to create an in-memory database note that for an in-memory database no data is persisted to disk i e all data is lost when you exit the r process if you would like to connect to an existing database in read-only mode set the read_only flag to true read-only mode is required if multiple r processes want to access the same database file at the same time library dbi to start an in-memory database con dbconnect duckdb duckdb dbdir memory to use a database file not shared between processes con dbconnect duckdb duckdb dbdir my-db duckdb read_only false to use a database file shared between processes con dbconnect duckdb duckdb dbdir my-db duckdb read_only true connections are closed implicitly when they go out of scope or if they are explicitly closed using dbdisconnect querying duckdb supports the standard dbi methods to send queries and retreive result sets dbexecute is meant for queries where no results are expected like create table or update etc and dbgetquery is meant to be used for queries that produce results e g select below an example create a table dbexecute con create table items item varchar value decimal 10 2 count integer insert two items into the table dbexecute con insert into items values jeans 20 0 1 hammer 42 2 2 retrieve the items again res dbgetquery con select from items print res item value count 1 jeans 20 0 1 2 hammer 42 2 2 duckdb also supports prepared statements in the r api with the dbexecute and dbgetquery methods here is an example prepared statement parameters are given as a list dbexecute con insert into items values list laptop 2000 1 if you want to reuse a prepared statement multiple times use dbsendstatement and dbbind stmt dbsendstatement con insert into items values dbbind stmt list iphone 300 2 dbbind stmt list android 3 5 1 dbclearresult stmt query the database using a prepared statement res dbgetquery con select item from items where value list 400 print res item 1 laptop do not use prepared statements to insert large amounts of data into duckdb see below for better options efficient transfer to write a r data frame into duckdb use the standard dbi function dbwritetable this creates a table in duckdb and populates it with the data frame contents for example dbwritetable con iris_table iris res dbgetquery con select from iris_table limit 1 print res sepal length sepal width petal length petal width species 1 5 1 3 5 1 4 0 2 setosa it is also possible to register a r data frame as a virtual table comparable to a sql view this does not actually transfer data into duckdb yet below is an example duckdb duckdb_register con iris_view iris res dbgetquery con select from iris_view limit 1 print res sepal length sepal width petal length petal width species 1 5 1 3 5 1 4 0 2 setosa duckdb keeps a reference to the r data frame after registration this prevents the data frame from being garbage-collected the reference is cleared when the connection is closed but can also be cleared manually using the duckdb duckdb_unregister method also refer to the data import documentation for more options of efficiently importing data dbplyr duckdb also plays well with the dbplyr dplyr packages for programmatic query construction from r here is an example library dbi library dplyr con - dbconnect duckdb duckdb duckdb duckdb_register con flights nycflights13 flights tbl con flights group_by dest summarise delay mean dep_time",
			"category": "docs",
			"url": "../docs/api/r",
			"blurb": "Installation  The DuckDB R API can be installed using  install.packages . Please see the   installation page  for..."
		},
		{
			"title": "Java JDBC API",
			"text": "installation the duckdb java jdbc api can be installed from maven central please see the installation page for details basic api usage duckdb s jdbc api implements the main parts of the standard java database connectivity jdbc api version 4 0 describing jdbc is beyond the scope of this page see the official documentation for details below we focus on the duckdb-specific parts startup shutdown in jdbc database connections are created through the standard java sql drivermanager class the driver should auto-register in the drivermanager if that does not work for some reason you can enforce registration like so class forname org duckdb duckdbdriver to create a duckdb connection call drivermanager with the jdbc duckdb jdbc url prefix like so connection conn drivermanager getconnection jdbc duckdb when using the jdbc duckdb url alone an in-memory database is created note that for an in-memory database no data is persisted to disk i e all data is lost when you exit the java program if you would like to access or create a persistent database append its file name after the path for example if your database is stored in tmp my_database use the jdbc url jdbc duckdb tmp my_database to create a connection to it it is possible to open a duckdb database file in read-only mode this is for example useful if multiple java processes want to read the same database file at the same time to open an existing database file in read-only mode set the connection property duckdb read_only like so properties ro_prop new properties ro_prop setproperty duckdb read_only true connection conn_ro drivermanager getconnection jdbc duckdb tmp my_database ro_prop the jdbc drivermanager api is a relatively poor fit for embedded database management systems such as duckdb if you would like to create multiple connections to the same database it would be somewhat logical to just create additional connections with the same url this is however only supported for read-only connections if you would like to create multiple read-write connections to the same database file or the same in-memory database instance you can use the custom duplicate method like so connection conn2 duckdbconnection conn duplicate querying duckdb supports the standard jdbc methods to send queries and retreive result sets first a statement object has to be created from the connection this object can then be used to send queries using execute and executequery execute is meant for queries where no results are expected like create table or update etc and executequery is meant to be used for queries that produce results e g select below two examples see also the jdbc statement and resultset documentations create a table statement stmt conn createstatement stmt execute create table items item varchar value decimal 10 2 count integer insert two items into the table stmt execute insert into items values jeans 20 0 1 hammer 42 2 2 resultset rs stmt executequery select from items while rs next system out println rs getstring 1 system out println rs getint 3 rs close jeans 1 hammer 2 duckdb also supports prepared statements as per the jdbc api preparedstatement p_stmt conn preparestatement insert into test values p_stmt setstring 1 chainsaw p_stmt setdouble 2 500 0 p_stmt setint 3 42 p_stmt execute more calls to execute possible p_stmt close do not use prepared statements to insert large amounts of data into duckdb see the data import documentation for better options",
			"category": "docs",
			"url": "../docs/api/java",
			"blurb": "Installation  The DuckDB Java JDBC API can be installed from   Maven Central . Please see the   installation page  for..."
		},
		{
			"title": "C++ API",
			"text": "installation the duckdb c api can be installed as part of the libduckdb packages please see the installation page for details basic api usage duckdb implements a custom c api this is built around the abstractions of a database instance duckdb class multiple connection s to the database instance and queryresult instances as the result of queries the header file for the c api is duckdb hpp the standard source distribution of libduckdb contains an amalgamation of the duckdb sources which combine all sources into two files duckdb hpp and duckdb cpp the duckdb hpp header is much larger in this case regardless of whether you are using the amalgamation or not just include duckdb hpp startup shutdown to use duckdb you must first initialize a duckdb instance using its constructor duckdb takes as parameter the database file to read and write from the special value nullptr can be used to create an in-memory database note that for an in-memory database no data is persisted to disk i e all data is lost when you exit the process the second parameter to the duckdb constructor is an optional dbconfig object in dbconfig you can set various database parameters for example the read write mode or memory limits the duckdb constructor may throw exceptions for example if the database file is not usable with the duckdb instance you can create one or many connection instances using the connection constructor while connections should be thread-safe they will be locked during querying it is therefore recommended that each thread uses its own connection if you are in a multithreaded environment duckdb db nullptr connection con db querying connections expose the query method to send a sql query string to duckdb from c query fully materializes the query result as a materializedqueryresult in memory before returning at which point the query result can be consumed there is also a streaming api for queries see further below create a table con query create table integers i integer j integer insert three rows into the table con query insert into integers values 3 4 5 6 7 null materializedqueryresult result con query select from integers if result- success cerr result- error the materializedqueryresult instance contains firstly two fields that indicate whether the query was successful query will not throw exceptions under normal circumstances instead invalid queries or other issues will lead to the success boolean field in the query result instance to be set to false in this case an error message may be available in error as a string if successful other fields are set the type of statement that was just executed e g statementtype insert_statement is contained in statement_type the high-level sql type types of the result set columns are in sql_types and the low-level data representation types are in types the names of the result columns are in the names string vector in case multiple result sets are returned for example because the result set contained multiple statements the result set can be chained using the next field todo duckdb also supports prepared statements in the c api with the prepare method this returns an instance of preparedstatement this instance can be used to execute the prepared statement with parameters below is an example todo do not use prepared statements to insert large amounts of data into duckdb see the data import documentation for better options streaming queries udf api the udf api is exposed in duckdb connection through the methods createscalarfunction and createvectorizedfunction and variants these methods created udfs into the temporary schema temp_schema of the owner connection that is the only one allowed to use and change them createscalarfunction the user can code an ordinary scalar function and invoke the createscalarfunction to register and afterward use the udf in a select statement for instance bool bigger_than_four int value return value 4 connection createscalarfunction bool int bigger_than_four bigger_than_four connection query select bigger_than_four i from values 3 5 tbl i - print the createscalarfunction methods automatically creates vectorized scalar udfs so they are as efficient as built-in functions we have two variants of this method interface as follows 1 template typename tr typename args nbsp nbsp nbsp void createscalarfunction string name tr udf_func args template parameters tr is the return type of the udf function args are the arguments up to 3 for the udf function this method only supports until ternary functions name is the name to register the udf function udf_func is a pointer to the udf function this method automatically discovers from the template typenames the corresponding sqltypes bool sqltype boolean int8_t sqltype tinyint int16_t sqltype smallint int32_t sqltype integer int64_t sqltype bigint float sqltype float double sqltype double string_t sqltype varchar in duckdb some primitive types e g int32_t are mapped to the same sqltype integer time and date then for disambiguation the users can use the following overloaded method 2 template typename tr typename args nbsp nbsp nbsp void createscalarfunction string name vector sqltype args sqltype ret_type tr udf_func args an example of use would be int32_t udf_date int32_t a return a con query create table dates d date con query insert into dates values 1992-01-01 con createscalarfunction int32_t int32_t udf_date sqltype date sqltype date udf_date con query select udf_date d from dates - print template parameters tr is the return type of the udf function args are the arguments up to 3 for the udf function this method only supports until ternary functions name is the name to register the udf function args are the sqltype arguments that the function uses which should match with the template args types ret_type is the sqltype of return of the function which should match with the template tr type udf_func is a pointer to the udf function this function checks the template types against the sqltypes passed as arguments and they must match as follow sqltypeid boolean bool sqltypeid tinyint int8_t sqltypeid smallint int16_t sqltypeid date sqltypeid time sqltypeid integer int32_t sqltypeid bigint sqltypeid timestamp int64_t sqltypeid float sqltypeid double sqltypeid decimal double sqltypeid varchar sqltypeid char sqltypeid blob string_t sqltypeid varbinary blob_t createvectorizedfunction the createvectorizedfunction methods register a vectorized udf such as this vectorized function copies the input values to the result vector template typename type static void udf_vectorized datachunk args expressionstate state vector result set the result vector type result vector_type vectortype flat_vector get a raw array from the result auto result_data flatvector getdata type result get the solely input vector auto input args data 0 now get an orrified vector vectordata vdata input orrify args size vdata get a raw array from the orrified input auto input_data type vdata data handling the data for idx_t i 0 i args size i auto idx vdata sel- get_index i if vdata nullmask idx continue result_data i input_data idx con query create table integers i integer con query insert into integers values 1 2 3 999 con createvectorizedfunction int int udf_vectorized_int udf_vectorized int con query select udf_vectorized_int i from integers - print the vectorized udf is a pointer of the type scalar_function_t typedef std function void datachunk args expressionstate expr vector result scalar_function_t args is a datachunk that holds a set of input vectors for the udf that all have the same length expr is an expressionstate that provides information to the query s expression state result is a vector to store the result values there are different vector types to handle in a vectorized udf constantvector dictionaryvector flatvector listvector stringvector structvector sequencevector the general api of the createvectorizedfunction method is as follows 1 template typename tr typename args nbsp nbsp nbsp void createvectorizedfunction string name scalar_function_t udf_func sqltype varargs sqltype invalid template parameters tr is the return type of the udf function args are the arguments up to 3 for the udf function name is the name to register the udf function udf_func is a vectorized udf function varargs the type of varargs to support or sqltypeid invalid default value if the function does not accept variable length arguments this method automatically discovers from the template typenames the corresponding sqltypes bool sqltype boolean int8_t sqltype tinyint int16_t sqltype smallint int32_t sqltype integer int64_t sqltype bigint float sqltype float double sqltype double string_t sqltype varchar 2 template typename tr typename args nbsp nbsp nbsp void createvectorizedfunction string name vector sqltype args sqltype ret_type scalar_function_t udf_func sqltype varargs sqltype invalid todo",
			"category": "docs",
			"url": "../docs/api/cpp",
			"blurb": "Installation  The DuckDB C++ API can be installed as part of the  libduckdb  packages. Please see the   installation page..."
		},
		{
			"title": "C API - Overview",
			"text": "duckdb implements a custom c api modelled somewhat following the sqlite c api the api is contained in the duckdb h header continue to startup shutdown to get started or check out the full api overview we also provide a sqlite api wrapper which means that if your applications is programmed against the sqlite c api you can re-link to duckdb and it should continue working see the sqlite_api_wrapper folder in our source repository for more information installation the duckdb c api can be installed as part of the libduckdb packages please see the installation page for details pages in this section",
			"category": "docs",
			"url": "../docs/api/c/overview",
			"blurb": "DuckDB implements a custom C API modelled somewhat following the SQLite C API. The API is contained in the  duckdb.h ..."
		},
		{
			"title": "C API - Complete API",
			"text": "api reference open connect syntax path to the database file on disk or nullptr or memory to open an in-memory database out_database the result database object returns duckdbsuccess on success or duckdberror on failure duckdb_open_ext extended version of duckdb_open creates a new database or opens an existing database file stored at the the given path syntax path to the database file on disk or nullptr or memory to open an in-memory database out_database the result database object config optional configuration used to start up the database system out_error if set and the function returns duckdberror this will contain the reason why the start-up failed note that the error must be freed using duckdb_free returns duckdbsuccess on success or duckdberror on failure duckdb_close closes the specified database and de-allocates all memory allocated for that database this should be called after you are done with any database allocated through duckdb_open note that failing to call duckdb_close in case of e g a program crash will not cause data corruption still it is recommended to always correctly close a database object after you are done with it syntax the database object to shut down duckdb_connect opens a connection to a database connections are required to query the database and store transactional state associated with the connection syntax the database file to connect to out_connection the result connection object returns duckdbsuccess on success or duckdberror on failure duckdb_disconnect closes the specified connection and de-allocates all memory allocated for that connection syntax the connection to close duckdb_create_config initializes an empty configuration object that can be used to provide start-up options for the duckdb instance through duckdb_open_ext this will always succeed unless there is a malloc failure syntax the result configuration object returns duckdbsuccess on success or duckdberror on failure duckdb_config_count this returns the total amount of configuration options available for usage with duckdb_get_config_flag this should not be called in a loop as it internally loops over all the options syntax the amount of config options available duckdb_get_config_flag obtains a human-readable name and description of a specific configuration option this can be used to e g display configuration options this will succeed unless index is out of range i e duckdb_config_count the result name or description must not be freed syntax the index of the configuration option between 0 and duckdb_config_count out_name a name of the configuration flag out_description a description of the configuration flag returns duckdbsuccess on success or duckdberror on failure duckdb_set_config sets the specified option for the specified configuration the configuration option is indicated by name to obtain a list of config options see duckdb_get_config_flag in the source code configuration options are defined in config cpp this can fail if either the name is invalid or if the value provided for the option is invalid syntax the configuration object to set the option on name the name of the configuration flag to set option the value to set the configuration flag to returns duckdbsuccess on success or duckdberror on failure duckdb_destroy_config destroys the specified configuration option and de-allocates all memory allocated for the object syntax the configuration object to destroy duckdb_query executes a sql query within a connection and stores the full materialized result in the out_result pointer if the query fails to execute duckdberror is returned and the error message can be retrieved by calling duckdb_result_error note that after running duckdb_query duckdb_destroy_result must be called on the result object even if the query fails otherwise the error stored within the result will not be freed correctly syntax the connection to perform the query in query the sql query to run out_result the query result returns duckdbsuccess on success or duckdberror on failure duckdb_destroy_result closes the result and de-allocates all memory allocated for that connection syntax the result to destroy duckdb_column_name returns the column name of the specified column the result should not need be freed the column names will automatically be destroyed when the result is destroyed returns null if the column is out of range syntax the result object to fetch the column name from col the column index returns the column name of the specified column duckdb_column_type returns the column type of the specified column returns duckdb_type_invalid if the column is out of range syntax the result object to fetch the column type from col the column index returns the column type of the specified column duckdb_column_count returns the number of columns present in a the result object syntax the result object returns the number of columns present in the result object duckdb_row_count returns the number of rows present in a the result object syntax the result object returns the number of rows present in the result object duckdb_rows_changed returns the number of rows changed by the query stored in the result this is relevant only for insert update delete queries for other queries the rows_changed will be 0 syntax the result object returns the number of rows changed duckdb_column_data returns the data of a specific column of a result in columnar format this is the fastest way of accessing data in a query result as no conversion or type checking must be performed outside of the original switch if performance is a concern it is recommended to use this api over the duckdb_value functions the function returns a dense array which contains the result data the exact type stored in the array depends on the corresponding duckdb_type as provided by duckdb_column_type for the exact type by which the data should be accessed see the comments in the types section or the duckdb_type enum for example for a column of type duckdb_type_integer rows can be accessed in the following manner int32_t data int32_t duckdb_column_data result 0 printf data for row d d n row data row syntax the result object to fetch the column data from col the column index returns the column data of the specified column duckdb_nullmask_data returns the nullmask of a specific column of a result in columnar format the nullmask indicates for every row whether or not the corresponding row is null if a row is null the values present in the array provided by duckdb_column_data are undefined int32_t data int32_t duckdb_column_data result 0 bool nullmask duckdb_nullmask_data result 0 if nullmask row printf data for row d null n row else printf data for row d d n row data row syntax the result object to fetch the nullmask from col the column index returns the nullmask of the specified column duckdb_result_error returns the error message contained within the result the error is only set if duckdb_query returns duckdberror the result of this function must not be freed it will be cleaned up when duckdb_destroy_result is called syntax the result object to fetch the nullmask from returns the error of the result duckdb_value_boolean syntax the boolean value at the specified location or false if the value cannot be converted duckdb_value_int8 syntax the int8_t value at the specified location or 0 if the value cannot be converted duckdb_value_int16 syntax the int16_t value at the specified location or 0 if the value cannot be converted duckdb_value_int32 syntax the int32_t value at the specified location or 0 if the value cannot be converted duckdb_value_int64 syntax the int64_t value at the specified location or 0 if the value cannot be converted duckdb_value_hugeint syntax the duckdb_hugeint value at the specified location or 0 if the value cannot be converted duckdb_value_uint8 syntax the uint8_t value at the specified location or 0 if the value cannot be converted duckdb_value_uint16 syntax the uint16_t value at the specified location or 0 if the value cannot be converted duckdb_value_uint32 syntax the uint32_t value at the specified location or 0 if the value cannot be converted duckdb_value_uint64 syntax the uint64_t value at the specified location or 0 if the value cannot be converted duckdb_value_float syntax the float value at the specified location or 0 if the value cannot be converted duckdb_value_double syntax the double value at the specified location or 0 if the value cannot be converted duckdb_value_date syntax the duckdb_date value at the specified location or 0 if the value cannot be converted duckdb_value_time syntax the duckdb_time value at the specified location or 0 if the value cannot be converted duckdb_value_timestamp syntax the duckdb_timestamp value at the specified location or 0 if the value cannot be converted duckdb_value_interval syntax the duckdb_interval value at the specified location or 0 if the value cannot be converted duckdb_value_varchar syntax the char value at the specified location or nullptr if the value cannot be converted the result must be freed with duckdb_free duckdb_value_varchar_internal syntax the char value at the specified location only works on varchar columns and does not auto-cast if the column is not a varchar column this function will return null the result must not be freed duckdb_value_blob syntax the duckdb_blob value at the specified location returns a blob with blob data set to nullptr if the value cannot be converted the resulting blob data must be freed with duckdb_free duckdb_value_is_null syntax returns true if the value at the specified index is null and false otherwise duckdb_malloc allocate size bytes of memory using the duckdb internal malloc function any memory allocated in this manner should be freed using duckdb_free syntax the number of bytes to allocate returns a pointer to the allocated memory region duckdb_free free a value returned from duckdb_malloc duckdb_value_varchar or duckdb_value_blob syntax the memory region to de-allocate duckdb_from_date decompose a duckdb_date object into year month and date stored as duckdb_date_struct syntax the date object as obtained from a duckdb_type_date column returns the duckdb_date_struct with the decomposed elements duckdb_to_date re-compose a duckdb_date from year month and date duckdb_date_struct syntax the year month and date stored in a duckdb_date_struct returns the duckdb_date element duckdb_from_time decompose a duckdb_time object into hour minute second and microsecond stored as duckdb_time_struct syntax the time object as obtained from a duckdb_type_time column returns the duckdb_time_struct with the decomposed elements duckdb_to_time re-compose a duckdb_time from hour minute second and microsecond duckdb_time_struct syntax the hour minute second and microsecond in a duckdb_time_struct returns the duckdb_time element duckdb_from_timestamp decompose a duckdb_timestamp object into a duckdb_timestamp_struct syntax the ts object as obtained from a duckdb_type_timestamp column returns the duckdb_timestamp_struct with the decomposed elements duckdb_to_timestamp re-compose a duckdb_timestamp from a duckdb_timestamp_struct syntax the de-composed elements in a duckdb_timestamp_struct returns the duckdb_timestamp element duckdb_hugeint_to_double converts a duckdb_hugeint object as obtained from a duckdb_type_hugeint column into a double syntax the hugeint value returns the converted double element duckdb_double_to_hugeint converts a double value to a duckdb_hugeint object if the conversion fails because the double value is too big the result will be 0 syntax the double value returns the converted duckdb_hugeint element duckdb_prepare create a prepared statement object from a query note that after calling duckdb_prepare the prepared statement should always be destroyed using duckdb_destroy_prepare even if the prepare fails if the prepare fails duckdb_prepare_error can be called to obtain the reason why the prepare failed syntax the connection object query the sql query to prepare out_prepared_statement the resulting prepared statement object returns duckdbsuccess on success or duckdberror on failure duckdb_destroy_prepare closes the prepared statement and de-allocates all memory allocated for that connection syntax the prepared statement to destroy duckdb_prepare_error returns the error message associated with the given prepared statement if the prepared statement has no error message this returns nullptr instead the error message should not be freed it will be de-allocated when duckdb_destroy_prepare is called syntax the prepared statement to obtain the error from returns the error message or nullptr if there is none duckdb_nparams returns the number of parameters that can be provided to the given prepared statement returns 0 if the query was not successfully prepared syntax the prepared statement to obtain the number of parameters for duckdb_param_type returns the parameter type for the parameter at the given index returns duckdb_type_invalid if the parameter index is out of range or the statement was not successfully prepared syntax the prepared statement param_idx the parameter index returns the parameter type duckdb_bind_boolean binds a bool value to the prepared statement at the specified index syntax duckdb_bind_int8 binds an int8_t value to the prepared statement at the specified index syntax duckdb_bind_int16 binds an int16_t value to the prepared statement at the specified index syntax duckdb_bind_int32 binds an int32_t value to the prepared statement at the specified index syntax duckdb_bind_int64 binds an int64_t value to the prepared statement at the specified index syntax duckdb_bind_hugeint binds an duckdb_hugeint value to the prepared statement at the specified index syntax duckdb_bind_uint8 binds an uint8_t value to the prepared statement at the specified index syntax duckdb_bind_uint16 binds an uint16_t value to the prepared statement at the specified index syntax duckdb_bind_uint32 binds an uint32_t value to the prepared statement at the specified index syntax duckdb_bind_uint64 binds an uint64_t value to the prepared statement at the specified index syntax duckdb_bind_float binds an float value to the prepared statement at the specified index syntax duckdb_bind_double binds an double value to the prepared statement at the specified index syntax duckdb_bind_date binds a duckdb_date value to the prepared statement at the specified index syntax duckdb_bind_time binds a duckdb_time value to the prepared statement at the specified index syntax duckdb_bind_timestamp binds a duckdb_timestamp value to the prepared statement at the specified index syntax duckdb_bind_interval binds a duckdb_interval value to the prepared statement at the specified index syntax duckdb_bind_varchar binds a null-terminated varchar value to the prepared statement at the specified index syntax duckdb_bind_varchar_length binds a varchar value to the prepared statement at the specified index syntax duckdb_bind_blob binds a blob value to the prepared statement at the specified index syntax duckdb_bind_null binds a null value to the prepared statement at the specified index syntax duckdb_execute_prepared executes the prepared statement with the given bound parameters and returns a materialized query result this method can be called multiple times for each prepared statement and the parameters can be modified between calls to this function syntax the prepared statement to execute out_result the query result returns duckdbsuccess on success or duckdberror on failure duckdb_execute_prepared_arrow executes the prepared statement with the given bound parameters and returns an arrow query result syntax the prepared statement to execute out_result the query result returns duckdbsuccess on success or duckdberror on failure duckdb_appender_create creates an appender object syntax the connection context to create the appender in schema the schema of the table to append to or nullptr for the default schema table the table name to append to out_appender the resulting appender object returns duckdbsuccess on success or duckdberror on failure duckdb_appender_error returns the error message associated with the given appender if the appender has no error message this returns nullptr instead the error message should be freed using duckdb_free syntax the appender to get the error from returns the error message or nullptr if there is none duckdb_appender_flush flush the appender to the table forcing the cache of the appender to be cleared and the data to be appended to the base table this should generally not be used unless you know what you are doing instead call duckdb_appender_destroy when you are done with the appender syntax the appender to flush returns duckdbsuccess on success or duckdberror on failure duckdb_appender_close close the appender flushing all intermediate state in the appender to the table and closing it for further appends this is generally not necessary call duckdb_appender_destroy instead syntax the appender to flush and close returns duckdbsuccess on success or duckdberror on failure duckdb_appender_destroy close the appender and destroy it flushing all intermediate state in the appender to the table and de-allocating all memory associated with the appender syntax the appender to flush close and destroy returns duckdbsuccess on success or duckdberror on failure duckdb_appender_begin_row a nop function provided for backwards compatibility reasons does nothing only duckdb_appender_end_row is required syntax duckdb_appender_end_row finish the current row of appends after end_row is called the next row can be appended syntax the appender returns duckdbsuccess on success or duckdberror on failure duckdb_append_bool append a bool value to the appender syntax duckdb_append_int8 append an int8_t value to the appender syntax duckdb_append_int16 append an int16_t value to the appender syntax duckdb_append_int32 append an int32_t value to the appender syntax duckdb_append_int64 append an int64_t value to the appender syntax duckdb_append_hugeint append a duckdb_hugeint value to the appender syntax duckdb_append_uint8 append a uint8_t value to the appender syntax duckdb_append_uint16 append a uint16_t value to the appender syntax duckdb_append_uint32 append a uint32_t value to the appender syntax duckdb_append_uint64 append a uint64_t value to the appender syntax duckdb_append_float append a float value to the appender syntax duckdb_append_double append a double value to the appender syntax duckdb_append_date append a duckdb_date value to the appender syntax duckdb_append_time append a duckdb_time value to the appender syntax duckdb_append_timestamp append a duckdb_timestamp value to the appender syntax duckdb_append_interval append a duckdb_interval value to the appender syntax duckdb_append_varchar append a varchar value to the appender syntax duckdb_append_varchar_length append a varchar value to the appender syntax duckdb_append_blob append a blob value to the appender syntax duckdb_append_null append a null value to the appender of any type syntax duckdb_query_arrow executes a sql query within a connection and stores the full materialized result in an arrow structure if the query fails to execute duckdberror is returned and the error message can be retrieved by calling duckdb_query_arrow_error note that after running duckdb_query_arrow duckdb_destroy_arrow must be called on the result object even if the query fails otherwise the error stored within the result will not be freed correctly syntax the connection to perform the query in query the sql query to run out_result the query result returns duckdbsuccess on success or duckdberror on failure duckdb_query_arrow_schema fetch the internal arrow schema from the arrow result syntax the result to fetch the schema from out_schema the output schema returns duckdbsuccess on success or duckdberror on failure duckdb_query_arrow_array fetch an internal arrow array from the arrow result this function can be called multiple time to get next chunks which will free the previous out_array so consume the out_array before calling this function again syntax the result to fetch the array from out_array the output array returns duckdbsuccess on success or duckdberror on failure duckdb_arrow_column_count returns the number of columns present in a the arrow result object syntax the result object returns the number of columns present in the result object duckdb_arrow_row_count returns the number of rows present in a the arrow result object syntax the result object returns the number of rows present in the result object duckdb_arrow_rows_changed returns the number of rows changed by the query stored in the arrow result this is relevant only for insert update delete queries for other queries the rows_changed will be 0 syntax the result object returns the number of rows changed duckdb_query_arrow_error returns the error message contained within the result the error is only set if duckdb_query_arrow returns duckdberror the result should be freed using duckdb_free syntax the result object to fetch the nullmask from returns the error of the result duckdb_destroy_arrow closes the result and de-allocates all memory allocated for the arrow result syntax the result to destroy",
			"category": "docs",
			"url": "../docs/api/c/api",
			"blurb": "API Reference   Open/Connect     Syntax     Path to the database file on disk, or  nullptr  or  :memory:  to open an..."
		},
		{
			"title": "C API - Appender",
			"text": "appenders are the most efficient way of loading data into duckdb from within the c interface and are recommended for fast data loading the appender is much faster than using prepared statements or individual insert into statements appends are made in row-wise format for every column a duckdb_append_ type call should be made after which the row should be finished by calling duckdb_appender_end_row after all rows have been appended duckdb_appender_destroy should be used to finalize the appender and clean up the resulting memory note that duckdb_appender_destroy should always be called on the resulting appender even if the function returns duckdberror example duckdb_query con create table people id integer name varchar null duckdb_appender appender if duckdb_appender_create con null people appender duckdberror handle error append the first row 1 mark duckdb_append_int32 appender 1 duckdb_append_varchar appender mark duckdb_appender_end_row appender append the second row 2 hannes duckdb_append_int32 appender 2 duckdb_append_varchar appender hannes duckdb_appender_end_row appender finish appending and flush all the rows to the table duckdb_appender_destroy appender api reference syntax the connection context to create the appender in schema the schema of the table to append to or nullptr for the default schema table the table name to append to out_appender the resulting appender object returns duckdbsuccess on success or duckdberror on failure duckdb_appender_error returns the error message associated with the given appender if the appender has no error message this returns nullptr instead the error message should be freed using duckdb_free syntax the appender to get the error from returns the error message or nullptr if there is none duckdb_appender_flush flush the appender to the table forcing the cache of the appender to be cleared and the data to be appended to the base table this should generally not be used unless you know what you are doing instead call duckdb_appender_destroy when you are done with the appender syntax the appender to flush returns duckdbsuccess on success or duckdberror on failure duckdb_appender_close close the appender flushing all intermediate state in the appender to the table and closing it for further appends this is generally not necessary call duckdb_appender_destroy instead syntax the appender to flush and close returns duckdbsuccess on success or duckdberror on failure duckdb_appender_destroy close the appender and destroy it flushing all intermediate state in the appender to the table and de-allocating all memory associated with the appender syntax the appender to flush close and destroy returns duckdbsuccess on success or duckdberror on failure duckdb_appender_begin_row a nop function provided for backwards compatibility reasons does nothing only duckdb_appender_end_row is required syntax duckdb_appender_end_row finish the current row of appends after end_row is called the next row can be appended syntax the appender returns duckdbsuccess on success or duckdberror on failure duckdb_append_bool append a bool value to the appender syntax duckdb_append_int8 append an int8_t value to the appender syntax duckdb_append_int16 append an int16_t value to the appender syntax duckdb_append_int32 append an int32_t value to the appender syntax duckdb_append_int64 append an int64_t value to the appender syntax duckdb_append_hugeint append a duckdb_hugeint value to the appender syntax duckdb_append_uint8 append a uint8_t value to the appender syntax duckdb_append_uint16 append a uint16_t value to the appender syntax duckdb_append_uint32 append a uint32_t value to the appender syntax duckdb_append_uint64 append a uint64_t value to the appender syntax duckdb_append_float append a float value to the appender syntax duckdb_append_double append a double value to the appender syntax duckdb_append_date append a duckdb_date value to the appender syntax duckdb_append_time append a duckdb_time value to the appender syntax duckdb_append_timestamp append a duckdb_timestamp value to the appender syntax duckdb_append_interval append a duckdb_interval value to the appender syntax duckdb_append_varchar append a varchar value to the appender syntax duckdb_append_varchar_length append a varchar value to the appender syntax duckdb_append_blob append a blob value to the appender syntax duckdb_append_null append a null value to the appender of any type syntax",
			"category": "docs",
			"url": "../docs/api/c/appender",
			"blurb": "Appenders are the most efficient way of loading data into DuckDB from within the C interface, and are recommended for ..."
		},
		{
			"title": "C API - Configuration",
			"text": "configuration options can be provided to change different settings of the database system note that many of these settings can be changed later on using pragma statements as well the configuration object should be created filled with values and passed to duckdb_open_ext example duckdb_database db duckdb_config config create the configuration object if duckdb_create_config config duckdberror handle error set some configuration options duckdb_set_config config access_mode read_write or read_only duckdb_set_config config threads 8 duckdb_set_config config max_memory 8gb duckdb_set_config config default_order desc open the database using the configuration if duckdb_open_ext null db config null duckdberror handle error cleanup the configuration object duckdb_destroy_config config run queries cleanup duckdb_close db api reference this will always succeed unless there is a malloc failure syntax the result configuration object returns duckdbsuccess on success or duckdberror on failure duckdb_config_count this returns the total amount of configuration options available for usage with duckdb_get_config_flag this should not be called in a loop as it internally loops over all the options syntax the amount of config options available duckdb_get_config_flag obtains a human-readable name and description of a specific configuration option this can be used to e g display configuration options this will succeed unless index is out of range i e duckdb_config_count the result name or description must not be freed syntax the index of the configuration option between 0 and duckdb_config_count out_name a name of the configuration flag out_description a description of the configuration flag returns duckdbsuccess on success or duckdberror on failure duckdb_set_config sets the specified option for the specified configuration the configuration option is indicated by name to obtain a list of config options see duckdb_get_config_flag in the source code configuration options are defined in config cpp this can fail if either the name is invalid or if the value provided for the option is invalid syntax the configuration object to set the option on name the name of the configuration flag to set option the value to set the configuration flag to returns duckdbsuccess on success or duckdberror on failure duckdb_destroy_config destroys the specified configuration option and de-allocates all memory allocated for the object syntax the configuration object to destroy",
			"category": "docs",
			"url": "../docs/api/c/config",
			"blurb": "Configuration options can be provided to change different settings of the database system. Note that many of these ..."
		},
		{
			"title": "C API - Startup & Shutdown",
			"text": "to use duckdb you must first initialize a duckdb_database handle using duckdb_open duckdb_open takes as parameter the database file to read and write from the special value null nullptr can be used to create an in-memory database note that for an in-memory database no data is persisted to disk i e all data is lost when you exit the process with the duckdb_database handle you can create one or many duckdb_connection using duckdb_connect while individual connections are thread-safe they will be locked during querying it is therefore recommended that each thread uses its own connection to allow for the best parallel performance all duckdb_connection s have to explicitly be disconnected with duckdb_disconnect and the duckdb_database has to be explicitly closed with duckdb_close to avoid memory and file handle leaking example duckdb_database db duckdb_connection con if duckdb_open null db duckdberror handle error if duckdb_connect db con duckdberror handle error run queries cleanup duckdb_disconnect con duckdb_close db api reference syntax path to the database file on disk or nullptr or memory to open an in-memory database out_database the result database object returns duckdbsuccess on success or duckdberror on failure duckdb_open_ext extended version of duckdb_open creates a new database or opens an existing database file stored at the the given path syntax path to the database file on disk or nullptr or memory to open an in-memory database out_database the result database object config optional configuration used to start up the database system out_error if set and the function returns duckdberror this will contain the reason why the start-up failed note that the error must be freed using duckdb_free returns duckdbsuccess on success or duckdberror on failure duckdb_close closes the specified database and de-allocates all memory allocated for that database this should be called after you are done with any database allocated through duckdb_open note that failing to call duckdb_close in case of e g a program crash will not cause data corruption still it is recommended to always correctly close a database object after you are done with it syntax the database object to shut down duckdb_connect opens a connection to a database connections are required to query the database and store transactional state associated with the connection syntax the database file to connect to out_connection the result connection object returns duckdbsuccess on success or duckdberror on failure duckdb_disconnect closes the specified connection and de-allocates all memory allocated for that connection syntax the connection to close",
			"category": "docs",
			"url": "../docs/api/c/connect",
			"blurb": "To use DuckDB, you must first initialize a  duckdb_database  handle using  duckdb_open() .  duckdb_open()  takes as..."
		},
		{
			"title": "C API - Query",
			"text": "the duckdb_query method allows sql queries to be run in duckdb from c this method takes two parameters a null-terminated sql query string and a duckdb_result result pointer the result pointer may be null if the application is not interested in the result set or if the query produces no result after the result is consumed the duckdb_destroy_result method should be used to clean up the result elements can be extracted from the duckdb_result object using a variety of methods the duckdb_column_count and duckdb_row_count methods can be used to extract the number of columns and the number of rows respectively duckdb_column_name and duckdb_column_type can be used to extract the names and types of individual columns example duckdb_state state duckdb_result result create a table state duckdb_query con create table integers i integer j integer null if state duckdberror handle error insert three rows into the table state duckdb_query con insert into integers values 3 4 5 6 7 null null if state duckdberror handle error query rows again state duckdb_query con select from integers result if state duckdberror handle error handle the result destroy the result after we are done with it duckdb_destroy_result result value extraction values can be extracted using either the duckdb_column_data duckdb_nullmask_data functions or using the duckdb_value convenience functions the duckdb_column_data duckdb_nullmask_data functions directly hand you a pointer to the result arrays in columnar format and can therefore be very fast the duckdb_value functions perform bounds- and type-checking and will automatically cast values to the desired type this makes them more convenient and easier to use at the expense of being slower see the types page for more information for optimal performance use duckdb_column_data and duckdb_nullmask_data to extract data from the query result the duckdb_value functions perform internal type-checking bounds-checking and casting which makes them slower duckdb_value below is an example that prints the above result to csv format using the duckdb_value_varchar function note that the function is generic we do not need to know about the types of the individual result columns print the above result to csv format using duckdb_value_varchar idx_t row_count duckdb_row_count result idx_t column_count duckdb_column_count result for idx_t row 0 row row_count row for idx_t col 0 col column_count col if col 0 printf auto str_val duckdb_value_varchar result col row printf s str_val duckdb_free str_val printf n duckdb_column_data below is an example that prints the above result to csv format using the duckdb_column_data function note that the function is not generic we do need to know exactly what the types of the result columns are int32_t i_data int32_t duckdb_column_data result 0 int32_t j_data int32_t duckdb_column_data result 1 bool i_mask duckdb_nullmask_data result 0 bool j_mask duckdb_nullmask_data result 1 idx_t row_count duckdb_row_count result for idx_t row 0 row row_count row if i_mask row printf null else printf d i_data row printf if j_mask row printf null else printf d j_data row printf n when using duckdb_column_data be careful that the type matches exactly what you expect it to be as the code directly accesses an internal array there is no type-checking accessing a duckdb_type_integer column as if it was a duckdb_type_bigint column will provide unpredictable results api reference note that after running duckdb_query duckdb_destroy_result must be called on the result object even if the query fails otherwise the error stored within the result will not be freed correctly syntax the connection to perform the query in query the sql query to run out_result the query result returns duckdbsuccess on success or duckdberror on failure duckdb_destroy_result closes the result and de-allocates all memory allocated for that connection syntax the result to destroy duckdb_column_name returns the column name of the specified column the result should not need be freed the column names will automatically be destroyed when the result is destroyed returns null if the column is out of range syntax the result object to fetch the column name from col the column index returns the column name of the specified column duckdb_column_type returns the column type of the specified column returns duckdb_type_invalid if the column is out of range syntax the result object to fetch the column type from col the column index returns the column type of the specified column duckdb_column_count returns the number of columns present in a the result object syntax the result object returns the number of columns present in the result object duckdb_row_count returns the number of rows present in a the result object syntax the result object returns the number of rows present in the result object duckdb_rows_changed returns the number of rows changed by the query stored in the result this is relevant only for insert update delete queries for other queries the rows_changed will be 0 syntax the result object returns the number of rows changed duckdb_column_data returns the data of a specific column of a result in columnar format this is the fastest way of accessing data in a query result as no conversion or type checking must be performed outside of the original switch if performance is a concern it is recommended to use this api over the duckdb_value functions the function returns a dense array which contains the result data the exact type stored in the array depends on the corresponding duckdb_type as provided by duckdb_column_type for the exact type by which the data should be accessed see the comments in the types section or the duckdb_type enum for example for a column of type duckdb_type_integer rows can be accessed in the following manner int32_t data int32_t duckdb_column_data result 0 printf data for row d d n row data row syntax the result object to fetch the column data from col the column index returns the column data of the specified column duckdb_nullmask_data returns the nullmask of a specific column of a result in columnar format the nullmask indicates for every row whether or not the corresponding row is null if a row is null the values present in the array provided by duckdb_column_data are undefined int32_t data int32_t duckdb_column_data result 0 bool nullmask duckdb_nullmask_data result 0 if nullmask row printf data for row d null n row else printf data for row d d n row data row syntax the result object to fetch the nullmask from col the column index returns the nullmask of the specified column duckdb_result_error returns the error message contained within the result the error is only set if duckdb_query returns duckdberror the result of this function must not be freed it will be cleaned up when duckdb_destroy_result is called syntax the result object to fetch the nullmask from returns the error of the result",
			"category": "docs",
			"url": "../docs/api/c/query",
			"blurb": "The  duckdb_query  method allows SQL queries to be run in DuckDB from C. This method takes two parameters, a..."
		},
		{
			"title": "C API - Types",
			"text": "duckdb is a strongly typed database system as such every column has a single type specified this type is constant over the entire column that is to say a column that is labeled as an integer column will only contain integer values the supported types are listed below typedef enum duckdb_type duckdb_type_invalid duckdb_type_boolean duckdb_type_tinyint duckdb_type_smallint duckdb_type_integer duckdb_type_bigint duckdb_type_utinyint duckdb_type_usmallint duckdb_type_uinteger duckdb_type_ubigint duckdb_type_float duckdb_type_double duckdb_type_timestamp duckdb_type_date duckdb_type_time duckdb_type_interval duckdb_type_hugeint duckdb_type_varchar duckdb_type_blob duckdb_type the type of a column in the result can be obtained using the duckdb_column_type function the internal type of the columns is especially relevant for correct usage of the duckdb_column_data function the mapping from column type to internal type is provided in the table below column type data type ----------------------- ------------------ duckdb_type_boolean bool duckdb_type_tinyint int8_t duckdb_type_smallint int16_t duckdb_type_integer int32_t duckdb_type_bigint int64_t duckdb_type_utinyint uint8_t duckdb_type_usmallint uint16_t duckdb_type_uinteger uint32_t duckdb_type_ubigint uint64_t duckdb_type_float float duckdb_type_double double duckdb_type_timestamp duckdb_timestamp duckdb_type_date duckdb_date duckdb_type_time duckdb_time duckdb_type_interval duckdb_interval duckdb_type_hugeint duckdb_hugeint duckdb_type_varchar const char duckdb_type_blob duckdb_blob duckdb_value the duckdb_value functions will auto-cast values as required for example it is no problem to use duckdb_value_double on a column of type duckdb_value_int32 the value will be auto-cast and returned as a double note that in certain cases the cast may fail for example this can happen if we request a duckdb_value_int8 and the value does not fit within an int8 value in this case a default value will be returned usually 0 or nullptr the same default value will also be returned if the corresponding value is null the duckdb_value_is_null function can be used to check if a specific value is null or not the exception to the auto-cast rule is the duckdb_value_varchar_internal function this function does not auto-cast and only works for varchar columns the reason this function exists is that the result does not need to be freed note that duckdb_value_varchar and duckdb_value_blob require the result to be de-allocated using duckdb_free duckdb_column_data the duckdb_column_data returns a pointer to an internal array within the result the type of the elements of the array depends on the internal type of the column as obtained using duckdb_column_type the corresponding null values can be obtained using the duckdb_nullmask_data function the exact data types can be seen in the table above for numeric types the internal types of the columns are standard built-in types for example for a column of type duckdb_type_integer the internal type is a int32_t for text varchar columns the internal type is a pointer to a null-terminated string for dates times timestamps blobs intervals and hugeints the internal types are duckdb-specific structs as defined below days are stored as days since 1970-01-01 use the duckdb_from_date duckdb_to_date function to extract individual information typedef struct int32_t days duckdb_date time is stored as microseconds since 00 00 00 use the duckdb_from_time duckdb_to_time function to extract individual information typedef struct int64_t micros duckdb_time timestamps are stored as microseconds since 1970-01-01 use the duckdb_from_timestamp duckdb_to_timestamp function to extract individual information typedef struct int64_t micros duckdb_timestamp typedef struct int32_t months int32_t days int64_t micros duckdb_interval hugeints are composed in a lower upper component the value of the hugeint is upper 2 64 lower for easy usage the functions duckdb_hugeint_to_double duckdb_double_to_hugeint are recommended typedef struct uint64_t lower int64_t upper duckdb_hugeint typedef struct void data idx_t size duckdb_blob for hugeints dates times and timestamps there are also several helper functions available in order to facilitate easier usage of these types see the api reference below for the available functions helper structs as used by the date time timestamp helpers typedef struct int32_t year int8_t month int8_t day duckdb_date_struct typedef struct int8_t hour int8_t min int8_t sec int32_t micros duckdb_time_struct typedef struct duckdb_date_struct date duckdb_time_struct time duckdb_timestamp_struct api reference the boolean value at the specified location or false if the value cannot be converted duckdb_value_int8 syntax the int8_t value at the specified location or 0 if the value cannot be converted duckdb_value_int16 syntax the int16_t value at the specified location or 0 if the value cannot be converted duckdb_value_int32 syntax the int32_t value at the specified location or 0 if the value cannot be converted duckdb_value_int64 syntax the int64_t value at the specified location or 0 if the value cannot be converted duckdb_value_hugeint syntax the duckdb_hugeint value at the specified location or 0 if the value cannot be converted duckdb_value_uint8 syntax the uint8_t value at the specified location or 0 if the value cannot be converted duckdb_value_uint16 syntax the uint16_t value at the specified location or 0 if the value cannot be converted duckdb_value_uint32 syntax the uint32_t value at the specified location or 0 if the value cannot be converted duckdb_value_uint64 syntax the uint64_t value at the specified location or 0 if the value cannot be converted duckdb_value_float syntax the float value at the specified location or 0 if the value cannot be converted duckdb_value_double syntax the double value at the specified location or 0 if the value cannot be converted duckdb_value_date syntax the duckdb_date value at the specified location or 0 if the value cannot be converted duckdb_value_time syntax the duckdb_time value at the specified location or 0 if the value cannot be converted duckdb_value_timestamp syntax the duckdb_timestamp value at the specified location or 0 if the value cannot be converted duckdb_value_interval syntax the duckdb_interval value at the specified location or 0 if the value cannot be converted duckdb_value_varchar syntax the char value at the specified location or nullptr if the value cannot be converted the result must be freed with duckdb_free duckdb_value_varchar_internal syntax the char value at the specified location only works on varchar columns and does not auto-cast if the column is not a varchar column this function will return null the result must not be freed duckdb_value_blob syntax the duckdb_blob value at the specified location returns a blob with blob data set to nullptr if the value cannot be converted the resulting blob data must be freed with duckdb_free duckdb_value_is_null syntax returns true if the value at the specified index is null and false otherwise duckdb_from_date decompose a duckdb_date object into year month and date stored as duckdb_date_struct syntax the date object as obtained from a duckdb_type_date column returns the duckdb_date_struct with the decomposed elements duckdb_to_date re-compose a duckdb_date from year month and date duckdb_date_struct syntax the year month and date stored in a duckdb_date_struct returns the duckdb_date element duckdb_from_time decompose a duckdb_time object into hour minute second and microsecond stored as duckdb_time_struct syntax the time object as obtained from a duckdb_type_time column returns the duckdb_time_struct with the decomposed elements duckdb_to_time re-compose a duckdb_time from hour minute second and microsecond duckdb_time_struct syntax the hour minute second and microsecond in a duckdb_time_struct returns the duckdb_time element duckdb_from_timestamp decompose a duckdb_timestamp object into a duckdb_timestamp_struct syntax the ts object as obtained from a duckdb_type_timestamp column returns the duckdb_timestamp_struct with the decomposed elements duckdb_to_timestamp re-compose a duckdb_timestamp from a duckdb_timestamp_struct syntax the de-composed elements in a duckdb_timestamp_struct returns the duckdb_timestamp element duckdb_hugeint_to_double converts a duckdb_hugeint object as obtained from a duckdb_type_hugeint column into a double syntax the hugeint value returns the converted double element duckdb_double_to_hugeint converts a double value to a duckdb_hugeint object if the conversion fails because the double value is too big the result will be 0 syntax the double value returns the converted duckdb_hugeint element",
			"category": "docs",
			"url": "../docs/api/c/types",
			"blurb": "DuckDB is a strongly typed database system. As such, every column has a single type specified. This type is constant ..."
		},
		{
			"title": "C API - Prepared Statements",
			"text": "a prepared statement is a parameterized query the query is prepared with question marks or dollar symbols 1 indicating the parameters of the query values can then be bound to these parameters after which the prepared statement can be executed using those parameters a single query can be prepared once and executed many times prepared statements are useful to easily supply parameters to functions while avoiding string concatenation sql injection attacks speeding up queries that will be executed many times with different parameters duckdb supports prepared statements in the c api with the duckdb_prepare method the duckdb_bind family of functions is used to supply values for subsequent execution of the prepared statement using duckdb_execute_prepared after we are done with the prepared statement it can be cleaned up using the duckdb_destroy_prepare method example duckdb_prepared_statement stmt duckdb_result result if duckdb_prepare con insert into integers values 1 2 stmt duckdberror handle error duckdb_bind_int32 stmt 1 42 the parameter index starts counting at 1 duckdb_bind_int32 stmt 2 43 null as second parameter means no result set is requested duckdb_execute_prepared stmt null duckdb_destroy_prepare stmt we can also query result sets using prepared statements if duckdb_prepare con select from integers where i stmt duckdberror handle error duckdb_bind_int32 stmt 1 42 duckdb_execute_prepared stmt result do something with result clean up duckdb_destroy_result result duckdb_destroy_prepare stmt after calling duckdb_prepare the prepared statement parameters can be inspected using duckdb_nparams and duckdb_param_type in case the prepare fails the error can be obtained through duckdb_prepare_error it is not required that the duckdb_bind family of functions matches the prepared statement parameter type exactly the values will be auto-cast to the required value as required for example calling duckdb_bind_int8 on a parameter type of duckdb_type_integer will work as expected do not use prepared statements to insert large amounts of data into duckdb instead it is recommended to use the appender api reference note that after calling duckdb_prepare the prepared statement should always be destroyed using duckdb_destroy_prepare even if the prepare fails if the prepare fails duckdb_prepare_error can be called to obtain the reason why the prepare failed syntax the connection object query the sql query to prepare out_prepared_statement the resulting prepared statement object returns duckdbsuccess on success or duckdberror on failure duckdb_destroy_prepare closes the prepared statement and de-allocates all memory allocated for that connection syntax the prepared statement to destroy duckdb_prepare_error returns the error message associated with the given prepared statement if the prepared statement has no error message this returns nullptr instead the error message should not be freed it will be de-allocated when duckdb_destroy_prepare is called syntax the prepared statement to obtain the error from returns the error message or nullptr if there is none duckdb_nparams returns the number of parameters that can be provided to the given prepared statement returns 0 if the query was not successfully prepared syntax the prepared statement to obtain the number of parameters for duckdb_param_type returns the parameter type for the parameter at the given index returns duckdb_type_invalid if the parameter index is out of range or the statement was not successfully prepared syntax the prepared statement param_idx the parameter index returns the parameter type duckdb_bind_boolean binds a bool value to the prepared statement at the specified index syntax duckdb_bind_int8 binds an int8_t value to the prepared statement at the specified index syntax duckdb_bind_int16 binds an int16_t value to the prepared statement at the specified index syntax duckdb_bind_int32 binds an int32_t value to the prepared statement at the specified index syntax duckdb_bind_int64 binds an int64_t value to the prepared statement at the specified index syntax duckdb_bind_hugeint binds an duckdb_hugeint value to the prepared statement at the specified index syntax duckdb_bind_uint8 binds an uint8_t value to the prepared statement at the specified index syntax duckdb_bind_uint16 binds an uint16_t value to the prepared statement at the specified index syntax duckdb_bind_uint32 binds an uint32_t value to the prepared statement at the specified index syntax duckdb_bind_uint64 binds an uint64_t value to the prepared statement at the specified index syntax duckdb_bind_float binds an float value to the prepared statement at the specified index syntax duckdb_bind_double binds an double value to the prepared statement at the specified index syntax duckdb_bind_date binds a duckdb_date value to the prepared statement at the specified index syntax duckdb_bind_time binds a duckdb_time value to the prepared statement at the specified index syntax duckdb_bind_timestamp binds a duckdb_timestamp value to the prepared statement at the specified index syntax duckdb_bind_interval binds a duckdb_interval value to the prepared statement at the specified index syntax duckdb_bind_varchar binds a null-terminated varchar value to the prepared statement at the specified index syntax duckdb_bind_varchar_length binds a varchar value to the prepared statement at the specified index syntax duckdb_bind_blob binds a blob value to the prepared statement at the specified index syntax duckdb_bind_null binds a null value to the prepared statement at the specified index syntax duckdb_execute_prepared executes the prepared statement with the given bound parameters and returns a materialized query result this method can be called multiple times for each prepared statement and the parameters can be modified between calls to this function syntax the prepared statement to execute out_result the query result returns duckdbsuccess on success or duckdberror on failure duckdb_execute_prepared_arrow executes the prepared statement with the given bound parameters and returns an arrow query result syntax the prepared statement to execute out_result the query result returns duckdbsuccess on success or duckdberror on failure",
			"category": "docs",
			"url": "../docs/api/c/prepared",
			"blurb": "A prepared statement is a parameterized query. The query is prepared with question marks ( ? ) or dollar symbols ( $1 )..."
		},
		{
			"title": "Profiling",
			"text": "profiling is important to help understand why certain queries exhibit specific performance characteristics duckdb contains several built-in features to enable query profiling that will be explained on this page for the examples on this page we will use the following example data set create table students sid integer primary key name varchar create table exams cid integer sid integer grade integer primary key cid sid insert into students values 1 mark 2 hannes 3 pedro insert into exams values 1 1 8 1 2 8 1 3 7 2 1 9 2 2 10 explain statement the first step to profiling a database engine is figuring out what execution plan the engine is using the explain statement allows you to peek into the query plan and see what is going on under the hood the explain statement displays three query plans that show what the plan looks like as it passes the various stages of the execution engine the logical_plan is the initial unoptimized plan as it is created right after parsing the logical_opt is the optimized logical plan that demonstrates the equivalent but optimized logical plan after it passes the optimization phase this optimized plan is then transformed into the physical_plan which is the plan that will actually get executed to demonstrate see the below example explain select name from students join exams using sid where name like ma physical plan projection name hash_join inner sid sid seq_scan filter exams prefix name ma sid seq_scan students sid name filters name ma and name mb and name is not null we can see that the logical_plan contains the unoptimized query plan involving a cross product and a like operation the optimized plan transforms this plan and pushes down the filters transforming the cross product into a comparison join it also recognizes that the like operator only does prefix filtering and transforms the more expensive like operator into a cheaper prefix selection run-time profiling the query plan helps understand the performance characteristics of the system however often it is also necessary to look at the performance numbers of individual operators and the cardinalities that pass through them for this you can create a query-profile graph examples of these can be found in the benchmarks section to create the query graphs it is first necessary to gather the necessary data by running the query in order to do that we must first enable the run-time profiling this can be done as follows using the default output format of the profiler query_tree pragma enable_profiling select name from students join exams using sid where name like ma total time 0 0003s projection name 2 0 00s hash_join inner sid sid 2 0 00s seq_scan filter exams prefix name ma sid 1 0 00s 5 0 00s seq_scan students sid name filters name ma and name mb and name is not null 1 0 00s name mark mark it is also possible to save the query plan to a file e g in json format -- enable profiling in json format pragma enable_profiling json -- write the profiling output to a specific file on disk pragma profile_output path to file json after these commands have been issued any query performed will write its profiling output to the specified json file this file is overwritten with each query that is issued if you want to store the profile output for later it should be copied to a different file now let us run the query that we inspected before select name from students join exams using sid where name like ma after the query is completed the json file containing the profiling output has been written to the specified file we can then render the query graph using the python script provided we have the duckdb python module installed this script will generate a html file and open it in your web browser python scripts generate_querygraph py path to file json alternatively we can also use the python api directly to generate the query graph file this can be done using the following python snippet import duckdb_query_graph generate duckdb_query_graph generate generate path to file json path to out html this will show us the following query graph example query graph",
			"category": "docs",
			"url": "../docs/dev/profiling",
			"blurb": "Profiling is important to help understand why certain queries exhibit specific performance characteristics. DuckDB..."
		},
		{
			"title": "Testing",
			"text": "testing is vital to make sure that duckdb works properly and keeps working properly for that reason we put a large emphasis on thorough and frequent testing we run a batch of small tests on every commit using github actions and run a more exhaustive batch of tests on pull requests and commits in the master branch it is crucial that any new features that get added have correct tests that not only test the happy path but also test edge cases and incorrect usage of the feature in this section we describe how duckdb tests are structured and how to make new tests for duckdb the tests can be run by running the unittest program located in the test folder for the default compilations this is located in either build release test unittest release or build debug test unittest debug sqllogictests when testing duckdb we aim to route all the tests through sql we try to avoid testing components individually because that makes those components more difficult to change later on as such almost all of our tests can and should be expressed in pure sql there are certain exceptions to this which we will discuss in the section catch tests however in most cases you should write your tests in plain sql for testing plain sql we use an extended version of the sql logic test suite adopted from sqlite every test is a single self-contained file located in the test sql directory the test describes a series of sql statements together with either the expected result a statement ok indicator or a statement error indicator an example of a test file is shown below name test sql projection test_simple_projection test group projection create table statement ok create table a i integer j integer insertion 1 affected row statement ok insert into a values 42 84 query ii select from a ---- 42 84 in this example three statements are executed the first statements are expected to succeed prefixed by statement ok the third statement is expected to return a single row with two columns indicated by query ii the values of the row are expected to be 42 and 84 separated by a tab character the top of every file should contain a comment describing the name and group of the test the name of the test is always the relative file path of the file the group is the folder that the file is in the name and group of the test are relevant because they can be used to execute only that test in the unittest group for example if we wanted to execute only the above test we would run the command unittest test sql projection test_simple_projection test if we wanted to run all tests in a specific directory we would run the command unittest projection any tests that are placed in the test directory are automatically added to the test suite note that the extension of the test is significant sqllogictests should either use the test extension or the test_slow extension the test_slow extension indicates that the test takes a while to run and will only be run when all tests are explicitly run using unittest tests with the extension test will be included in the fast set of tests row-wise vs value-wise result ordering the result values of a query can be either supplied in row-wise order with the individual values separated by tabs or in value-wise order in value wise order the individual values of the query must appear in row column order each on an individual line consider the following example in both row-wise and value-wise order row-wise query ii select 42 84 union all select 10 20 ---- 42 84 10 20 value-wise query ii select 42 84 union all select 10 20 ---- 42 84 10 20 null values and empty strings empty lines have special significance for the sqllogic test runner they signify an end of the current statement or query for that reason empty strings and null values have special syntax that must be used in result verification null values should use the string null and empty strings should use the string empty e g query ii select null ---- null empty hashes outputting values besides direct result verification the sqllogic test suite also has the option of using md5 hashes for value comparisons a test using hashes for result verification looks like this query i select g string_agg x from strings group by g ---- 200 values hashing to b8126ea73f21372cdb3f2dc483106a12 this approach is useful for reducing the size of tests when results have many output rows however it should be used sparingly as hash values make the tests more difficult to debug if they do break after it is ensured that the system outputs the correct result hashes of the queries in a test file can be computed by adding mode output_hash to the test file for example mode output_hash query ii select 42 84 union all select 10 20 ---- 42 84 10 20 the expected output hashes for every query in the test file will then be printed to the terminal as follows sql query select 42 84 union all select 10 20 4 values hashing to 498c69da8f30c24da3bd5b322a2fd455 in a similar manner mode output_result can be used in order to force the program to print the result to the terminal for every query run in the test file multiple connections for tests whose purpose is to verify that the transactional management or versioning of data works correctly it is generally necessary to use multiple connections for example if we want to verify that the creation of tables is correctly transactional we might want to start a transaction and create a table in con1 then fire a query in con2 that checks that the table is not accessible yet until committed we can use multiple connections in the sqllogictests using connection labels the connection label can be optionally appended to any statement or query all queries with the same connection label will be executed in the same connection a test that would verify the above property would look as follows statement ok con1 begin transaction statement ok con1 create table integers i integer statement error con2 select from integers result sorting queries can have an optional field that indicates that the result should be sorted in a specific manner this field goes in the same location as the connection label because of that connection labels and result sorting cannot be mixed the possible values of this field are nosort rowsort and valuesort an example of how this might be used is given below query i rowsort select world union all select hello ---- hello world in general we prefer not to use this field and rely on order by in the query to generate deterministic query answers however existing sqllogictests use this field extensively hence it is important to know of its existance query labels another feature that can be used for result verification are query labels these can be used to verify that different queries provide the same result this is useful for comparing queries that are logically equivalent but formulated differently query labels are provided after the connection label or sorting specifier queries that have a query label do not need to have a result provided instead the results of each of the queries with the same label are compared to each other for example the following script verifies that the queries select 42 1 and select 44-1 provide the same result query i nosort r43 select 42 1 ---- query i nosort r43 select 44-1 ---- query verification many simple tests start by enabling query verification this can be done through the following pragma statement statement ok pragma enable_verification query verification performs extra validation to ensure that the underlying code runs correctly the most important part of that is that it verifies that optimizers do not cause bugs in the query it does this by running both an unoptimized and optimized version of the query and verifying that the results of these queries are identical query verification is very useful because it not only discovers bugs in optimizers but also finds bugs in e g join implementations this is because the unoptimized version will typically run using cross products instead because of this query verification can be very slow to do when working with larger data sets it is therefore recommended to turn on query verification for all unit tests except those involving larger data sets more than 10-100 rows temporary files for some tests e g csv parquet file format tests it is necessary to create temporary files any temporary files should be created in the temporary testing directory this directory can be used by placing the string __test_dir__ in a query this string will be replaced by the path of the temporary testing directory require extensions to avoid bloating the core system certain functionality of duckdb is available only as an extension tests can be build for those extensions by adding a require field in the test if the extension is not loaded any statements that occurs after the require field will be skipped examples of this are require parquet or require icu another usage is to limit a test to a specific vector size for example adding require vector_size 512 to a test will prevent the test from being run unless the vector size greater than or equal to 512 this is useful because certain functionality is not supported for low vector sizes but we run tests using a vector size of 2 in our ci loops loops can be used in sqllogictests when it is required to execute the same query many times but with slight modifications in constant values only simple non-nested loops are supported for example suppose we want to fire off 100 queries that check for the presence of the values 0 100 in a table create the table integers with the values 0 100 statement ok create table integers as select from range 0 100 1 t1 i verify individually that all 100 values are there loop i 0 100 execute the query replacing the value query i select count from integers where i i ---- 1 end the loop note that multiple statements can be part of a loop endloop data generation loops should be used sparingly while it might be tempting to use loops for inserting data using insert statements this will considerably slow down the test cases instead it is better to generate data using the built-in range and repeat functions -- create the table integers with the values 0 1 98 99 create table integers as select from range 0 100 1 t1 i -- create the table strings with 100x the value hello create table strings as select from repeat hello 100 t2 s using these two functions together with clever use of cross products and other expressions many different types of datasets can be efficiently generated the random function can also be used to generate random data an alternative option is to read data from an existing csv file there are several large csv files that can be loaded from the directory test sql copy csv data real using a copy into statement or the read_csv_auto function debugging the purpose of the tests is to figure out when things break inevitably changes made to the system will cause one of the tests to fail and when that happens the test needs to be debugged first it is always recommended to run in debug mode this can be done by compiling the system using the command make debug second it is recommended to only run the test that breaks this can be done by passing the filename of the breaking test to the test suite as a command line parameter e g build debug test unittest test sql projection test_simple_projection test after that a debugger can be attached to the program and the test can be debugged in the sqllogictests it is normally difficult to break on a specific query however we have expanded the test suite so that a function called query_break is called with the line number line as parameter for every query that is run this allows you to put a conditional breakpoint on a specific query for example if we want to break on line number 43 of the test file we can create the following break point gdb break query_break if line 43 lldb break -n query_break -c line 43 you can also skip certain queries from executing by placing mode skip in the file followed by an optional mode unskip any queries between the two statements will not be executed editors syntax highlighting the sqllogictests are not exactly an industry standard but several other systems have adopted them as well parsing sqllogictests is intentionally simple all statements have to be separated by empty lines for that reason writing a syntax highlighter is not extremely difficult a syntax highlighter exists for visual studio code we have also made a fork that supports the duckdb dialect of the sqllogictests you can use the fork by installing the original then copying the syntaxes sqllogictest tmlanguage json into the installed extension on macos this is located in vscode extensions benesch sqllogictest-0 1 1 a syntax highlighter is also available for clion it can be installed directly on the ide by searching sqltest on the marketplace a github repository is also available with extensions and bug reports being welcome catch tests while we prefer the sqllogic tests for testing most functionality for certain tests only sql is not sufficient this typically happens when you want to test the c api when you want to stress test the system e g using multiple concurrent threads or when you want to test persistent storage involving database restarts when using pure sql is really not an option it might be necessary to make a c test using catch catch tests reside in the test directory as well here is an example of a catch test that tests the storage of the system include catch hpp include test_helpers hpp test_case test simple storage storage auto config gettestconfig unique_ptr queryresult result auto storage_database testcreatepath storage_test make sure the database does not exist deletedatabase storage_database create a database and insert values duckdb db storage_database config get connection con db require_no_fail con query create table test a integer b integer require_no_fail con query insert into test values 11 22 13 22 12 21 null null require_no_fail con query create table test2 a integer require_no_fail con query insert into test2 values 13 12 11 reload the database from disk a few times for idx_t i 0 i 2 i duckdb db storage_database config get connection con db result con query select from test order by a require check_column result 0 value 11 12 13 require check_column result 1 value 22 21 22 result con query select from test2 order by a require check_column result 0 11 12 13 deletedatabase storage_database the test uses the test_case wrapper to create each test the database is created and queried using the c api results are checked using either require_fail require_no_fail corresponding to statement ok and statement error or require check_column corresponding to query with a result check every test that is created in this way needs to be added to the corresponding cmakelists txt",
			"category": "docs",
			"url": "../docs/dev/testing",
			"blurb": "Testing is vital to make sure that DuckDB works properly and keeps working properly. For that reason, we put a large..."
		},
		{
			"title": "Why DuckDB",
			"text": "there are many database management systems dbms out there but there is no one-size-fits all database system all take different trade-offs to better adjust to specific use cases duckdb is no different here we try to explain what goals duckdb has and why and how we try to achieve those goals through technical means to start with duckdb is a relational table-oriented dbms that supports the structured query language sql to efficiently support this workload it is critical to reduce the amount of cpu cycles that are expended per individual value the state of the art in data management to achieve this are either vectorized or just-in-time query execution engines duckdb contains a columnar-vectorized query execution engine where queries are still interpreted but a large batch of values a vector are processed in one operation this greatly reduces overhead present in traditional systems such as postgresql mysql or sqlite which process each row sequentially vectorized query execution leads to far better performance in olap queries duckdb has no external dependencies neither for compilation nor during run-time for releases the entire source tree of duckdb is compiled into two files a header and an implementation file a so-called amalgamation this greatly simplifies deployment and integration in other build processes for building all that is required to build duckdb is a working c 11 compiler for duckdb there is no dbms server software to install update and maintain duckdb does not run as a separate process but completely embedded within a host process for the analytical use cases that duckdb targets this has the additional advantage of high-speed data transfer to and from the database in some cases duckdb can process foreign data without copying for example the duckdb python package can run queries directly on pandas data without ever importing or copying any data duckdb is deeply integrated into python and r for efficient interactive data analysis duckdb provides apis for java c c and others to facilitate this stability duckdb is intensively tested using continuous integration duckdb s test suite currently contains millions of queries and includes queries adapted from the test suites of sqlite postgresql and monetdb tests are repeated on a wide variety of platforms and compilers every pull request is checked against the full test setup and only merged if it passes in addition to this test suite we run various tests that stress duckdb under heavy loads we run the tpc-h and tpc-ds benchmarks and run various tests where duckdb is used by many clients in parallel peer-reviewed papers data management for data science - towards embedded analytics cidr 2020 duckdb an embeddable analytical database sigmod 2019 demo presentations duckdb the sqlite for analytics video presentation ca 1h duckdb - an embeddable analytical database video presentation ca 15min duckdb - an embeddable analytical rdbms slides 1000 days of duckdb - the pareto principle still holds slides the case for in-process analytics slides push-based execution in duckdb video presentation ca 1h fastest table sort in the west - redesigning duckdb s sort video presentation ca 20min other projects here are some projects that we know of that use duckdb if you would like your project to be added here open a github issue taxadb a high-performance local taxonomic database interface duckdb js - duckdb compiled to javascript poc sql for r dataframes with duckdb duckdb conda support dbt adapter for duckdb newlisp bindings for duckdb duckdb_engine - very very very basic sqlalchemy driver for duckdb toy duckdb based timeseries database php example to integrate duckdb using php-ffi duckdb foreign data wrapper for postgresql demo crud operations with qt5 and duckdb elixir driver and ecto adapter for duckdb testimonials see our duckdb testimonial twitter wall standing on the shoulders of giants duckdb uses some components from various open-source projects and draws inspiration from scientific publications we are very greatful for this here is an overview execution engine the vectorized execution engine is inspired by the paper monetdb x100 hyper-pipelining query execution by peter boncz marcin zukowski and niels nes optimizer duckdb s optimizer draws inspiration from the papers dynamic programming strikes back by guido moerkotte and thomas neumann as well as unnesting arbitrary queries by thomas neumann and alfons kemper concurrency control our mvcc implementation is inspired by the paper fast serializable multi-version concurrency control for main-memory database systems by thomas neumann tobias mühlbauer and alfons kemper secondary indexes duckdb has support for secondary indexes based on the paper the adaptive radix tree artful indexing for main-memory databases by viktor leis alfons kemper and thomas neumann sql window functions duckdb s window functions implementation uses segment tree aggregation as described in the paper efficient processing of window functions in analytical sql queries by viktor leis kan kundhikanjana alfons kemper and thomas neumann sql parser we use the postgresql parser that was repackaged as a stand-alone library the translation to our own parse tree is inspired by peloton shell we use the sqlite shell to work with duckdb regular expressions duckdb uses google s re2 regular expression engine string formatting duckdb uses the fmt string formatting library utf wrangling duckdb uses the utf8proc library to check and normalize utf8 test framework duckdb uses the catch2 unit test framework test cases we use the sql logic tests from sqlite to test duckdb result validation manuel rigger used his excellent sqlancer tool to verify duckdb result correctness query fuzzing we use sqlsmith to generate random queries for additional testing",
			"category": "docs",
			"url": "../docs/why_duckdb",
			"blurb": "There are many database management systems (DBMS) out there. But there is   no one-size-fits all database system . All..."
		},
		{
			"title": "Insert Statements",
			"text": "insert statements are the standard way of loading data into a relational database when using insert statements the values are supplied row-by-row while simple there is significant overhead involved in parsing and processing individual insert statements this makes them very inefficient for bulk insertion as a rule-of-thumb avoid insert statements when inserting more than a few rows i e avoid using insert statements as part of a loop if you must use insert statements to load data in a loop avoid executing the statements in auto-commit mode after every commit the database is required to sync the changes made to disk to ensure no data is lost in auto-commit mode every single statement will be wrapped in a separate transaction meaning fsync will be called for every statement this is typically unnecessary when bulk loading and will significantly slow down your program if you absolutely must use insert statements in a loop to load data wrap them in calls to begin transaction and commit syntax an example of using insert into to load data in a table is as follows create table people id integer name varchar insert into people values 1 mark 2 hannes a more detailed description together with syntax diagram can be found here",
			"category": "docs",
			"url": "../docs/data/insert",
			"blurb": "Insert statements are the standard way of loading data into a relational database. When using insert statements, the..."
		},
		{
			"title": "CSV Loading",
			"text": "csv loading is a very common and yet surprisingly tricky task while csvs seem simple on the surface there are a lot of inconsistencies found within csv files that can make loading them a challenge csv files exist with different delimiters they can contain quoted values have an optional header row or even multiple or even be completely deformed the csv reader needs to cope with all of these different situations the duckdb csv reader can automatically infer which configuration flags to use by analyzing the csv file this will work correctly in most situations and should be the first option attempted in rare situations where the csv reader cannot figure out the correct configuration it is possible to manually configure the csv reader to correctly parse the csv file we use the following csv file in our examples keep in mind that you can also use compressed csv files in the following examples e g a gzipped file such as test csv gz will work just fine test csv flightdate uniquecarrier origincityname destcityname 1988-01-01 aa new york ny los angeles ca 1988-01-02 aa new york ny los angeles ca 1988-01-03 aa new york ny los angeles ca examples -- read a csv file from disk auto-infer options select from test csv -- read_csv with custom options select from read_csv_auto test csv delim header true columns flightdate date uniquecarrier varchar origincityname varchar destcityname varchar -- read a csv file into a table create table ontime flightdate date uniquecarrier varchar origincityname varchar destcityname varchar copy ontime from test csv auto_detect true -- alternatively create a table without specifying the schema manually create table ontime as select from test csv -- write the result of a query to a csv file copy select from ontime to test csv with header 1 delimiter read_csv_auto function the read_csv_auto is the simplest method of loading csv files it automatically attempts to figure out the correct configuration of the csv reader it also automatically deduces types of columns if the csv file has a header it will use the names found in that header to name the columns otherwise the columns will be named column0 column1 column2 select from read_csv_auto test csv flightdate uniquecarrier origincityname destcityname --------- ------------ ---------------- -------------- 1988-01-01 aa new york ny los angeles ca 1988-01-02 aa new york ny los angeles ca 1988-01-03 aa new york ny los angeles ca the path can either be a relative path relative to the current working directory or an absolute path we can use read_csv_auto to create a persistent table as well create table ontime as select from read_csv_auto test csv describe ontime field type null key default extra ------------- ------ --- --- ------ ---- flightdate date yes null null null uniquecarrier varchar yes null null null origincityname varchar yes null null null destcityname varchar yes null null null sample_size option to define number of sample rows for automatic csv type detection chunks of sample rows will be drawn from different locations of the input file set to -1 to scan the entire input file note only the first max 1024 rows will be used for dialect detection all_varchar option to skip type detection for csv parsing and assume all columns to be of type varchar select from read_csv_auto test csv sample_size 20000 if we set delim sep quote escape or header explicitly we can bypass the automatic detection of this particular parameter select from read_csv_auto test csv header true note read_csv_auto is an alias for read_csv auto_detect true copy statement the copy statement can be used to load data from a csv file into a table this statement has the same syntax as the copy statement supported by postgresql for the copy statement we must first create a table with the correct schema to load the data into we then specify the csv file to load from plus any configuration options separately create table ontime flightdate date uniquecarrier varchar origincityname varchar destcityname varchar copy ontime from test csv delimiter header select from ontime flightdate uniquecarrier origincityname destcityname --------- ------------ ---------------- -------------- 1988-01-01 aa new york ny los angeles ca 1988-01-02 aa new york ny los angeles ca 1988-01-03 aa new york ny los angeles ca if we want to use the automatic format detection we can set auto_detect to true and omit the otherwise required configuration options create table ontime flightdate date uniquecarrier varchar origincityname varchar destcityname varchar copy ontime from test csv auto_detect true select from ontime more on the copy statement can be found here",
			"category": "docs",
			"url": "../docs/data/csv",
			"blurb": "CSV loading is a very common, and yet surprisingly tricky, task. While CSVs seem simple on the surface, there are a lot..."
		},
		{
			"title": "Importing Data",
			"text": "the first step to using a database system is to insert data into that system duckdb provides several data ingestion methods that allow you to easily and efficiently fill up the database in this section we provide an overview of these methods so you can select which one is correct for you insert statements insert statements are the standard way of loading data into a database system they are suitable for quick prototyping but should be avoided for bulk loading as they have significant per-row overhead insert into people values 1 mark see here for a more detailed description of insert statements csv loading data can be efficiently loaded from csv files using the read_csv_auto function or the copy statement select from read_csv_auto test csv you can also load data from compressed e g compressed with gzip csv files for example select from read_csv_auto test csv gz see here for a detailed description of csv loading parquet loading parquet files can be efficiently loaded and queried using the parquet_scan function select from parquet_scan test parquet see here for a detailed description of parquet loading appender c and java in c and java the appender can be used as an alternative for bulk data loading this class can be used to efficiently add rows to the database system without needing to use sql c appender appender con people appender appendrow 1 mark appender close java con createappender main people appender beginrow appender append mark appender endrow appender close see here for a detailed description of the c appender",
			"category": "docs",
			"url": "../docs/data/overview",
			"blurb": "The first step to using a database system is to insert data into that system. DuckDB provides several data ingestion..."
		},
		{
			"title": "Appender",
			"text": "the c appender can be used to load bulk data into a duckdb database the appender is tied to a connection and will use the transaction context of that connection when appending an appender always appends to a single table in the database file duckdb db connection con db create the table con query create table people id integer name varchar initialize the appender appender appender con people the appendrow function is the easiest way of appending data it uses recursive templates to allow you to put all the values of a single row within one function call as follows appender appendrow 1 mark rows can also be individually constructed using the beginrow endrow and append methods this is done internally by appendrow and hence has the same performance characteristics appender beginrow appender append int32_t 2 appender append string hannes appender endrow any values added to the appender are cached prior to being inserted into the database system for performance reasons that means that while appending the rows might not be immediately visible in the system the cache is automatically flushed when the appender goes out of scope or when appender close is called the cache can also be manually flushed using the appender flush method after either flush or close is called all the data has been written to the database system date time and timestamps while numbers and strings are rather self-explanatory dates times and timestamps require some explanation they can be directly appended using the methods provided by duckdb date duckdb time or duckdb timestamp they can also be appended using the internal duckdb value type however this adds some additional overheads and should be avoided if possible below is a short example con query create table dates d date t time ts timestamp appender appender con dates construct the values using the date time timestamp types - this is the most efficient appender appendrow date fromdate 1992 1 1 time fromtime 1 1 1 0 timestamp fromdatetime date fromdate 1992 1 1 time fromtime 1 1 1 0 construct duckdb value objects appender appendrow value date 1992 1 1 value time 1 1 1 0 value timestamp 1992 1 1 1 1 1 0",
			"category": "docs",
			"url": "../docs/data/appender",
			"blurb": "The C++ Appender can be used to load bulk data into a DuckDB database. The Appender is tied to a connection, and will use..."
		},
		{
			"title": "Parquet",
			"text": "parquet files are compressed columnar files that are efficient to load and process duckdb provides support for both reading and writing parquet files in an efficient manner as well as support for pushing filters and projections into the parquet file scans examples -- read a single parquet file select from test parquet -- figure out which columns types are in a parquet file describe select from test parquet -- create a table from a parquet file create table test as select from test parquet -- if the file does not end in parquet use the parquet_scan function select from parquet_scan test parq -- use list parameter to read 3 parquet files and treat them as a single table select from parquet_scan file1 parquet file2 parquet file3 parquet -- read all files that match the glob pattern select from test parquet -- use a list of globs to read all parquet files from 2 specific folders select from parquet_scan folder1 parquet folder2 parquet -- query the metadata of a parquet file select from parquet_metadata test parquet -- query the schema of a parquet file select from parquet_schema test parquet -- write the results of a query to a parquet file copy select from tbl to result-snappy parquet format parquet -- export the table contents of the entire database as parquet export database target_directory format parquet single-file reads duckdb includes an efficient parquet reader in the form of the parquet_scan function select from parquet_scan test parquet if your file ends in parquet the parquet_scan syntax is optional the system will automatically infer that you are reading a parquet file select from test parquet unlike csv files parquet files are structured and as such are unambiguous to read no parameters need to be passed to this function the parquet_scan function will figure out the column names and column types present in the file and emit them multi-file reads and globs duckdb can also read a series of parquet files and treat them as if they were a single table note that this only works if the parquet files have the same schema you can specify which parquet files you want to read using a list parameter glob pattern matching syntax or a combination of both list parameter the parquet_scan function can accept a list of filenames as the input parameter see the nested types documentation for more details on lists -- read 3 parquet files and treat them as a single table select from parquet_scan file1 parquet file2 parquet file3 parquet glob syntax any file name input to the parquet_scan function can either be an exact filename or use a glob syntax to read multple files that match a pattern wildcard description ------------ ----------------------------------------------------------- matches any number of any characters including none matches any single character abc matches one character given in the bracket a-z matches one character from the range given in the bracket here is an example that reads all the files that end with parquet located in the test folder -- read all files that match the glob pattern select from parquet_scan test parquet list of globs the glob syntax and the list input parameter can be combined to scan files that meet one of multiple patterns -- read all parquet files from 2 specific folders select from parquet_scan folder1 parquet folder2 parquet partial reading duckdb supports projection pushdown into the parquet file itself that is to say when querying a parquet file only the columns required for the query are read this allows you to read only the part of the parquet file that you are interested in this will be done automatically by the system duckdb also supports filter pushdown into the parquet reader when you apply a filter to a column that is scanned from a parquet file the filter will be pushed down into the scan and can even be used to skip parts of the file using the built-in zonemaps note that this will depend on whether or not your parquet file contains zonemaps filter and projection pushdown provide significant performance benefits see our blog post on this for more information inserts and views you can also insert the data into a table or create a table from the parquet file directly this will load the data from the parquet file and insert it into the database -- insert the data from the parquet file in the table insert into people select from parquet_scan test parquet -- create a table directly from a parquet file create table people as select from parquet_scan test parquet if you wish to keep the data stored inside the parquet file but want to query the parquet file directly you can create a view over the parquet_scan function you can then query the parquet file as if it were a built-in table -- create a view over the parquet file create view people as select from parquet_scan test parquet -- query the parquet file select from people parquet metadata the parquet_metadata function can be used to query the metadata contained within a parquet file which reveals various internal details of the parquet file such as the statistics of the different columns this can be useful for figuring out what kind of skipping is possible in parquet files or even to obtain a quick overview of what the different columns contain select from parquet_metadata test parquet below is a table of the columns returned by parquet_metadata field type ------------------------- --------- file_name varchar row_group_id bigint row_group_num_rows bigint row_group_num_columns bigint row_group_bytes bigint column_id bigint file_offset bigint num_values bigint path_in_schema varchar type varchar stats_min varchar stats_max varchar stats_null_count bigint stats_distinct_count bigint stats_min_value varchar stats_max_value varchar compression varchar encodings varchar index_page_offset bigint dictionary_page_offset bigint data_page_offset bigint total_compressed_size bigint total_uncompressed_size bigint parquet schema the parquet_schema function can be used to query the internal schema contained within a parquet file note that this is the schema as it is contained within the metadata of the parquet file if you want to figure out the column names and types contained within a parquet file it is easier to use describe -- fetch the column names and column types describe select from test parquet -- fetch the internal schema of a parquet file select from parquet_schema test parquet below is a table of the columns returned by parquet_schema field type ----------------- --------- file_name varchar name varchar type varchar type_length varchar repetition_type varchar num_children bigint converted_type varchar scale bigint precision bigint field_id bigint logical_type varchar writing to parquet files duckdb also has support for writing to parquet files using the copy statement syntax you can specify which compression format should be used using the codec parameter options uncompressed snappy default zstd gzip -- write a query to a snappy compressed parquet file copy select from tbl to result-snappy parquet format parquet -- write tbl to a zstd compressed parquet file copy tbl to result-zstd parquet format parquet codec zstd -- write a csv file to an uncompressed parquet file copy test csv to result-uncompressed parquet format parquet codec uncompressed duckdb s export command can be used to export an entire database to a series of parquet files see the export statement documentation for more details -- export the table contents of the entire database as parquet export database target_directory format parquet",
			"category": "docs",
			"url": "../docs/data/parquet",
			"blurb": "Parquet files are compressed columnar files that are efficient to load and process. DuckDB provides support for both..."
		},
		{
			"title": "Aggregate Functions",
			"text": "aggregates are functions that combine multiple rows into a single value aggregates are different from scalar functions and window functions because they change the cardinality of the result as such aggregates can only be used in the select and having clauses of a sql query when the distinct clause is provided only distinct values are considered in the computation of the aggregate this is typically used in combination with the count aggregate to get the number of distinct elements but it can be used together with any aggregate function in the system when the order by clause is provided the values being aggregated are sorted before applying the function usually this is not important but there are some order-sensitive aggregates that can have indeterminate results e g first last list and string_agg these can be made deterministic by ordering the arguments for order-insensitive aggregates this clause is parsed but ignored general aggregate functions the table below shows the available general aggregate functions function description example alias es --- --- --- --- arg_max arg val calculates the arg value for a maximum val value arg_max a b argmax a b max_by a b arg_min arg val calculates the arg value for a minimum val value arg_min a b argmin a b min_by a b avg arg calculates the average value for all tuples in arg avg a - bit_and arg returns the bitwise and of all bits in a given expression bit_and a - bit_or arg returns the bitwise or of all bits in a given expression bit_or a - bit_xor arg returns the bitwise xor of all bits in a given expression bit_xor a - bool_and arg returns true if every input value is true otherwise false bool_and a - bool_or arg returns true if any input value is true otherwise false bool a - count arg calculates the number of tuples tuples in arg count a - favg arg calculates the average using a more accurate floating point summation kahan sum favg a - first arg returns the first value of a column first a arbitrary a fsum arg calculates the sum using a more accurate floating point summation kahan sum fsum a sumkahan kahan_sum histogram arg returns a list of struct s with the fields bucket and count histogram a - last arg returns the last value of a column last a - list arg returns a list containing all the values of a column list a array_agg max arg returns the maximum value present in arg max a - min arg returns the minumum value present in arg min a - product arg calculates the product of all tuples in arg product a - string_agg arg sep concatenates the column string values with a separator string_agg s group_concat sum arg calculates the sum value for all tuples in arg sum a - approximate aggregates the table below shows the available approximate aggregate functions function description example --- --- --- approx_count_distinct x gives the approximate count of distintinct elements using hyperloglog approx_count_distinct a approx_quantile x pos gives the approximate quantile using t-digest approx_quantile a 0 5 reservoir_quantile x quantile sample_size 8192 gives the approximate quantile using reservoir sampling the sample size is optional and uses 8192 as a default size reservoir_quantile a 0 5 1024 statistical aggregates the table below shows the available statistical aggregate functions function description formula alias --- --- --- --- corr y x returns the correlation coefficient for non-null pairs in a group covar_pop y x stddev_pop x stddev_pop y - covar_pop y x returns the population covariance of input values sum x y - sum x sum y count count - entropy x returns the log-2 entropy of count input-values - - kurtosis x returns the excess kurtosis of all input values - - mad x returns the median absolute deviation for the values within x null values are ignored temporal types return a positive interval median abs x-median x - median x returns the middle value of the set null values are ignored for even value counts quantitiative values are averaged and ordinal values return the lower value quantile_cont x 0 5 - mode x returns the most frequent value for the values within x null values are ignored - - quantile_cont x pos returns the intepolated quantile number between 0 and 1 if pos is a list of float s then the result is a list of the corresponding intepolated quantiles - - quantile_disc x pos returns the exact quantile number between 0 and 1 if pos is a list of float s then the result is a list of the corresponding exact quantiles - quantile regr_avgx y x returns the average of the independent variable for non-null pairs in a group where x is the independent variable and y is the dependent variable - - regr_avgy y x returns the average of the dependent variable for non-null pairs in a group where x is the independent variable and y is the dependent variable - - regr_count y x returns the number of non-null number pairs in a group sum x y - sum x sum y count count - regr_intercept y x returns the intercept of the univariate linear regression line for non-null pairs in a group avg y -regr_slope y x avg x - stddev_pop y x returns the population standard deviation square root of variance of non-null values - - regr_r2 y x returns the coefficient of determination for non-null pairs in a group - - regr_slope y x returns the slope of the linear regression line for non-null pairs in a group covar_pop x y var_pop x - regr_sxx y x - regr_count y x var_pop x - regr_sxy y x returns the population covariance of input values regr_count y x covar_pop y x - regr_syy y x - regr_count y x var_pop y f - skewness x returns the skewness of all input values - - stddev_pop x returns the population standard deviation sqrt var_pop x - stddev_samp x returns the sample standard deviation sqrt var_samp x stddev x var_pop x returns the population variance - - var_samp x returns the sample variance of all input values sum x 2 - sum x 2 count x count x - 1 variance arg val ordered set aggregate functions the table below shows the available ordered set aggregate functions these functions are specified using the within group order by sort_expression syntax and they are converted to an equivalent aggregate function that takes the ordering expression as the first argument function equivalent --- --- mode within group order by sort_expression mode sort_expression percentile_cont fraction within group order by sort_expression quantile_cont sort_expression fraction percentile_cont fractions within group order by sort_expression quantile_cont sort_expression fractions percentile_disc fraction within group order by sort_expression quantile_disc sort_expression fraction percentile_disc fractions within group order by sort_expression quantile_disc sort_expression fractions",
			"category": "docs",
			"url": "../docs/sql/aggregates",
			"blurb": "Aggregates are functions that   combine  multiple rows into a single value. Aggregates are different from scalar..."
		},
		{
			"title": "Numeric Types",
			"text": "integer types the types tinyint smallint integer bigint and hugeint store whole numbers that is numbers without fractional components of various ranges attempts to store values outside of the allowed range will result in an error the types utinyint usmallint uinteger ubigint store whole unsigned numbers attempts to store negative numbers or values outside of the allowed range will result in an error name aliases min max --- --- --- --- tinyint int1 -128 127 smallint int2 short -32768 32767 integer int4 int signed -2147483648 2147483647 bigint int8 long -9223372036854775808 9223372036854775808 hugeint -170141183460469231731687303715884105727 170141183460469231731687303715884105727 utinyint - 0 255 usmallint - 0 65535 uinteger - 0 4294967295 ubigint - 0 18446744073709551615 the type integer is the common choice as it offers the best balance between range storage size and performance the smallint type is generally only used if disk space is at a premium the bigint and hugeint types are designed to be used when the range of the integer type is insufficient fixed-point decimals the data type decimal width scale represents an exact fixed-point decimal value when creating a value of type decimal the width and scale can be specified to define which size of decimal values can be held in the field the width field determines how many digits can be held and the scale determines the amount of digits after the decimal point for example the type decimal 3 2 can fit the value 1 23 but cannot fit the value 12 3 or the value 1 234 the default width and scale is decimal 18 3 if none are specified internally decimals are represented as integers depending on their specified width width internal size bytes --- --- --- 1-4 int16 2 5-9 int32 4 10-18 int64 8 19-38 int128 16 performance can be impacted by using too large decimals when not required in particular decimal values with a width above 19 are very slow as arithmetic involving the int128 type is much more expensive than operations involving the int32 or int64 types it is therefore recommended to stick with a width of 18 or below unless there is a good reason for why this is insufficient floating-point types the data types real and double precision are inexact variable-precision numeric types in practice these types are usually implementations of ieee standard 754 for binary floating-point arithmetic single and double precision respectively to the extent that the underlying processor operating system and compiler support it name aliases description --- --- --- real float4 single precision floating-point number 4 bytes double float8 double precision floating-point number 8 bytes inexact means that some values cannot be converted exactly to the internal format and are stored as approximations so that storing and retrieving a value might show slight discrepancies managing these errors and how they propagate through calculations is the subject of an entire branch of mathematics and computer science and will not be discussed here except for the following points if you require exact storage and calculations such as for monetary amounts use the numeric type instead if you want to do complicated calculations with these types for anything important especially if you rely on certain behavior in boundary cases infinity underflow you should evaluate the implementation carefully comparing two floating-point values for equality might not always work as expected on most platforms the real type has a range of at least 1e-37 to 1e 37 with a precision of at least 6 decimal digits the double type typically has a range of around 1e-307 to 1e 308 with a precision of at least 15 digits values that are too large or too small will cause an error rounding might take place if the precision of an input number is too high numbers too close to zero that are not representable as distinct from zero will cause an underflow error in addition to ordinary numeric values the floating-point types have several special values infinity -infinity nan these represent the ieee 754 special values infinity negative infinity and not-a-number respectively on a machine whose floating-point arithmetic does not follow ieee 754 these values will probably not work as expected when writing these values as constants in an sql command you must put quotes around them for example update table set x -infinity on input these strings are recognized in a case-insensitive manner functions see numeric functions and operators",
			"category": "docs",
			"url": "../docs/sql/data_types/numeric",
			"blurb": "Numeric types are used to store numbers, and come in different shapes and sizes."
		},
		{
			"title": "Text Types",
			"text": "in duckdb strings can be stored in the varchar field name aliases description --- --- --- varchar char bpchar text string variable-length character string varchar n variable-length character string with maximum length n it is possible to supply a maximum length along with the type by initializing a type as varchar n where n is a positive integer note that specifying this length is not required and specifying this length will not improve performance or reduce storage space of the strings in the database specifying a maximum length is useful only for data integrity reasons not for performance reasons in fact the following sql statements are equivalent -- the following statements are equivalent create table strings val varchar 10 -- val has a maximum length of 10 characters create table strings val varchar check length val 10 -- val has a maximum length of 10 characters the varchar field allows storage of unicode characters internally the data is encoded as utf-8 functions see character functions and pattern matching",
			"category": "docs",
			"url": "../docs/sql/data_types/text",
			"blurb": "In DuckDB, strings can be stored in the VARCHAR field."
		},
		{
			"title": "Data Types",
			"text": "the table below shows all the built-in general-purpose data types the alternatives listed in the aliases column can be used to refer to these types as well however note that the aliases are not part of the sql standard and hence might not be accepted by other database engines name aliases description --- --- --- bigint int8 long signed eight-byte integer boolean bool logical logical boolean true false blob bytea binary varbinary variable-length binary data date calendar date year month day double float8 numeric decimal double precision floating-point number 8 bytes decimal s p fixed-precision floating point number with the given scale and precision hugeint signed sixteen-byte integer integer int4 int signed signed four-byte integer real float4 float single precision floating-point number 4 bytes smallint int2 short signed two-byte integer time time of day no time zone timestamp datetime combination of time and date tinyint int1 signed one-byte integer ubigint unsigned eight-byte integer uinteger unsigned four-byte integer usmallint unsigned two-byte integer utinyint unsigned one-byte integer uuid uuid data type varchar char bpchar text string variable-length character string in addition the composite types row map and array are supported multi-dimensional arrays can be created by appending square brackets after the type e g int to create an integer array row can be created by specifying the individual columns that reside within the row e g row a integer b varchar note that rows can be nested and can also contain arrays more",
			"category": "docs",
			"url": "../docs/sql/data_types/overview",
			"blurb": "The table below shows all the built-in general-purpose data types."
		},
		{
			"title": "Blob Type",
			"text": "name aliases description --- --- --- blob bytea variable-length binary data the blob b inary l arge ob ject type represents an arbitrary binary object stored in the database system the blob type can contain any type of binary data with no restrictions what the actual bytes represent is opaque to the database system -- create a blob value with a single byte 170 select xaa blob -- create a blob value with two bytes 65 66 select ab blob blobs are typically used to store non-textual objects that the database does not provide explicit support for such as images while blobs can hold objects up to 4gb in size typically it is not recommended to store very large objects within the database system in many situations it is better to store the large file on the file system and store the path to the file in the database system in a varchar field functions see blob functions",
			"category": "docs",
			"url": "../docs/sql/data_types/blob",
			"blurb": "The blob (Binary Large OBject) type represents an arbitrary binary object stored in the database system."
		},
		{
			"title": "Date Types",
			"text": "name aliases description --- --- --- date calendar date year month day a date specifies a combination of year month and day duckdb follows the sql standard s lead by counting dates exclusively in the gregorian calendar even for years before that calendar was in use dates can be created using the date keyword where the data must be formatted according to the iso 8601 format yyyy-mm-dd -- 20 september 1992 select date 1992-09-20 functions see date functions",
			"category": "docs",
			"url": "../docs/sql/data_types/date",
			"blurb": "A date specifies a combination of year, month and day."
		},
		{
			"title": "Nested Types",
			"text": "this section describes functions and operators for examining and manipulating nested values duckdb supports three nested data types lists structs and maps name description rules when used in a column build from values define in ddl create --- --- --- --- --- list an ordered sequence of data values of the same type each row must have the same data type within each list but can have any number of elements 1 2 3 int struct a dictionary of multiple named values where each key is a string but the value can be a different type for each key each row must have the same keys i 42 j a struct i int j varchar map a dictionary of multiple named values each key having the same type and each value having the same type keys and values can be any type and can be different types from one another rows may have different keys map 1 2 a b map int varchar lists a list column can have values with different lengths but they must all have the same underlying type list s are typically used to store arrays of numbers but can contain any uniform data type including other list s and struct s list s are similar to postgres s array type duckdb uses the list terminology but some array functions are provided for postgres compatibility lists can be created using the list_value expr function or the equivalent bracket notation expr the expressions can be constants or arbitrary expressions creating lists -- list of integers select 1 2 3 -- list of strings with a null value select duck goose null heron -- list of lists with null values select duck goose heron null frog toad -- create a list with the list_value function select list_value 1 2 3 -- create a table with an integer list column and a varchar list column create table list_table int_list int varchar_list varchar retrieving from lists retrieving one or more values from a list can be accomplished using brackets and slicing notation or through list functions like list_extract multiple equivalent functions are provided as aliases for compatibility with systems that refer to lists as arrays for example the function array_slice -- retrieve an element from a list using brackets this returns c -- note that we wrap the list creation in parenthesis so that it happens first -- this is only needed in our basic examples here not when working with a list column -- for example this can t be parsed select a b c 1 select a b c 2 -- use a negative index to grab the nth element from the end of the list this returns c select a b c -1 -- any expression that evaluates to an integer can be used to retrieve a list value -- this includes using a column to determine which index to retrieve -- this returns c select a b c 1 1 -- the list_extract function may also be used in place of brackets for selecting individual elements -- this returns c select list_extract a b c 2 -- retrieve multiple list values using a bracketed slice syntax this returns b c select a b c 1 3 -- single sided slices are also supported here grab the first 2 elements this returns a b select a b c 2 -- use a negative index to grab the last 2 elements this returns b c select a b c -2 -- the array_slice function is also supported this returns b c select array_slice a b c 1 3 structs conceptually a struct column contains an ordered list of other columns called entries the entries are referenced by name using strings this document refers to those entry names as keys each row in the struct column must have the same keys each key must have the same type of value for each row struct s are typically used to nest multiple columns into a single column and the nested column can be of any type including other struct s and list s struct s are similar to postgres s row type the key difference is that duckdb struct s require the same keys in each row of a struct column this allows duckdb to provide significantly improved performance by fully utilizing its vectorized execution engine and also enforces type consistency for improved correctness duckdb includes a row function as a special way to produce a struct but does not have a row data type see an example below and the nested functions docs for details structs can be created using the struct_pack name expr function or the equivalent array notation name expr notation the expressions can be constants or arbitrary expressions creating structs -- struct of integers select x 1 y 2 z 3 -- struct of strings with a null value select yes duck maybe goose huh null no heron -- struct with a different type for each key select key1 string key2 1 key3 12 345 -- struct using the struct_pack function -- note the lack of single quotes around the keys and the use of the operator select struct_pack key1 value1 key2 42 -- struct of structs with null values select birds yes duck maybe goose huh null no heron aliens null amphibians yes frog maybe salamander huh dragon no toad -- create a struct from columns and or expressions using the row function -- this returns x 1 v2 2 y a select row x x 1 y from select 1 as x a as y -- if using multiple expressions when creating a struct the row function is optional -- this also returns x 1 v2 2 y a select x x 1 y from select 1 as x a as y retrieving from structs retrieving a value from a struct can be accomplished using dot notation bracket notation or through struct functions like struct_extract -- use dot notation to retrieve the value at a key s location this returns 1 -- the subquery generates a struct column a which we then query with a x select a x from select x 1 y 2 z 3 as a -- if key contains a space simply wrap it in double quotes this returns 1 -- note use double quotes not single quotes -- this is because this action is most similar to selecting a column from within the struct select a x space from select x space 1 y 2 z 3 as a -- bracket notation may also be used this returns 1 -- note use single quotes since the goal is to specify a certain string key -- only constant expressions may be used inside the brackets no columns select a x space from select x space 1 y 2 z 3 as a -- the struct_extract function is also equivalent this returns 1 select struct_extract x space 1 y 2 z 3 x space referring to structs with dot notation can be ambiguous with referring to schemas and tables in general duckdb looks for columns first then for struct keys within columns duckdb resolves references in these orders using the first match to occur no dots select part1 from tbl part1 is a column one dot select part1 part2 from tbl part1 is a table part2 is a column part1 is a column part2 is a property of that column two or more dots select part1 part2 part3 from tbl part1 is a schema part2 is a table part3 is a column part1 is a table part2 is a column part3 is a property of that column part1 is a column part2 is a property of that column part3 is a property of that column any extra parts e g part4 part5 etc are always treated as properties creating structs with the row function the row function can be used to automatically convert multiple columns to a single struct column the name of each input column is used as a key and the value of each column becomes the struct s value at that key when converting multiple expressions into a struct the row function name is optional - a set of parenthesis is all that is needed example data table named t1 my_column another_column --- --- 1 a 2 b row function example select row my_column another_column as my_struct_column my_column another_column as identical_struct_column from t1 example output my_struct_column identical_struct_column --- --- my_column 1 another_column a my_column 1 another_column a my_column 2 another_column b my_column 2 another_column b the row function or simplified parenthesis syntax may also be used with arbitrary expressions as input rather than column names in the case of an expression a key will be automatically generated in the format of vn where n is a number that refers to its parameter location in the row function ex v1 v2 etc this can be combined with column names as an input in the same call to the row function this example uses the same input table as above row function example with a column name a constant and an expression as input select row my_column 42 my_column 1 as my_struct_column my_column 42 my_column 1 as identical_struct_column from t1 example output my_struct_column identical_struct_column --- --- my_column 1 v2 42 v3 2 my_column 1 v2 42 v3 2 my_column 2 v2 42 v3 3 my_column 2 v2 42 v3 3 maps map s are similar to struct s in that they are an ordered list of entries where a key maps to a value however map s do not need to have the same keys present on each row and thus open additional use cases map s are useful when the schema is unknown beforehand and when adding or removing keys in subsequent rows their flexibility is a key differentiator map s must have a single type for all keys and a single type for all values keys and values can be any type and the type of the keys does not need to match the type of the values ex a map of int s to varchar s map s may also have duplicate keys this is possible and useful because maps are ordered map s are also more forgiving when extracting values as they return an empty list if a key is not found rather than throwing an error as structs do in contrast struct s must have string keys but each key may have a value of a different type struct s may not have duplicate keys to construct a map use the map function provide a list of keys as the first parameter and a list of values for the second creating maps -- a map with integer keys and varchar values this returns 1 a 5 e select map 1 5 a e -- a map with integer keys and numeric values this returns 1 42 001 5 -32 100 select map 1 5 42 001 -32 1 -- keys and or values can also be nested types -- this returns a b 1 1 2 2 c d 3 3 4 4 select map a b c d 1 1 2 2 3 3 4 4 -- create a table with a map column that has integer keys and double values create table map_table map_col map int double retrieving from maps map s use bracket notation for retrieving values this is due to the variety of types that can be used as a map s key selecting from a map also returns a list rather than an individual value -- use bracket notation to retrieve a list containing the value at a key s location this returns 42 -- note that the expression in bracket notation must match the type of the map s key select map 100 5 42 43 100 -- to retrieve the underlying value use list selection syntax to grab the 0th element -- this returns 42 select map 100 5 42 43 100 0 -- if the element is not in the map an empty list will be returned returns -- note that the expression in bracket notation must match the type of the map s key else an error is returned select map 100 5 42 43 123 -- the element_at function can also be used to retrieve a map value this returns 42 select element_at map 100 5 42 43 100 nesting list s and struct s can be arbitrarily nested to any depth so long as the type rules are observed -- struct with lists select birds duck goose heron aliens null amphibians frog toad comparison nested types can be compared using all the comparison operators these comparisons can be used in logical expressions for both where and having clauses as well as for creating boolean values the ordering is defined positionally in the same way that words can be ordered in a dictionary null values compare greater than all other values and are considered equal to each other at the top level null nested values obey standard sql null comparison rules comparing a null nested value to a non- null nested value produces a null result comparing nested value members however uses the internal nested value rules for null s and a null nested value member will compare above a non- null nested value member grouping and joining nested types can be used in group by clauses and as expressions for join s functions see nested functions",
			"category": "docs",
			"url": "../docs/sql/data_types/nested",
			"blurb": "This section describes functions and operators for examining and manipulating nested values. DuckDB supports three nested..."
		},
		{
			"title": "Enum Types",
			"text": "name description --- --- enum dictionary encoding representing all possible string values of a column enums the enum type represents a dictionary data structure with all possible unique values of a column for example a column storing the days of the week can be an enum holding all possible days enums are particularly interesting for string columns with high cardinality this is because the column only stores a numerical reference to the string in the enum dictionary resulting in immense savings in disk storage and faster query performance enum definition enum types are created using the command create type enum_name as enum value_1 value_2 for example -- creates new user defined type mood as an enum create type mood as enum sad ok happy -- this will fail since the mood type already exists create type mood as enum sad ok happy anxious -- this will fail since enums cannot hold null values create type breed as enum maltese null -- this will fail since enum values must be unique create type breed as enum maltese maltese enum usage after an enum has been created it can be used anywhere a standard built-in type is used for example we can create a table with a column that references the enum -- creates a table person with attributes name string type and current_mood mood type create table person name text current_mood mood -- inserts tuples in the person table insert into person values pedro happy mark null pagliacci sad mr mackey ok -- this will fail since the mood type does not have a quackity-quack value insert into person values hannes quackity-quack -- the string sad is cast to the type mood returning a numerical reference value -- this makes the comparison a numerical comparison instead of a string comparison select from person where current_mood sad ---- pagliacci enum vs strings duckdb enums are automatically cast to varchar types whenever necessary this characteristic allows for enum columns to be used in any varchar function in addition it also allows for comparisons between different enum columns or an enum and a varchar column for example -- regexp_matches is a function that takes a varchar hence current_mood is cast to varchar select regexp_matches current_mood a from person ---- true false true false create type new_mood as enum happy anxious create table person_2 name text current_mood mood future_mood new_mood past_mood varchar -- since current_mood and future_mood are constructed on different enums -- duckdb will cast both enums to strings and perform a string comparison select from person_2 where current_mood future_mood -- since current_mood is an enum -- duckdb will cast the current_mood enum to varchar and perform a string comparison select from person_2 where current_mood past_mood enum removal enum types are stored in the catalog and a catalog dependency is added to each table that uses them it is possible to drop an enum from the catalog using the following command drop type enum_name note that any dependent must be removed before dropping the enum or the enum must be dropped with the additional cascade parameter for example -- this will fail since person has a catalog dependency to the mood type drop type mood drop table person drop table person_2 -- this successfully removes the mood type -- another option would be to drop type mood cascade drops the type and its dependents drop table mood",
			"category": "docs",
			"url": "../docs/sql/data_types/enum",
			"blurb": "The ENUM type represents a dictionary data structure with all possible unique values of a column."
		},
		{
			"title": "Timestamp Type",
			"text": "name aliases description --- --- --- timestamp datetime time of day no time zone a timestamp specifies a combination of date year month day and a time hour minute second millisecond timestamps can be created using the timestamp keyword where the data must be formatted according to the iso 8601 format yyyy-mm-dd hh mm ss -- 11 30 am at 20 september 1992 select timestamp 1992-09-20 11 30 00 -- 2 30 pm at 20 september 1992 select timestamp 1992-09-20 14 30 00 functions see timestamp functions",
			"category": "docs",
			"url": "../docs/sql/data_types/timestamp",
			"blurb": "A timestamp specifies a combination of date (year, month, day) and a time (hour, minute, second, millisecond)."
		},
		{
			"title": "Boolean Type",
			"text": "name aliases description --- --- --- boolean bool logical boolean true false the boolean type represents a statement of truth true or false in sql the boolean field can also have a third state unknown which is represented by the sql null value -- select the three possible values of a boolean column select true false null boolean boolean values can be explicitly created using the literals true and false however they are most often created as a result of comparisons or conjunctions for example the comparison i 10 results in a boolean value boolean values can be used in the where and having clauses of a sql statement to filter out tuples from the result in this case tuples for which the predicate evaluates to true will pass the filter and tuples for which the predicate evaluates to false or null will be filtered out consider the following example -- create a table with the value 5 15 and null create table integers i integer insert into integers values 5 15 null -- select all entries where i 10 select from integers where i 10 -- in this case 5 and null are filtered out -- 5 10 false -- null 10 null -- the result is 15 expressions see logical operators and comparison operators",
			"category": "docs",
			"url": "../docs/sql/data_types/boolean",
			"blurb": "The BOOLEAN type represents a statement of truth (true or false)."
		},
		{
			"title": "Indexes",
			"text": "index types duckdb currently uses two index types a min-max index is automatically created for columns of all general-purpose data types an adaptive radix tree art is mainly used to ensure primary key constraints and to speed up point and very highly selective i e 0 1 queries such an index is automatically created for columns with a unique or primary key constraint and can be defined using create index joins on columns with an art index can make use of the index join algorithm forcing index joins is possible using pragmas persistence min-max indexes are persisted currently art indexes are not persisted unique and primary key indexes are rebuilt upon startup while user-defined indexes are discarded create index create index constructs an index on the specified column s of the specified table compound indexes on multiple columns expressions are supported currently unidimensional indexes are supported multidimensional indexes are not supported parameters name description --- --- unique causes the system to check for duplicate values in the table when the index is created if data already exist and each time data is added attempts to insert or update data that would result in duplicate entries will generate an error name the name of the index to be created table the name of the table to be indexed column the name of the column to be indexed expression an expression based on one or more columns of the table the expression usually must be written with surrounding parentheses as shown in the syntax however the parentheses can be omitted if the expression has the form of a function call examples -- create an unique index films_id_idx on the column id of table films create unique index films_id_idx on films id -- create index s_idx that allows for duplicate values on column revenue of table films create index revenue_idx on films revenue -- create compound index gy_idx on genre and year columns create index gy_idx on films genre year -- create index i_index on the expression of the sum of columns j and k from table integers create index i_index on integers j k drop index drop index drops an existing index from the database system parameters name description --- --- if exists do not throw an error if the index does not exist name the name of an index to remove examples -- remove the index title_idx drop index title_idx",
			"category": "docs",
			"url": "../docs/sql/indexes",
			"blurb": "Index types   DuckDB currently uses two index types:     A   min-max index  is automatically created for columns of all  ..."
		},
		{
			"title": "Samples",
			"text": "samples are used to randomly select a subset of a dataset examples -- select a sample of 5 rows from tbl using reservoir sampling select from tbl using sample 5 -- select a sample of 10 of the table using system sampling cluster sampling select from tbl using sample 10 -- select a sample of 10 of the table using bernoulli sampling select from tbl using sample 10 percent bernoulli -- select a sample of 50 rows of the table using reservoir sampling with a fixed seed 100 select from tbl using sample reservoir 50 rows repeatable 100 -- select a sample of 20 of the table using system sampling with a fixed seed 377 select from tbl using sample 10 system 377 -- select a sample of 10 of tbl before the join with tbl2 select from tbl tablesample reservoir 20 tbl2 where tbl i tbl2 i -- select a sample of 10 of tbl after the join with tbl2 select from tbl tbl2 where tbl i tbl2 i using sample reservoir 20 syntax samples allow you to randomly extract a subset of a dataset samples are useful for exploring a dataset faster as often you might not be interested in the exact answers to queries but only in rough indications of what the data looks like and what is in the data samples allow you to get approximate answers to queries faster as they reduce the amount of data that needs to pass through the query engine duckdb supports three different types of sampling methods reservoir bernoulli and system by default duckdb uses reservoir sampling when an exact number of rows is sampled and system sampling when a percentage is specified the sampling methods are described in detail below samples require a sample size which is an indication of how many elements will be sampled from the total population samples can either be given as a percentage 10 or as a fixed number of rows 10 rows all three sampling methods support sampling over a percentage but only reservoir sampling supports sampling a fixed number of rows samples are probablistic that is to say samples can be different between runs unless the seed is specifically specified specifying the seed only guarantees that the sample is the same if multi-threading is not enabled i e pragma threads 1 in the case of multiple threads running over a sample samples are not necessarily consistent even with a fixed seed reservoir reservoir sampling is a stream sampling technique that selects a random sample by keeping a reservoir of size equal to the sample size and randomly replacing elements as more elements come in reservoir sampling allows us to specify exactly how many elements we want in the resulting sample by selecting the size of the reservoir as a result reservoir sampling always outputs the same amount of elements unlike system and bernoulli sampling reservoir sampling is only recommended for small sample sizes and is not recommended for use with percentages that is because reservoir sampling needs to materialize the entire sample and randomly replace tuples within the materialized sample the larger the sample size the higher the performance hit incurred by this process reservoir sampling also incurs an additional performance penalty when multi-processing is used since the reservoir is to be shared amongst the different threads to ensure unbiased sampling this is not a big problem when the reservoir is very small but becomes costly when the sample is large bernoulli bernoulli sampling can only be used when a sampling percentage is specified it is rather straightforward every tuple in the underlying table is included with a chance equal to the specified percentage as a result bernoulli sampling can return a different number of tuples even if the same percentage is specified the amount of rows will generally be more or less equal to the specified percentage of the table but there will be some variance because bernoulli sampling is completely independent there is no shared state there is no penalty for using bernoulli sampling together with multiple threads system system sampling is a variant of bernoulli sampling with one crucial difference every vector is included with a chance equal to the sampling percentage this is a form of cluster sampling system sampling is more efficient than bernoulli sampling as no per-tuple selections have to be performed there is almost no extra overhead for using system sampling whereas bernoulli sampling can add additional cost as it has to perform random number generation for every single tuple system sampling is not suitable for smaller data sets as the granularity of the sampling is on the order of 1000 tuples that means that if system sampling is used for small data sets e g 100 rows either all the data will be filtered out or all the data will be included table samples the tablesample and using sample clauses are identical in terms of syntax and effect with one important difference tablesamples sample directly from the table for which they are specified whereas the sample clause samples after the entire from clause has been resolved this is relevant when there are joins present in the query plan the tablesample clause is essentially equivalent to creating a subquery with the using sample clause i e the following two queries are identical -- sample 20 of tbl before the join select from tbl tablesample reservoir 20 tbl2 where tbl i tbl2 i -- sample 20 of tbl before the join select from select from tbl using sample reservoir 20 tbl tbl2 where tbl i tbl2 i -- sample 20 after the join i e sample 20 of the join result select from tbl tbl2 where tbl i tbl2 i using sample reservoir 20",
			"category": "docs",
			"url": "../docs/sql/samples",
			"blurb": "Samples are used to randomly select a subset of a dataset.   Examples  -- select a sample of 5 rows from tbl using..."
		},
		{
			"title": "SQL Introduction",
			"text": "here we provide an overview of how to perform simple operations in sql this tutorial is only intended to give you an introduction and is in no way a complete tutorial on sql this tutorial is adapted from the postgresql tutorial in the examples that follow we assume that you have installed the duckdb command line interface cli shell see here for information on how to install the cli if you build from the source tree you can launch the cli from the build directory build release duckdb launching the shell should give you the following prompt duckdb 5fb6fe57ab enter help for usage hints connected to a transient in-memory database use open filename to reopen on a persistent database d by launching the database like this an in-memory database is launched that means that no data is persisted on disk to persist data on disk you should also pass a database path to the shell the database will then be stored at that path and can be reloaded from disk later concepts duckdb is a relational database management system rdbms that means it is a system for managing data stored in relations a relation is essentially a mathematical term for a table each table is a named collection of rows each row of a given table has the same set of named columns and each column is of a specific data type tables themselves are stored inside schemas and a collection of schemas constitutes the entire database that you can access creating a new table you can create a new table by specifying the table name along with all column names and their types create table weather city varchar temp_lo integer -- minimum temperature on a day temp_hi integer -- maximum temperature on a day prcp real date date you can enter this into the shell with the line breaks the command is not terminated until the semicolon white space i e spaces tabs and newlines can be used freely in sql commands that means you can type the command aligned differently than above or even all on one line two dash characters -- introduce comments whatever follows them is ignored up to the end of the line sql is case insensitive about key words and identifiers except when identifiers are double-quoted to preserve the case not done above in the sql command we first specify the type of command that we want to perform create table after that follows the parameters for the command first the table name weather is given then the column names and column types follow city varchar specifies that the table has a column called city that is of type varchar varchar specifies a data type that can store text of arbitrary length the temperature fields are stored in an integer type a type that stores integer numbers i e whole numbers without a decimal point real columns store single precision floating-point numbers i e numbers with a decimal point date stores a date i e year month day combination date only stores the specific day not a time associated with that day duckdb supports the standard sql types integer smallint real double decimal char n varchar n date time and timestamp the second example will store cities and their associated geographical location create table cities name varchar lat decimal lon decimal finally it should be mentioned that if you don t need a table any longer or want to recreate it differently you can remove it using the following command drop table tablename populating a table with rows the insert statement is used to populate a table with rows insert into weather values san francisco 46 50 0 25 1994-11-27 constants that are not numeric values e g text and dates must be surrounded by single quotes as in the example input dates for the date type must be formatted as yyyy-mm-dd we can insert into the cities table in the same manner insert into cities values san francisco -194 0 53 0 the syntax used so far requires you to remember the order of the columns an alternative syntax allows you to list the columns explicitly insert into weather city temp_lo temp_hi prcp date values san francisco 43 57 0 0 1994-11-29 you can list the columns in a different order if you wish or even omit some columns e g if the prcp is unknown insert into weather date city temp_hi temp_lo values 1994-11-29 hayward 54 37 many developers consider explicitly listing the columns better style than relying on the order implicitly please enter all the commands shown above so you have some data to work with in the following sections you could also have used copy to load large amounts of data from csv files this is usually faster because the copy command is optimized for this application while allowing less flexibility than insert an example would be copy weather from home user weather csv where the file name for the source file must be available on the machine running the process there are many other ways of loading data into duckdb see the corresponding documentation section for more information querying a table to retrieve data from a table the table is queried a sql select statement is used to do this the statement is divided into a select list the part that lists the columns to be returned a table list the part that lists the tables from which to retrieve the data and an optional qualification the part that specifies any restrictions for example to retrieve all the rows of table weather type select from weather here is a shorthand for all columns so the same result would be had with select city temp_lo temp_hi prcp date from weather the output should be city temp_lo temp_hi prcp date --------------- --------- ---------- ------ ------------ san francisco 46 50 0 25 1994-11-27 san francisco 43 57 0 0 1994-11-29 hayward 37 54 1994-11-29 3 rows you can write expressions not just simple column references in the select list for example you can do select city temp_hi temp_lo 2 as temp_avg date from weather this should give city temp_avg date --------------- ---------- ------------ san francisco 48 1994-11-27 san francisco 50 1994-11-29 hayward 45 1994-11-29 3 rows notice how the as clause is used to relabel the output column the as clause is optional a query can be qualified by adding a where clause that specifies which rows are wanted the where clause contains a boolean truth value expression and only rows for which the boolean expression is true are returned the usual boolean operators and or and not are allowed in the qualification for example the following retrieves the weather of san francisco on rainy days select from weather where city san francisco and prcp 0 0 result city temp_lo temp_hi prcp date --------------- --------- --------- ------ ------------ san francisco 46 50 0 25 1994-11-27 you can request that the results of a query be returned in sorted order select from weather order by city city temp_lo temp_hi prcp date --------------- --------- --------- ------ ------------ hayward 37 54 1994-11-29 san francisco 43 57 0 0 1994-11-29 san francisco 46 50 0 25 1994-11-27 in this example the sort order isn t fully specified and so you might get the san francisco rows in either order but you d always get the results shown above if you do select from weather order by city temp_lo you can request that duplicate rows be removed from the result of a query select distinct city from weather city --------------- hayward san francisco 2 rows here again the result row ordering might vary you can ensure consistent results by using distinct and order by together select distinct city from weather order by city joins between tables thus far our queries have only accessed one table at a time queries can access multiple tables at once or access the same table in such a way that multiple rows of the table are being processed at the same time a query that accesses multiple rows of the same or different tables at one time is called a join query as an example say you wish to list all the weather records together with the location of the associated city to do that we need to compare the city column of each row of the weather table with the name column of all rows in the cities table and select the pairs of rows where these values match this would be accomplished by the following query select from weather cities where city name city temp_lo temp_hi prcp date name lon lat --------------- --------- --------- ------ ------------ --------------- ----- ---- san francisco 46 50 0 25 1994-11-27 san francisco -194 53 san francisco 43 57 0 1994-11-29 san francisco -194 53 2 rows observe two things about the result set there is no result row for the city of hayward this is because there is no matching entry in the cities table for hayward so the join ignores the unmatched rows in the weather table we will see shortly how this can be fixed there are two columns containing the city name this is correct because the lists of columns from the weather and cities tables are concatenated in practice this is undesirable though so you will probably want to list the output columns explicitly rather than using select city temp_lo temp_hi prcp date lon lat from weather cities where city name since the columns all had different names the parser automatically found which table they belong to if there were duplicate column names in the two tables you d need to qualify the column names to show which one you meant as in select weather city weather temp_lo weather temp_hi weather prcp weather date cities lon cities lat from weather cities where cities name weather city it is widely considered good style to qualify all column names in a join query so that the query won t fail if a duplicate column name is later added to one of the tables join queries of the kind seen thus far can also be written in this alternative form select from weather inner join cities on weather city cities name this syntax is not as commonly used as the one above but we show it here to help you understand the following topics now we will figure out how we can get the hayward records back in what we want the query to do is to scan the weather table and for each row to find the matching cities row s if no matching row is found we want some empty values to be substituted for the cities table s columns this kind of query is called an outer join the joins we have seen so far are inner joins the command looks like this select from weather left outer join cities on weather city cities name city temp_lo temp_hi prcp date name lon lat --------------- --------- --------- ------ ------------ --------------- ----- ---- san francisco 46 50 0 25 1994-11-27 san francisco -194 53 san francisco 43 57 0 1994-11-29 san francisco -194 53 hayward 37 54 1994-11-29 3 rows this query is called a left outer join because the table mentioned on the left of the join operator will have each of its rows in the output at least once whereas the table on the right will only have those rows output that match some row of the left table when outputting a left-table row for which there is no right-table match empty null values are substituted for the right-table columns aggregate functions like most other relational database products duckdb supports aggregate functions an aggregate function computes a single result from multiple input rows for example there are aggregates to compute the count sum avg average max maximum and min minimum over a set of rows as an example we can find the highest low-temperature reading anywhere with select max temp_lo from weather max ----- 46 1 row if we wanted to know what city or cities that reading occurred in we might try select city from weather where temp_lo max temp_lo -- wrong but this will not work since the aggregate max cannot be used in the where clause this restriction exists because the where clause determines which rows will be included in the aggregate calculation so obviously it has to be evaluated before aggregate functions are computed however as is often the case the query can be restated to accomplish the desired result here by using a subquery select city from weather where temp_lo select max temp_lo from weather city --------------- san francisco 1 row this is ok because the subquery is an independent computation that computes its own aggregate separately from what is happening in the outer query aggregates are also very useful in combination with group by clauses for example we can get the maximum low temperature observed in each city with select city max temp_lo from weather group by city city max --------------- ----- hayward 37 san francisco 46 2 rows which gives us one output row per city each aggregate result is computed over the table rows matching that city we can filter these grouped rows using having select city max temp_lo from weather group by city having max temp_lo 40 city max --------- ----- hayward 37 1 row which gives us the same results for only the cities that have all temp_lo values below 40 finally if we only care about cities whose names begin with s we can use the like operator select city max temp_lo from weather where city like s -- 1 group by city having max temp_lo 40 more information about the like operator can be found here it is important to understand the interaction between aggregates and sql s where and having clauses the fundamental difference between where and having is this where selects input rows before groups and aggregates are computed thus it controls which rows go into the aggregate computation whereas having selects group rows after groups and aggregates are computed thus the where clause must not contain aggregate functions it makes no sense to try to use an aggregate to determine which rows will be inputs to the aggregates on the other hand the having clause always contains aggregate functions in the previous example we can apply the city name restriction in where since it needs no aggregate this is more efficient than adding the restriction to having because we avoid doing the grouping and aggregate calculations for all rows that fail the where check updates you can update existing rows using the update command suppose you discover the temperature readings are all off by 2 degrees after november 28 you can correct the data as follows update weather set temp_hi temp_hi - 2 temp_lo temp_lo - 2 where date 1994-11-28 look at the new state of the data select from weather city temp_lo temp_hi prcp date --------------- --------- --------- ------ ------------ san francisco 46 50 0 25 1994-11-27 san francisco 41 55 0 1994-11-29 hayward 35 52 1994-11-29 3 rows deletions rows can be removed from a table using the delete command suppose you are no longer interested in the weather of hayward then you can do the following to delete those rows from the table delete from weather where city hayward all weather records belonging to hayward are removed select from weather city temp_lo temp_hi prcp date --------------- --------- --------- ------ ------------ san francisco 46 50 0 25 1994-11-27 san francisco 41 55 0 1994-11-29 2 rows one should be wary of statements of the form delete from tablename without a qualification delete will remove all rows from the given table leaving it empty the system will not request confirmation before doing this",
			"category": "docs",
			"url": "../docs/sql/introduction",
			"blurb": "Here we provide an overview of how to perform simple operations in SQL. This tutorial is only intended to give you an..."
		},
		{
			"title": "Catalog Functions",
			"text": "catalog functions are table functions that return tables describing the catalog entries of the database these tables can be filtered to obtain information about a specific column or table database catalog and schema the top level catalog table function is information_schema_schemata it lists the catalogs and the schemas present in the database and has the following layout column description type example --- --- --- --- catalog_name name of the database that the schema is contained in not yet implemented varchar null schema_name name of the schema varchar main schema_owner name of the owner of the schema not yet implemented varchar null default_character_set_catalog applies to a feature not available in duckdb varchar null default_character_set_schema applies to a feature not available in duckdb varchar null default_character_set_name applies to a feature not available in duckdb varchar null sql_path the file system location of the database currently unimplemented varchar null tables and views the table function that describes the catalog information for tables and views is information_schema_tables it lists the tables present in the database and has the following layout column description type example --- --- --- --- table_catalog the catalog the table or view belongs to not yet implemented varchar null table_schema the schema the table or view belongs to varchar main table_name the name of the table or view varchar widgets table_type the type of table one of base table local temporary view varchar base table self_referencing_column_name applies to a feature not available in duckdb varchar null reference_generation applies to a feature not available in duckdb varchar null user_defined_type_catalog if the table is a typed table the name of the database that contains the underlying data type always the current database else null currently unimplemented varchar null user_defined_type_schema if the table is a typed table the name of the schema that contains the underlying data type else null currently unimplemented varchar null user_defined_type_name if the table is a typed table the name of the underlying data type else null currently unimplemented varchar null is_insertable_into yes if the table is insertable into no if not base tables are always insertable into views not necessarily varchar yes is_typed yes if the table is a typed table no if not varchar no commit_action not yet implemented varchar no columns the table function that describes the catalog information for columns is information_schema_columns it lists the column present in the database and has the following layout column description type example --- --- --- --- table_catalog name of the database containing the table not yet implemented varchar null table_schema name of the schema containing the table varchar main table_name name of the table varchar widgets column_name name of the column varchar price ordinal_position ordinal position of the column within the table count starts at 1 integer 5 column_default default expression of the column varchar 1 99 is_nullable yes if the column is possibly nullable no if it is known not nullable varchar yes data_type data type of the column varchar decimal 18 2 character_maximum_length if data_type identifies a character or bit string type the declared maximum length null for all other data types or if no maximum length was declared integer 255 character_octet_length if data_type identifies a character type the maximum possible length in octets bytes of a datum null for all other data types the maximum octet length depends on the declared character maximum length see above and the character encoding integer 1073741824 numeric_precision if data_type identifies a numeric type this column contains the declared or implicit precision of the type for this column the precision indicates the number of significant digits for all other data types this column is null integer 18 numeric_scale if data_type identifies a numeric type this column contains the declared or implicit scale of the type for this column the precision indicates the number of significant digits for all other data types this column is null integer 2 datetime_precision if data_type identifies a date time timestamp or interval type this column contains the declared or implicit fractional seconds precision of the type for this column that is the number of decimal digits maintained following the decimal point in the seconds value no fractional seconds are currently supported in duckdb for all other data types this column is null integer 0",
			"category": "docs",
			"url": "../docs/sql/catalog_functions",
			"blurb": "Catalog functions are table functions that return tables describing the catalog entries of the database. These tables can..."
		},
		{
			"title": "Full Text Search",
			"text": "full text search is an extension to duckdb that allows for search through strings similar to sqlite s fts5 extension api the extension adds two pragma statements to duckdb one to create and one to drop an index additionally a scalar macro stem is added which is used internally by the extension pragma create_fts_index create_fts_index input_table input_id input_values stemmer porter stopwords english ignore a-z strip_accents true lower true overwrite false pragma that creates a fts index for the specified table name type description -- -- -- input_table varchar qualified name of specified table e g table_name or main table_name input_id varchar column name of document identifier e g document_identifier input_values varchar column names of the text fields to be indexed vararg e g text_field_1 text_field_2 text_field_n or for all columns in input_table of type varchar stemmer varchar the type of stemmer to be used one of arabic basque catalan danish dutch english finnish french german greek hindi hungarian indonesian irish italian lithuanian nepali norwegian porter portuguese romanian russian serbian spanish swedish tamil turkish or none if no stemming is to be used defaults to porter stopwords varchar qualified name of table containing a single varchar column containing the desired stopwords or none if no stopwords are to be used defaults to english for a pre-defined list of 571 english stopwords ignore varchar regular expression of patterns to be ignored defaults to a-z ignoring all escaped and non-alphabetic lowercase characters strip_accents boolean whether to remove accents e g convert á to a defaults to true lower boolean whether to convert all text to lowercase defaults to true overwrite boolean whether to overwrite an existing index on a table defaults to false this pragma builds the index under a newly created schema the schema will be named after the input table if an index is created on table main table_name then the schema will be named fts_main_table_name pragma drop_fts_index drop_fts_index input_table drops a fts index for the specified table name type description -- -- -- input_table varchar qualified name of input table e g table_name or main table_name match_bm25 match_bm25 input_id query_string fields null k 1 2 b 0 75 conjunctive 0 when an index is built this retrieval macro is created that can be used to search the index name type description -- -- -- input_id varchar column name of document identifier e g document_identifier query_string varchar the string to search the index for fields varchar comma-separarated list of fields to search in e g text_field_2 text_field_n defaults to null to search all indexed fields k double parameter k sub 1 sub in the okapi bm25 retrieval model defaults to 1 2 b double parameter b in the okapi bm25 retrieval model defaults to 0 75 conjunctive boolean whether to make the query conjunctive i e all terms in the query string must be present in order for a document to be retrieved stem stem input_string stemmer reduces words to their base used internally by the extension name type description -- -- -- input_string varchar the column or constant to be stemmed stemmer varchar the type of stemmer to be used one of arabic basque catalan danish dutch english finnish french german greek hindi hungarian indonesian irish italian lithuanian nepali norwegian porter portuguese romanian russian serbian spanish swedish tamil turkish or none if no stemming is to be used example usage -- create a table and fill it with text data create table documents document_identifier varchar text_content varchar author varchar doc_version integer insert into documents values doc1 the mallard is a dabbling duck that breeds throughout the temperate hannes mühleisen 3 doc2 the cat is a domestic species of small carnivorous mammal laurens kuiper 2 -- build the index make both the text_content and author columns searchable pragma create_fts_index documents document_identifier text_content author -- search the author field index for documents that are written by hannes - this retrieves doc1 select text_content score from select fts_main_documents match_bm25 document_identifier muhleisen fields author as score from documents sq where score is not null and doc_version 2 order by score desc -- search for documents about small cats - this retrieves doc2 select text_content score from select fts_main_documents match_bm25 document_identifier small cats as score from documents sq where score is not null order by score desc",
			"category": "docs",
			"url": "../docs/sql/full_text_search",
			"blurb": "Full Text Search is an extension to DuckDB that allows for search through strings, similar to SQLite's FTS5 extension.  ..."
		},
		{
			"title": "Insert Statement",
			"text": "the insert statement inserts new data into a table examples -- insert the values 1 2 3 into tbl insert into tbl values 1 2 3 -- insert the result of a query into a table insert into tbl select from other_tbl -- insert values into the i column inserting the default value into other columns insert into tbl i values 1 2 3 -- explicitly insert the default value into a column insert into tbl i values 1 default 3 syntax insert into inserts new rows into a table one can insert one or more rows specified by value expressions or zero or more rows resulting from a query the target column names can be listed in any order if no list of column names is given at all the default is all the columns of the table in their declared order the values supplied by the values clause or query are associated with the column list left-to-right each column not present in the explicit or implicit column list will be filled with a default value either its declared default value or null if there is none if the expression for any column is not of the correct data type automatic type conversion will be attempted",
			"category": "docs",
			"url": "../docs/sql/statements/insert",
			"blurb": "The  INSERT  statement inserts new data into a table.   Examples  -- insert the values (1), (2), (3) into tbl INSERT INTO..."
		},
		{
			"title": "Create Table",
			"text": "the create table statement creates a table in the catalog examples -- create a table with two integer columns i and j create table t1 i integer j integer -- create a table with a primary key create table t1 id integer primary key j varchar -- create a table with a composte primary key create table t1 id integer j varchar primary key id j -- create a table with various different types and constraints create table t1 i integer not null decimalnr double check decimalnr 10 date date unique time timestamp -- create a table from the result of a query create table t1 as select 42 as i 84 as j -- create a table from a csv file using auto-detect i e automatically detecting column names and types create table t1 as select from read_csv_auto path file csv syntax",
			"category": "docs",
			"url": "../docs/sql/statements/create_table",
			"blurb": "The  CREATE TABLE  statement creates a table in the catalog.   Examples  -- create a table with two integer columns (i..."
		},
		{
			"title": "Alter Table",
			"text": "the alter table statement changes the schema of an existing table in the catalog examples -- add a new column with name k to the table integers it will be filled with the default value null alter table integers add column k integer -- add a new column with name l to the table integers it will be filled with the default value 10 alter table integers add column l integer default 10 -- drop the column k from the table integers alter table integers drop k -- change the type of the column i to the type varchar using a standard cast alter table integers alter i type varchar -- change the type of the column i to the type varchar using the specified expression to convert the data for each row alter table integers alter i set data type varchar using concat i _ j -- set the default value of a column alter table integers alter column i set default 10 -- drop the default value of a column alter table integers alter column i drop default -- rename a table alter table integers rename to integers_old -- rename a column of a table alter table integers rename i to j syntax alter table changes the schema of an existing table all the changes made by alter table fully respect the transactional semantics - that is - they will not be visible to other transactions until committed and can be fully reverted through a rollback rename table -- rename a table alter table integers rename to integers_old the rename to clause renames an entire table changing its name in the schema note that any views that rely on the table are not automatically updated rename column -- rename a column of a table alter table integers rename i to j alter table integers rename column j to k the rename column clause renames a single column within a table any constraints that rely on this name e g check constraints are automatically updated however note that any views that rely on this column name are not automatically updated add column -- add a new column with name k to the table integers it will be filled with the default value null alter table integers add column k integer -- add a new column with name l to the table integers it will be filled with the default value 10 alter table integers add column l integer default 10 the add column clause can be used to add a new column of a specified type to a table the new column will be filled with the specified default value or null if none is specified drop column -- drop the column k from the table integers alter table integers drop k the drop column clause can be used to remove a column from a table note that columns can only be removed if they do not have any indexes that rely on them this includes any indexes created as part of a primary key or unique constraint columns that are part of multi-column check constraints cannot be dropped either alter type -- change the type of the column i to the type varchar using a standard cast alter table integers alter i type varchar -- change the type of the column i to the type varchar using the specified expression to convert the data for each row alter table integers alter i set data type varchar using concat i _ j the set data type clause changes the type of a column in a table any data present in the column is converted according to the provided expression in the using clause or if the using clause is absent cast to the new data type note that columns can only have their type changed if they do not have any indexes that rely on them and are not part of any check constraints set drop default -- set the default value of a column alter table integers alter column i set default 10 -- drop the default value of a column alter table integers alter column i drop default the set drop default clause modifies the default value of an existing column note that this does not modify any existing data in the column dropping the default is equivalent to setting the default value to null",
			"category": "docs",
			"url": "../docs/sql/statements/alter_table",
			"blurb": "The ALTER TABLE statement changes the schema of an existing table in the catalog."
		},
		{
			"title": "Statements Overview",
			"text": "more",
			"category": "docs",
			"url": "../docs/sql/statements/overview",
			"blurb": ""
		},
		{
			"title": "Create Macro",
			"text": "the create macro statement creates a scalar macro in the catalog examples -- create a macro that adds two expressions a and b create macro add a b as a b -- create a macro for a case expression create macro ifelse a b c as case when a then b else c end -- create a macro that does a subquery create macro one as select 1 -- create a macro with a common table expression -- parameter names get priority over column names disambiguate using the table name create macro plus_one a as with cte as select 1 as a select cte a a from cte -- macro s are schema-dependent and have an alias function create function main myavg x as sum x count x -- create a macro with default constant parameters create macro add_default a b 5 as a b -- create a macro arr_append with a functionality equivalent to array_append create macro arr_append l e as list_concat l list_value e syntax macros allow you to create shortcuts for combinations of expressions -- failure cannot find column b create macro add a as a b -- this works create macro add a b as a b -- error cannot bind varchar integer select add hello 3 -- success select add 1 2 -- 3 macro s can have default parameters -- b is a default parameter create macro add_default a b 5 as a b -- the following will result in 42 select add_default 37 -- error add_default only has one positional parameter select add_default 40 2 -- success default parameters are used by assigning them like so select add_default 40 b 2 -- error default parameters must come after positional parameters select add_default b 2 40 -- the order of default parameters does not matter create macro triple_add a b 5 c 10 as a b c -- success select triple_add 40 c 1 b 1 -- 42 when macro s are used they are expanded i e replaced with the original expression and the parameters within the expanded expression are replaced with the supplied arguments step by step -- the add macro we defined above is used in a query select add 40 2 -- internally add is replaced with its definition of a b select a b -- then the parameters are replaced by the supplied arguments select 40 2 -- 42",
			"category": "docs",
			"url": "../docs/sql/statements/create_macro",
			"blurb": "The  CREATE MACRO  statement creates a scalar macro in the catalog.   Examples  -- create a macro that adds two..."
		},
		{
			"title": "Create View",
			"text": "the create view statement defines a new view in the catalog examples -- create a simple view create view v1 as select from tbl -- create a view or replace it if a view with that name already exists create or replace view v1 as select 42 -- create a view and replace the column names create view v1 a as select 42 syntax create view defines a view of a query the view is not physically materialized instead the query is run every time the view is referenced in a query create or replace view is similar but if a view of the same name already exists it is replaced if a schema name is given then the view is created in the specified schema otherwise it is created in the current schema temporary views exist in a special schema so a schema name cannot be given when creating a temporary view the name of the view must be distinct from the name of any other view or table in the same schema",
			"category": "docs",
			"url": "../docs/sql/statements/create_view",
			"blurb": "The  CREATE VIEW  statement defines a new view in the catalog.   Examples  -- create a simple view CREATE VIEW v1 AS..."
		},
		{
			"title": "Drop Statement",
			"text": "the drop statement removes a catalog entry added previously with the create command examples -- delete the table with the name tbl drop table tbl -- drop the view with the name v1 do not throw an error if the view does not exist drop view if exists v1 syntax the optional if exists clause suppresses the error that would normally result if the table does not exist by default or if the restrict clause is provided the entry will not be dropped if there are any other objects that depend on it if the cascade clause is provided then all the objects that are dependent on the object will be dropped as well create schema myschema create table myschema t1 i integer -- error cannot drop myschema because the table myschema t1 depends on it drop schema myschema -- cascade drops both myschema and myschema 1 drop schema myschema cascade",
			"category": "docs",
			"url": "../docs/sql/statements/drop",
			"blurb": "The  DROP  statement removes a catalog entry added previously with the  CREATE  command.   Examples  -- delete the table..."
		},
		{
			"title": "Select Statement",
			"text": "the select statement retrieves rows from the database examples -- select all columns from the table tbl select from tbl -- select the rows from tbl select j from tbl where i 3 -- perform an aggregate grouped by the column i select i sum j from tbl group by i -- select only the top 3 rows from the tbl select from tbl order by i desc limit 3 -- join two tables together using the using clause select from t1 join t2 using a b syntax the select statement retrieves rows from the database the canonical order of a select statement is as follows select select_list from tables where condition group by groups having group_filter order by order_expr limit n optionally the select statement can be prefixed with a with clause as the select statement is so complex we have split up the syntax diagrams into several parts the full syntax diagram can be found at the bottom of the page select clause the select clause specifies the list of columns that will be returned by the query while it appears first in the clause logically the expressions here are executed only at the end the select clause can contain arbitrary expressions that transform the output as well as aggregates and window functions from clause the from clause specifies the source of the data on which the remainder of the query should operate logically the from clause is where the query starts execution the from clause can contain a single table a combination of multiple tables that are joined together or another select query inside a subquery node where clause the where clause specifies any filters to apply to the data this allows you to select only a subset of the data in which you are interested logically the where clause is applied immediately after the from clause group by having clause the group by clause specifies which grouping columns should be used to perform any aggregations in the select clause if the group by clause is specified the query is always an aggregate query even if no aggregations are present in the select clause window clause the window clause allows you to specify named windows that can be used within window functions these are useful when you have multiple window functions as they allow you to avoid repeating the same window clause sample clause the sample clause allows you to run the query on a sample from the base table this can significantly speed up processing of queries at the expense of accuracy in the result samples can also be used to quickly see a snapshot of the data when exploring a data set the sample clause is applied right after anything in the from clause i e after any joins but before the where clause or any aggregates see the sample page for more information order by limit clause order by and limit are output modifiers logically they are applied at the very end of the query the limit clause restricts the amount of rows fetched and the order by clause sorts the rows on the sorting criteria in either ascending or descending order values list a values list is a set of values that is supplied instead of a select statement row ids for each table the rowid pseudocolumn returns the row identifiers based on the physical storage d create table id int content string d insert into t values 42 hello 43 world d select rowid id content from t rowid id content 0 42 hello 1 43 world in the current storage these identifiers are contiguous unsigned integers 0 1 if no rows were deleted deletions introduce gaps in the rowids which may be reclaimed later therefore it is strongly recommended not to use rowids as identifiers the rowid values are stable within a transaction if there is a user-defined column named rowid it shadows the rowid pseudocolumn common table expressions full syntax diagram below is the full syntax diagram of the select statement",
			"category": "docs",
			"url": "../docs/sql/statements/select",
			"blurb": "The SELECT statement retrieves rows from the database."
		},
		{
			"title": "Create Schema",
			"text": "the create schema statement creates a schema in the catalog the default schema is main examples -- create a schema create schema s1 -- create a schema if it does not exist yet create schema if not exists s2 -- create table in the schemas create table s1 t id integer primary key other_id integer create table s2 t id integer primary key j varchar -- compute a join between tables from two schemas select from s1 t s1t s2 t s2t where s1t other_id s2t id syntax",
			"category": "docs",
			"url": "../docs/sql/statements/create_schema",
			"blurb": "The  CREATE SCHEMA  statement creates a schema in the catalog. The default schema is  main .   Examples  -- create a..."
		},
		{
			"title": "Delete Statement",
			"text": "the delete statement removes rows from the table identified by the table-name examples -- remove the rows matching the condition i 2 from the database delete from tbl where i 2 -- delete all rows in the table tbl delete from tbl syntax the delete statement removes rows from the table identified by the table-name if the where clause is not present all records in the table are deleted if a where clause is supplied then only those rows for which the where clause results in true are deleted rows for which the expression is false or null are retained the using clause allows deleting based on the content of other tables or subqueries",
			"category": "docs",
			"url": "../docs/sql/statements/delete",
			"blurb": "The  DELETE  statement removes rows from the table identified by the table-name.   Examples  -- remove the rows matching..."
		},
		{
			"title": "Update Statement",
			"text": "the update statement modifies the values of rows in a table examples -- for every row where i is null set the value to 0 instead update tbl set i 0 where i is null -- set all values of i to 1 and all values of j to 2 update tbl set i 1 j 2 syntax update changes the values of the specified columns in all rows that satisfy the condition only the columns to be modified need be mentioned in the set clause columns not explicitly modified retain their previous values",
			"category": "docs",
			"url": "../docs/sql/statements/update",
			"blurb": "The  UPDATE  statement modifies the values of rows in a table.   Examples  -- for every row where i is NULL, set the..."
		},
		{
			"title": "Create Sequence",
			"text": "the create sequence statement creates a new sequence number generator examples create an ascending sequence called serial starting at 101 create sequence serial start 101 select the next number from this sequence select nextval serial nextval --------- 101 use this sequence in an insert command insert into distributors values nextval serial nothing syntax create sequence creates a new sequence number generator if a schema name is given then the sequence is created in the specified schema otherwise it is created in the current schema temporary sequences exist in a special schema so a schema name may not be given when creating a temporary sequence the sequence name must be distinct from the name of any other sequence in the same schema after a sequence is created you use the function nextval to operate on the sequence parameters name description --- --- temporary or temp if specified the sequence object is created only for this session and is automatically dropped on session exit existing permanent sequences with the same name are not visible in this session while the temporary sequence exists unless they are referenced with schema-qualified names name the name optionally schema-qualified of the sequence to be created increment the optional clause increment by increment specifies which value is added to the current sequence value to create a new value a positive value will make an ascending sequence a negative one a descending sequence the default value is 1 minvalue the optional clause minvalue minvalue determines the minimum value a sequence can generate if this clause is not supplied or no minvalue is specified then defaults will be used the defaults are 1 and - 2 63 - 1 for ascending and descending sequences respectively maxvalue the optional clause maxvalue maxvalue determines the maximum value for the sequence if this clause is not supplied or no maxvalue is specified then default values will be used the defaults are 2 63 - 1 and -1 for ascending and descending sequences respectively start the optional clause start with start allows the sequence to begin anywhere the default starting value is minvalue for ascending sequences and maxvalue for descending ones cycle or no cycle the cycle option allows the sequence to wrap around when the maxvalue or minvalue has been reached by an ascending or descending sequence respectively if the limit is reached the next number generated will be the minvalue or maxvalue respectively if no cycle is specified any calls to nextval after the sequence has reached its maximum value will return an error if neither cycle or no cycle are specified no cycle is the default use drop sequence to remove a sequence sequences are based on bigint arithmetic so the range cannot exceed the range of an eight-byte integer -9223372036854775808 to 9223372036854775807",
			"category": "docs",
			"url": "../docs/sql/statements/create_sequence",
			"blurb": "The  CREATE SEQUENCE  statement creates a new sequence number generator.   Examples  Create an ascending sequence called..."
		},
		{
			"title": "Copy",
			"text": "copy moves data between duckdb tables and external comma separated value csv files csv import copy from imports data into duckdb from an external csv file into an existing table the data is appended to whatever data is in the table already the amount of columns inside the file must match the amount of columns in the table table_name and the contents of the columns must be convertible to the column types of the table in case this is not possible an error will be thrown if a list of columns is specified copy will only copy the data in the specified columns from the file if there are any columns in the table that are not in the column list copy from will insert the default values for those columns -- copy the contents of a comma-separated file test csv without a header into the table test copy test from test csv -- copy the contents of a comma-separated file with a header into the category table copy category from categories csv header -- copy the contents of lineitem tbl into the lineitem table where the contents are delimited by a pipe character copy lineitem from lineitem tbl delimiter -- copy the contents of lineitem tbl into the lineitem table where the delimiter quote character and presence of a header are automatically detected copy lineitem from lineitem tbl auto_detect true -- read the contents of a comma-separated file names csv into the name column of the category table any other columns of this table are filled with their default value copy category name from names csv syntax csv export copy to exports data from duckdb to an external csv file it has mostly the same set of options as copy from however in the case of copy to the options specify how the csv file should be written to disk any csv file created by copy to can be copied back into the database by using copy from with the same set of options the copy to function can be called specifying either a table name or a query when a table name is specified the contents of the entire table will be written into the resulting csv file when a query is specified the query is executed and the result of the query is written to the resulting file -- copy the contents of the lineitem table to the file lineitem tbl where the columns are delimited by a pipe character including a header line copy lineitem to lineitem tbl delimiter header -- copy the l_orderkey column of the lineitem table to the file orderkey tbl copy lineitem l_orderkey to orderkey tbl delimiter -- copy the result of a query to the file query csv including a header with column names copy select 42 as a hello as b to query csv with header 1 delimiter syntax parameters name description --- --- table_name the name optionally schema-qualified of an existing table column_name an optional list of columns to be copied if no column list is specified all columns of the table will be copied filename the path and name of the input or output file boolean specifies whether the selected option should be turned on or off you can write true on or 1 to enable the option and false off or 0 to disable it the boolean value can also be omitted in which case true is assumed format specifies the copy function to use this defaults to csv but other options can be available e g parquet delimiter specifies the string that separates columns within each row line of the file the default value is a comma null specifies the string that represents a null value the default is an empty string please note that in copy from both an unquoted empty string and a quoted empty string represent a null value if any other null string is specified again both its quoted and its unquoted appearance represent a null value copy to does not quote null values on output even if force_quote is true header specifies that the file contains a header line with the names of each column in the file thus copy from ignores the first line when importing data whereas on output copy to the first line of the file contains the column names of the exported columns quote specifies the quoting string to be used when a data value is quoted the default is double-quote escape specifies the string that should appear before a data character sequence that matches the quote value the default is the same as the quote value so that the quoting string is doubled if it appears in the data dateformat specifies the date format to use when parsing dates see date format timestampformat specifies the date format to use when parsing timestamps see date format force_quote forces quoting to be used for all non-null values in each specified column null output is never quoted if is specified non-null values will be quoted in all columns this option is allowed only in copy to force_not_null do not match the specified columns values against the null string in the default case where the null string is empty this means that empty values will be read as zero-length strings rather than nulls this option is allowed only in copy from encoding if this option is used its value must be utf8 with any other encoding an error will be thrown auto_detect option for csv parsing if true the parser will attempt to detect the input format and data types automatically delim sep quote escape and header parameters become optional sample_size option to define number of sample rows for automatic csv type detection chunks of sample rows will be drawn from different locations of the input file set to -1 to scan the entire input file only the first max 1024 rows will be used for dialect detection all_varchar option to skip type detection for csv parsing and assume all columns to be of type varchar it is recommended that the file name used in copy always be specified as an absolute path the values in each record are separated by the delimiter string if the value contains the delimiter string the quote string the null string a carriage return or line feed character then the whole value is prefixed and suffixed by the quote string and any occurrence within the value of a quote string or the escape string is preceded by the escape string you can also use force_quote to force quotes when outputting non-null values in specific columns the csv format has no standard way to distinguish a null value from an empty string you can use force_not_null to prevent null input comparisons for specific columns in csv format all characters are significant a quoted value surrounded by white space or any characters other than delimiter will include those characters this can cause errors if you import data from a system that pads csv lines with white space out to some fixed width if such a situation arises you might need to preprocess the csv file to remove the trailing white space before importing the data into duckdb",
			"category": "docs",
			"url": "../docs/sql/statements/copy",
			"blurb": "COPY moves data between DuckDB tables and external files."
		},
		{
			"title": "Export & Import Database",
			"text": "the export database command allows you to export the contents of the database to a specific directory the import database command allows you to then read the contents again examples -- export the database to the target directory export database target_directory -- export the table contents with the given options export database target_directory format csv delimiter -- export the table contents as parquet export database target_directory format parquet --reload the database again import database target_directory syntax the export database command exports the full contents of the database - including schema information tables views and sequences - to a specific directory that can then be loaded again the created directory will be structured as follows target_directory schema sql target_directory load sql target_directory t_1 csv target_directory t_n csv the schema sql file contains the schema statements that are found in the database it contains any create schema create table create view and create sequence commands that are necessary to re-construct the database the load sql file contains a set of copy statements that can be used to read the data from the csv files again the file contains a single copy statement for every table found in the schema the database can be reloaded by using the import database command again or manually by running schema sql followed by load sql to re-load the data",
			"category": "docs",
			"url": "../docs/sql/statements/export",
			"blurb": "The  EXPORT DATABASE  command allows you to export the contents of the database to a specific directory. The  IMPORT..."
		},
		{
			"title": "Configuration",
			"text": "duckdb has a number of configuration options that can be used to change the behavior of the system the configuration options can be set using either the set statement or the pragma statement examples -- set the memory limit of the system to 10gb set memory_limit 10gb -- configure the system to use 1 thread set threads to 1 -- enable printing of a progress bar during long-running queries set enable_progress_bar true -- set the default null order to nulls last pragma default_null_order nulls_last -- show a list of all available settings select from duckdb_settings configuration reference below is a list of all available settings name description input_type default_value ------------------------------------------ ------------------------------------------------------------------------------------------------------------------------------------------------- ------------ --------------- access_mode access mode of the database automatic read_only or read_write varchar automatic checkpoint_threshold wal_autocheckpoint the wal size threshold at which to automatically trigger a checkpoint e g 1gb varchar 16 7mb default_collation the collation setting used when none is specified varchar default_null_order null_order null ordering used when none is specified nulls_first or nulls_last varchar nulls_first default_order the order type used when none is specified asc or desc varchar asc enable_external_access allow the database to access external state through e g loading installing modules copy to from csv readers pandas replacement scans etc boolean true enable_object_cache whether or not object cache is used to cache e g parquet metadata boolean false enable_profiling enables profiling and sets the output format json query_tree query_tree_optimizer varchar none enable_progress_bar enables the progress bar printing progress to the terminal for long queries boolean false explain_output output of explain statements all optimized_only physical_only varchar physical_only log_query_path specifies the path to which queries should be logged default empty string queries are not logged varchar null max_memory memory_limit the maximum memory of the system e g 1gb varchar 75 of ram perfect_ht_threshold threshold in bytes for when to use a perfect hash table default 12 bigint 12 profile_output profiling_output the file to which profile output should be saved or empty to print to the terminal varchar profiler_history_size sets the profiler history size bigint null profiling_mode the profiling mode standard or detailed varchar standard progress_bar_time sets the time in milliseconds how long a query needs to take before we start printing a progress bar bigint 2000 s3_access_key_id s3 access key id varchar s3_endpoint s3 endpoint default s3 amazonaws com varchar s3_region s3 region varchar s3_secret_access_key s3 access key varchar s3_session_token s3 session token varchar schema sets the default search schema equivalent to setting search_path to a single value varchar search_path sets the default search search path as a comma-separated list of values varchar temp_directory set the directory to which to write temp files varchar threads worker_threads the number of total threads used by the system bigint cores",
			"category": "docs",
			"url": "../docs/sql/configuration",
			"blurb": "DuckDB has a number of configuration options that can be used to change the behavior of the system. The configuration..."
		},
		{
			"title": "Pragmas",
			"text": "the pragma statement is an sql extension adopted by duckdb from sqlite pragma statements can be issued in a similar manner to regular sql statements pragma commands may alter the internal state of the database engine and can influence the subsequent execution or behavior of the engine list of supported pragma statements below is a list of supported pragma statements database_list show_tables table_info show -- list all databases usually one pragma database_list -- list all tables pragma show_tables -- get info for a specific table pragma table_info table_name -- also show table structure but slightly different format for compatibility pragma show table_name table_info returns information about the columns of the table with name table_name the exact format of the table returned is given below cid integer -- cid of the column name varchar -- name fo the column type varchar -- type of the column notnull boolean -- if the column is marked as not null dflt_value varchar -- default value of the column or null if not specified pk boolean -- part of the primary key or not memory_limit threads -- set the memory limit pragma memory_limit 1gb -- set the amount of threads for parallel query execution pragma threads 4 collations default_collation -- list all available collations pragma collations -- set the default collation to one of the available ones pragma default_collation nocase default_null_order default_order -- set the ordering for nulls to be either nulls first or nulls last pragma default_null_order nulls last -- set the default result set ordering direction to ascending or descending pragma default_order descending version -- show duckdb version pragma version enable_progress_bar enable_profiling disable_profiling profiling_output -- show progress bar when running queries pragma enable_progress_bar -- enable profiling pragma enable_profiling -- enable profiling in a specified format pragma enable_profiling json query_tree query_tree_optimizer -- disable profiling pragma disable_profiling -- specify a file to save the profiling output to pragma profiling_output path to file json pragma profile_output path to file json enable the gathering and printing of profiling information after the execution of a query optionally the format of the resulting profiling information can be specified as either json query_tree or query_tree_optimizer the default format is query_tree which prints the physical operator tree together with the timings and cardinalities of each operator in the tree to the screen below is an example output of the profiling information for the simple query select 42 query profiling information select 42 total time 0 0001s projection 42 1 0 00s dummy_scan 1 0 00s the printing of profiling information can be disabled again using disable_profiling by default profiling information is printed to the console however if you prefer to write the profiling information to a file the pragma profiling_output can be used to write to a specified file note that the file contents will be overwritten for every new query that is issued hence the file will only contain the profiling information of the last query that is run log_query_path explain_output enable_verification disable_verification force_parallelism disable_force_parallelism -- set a path for query logging pragma log_query_path tmp duckdb_log -- disable query logging again pragma log_query_path -- either show all or only optimized plans in the explain output pragma explain_output optimized -- enable query verification for development pragma enable_verification -- disable query verification for development pragma disable_verification -- enable force parallel query processing for development pragma force_parallelism -- disable force parallel query processing for development pragma disable_force_parallelism -- force index joins where applicable pragma force_index_join these are pragma s mostly used for development and internal testing create_fts_index drop_fts_index only available when the fts extension is built documented here",
			"category": "docs",
			"url": "../docs/sql/pragmas",
			"blurb": "The  PRAGMA  statement is an SQL extension adopted by DuckDB from SQLite.  PRAGMA  statements can be issued in a similar..."
		},
		{
			"title": "Case Statement",
			"text": "the case statement performs a switch based on a condition the basic form is identical to the ternary condition used in many programming languages case when cond then a else b end is equivalent to cond a b -- integers 1 2 3 select i case when i 2 then 1 else 0 end from integers -- 1 2 3 -- 0 0 1 the when cond then expr part of the case statement can be chained whenever any of the conditions returns true for a single tuple the corresponding expression is evaluated and returned -- integers 1 2 3 select i case when i 1 then 10 when i 2 then 20 else 0 end from integers -- 1 2 3 -- 10 20 0 the else part of the case statement is optional if no else statement is provided and none of the conditions match the case statement will return null -- integers 1 2 3 select i case when i 1 then 10 end from integers -- 1 2 3 -- 10 null null after the case but before the when an individual expression can also be provided when this is done the case statement is essentially transformed into a switch statement -- integers 1 2 3 select i case i when 1 then 10 when 2 then 20 when 3 then 30 end from integers -- 1 2 3 -- 10 20 30 -- this is equivalent to select i case when i 1 then 10 when i 2 then 20 when i 3 then 30 end from integers",
			"category": "docs",
			"url": "../docs/sql/expressions/case",
			"blurb": "The  CASE  statement performs a switch based on a condition. The basic form is identical to the ternary condition used in..."
		},
		{
			"title": "Expressions",
			"text": "an expression is a combination of values operators and functions expressions are highly composable and range from very simple to arbitrarily complex they can be found in many different parts of sql statements in this section we provide the different types of operators and functions that can be used within expressions more",
			"category": "docs",
			"url": "../docs/sql/expressions/overview",
			"blurb": "An expression is a combination of values, operators and functions. Expressions are highly composable, and range from very..."
		},
		{
			"title": "Collations",
			"text": "collations provide rules for how text should be sorted or compared in the execution engine collations are useful for localization as the rules for how text should be ordered are different for different languages or for different countries these orderings are often incompatible with one another for example in english the letter y comes between x and z however in lithuanian the letter y comes between the i and j for that reason different collations are supported the user must choose which collation they want to use when performing sorting and comparison operations by default the binary collation is used that means that strings are ordered and compared based only on their binary contents this makes sense for standard ascii characters i e the letters a-z and numbers 0-9 but generally does not make much sense for special unicode characters it is however by far the fastest method of performing ordering and comparisons hence it is recommended to stick with the binary collation unless required otherwise using collations in the stand-alone installation of duckdb three collations are included nocase noaccent and nfc the nocase collation compares characters as equal regardless of their casing the noaccent collation compares characters as equal regardless of their accents the nfc collation performs nfc-normalized comparisons see here for more information select hello hello -- false select hello collate nocase hello -- true select hello hëllo -- false select hello collate noaccent hëllo -- true collations can be combined by chaining them using the dot operator note however that not all collations can be combined together in general the nocase collation can be combined with any other collator but most other collations cannot be combined select hello collate nocase hellö -- false select hello collate noaccent hellö -- false select hello collate nocase noaccent hellö -- true default collations the collations we have seen so far have all been specified per expression it is also possible to specify a default collator either on the global database level or on a base table column the pragma default_collation can be used to specify the global default collator this is the collator that will be used if no other one is specified pragma default_collation nocase select hello hello -- true collations can also be specified per-column when creating a table when that column is then used in a comparison the per-column collation is used to perform that comparison create table names name varchar collate noaccent insert into names values hännes select name from names where name hannes -- hännes be careful here however as different collations cannot be combined this can be problematic when you want to compare columns that have a different collation specified select name from names where name hannes collate nocase -- error cannot combine types with different collation create table other_names name varchar collate nocase insert into other_names values hännes select from names other_names where names name other_names name -- error cannot combine types with different collation -- need to manually overwrite the collation select from names other_names where names name collate noaccent nocase other_names name collate noaccent nocase -- hännes hännes icu collations the collations we have seen so far are not region dependent and do not follow any specific regional rules if you wish to follow the rules of a specific region or language you will need to use one of the icu collations for that you need to include the icu extension this can be found in the extension icu folder in the project using the c api the extension can be loaded as follows duckdb db db loadextension icuextension loading this extension will add a number of language and region specific collations to your database these can be queried using pragma collations command or by querying the pragma_collations function pragma collations select from pragma_collations -- af am ar as az be bg bn bo bs bs ca ceb chr cs cy da de de_at dsb dz ee el en en_us en_us eo es et fa fa_af fi fil fo fr fr_ca ga gl gu ha haw he he_il hi hr hsb hu hy id id_id ig is it ja ka kk kl km kn ko kok ku ky lb lkt ln lo lt lv mk ml mn mr ms mt my nb nb_no ne nl nn om or pa pa pa_in pl ps pt ro ru se si sk sl smn sq sr sr sr_ba sr_me sr_rs sr sr_ba sr_rs sv sw ta te th tk to tr ug uk ur uz vi wae wo xh yi yo zh zh zh_cn zh_sg zh zh_hk zh_mo zh_tw zu these collations can then be used as the other collations would be used before they can also be combined with the nocase collation for example to use the german collation rules you could use the following code snippet create table strings s varchar collate de insert into strings values gabel göbel goethe goldmann göthe götz select from strings order by s -- gabel göbel goethe goldmann göthe götz",
			"category": "docs",
			"url": "../docs/sql/expressions/collations",
			"blurb": "Collations provide rules for how text should be sorted or compared in the execution engine. Collations are useful for..."
		},
		{
			"title": "Comparisons",
			"text": "comparison operators the table below shows the standard comparison operators whenever either of the input arguments is null the output of the comparison is null operator description example result --- --- --- --- less than 2 3 true greater than 2 3 false less than or equal to 2 3 true greater than or equal to 4 null null equal null null null or not equal 2 2 false the table below shows the standard distinction operators these operators treat null values as equal operator description example result --- --- --- --- is distinct from equal including null 2 is distinct from null true is not distinct from not equal including null null is not distinct from null true between and is not null besides the stanadrd comparison operators there are also the between and is not null operators these behave much like operators but have special syntax mandated by the sql standard they are shown in the table below predicate description --- --- a between x and y equivalent to a x and a y a not between x and y equivalent to a x or a y expression is null true if expression is null false otherwise expression isnull alias for is null non-standard expression is not null false if expression is null true otherwise expression notnull alias for is not null non-standard",
			"category": "docs",
			"url": "../docs/sql/expressions/comparison_operators",
			"blurb": "Comparison Operators    The table below shows the standard comparison operators.  Whenever either of the input arguments..."
		},
		{
			"title": "Casting",
			"text": "casting refers to the process of changing the type of a row from one type to another the standard sql syntax for this is cast expr as typename duckdb also supports the easier to type shorthand expr typename which is also present in postgresql -- integers 1 2 3 select cast i as varchar i double -- 1 2 3 -- 1 0 2 0 3 0 the exact behavior of the cast depends on the source and destination types for example when casting from varchar to any other type the string will be attempted to be converted not all casts are possible for example it is not possible to convert an integer to a date casts may also throw errors when the cast could not be successfully performed for example trying trying to cast the string hello to an integer will result in an error being thrown implicit casting in many situations the system will add casts by itself this is called implicit casting this happens for example when a function is called with an argument that does not match the type of the function but can be casted to the desired type consider the function sin double this function takes as input argument a column of type double however it can be called with an integer as well sin 1 if we look at the explain output we will see that the integer is converted into a double before being passed to the sin function explain select sin 1 physical plan projection 0 841471 dummy_scan generally implicit casts only cast upwards that is to say we can implicitly cast an integer to a bigint but not the other way around",
			"category": "docs",
			"url": "../docs/sql/expressions/cast",
			"blurb": "Casting refers to the process of changing the type of a row from one type to another. The standard SQL syntax for this is..."
		},
		{
			"title": "Logical Operators",
			"text": "the following logical operators are available and or and not sql uses a three-valuad logic system with true false and null note that logical operators involving null do not always evaluate to null for example null and false will evaluate to false and null or true will evaluate to true below are the complete truth tables a b a and b a or b --- --- --- --- true true true true true false false true true null null true false false false false false null false null null null null null a not a --- --- true false false true null null the operators and and or are commutative that is you can switch the left and right operand without affecting the result",
			"category": "docs",
			"url": "../docs/sql/expressions/logical_operators",
			"blurb": "The following logical operators are available:  AND ,  OR  and  NOT . SQL uses a three-valuad logic system with  TRUE , ..."
		},
		{
			"title": "Subqueries",
			"text": "scalar subquery scalar subqueries are subqueries that return a single value they can be used anywhere where a regular expression can be used if a scalar subquery returns more than a single value the first value returned will be used consider the following table grades grade course --- --- 7 math 9 math 8 cs create table grades grade integer course varchar insert into grades values 7 math 9 math 8 cs we can run the following query to obtain the minimum grade select min grade from grades -- 7 by using a scalar subquery in the where clause we can figure out for which course this grade was obtained select course from grades where grade select min grade from grades -- math exists the exists operator is used to test for the existence of any row inside the subquery it returns either true when the subquery returns one or more records or false otherwise the exists clause is generally the most useful as a correlated subquery however it can be used as an uncorrelated subquery as well for example we can use it to figure out if there are any grades present for a given course select exists select from grades where course math -- true select exists select from grades where course history -- false in clause the in clause checks containment of the left expression inside the result defined by the subquery or the set of expressions on the right side the in clause returns true if the expression is present in the rhs false if the expression is not in the rhs and the rhs has no null values or null if the expression is not in the rhs and the rhs has null values we can use the in clause in a similar manner as we used the exists clause select math in select course from grades -- true correlated subqueries all the subqueries presented here so far have been uncorrelated subqueries where the subqueries themselves are entirely self-contained and can be run without the parent query there exists a second type of subqueries called correlated subqueries for correlated subqueries the subquery uses values from the parent subquery conceptually the subqueries are run once for every single row in the parent query perhaps a simple way of envisioning this is that the correlated subquery is a function that is applied to every row in the source data set for example suppose that we want to find the minimum grade for every course we could do that as follows select from grades grades_parent where grade select min grade from grades where grades course grades_parent course -- 7 math 8 cs the subquery uses a column from the parent query grades_parent course conceptually we can see the subquery as a function where the correlated column is a parameter to that function select min grade from grades where course now when we execute this function for each of the rows we can see that for math this will return 7 and for cs it will return 8 we then compare it against the grade for that actual row as a result the row math 9 will be filtered out as 9 7",
			"category": "docs",
			"url": "../docs/sql/expressions/subqueries",
			"blurb": "Scalar Subquery    Scalar subqueries are subqueries that return a single value. They can be used anywhere where a regular..."
		},
		{
			"title": "Numeric Functions",
			"text": "numeric operators the table below shows the available mathematical operators for numeric types operator description example result --- --- --- --- addition 2 3 5 - subtraction 2 - 3 -1 multiplication 2 3 6 division 4 2 2 modulo remainder 5 4 1 bitwise and 91 15 11 bitwise or 32 3 35 bitwise shift left 1 4 16 bitwise shift right 8 2 2 the modulo and bitwise operators work only on integral data types whereas the others are available for all numeric data types numeric functions the table below shows the available mathematical functions function description example result --- --- --- --- abs x absolute value abs -17 4 17 4 acos x computes the arccosine of x acos 0 5 1 0471975511965976 asin x computes the arcsine of x asin 0 5 0 5235987755982989 atan2 x computes the arctangent of x atan2 0 5 0 4636476090008061 atan2 x y computes the arctangent x y atan2 0 5 0 5 0 7853981633974483 bit_count x returns the number of bits that are set bit_count 31 5 cbrt x returns the cube root of the number cbrt 8 2 ceil x rounds the number up ceil 17 4 18 chr x returns a character which is corresponding the the ascii code value or unicode code point chr 65 a cos x computes the cosine of x cos 90 -0 4480736161291701 cot x computes the cotangent of x cot 0 5 1 830487721712452 degrees x converts radians to degrees degrees pi 180 floor x rounds the number down floor 17 4 17 greatest x1 x2 selects the largest value greatest 3 2 4 4 4 least x1 x2 selects the smallest value least 3 2 4 4 2 ln x computes the natural logarithm of x ln 2 0 693 log x computes the 10-log of x log 100 2 log2 x computes the 2-log of x log2 8 3 pi returns the value of pi pi 3 141592653589793 pow x y computes x to the power of y pow 2 3 8 radians x converts degrees to radians radians 90 1 5707963267948966 random returns a random number between 0 and 1 random round v numeric s int round to s decimal places round 42 4332 2 42 43 setseed x sets the seed to be used for the random function setseed 0 42 sin x computes the sin of x sin 90 0 8939966636005579 sign x returns the sign of x as -1 0 or 1 sign -349 -1 sqrt x returns the square root of the number sqrt 9 3 xor x bitwise xor xor 17 5 20 tan x computes the tangent of x tan 90 -1 995200412208242",
			"category": "docs",
			"url": "../docs/sql/functions/numeric",
			"blurb": "Numeric Operators  The table below shows the available mathematical operators for numeric types.   | Operator |..."
		},
		{
			"title": "Functions",
			"text": "functions are more",
			"category": "docs",
			"url": "../docs/sql/functions/overview",
			"blurb": "Functions are ...    More"
		},
		{
			"title": "Pattern Matching",
			"text": "pattern matching there are three separate approaches to pattern matching provided by duckdb the traditional sql like operator the more recent similar to operator added in sql 1999 and posix-style regular expressions like the like expression returns true if the string matches the supplied pattern as expected the not like expression returns false if like returns true and vice versa an equivalent expression is not string like pattern if pattern does not contain percent signs or underscores then the pattern only represents the string itself in that case like acts like the equals operator an underscore _ in pattern stands for matches any single character a percent sign matches any sequence of zero or more characters some examples abc like abc -- true abc like a -- true abc like _b_ -- true abc like c -- false abc like c -- false abc like c -- true like pattern matching always covers the entire string therefore if it s desired to match a sequence anywhere within a string the pattern must start and end with a percent sign similar to the similar to operator returns true or false depending on whether its pattern matches the given string it is similar to like except that it interprets the pattern using a regular expression like like the similar to operator succeeds only if its pattern matches the entire string this is unlike common regular expression behavior where the pattern can match any part of the string a regular expression is a character sequence that is an abbreviated definition of a set of strings a regular set a string is said to match a regular expression if it is a member of the regular set described by the regular expression as with like pattern characters match string characters exactly unless they are special characters in the regular expression language but regular expressions use different special characters than like does some examples abc similar to abc -- true abc similar to a -- false abc similar to b d -- true abc similar to b c -- false regular expressions function description --- --- regexp_matches string pattern returns true if string contains the regexp pattern false otherwise regexp_replace string pattern replacement if string contains the regexp pattern replaces the matching part with replacement regexp_extract string pattern idx if string contains the regexp pattern returns the capturing group specified by optional parameter idx the regexp_matches function is similar to the similar to operator however it does not require the entire string to match instead regexp_matches returns true if the string merely contains the pattern unless the special tokens and are used to anchor the regular expression to the start and end of the string below are some examples regexp_matches abc abc -- true regexp_matches abc abc -- true regexp_matches abc a -- true regexp_matches abc a -- false regexp_matches abc b d -- true regexp_matches abc b c -- true regexp_matches abc b c -- false the regexp_replace function can be used to replace the part of a string that matches the regexp pattern with a replacement string the notation d where d is a number indicating the group can be used to refer to groups captured in the regular expression in the replacement string below are some examples regexp_replace abc b c x -- axc regexp_replace abc b c 1 1 1 1 -- abbbbc regexp_replace abc c 1e -- abe regexp_replace abc a b 2 1 -- bac the regexp_extract function is used to extract a part of a string that matches the regexp pattern a specific capturing group within the pattern can be extracted using the idx parameter if idx is not specified it defaults to 0 extracting the first match with the whole pattern regexp_extract abc b -- abc regexp_extract abc b 0 -- abc regexp_extract abc b 1 -- empty regexp_extract abc a-z b 1 -- a regexp_extract abc a-z b 2 -- b",
			"category": "docs",
			"url": "../docs/sql/functions/patternmatching",
			"blurb": "Pattern Matching  There are three separate approaches to pattern matching provided by DuckDB: the traditional SQL  LIKE ..."
		},
		{
			"title": "Blob Functions",
			"text": "this section describes functions and operators for examining and manipulating blob values function description example result --- --- --- --- blob blob blob concatenation xaa blob xbb blob xaabb octet_length blob number of bytes in blob octet_length xaabb blob 2",
			"category": "docs",
			"url": "../docs/sql/functions/blob",
			"blurb": "This section describes functions and operators for examining and manipulating blob values.   | Function | Description |..."
		},
		{
			"title": "Date Functions",
			"text": "this section describes functions and operators for examining and manipulating date values date operators the table below shows the available mathematical operators for date types operator description example result --- --- --- --- addition of days integers date 1992-03-22 5 1992-03-27 addition of an interval date 1992-03-22 interval 5 day 1992-03-27 - subtraction of date s date 1992-03-27 - date 1992-03-22 5 - subtraction of an interval date 1992-03-27 - interval 5 day 1992-03-22 date functions the table below shows the available functions for date types dates can also be manipulated with the timestamp functions through type promotion function description example result --- --- --- --- current_date current date at start of current transaction date_diff part startdate enddate the number of partition boundaries between the dates date_diff month date 1992-09-15 date 1992-11-14 2 date_part part date get the subfield equivalent to extract date_part year date 1992-09-20 1992 date_sub part startdate enddate the number of complete partitions between the dates date_sub month date 1992-09-15 date 1992-11-14 1 date_trunc part date truncate to specified precision date_trunc month date 1992-03-07 1992-03-01 dayname date the english name of the weekday monthname date 1992-09-20 sunday extract part from date get subfield from a date extract year from date 1992-09-20 1992 greatest date date the later of two dates greatest date 1992-09-20 date 1992-03-07 1992-09-20 last_day date the last day of the month last_day date 1992-09-20 1992-09-30 least date date the earlier of two dates least date 1992-09-20 date 1992-03-07 1992-03-07 monthname date the english name of the month monthname date 1992-09-20 september weekofyear date the week of the year weekofyear date 1992-09-20 38 dayofmonth date day of month dayofmonth date 1992-09-20 20 last_day date last day of the corresponding month in the date last_day date 1992-09-20 1992-09-30 weekday date day of the week sunday 0 saturday 6 weekday date 1992-09-20 0 isodow date iso day of the week monday 1 sunday 7 isodow date 1992-09-20 7 yearweek date year and week of year yearweek date 1992-09-20 199238 strftime date format converts a date to a string according to the format string strftime date 1992-01-01 a -d b y wed 1 january 1992 there are also dedicated extraction functions to get the subfields",
			"category": "docs",
			"url": "../docs/sql/functions/date",
			"blurb": "This section describes functions and operators for examining and manipulating date values.   Date Operators  The table..."
		},
		{
			"title": "Nested Functions",
			"text": "this section describes functions and operators for examining and manipulating nested values there are two nested data types lists and structs list functions in the descriptions l is the three element list 4 5 6 function description example result --- --- --- --- list index bracket notation serves as an alias for list_extract l 2 6 array_extract list index alias for list_extract array_extract l 2 6 array_slice list begin end extract a sublist using slice conventions null s are interpreted as the bounds of the list negative values are accepted array_slice l 1 null 5 6 list_element list index alias for list_extract list_element l 2 6 list_extract list index extract the index th 0-based value from the list list_extract l 2 6 list_pack any alias for list_value list_pack 4 5 6 4 5 6 list_value any create a list containing the argument values list_value 4 5 6 4 5 6 list begin end alias for array_slice missing arguments are interprete as null s l 1 2 5 6 array_length list return the length of the list array_length 1 2 3 3 len list alias for array_length len 1 2 3 3 unnest list unnests a list by one level note that this is a special function that alters the cardinality of the result see the unnest page for more details unnest 1 2 3 1 2 3 list_concat list1 list2 concatenates two lists list_concat 2 3 4 5 6 2 3 4 5 6 list_cat list1 list2 alias for list_concat list_cat 2 3 4 5 6 2 3 4 5 6 array_concat list1 list2 alias for list_concat array_concat 2 3 4 5 6 2 3 4 5 6 array_cat list1 list2 alias for list_concat array_cat 2 3 4 5 6 2 3 4 5 6 list_append list element appends element to list list_append 2 3 4 2 3 4 list_prepend list element prepends element to list list_prepend 3 4 5 6 3 4 5 6 struct functions function description example result --- --- --- --- struct entry dot notation serves as an alias for struct_extract i 3 s string s string struct entry bracket notation serves as an alias for struct_extract i 3 s string s string row any create a struct containing the argument values if the values are column references the entry name will be the column name otherwise it will be the string vn where n is the 1-based position of the argument row i i 4 i 4 i 3 v2 3 v3 0 struct_extract struct entry extract the named entry from the struct struct_extract s i 4 struct_pack name any create a struct containing the argument values the entry name will be the bound variable name struct_pack i 4 s string i 3 s string map functions function description example result --- --- --- --- map entry alias for element_at map 100 5 a b 100 42 element_at map key return a list containing the value for a given key or an empty list if the key is not contained in the map the type of the key provided in the second parameter must match the type of the map s keys else an error is returned select element_at map 100 5 42 43 100 42 cardinality map return the size of the map or the number of entries in the map cardinality map 4 2 a b 2 map returns an empty map map range functions the functions range and generate_series create a list of values in the range between start and stop the start parameter is inclusive for the range function the stop parameter is exclusive while for generate_series it is inclusive based on the number of arguments the following variants exist range start stop step range start stop range stop generate_series start stop step generate_series start stop generate_series stop the default value of start is 0 and the default value of step is 1 select range 5 -- 0 1 2 3 4 select range 2 5 -- 2 3 4 select range 2 5 3 -- 2 select generate_series 5 -- 0 1 2 3 4 5 select generate_series 2 5 -- 2 3 4 5 select generate_series 2 5 3 -- 2 5 date ranges are also supported select from range date 1992-01-01 date 1992-03-01 interval 1 month range 1992-01-01 00 00 00 1992-02-01 00 00 00 generate_subscripts the generate_subscript arr dim function generates indexes along the dim th dimension of array arr select generate_subscripts 4 5 6 1 as i i 1 2 3 related functions there are also aggregate functions list and histogram that produces lists and lists of structs",
			"category": "docs",
			"url": "../docs/sql/functions/nested",
			"blurb": "This section describes functions and operators for examining and manipulating nested values. There are two nested data..."
		},
		{
			"title": "Date Format",
			"text": "the strftime and strptime functions can be used to convert between dates timestamps and strings this is often required when parsing csv files displaying output to the user or transferring information between programs because there are many possible date representations these functions accept a format string that describes how the date or timestamp should be structured strftime examples strftime timestamp format converts timestamps or dates to strings according to the specified pattern select strftime date 1992-03-02 d m y -- 02 03 1992 select strftime timestamp 1992-03-02 20 32 45 a -d b y - i m s p -- monday 2 march 1992 - 08 32 45 pm strptime examples strptime string format converts strings to timestamps according to the specified pattern select strptime 02 03 1992 d m y -- 1992-03-02 00 00 00 select strptime monday 2 march 1992 - 08 32 45 pm a -d b y - i m s p -- 1992-03-02 20 32 45 csv parsing the date formats can also be specified during csv parsing either in the copy statement or in the read_csv function this can be done by either specifying a dateformat or a timestampformat or both dateformat will be used for converting dates and timestampformat will be used for converting timestamps below are some examples for how to use this -- in copy statement copy dates from test csv dateformat d m y timestampformat a -d b y - i m s p -- in read_csv function select from read_csv test csv dateformat m d y format specifiers below is a full list of all available format specifiers specifier description example --- --- --- --- a abbreviated weekday name sun mon a full weekday name sunday monday w weekday as a decimal number 0 1 6 d day of the month as a zero-padded decimal 01 02 31 -d day of the month as a decimal number 1 2 30 b abbreviated month name jan feb dec b full month name january february m month as a zero-padded decimal number 01 02 12 -m month as a decimal number 1 2 12 y year without century as a zero-padded decimal number 00 01 99 -y year without century as a decimal number 0 1 99 y year with century as a decimal number 2013 2019 etc h hour 24-hour clock as a zero-padded decimal number 00 01 23 -h hour 24-hour clock as a decimal number 0 1 23 i hour 12-hour clock as a zero-padded decimal number 01 02 12 -i hour 12-hour clock as a decimal number 1 2 12 p locale s am or pm am pm m minute as a zero-padded decimal number 00 01 59 -m minute as a decimal number 0 1 59 s second as a zero-padded decimal number 00 01 59 -s second as a decimal number 0 1 59 g millisecond as a decimal number zero-padded on the left 000 - 999 f microsecond as a decimal number zero-padded on the left 000000 - 999999 z utc offset in the form hhmm or -hhmm z time zone name j day of the year as a zero-padded decimal number 001 002 366 -j day of the year as a decimal number 1 2 366 u week number of the year sunday as the first day of the week 00 01 53 w week number of the year monday as the first day of the week 00 01 53 c iso date and time representation 1992-03-02 10 30 20 x iso date representation 1992-03-02 x iso time representation 10 30 20 a literal character",
			"category": "docs",
			"url": "../docs/sql/functions/dateformat",
			"blurb": "The  strftime  and  strptime  functions can be used to convert between dates/timestamps and strings. This is often..."
		},
		{
			"title": "Timestamp Functions",
			"text": "this section describes functions and operators for examining and manipulating timestamp values timestamp operators the table below shows the available mathematical operators for timestamp types operator description example result --- --- --- --- addition of an interval timestamp 1992-03-22 01 02 03 interval 5 day 1992-03-27 01 02 03 - subtraction of timestamp s timestamp 1992-03-27 - timestamp 1992-03-22 5 days - subtraction of an interval timestamp 1992-03-27 01 02 03 - interval 5 day 1992-03-22 01 02 03 timestamp functions the table below shows the available scalar functions for timestamp types function description example result --- --- --- --- age timestamp timestamp subtract arguments resulting in the time difference between the two timestamps age timestamp 2001-04-10 timestamp 1992-09-20 8 years 6 months 20 days age timestamp subtract from current_date age timestamp 1992-09-20 29 years 1 month 27 days 12 39 00 844 century timestamp extracts the century of a timestamp century timestamp 1992-03-22 20 current_timestamp current date and time start of current transaction date_diff part startdate enddate the number of partition boundaries between the timestamps date_diff hour timestamp 1992-09-30 23 59 59 timestamp 1992-10-01 01 58 00 2 date_part part timestamp get subfield equivalent to extract date_part minute timestamp 1992-09-20 20 38 40 38 date_sub part startdate enddate the number of complete partitions between the timestamps date_sub hour timestamp 1992-09-30 23 59 59 timestamp 1992-10-01 01 58 00 1 date_trunc part timestamp truncate to specified precision date_trunc hour timestamp 1992-09-20 20 38 40 1992-09-20 20 00 00 day timestamp extracts the day of a timestamp day timestamp 1992-03-22 22 dayname timestamp the english name of the weekday dayname timestamp 1992-03-22 sunday dayofweek timestamp extracts the day of the week of a timestamp 0-6 0 sunday 6 saturday dayofweek timestamp 1992-03-22 0 dayofyear timestamp extracts the day of the year of a timestamp 1-366 dayofyear timestamp 1992-03-22 82 decade timestamp extracts the decade of a timestamp decade timestamp 1992-03-22 199 epoch timestamp extracts the epoch of a timestamp in seconds epoch timestamp 1992-03-22 701222400 epoch_ms ms converts ms since epoch to a timestamp epoch_ms 701222400000 1992-03-22 00 00 00 extract field from timestamp get subfield from a timestamp extract hour from timestamp 1992-09-20 20 38 48 20 greatest timestamp timestamp the later of two timestamps greatest timestamp 1992-09-20 20 38 48 timestamp 1992-03-22 01 02 03 1234 1992-09-20 20 38 48 hour timestamp extracts the hour component of a timestamp hour timestamp 1992-03-22 01 02 03 1234 1 isodow timestamp extracts the iso day of the week of a timestamp 1-7 1 monday 7 sunday isodow timestamp 1992-03-22 7 last_day timestamp the last day of the month last_day timestamp 1992-03-22 01 02 03 1234 1992-03-31 least timestamp timestamp the earlier of two timestamps least timestamp 1992-09-20 20 38 48 timestamp 1992-03-22 01 02 03 1234 1992-03-22 01 02 03 1234 microsecond timestamp extracts the sub-minute component of a timestamp in microseconds microsecond timestamp 1992-03-22 01 02 03 1234 3123400 millisecond timestamp extracts the sub-minute component of a timestamp in milliseconds millisecond timestamp 1992-03-22 01 02 03 1234 3123 millenium timestamp extracts the millenium of a timestamp millenium timestamp 1992-03-22 2 minute timestamp extracts the minute component of a timestamp minute timestamp 1992-03-22 01 02 03 1234 2 month timestamp extracts the month of a timestamp month timestamp 1992-03-22 3 monthname timestamp the english name of the month monthname timestamp 1992-09-20 september now current date and time start of current transaction quarter timestamp extracts the quarter of a timestamp quarter timestamp 1992-03-22 1 second timestamp extracts the second component of a timestamp second timestamp 1992-03-22 01 02 03 1234 3 strftime timestamp format converts timestamp to string according to the format string strftime timestamp 1992-01-01 20 38 40 a -d b y - i m s p wed 1 january 1992 - 08 38 40 pm strptime text format converts string to timestamp according to the format string strptime wed 1 january 1992 - 08 38 40 pm a -d b y - i m s p 1992-01-01 20 38 40 to_timestamp sec converts sec since epoch to a timestamp to_timestamp 701222400 1992-03-22 00 00 00 week timestamp extracts the week number of a timestamp 1-53 week timestamp 1992-03-22 12 year timestamp extracts the year of a timestamp year timestamp 1992-03-22 1992 timestamp table functions the table below shows the available scalar functions for timestamp types function description example --- --- --- --- generate_series timestamp timestamp interval generate a table of timestamps in the closed range stepping by the interval generate_series timestamp 2001-04-10 timestamp 2001-04-11 interval 30 minute range timestamp timestamp interval generate a table of timestamps in the half open range stepping by the interval range timestamp 2001-04-10 timestamp 2001-04-11 interval 30 minute",
			"category": "docs",
			"url": "../docs/sql/functions/timestamp",
			"blurb": "This section describes functions and operators for examining and manipulating  TIMESTAMP  values.   Timestamp Operators ..."
		},
		{
			"title": "Interval Functions",
			"text": "this section describes functions and operators for examining and manipulating interval values interval operators the table below shows the available mathematical operators for interval types operator description example result --- --- --- --- addition of an interval interval 1 hour interval 5 hour interval 6 hour addition to a date date 1992-03-22 interval 5 day 1992-03-27 addition to a timestamp timestamp 1992-03-22 01 02 03 interval 5 day 1992-03-27 01 02 03 addition to a time time 01 02 03 interval 5 hour 06 02 03 - subtraction of an interval interval 5 hour - interval 1 hour interval 4 hour - subtraction from a date date 1992-03-27 - interval 5 day 1992-03-22 - subtraction from a timestamp timestamp 1992-03-27 01 02 03 - interval 5 day 1992-03-22 01 02 03 - subtraction from a time time 06 02 03 - interval 5 hour 01 02 03 interval functions the table below shows the available scalar functions for interval types function description example result --- --- --- --- date_part part interval get subfield equivalent to extract date_part year interval 14 months 1 extract part from interval get subfield from a date extract month from interval 14 months 2 to_years integer construct a year interval to_years 5 interval 5 year to_months integer construct a month interval to_months 5 interval 5 month to_days integer construct a day interval to_days 5 interval 5 day to_hours integer construct a hour interval to_hours 5 interval 5 hour to_minutes integer construct a minute interval to_minutes 5 interval 5 minute to_seconds integer construct a second interval to_seconds 5 interval 5 second to_milliseconds integer construct a millisecond interval to_milliseconds 5 interval 5 millisecond to_microseconds integer construct a microsecond interval to_microseconds 5 interval 5 microsecond all date parts are defined for intervals except dow isodow doy week and yearweek",
			"category": "docs",
			"url": "../docs/sql/functions/interval",
			"blurb": "This section describes functions and operators for examining and manipulating  INTERVAL  values.   Interval Operators ..."
		},
		{
			"title": "Time Functions",
			"text": "this section describes functions and operators for examining and manipulating time values time operators the table below shows the available mathematical operators for time types operator description example result --- --- --- --- addition of an interval time 01 02 03 interval 5 hour 06 02 03 - subtraction of an interval time 06 02 03 - interval 5 hour 01 02 03 time functions the table below shows the available scalar functions for time types function description example result --- --- --- --- current_time current time start of current transaction date_diff part starttime endtime the number of partition boundaries between the times date_diff hour time 01 02 03 time 06 01 03 5 date_part part time get subfield equivalent to extract date_part minute time 14 21 13 21 date_sub part starttime endtime the number of complete partitions between the times date_sub hour time 01 02 03 time 06 01 03 4 epoch time the number of seconds since midnight epoch time 14 21 13 51673 extract part from time get subfield from a date extract hour from time 14 21 13 14 hour time extracts the hour component of a time hour time 01 02 03 1234 1 microsecond time extracts the sub-minute component of a time in microseconds microsecond time 01 02 03 1234 3123400 millisecond time extracts the sub-minute component of a time in milliseconds millisecond time 01 02 03 1234 3123 minute time extracts the minute component of a time minute time 01 02 03 1234 2 second time extracts the second component of a time second time 01 02 03 1234 3 the only date parts that are defined for times are epoch hours minutes seconds milliseconds and microseconds",
			"category": "docs",
			"url": "../docs/sql/functions/time",
			"blurb": "This section describes functions and operators for examining and manipulating  TIME  values.   Time Operators  The table..."
		},
		{
			"title": "Date Parts",
			"text": "the date_part and date_diff and date_trunc functions can be used to manipulate the fields of temporal types the fields are specified as strings that contain the part name of the field part specifiers below is a full list of all available date part specifiers the examples are the corresponding parts of the timestamp 2021-08-03 11 59 44 123456 specifier description synonyms example --- --- --- --- year gregorian year y years 2021 month gregorian month mon months mons 8 day gregorian day days d 3 decade gregorian decade decades 202 century gregorian century centuries 21 millennium gregorian millennium millenia 3 microseconds sub-minute microseconds microsecond 44123456 milliseconds sub-minute milliseconds millisecond ms msec msecs 44123 second seconds seconds s 44 minute minutes minutes m 59 hour hours hours h 11 epoch seconds since 1970-01-01 1627991984 dow day of the week sunday 0 saturday 6 2 isodow iso day of the week monday 1 sunday 7 2 week week number weeks w 31 yearweek year and week number in yyyyww format 202131 dayofyear day of the year 1-365 366 doy 215 quarter quarter of the year 1-4 quarters 3 the parts above epoch are also used to specify interval types there are dedicated extraction functions to get the certain subfields function description example result --- --- --- --- day date day day date 1992-02-15 15 month date month month date 1992-02-15 2 dayofyear date day of year dayofyear date 1992-02-15 46 week date week week date 1992-02-15 7 quarter date quarter quarter date 1992-02-15 1 year date year year date 1992-02-15 1992",
			"category": "docs",
			"url": "../docs/sql/functions/datepart",
			"blurb": "The  date_part  and  date_diff  and  date_trunc  functions can be used to manipulate the fields of temporal types.  The..."
		},
		{
			"title": "Text Functions",
			"text": "this section describes functions and operators for examining and manipulating string values denotes a space character function description example result --- --- --- --- string string string concatenation duck db duckdb string index alias for array_extract duckdb 3 k string begin end alias for array_slice missing arguments are interprete as null s duckdb 4 duck array_extract list index extract a single character using a 0-based index array_extract duckdb 1 u array_slice list begin end extract a string using slice conventions null s are interpreted as the bounds of the string negative values are accepted array_slice duckdb 4 null db ascii string returns an integer that represents the unicode code point of the first character of the string ascii ω 937 concat string concatenate many strings together concat hello world hello world concat_ws separator string concatenate strings together separated by the specified separator concat_ws banana apple melon banana apple melon format format parameters formats a string using fmt syntax format benchmark took seconds csv 42 benchmark csv took 42 seconds left string count extract the left-most count characters left hello 2 he length string number of characters in string length hello 5 string like target returns true if the string matches the like specifier see pattern matching hello like lo true list_element string index an alias for array_extract list_element duckdb 1 u list_extract string index an alias for array_extract list_extract duckdb 1 u lower string convert string to lower case lower hello hello lpad string count character pads the string with the character from the left until it has count characters lpad hello 10 hello ltrim string removes any spaces from the left side of the string ltrim test test ltrim string characters removes any occurrences of any of the characters from the left side of the string ltrim test test upper string convert string to upper case upper hello hello printf format parameters formats a string using printf syntax printf benchmark s took d seconds csv 42 benchmark csv took 42 seconds regexp_full_match string regex returns true if the entire string matches the regex see pattern matching regexp_full_match anabanana an false regexp_matches string regex returns true if a part of string matches the regex see pattern matching regexp_matches anabanana an true regexp_replace string regex replacement modifiers replaces the first occurrence of regex with the replacement use g modifier to replace all occurrences instead see pattern matching select regexp_replace hello lo - he-lo repeat string count repeats the string count number of times repeat a 5 aaaaa replace string source target replaces any occurrences of the source with target in string replace hello l - he--o reverse string reverses the string reverse hello olleh right string count extract the right-most count characters right hello 3 llo rpad string count character pads the string with the character from the right until it has count characters rpad hello 10 hello rtrim string removes any spaces from the right side of the string rtrim test test rtrim string characters removes any occurrences of any of the characters from the right side of the string rtrim test test string similar to regex returns true if the string matches the regex identical to regexp_full_match see pattern matching hello similar to l false strlen string number of bytes in string length 1 strip_accents string strips accents from string strip_accents mühleisen muhleisen string_split string separator splits the string along the separator string_split hello world hello world string_split_regex string regex splits the string along the regex string_split_regex hello world 42 hello world 42 substring string start length extract substring of length characters starting from character start note that a start value of 1 refers to the first character of the string substring hello 2 2 el trim string removes any spaces from either side of the string trim test test trim string characters removes any occurrences of any of the characters from either side of the string trim test test unicode string returns the unicode code of the first character of the string unicode ü 252",
			"category": "docs",
			"url": "../docs/sql/functions/char",
			"blurb": "This section describes functions and operators for examining and manipulating string values.  ␣  denotes a space..."
		},
		{
			"title": "SAMPLE Clause",
			"text": "the sample clause allows you to run the query on a sample from the base table this can significantly speed up processing of queries at the expense of accuracy in the result samples can also be used to quickly see a snapshot of the data when exploring a data set the sample clause is applied right after anything in the from clause i e after any joins but before the where clause or any aggregates see the sample page for more information examples -- select a sample of 1 of the addresses table using default system sampling select from addresses using sample 1 -- select a sample of 1 of the addresses table using bernoulli sampling select from addresses using sample 1 bernoulli -- select a sample of 10 rows from the subquery select from select from addresses using sample 10 rows syntax",
			"category": "docs",
			"url": "../docs/sql/query_syntax/sample",
			"blurb": "The SAMPLE clause  allows you to run the query on a sample from the base table. This can significantly speed up..."
		},
		{
			"title": "UNNEST",
			"text": "the unnest function is used to unnest a list by one level the function can be used as a regular scalar function but only in the select clause unnest is a special function in the sense that it changes the cardinality of the result the result of the unnest function is one tuple per entry in the list when unnest is combined with regular scalar expressions those expressions are repeated for every entry in the list when multiple lists are unnested in the same select clause the lists are unnested side-by-side if one list is longer than the other the shorter list will be padded with null values an empty list and a null list will both unnest to zero elements examples -- unnest a scalar list generating 3 rows 1 2 3 select unnest 1 2 3 -- unnest a scalar list generating 3 rows 1 10 2 11 3 null select unnest 1 2 3 unnest 10 11 -- unnest a scalar list generating 3 rows 1 10 2 10 3 10 select unnest 1 2 3 10 -- unnest a list column generated from a subquery select unnest l 10 from values 1 2 3 4 5 tbl l -- empty result select unnest",
			"category": "docs",
			"url": "../docs/sql/query_syntax/unnest",
			"blurb": "The  UNNEST  function is used to unnest a list by one level. The function can be used as a regular scalar function, but..."
		},
		{
			"title": "HAVING Clause",
			"text": "the having clause can be used after the group by clause to provide filter criteria after the grouping has been completed in terms of syntax the having clause is identical to the where clause but while the where clause occurs before the grouping the having clause occurs after the grouping examples -- count the number of entries in the addresses table that belong to each different city -- filtering out cities with a count below 50 select city count from addresses group by city having count 50 -- compute the average income per city per street_name -- filtering out cities with an average income bigger than twice the median income select city street_name avg income from addresses group by city street_name having avg income 2 median income syntax",
			"category": "docs",
			"url": "../docs/sql/query_syntax/having",
			"blurb": "The  HAVING  clause can be used after the  GROUP BY  clause to provide filter criteria   after  the grouping has been..."
		},
		{
			"title": "GROUP BY Clause",
			"text": "the group by clause specifies which grouping columns should be used to perform any aggregations in the select clause if the group by clause is specified the query is always an aggregate query even if no aggregations are present in the select clause when a group by clause is specified all tuples that have matching data in the grouping columns i e all tuples that belong to the same group will be combined the values of the grouping columns themselves are unchanged and any other columns can be combined using an aggregate function such as count sum avg etc normally the group by clause groups along a single dimension using the grouping sets cube or rollup clauses it is possible to group along multiple dimensions see the grouping sets page for more information examples -- count the number of entries in the addresses table that belong to each different city select city count from addresses group by city -- compute the average income per city per street_name select city street_name avg income from addresses group by city street_name syntax",
			"category": "docs",
			"url": "../docs/sql/query_syntax/groupby",
			"blurb": "The  GROUP BY  clause specifies which grouping columns should be used to perform any aggregations in the  SELECT  clause...."
		},
		{
			"title": "FROM Clause",
			"text": "the from clause specifies the source of the data on which the remainder of the query should operate logically the from clause is where the query starts execution the from clause can contain a single table a combination of multiple tables that are joined together or another select query inside a subquery node examples -- select all columns from the table called table_name select from table_name -- select all columns from the table called table_name in the schema schema_name select from schema_name table_name -- select the column i from the table function range where the first column of the range function is renamed to i select t i from range 100 as t i -- select all columns from the csv file called test csv select from test csv -- select all columns from a subquery select from select from table_name -- join two tables together select from table_name join other_table on table_name key other_table key -- select a 10 sample from a table select from table_name tablesample 10 -- select a sample of 10 rows from a table select from table_name tablesample 10 rows syntax",
			"category": "docs",
			"url": "../docs/sql/query_syntax/from",
			"blurb": "The FROM clause can contain a single table, a combination of multiple tables that are joined together, or another SELECT..."
		},
		{
			"title": "ORDER BY Clause",
			"text": "order by is an output modifier logically it is applied at the very end of the query the order by clause sorts the rows on the sorting criteria in either ascending or descending order in addition every order clause can specify whether null values should be moved to the beginning or to the end by default if no modifiers are provided duckdb sorts asc nulls first i e the values are sorted in ascending order and null values are placed first this is identical to the default sort order of sqlite postgresql by default sorts in asc nulls last order the default sort order can be changed using the following pragma statements -- change the default null sorting order to either nulls first and nulls last pragma default_null_order nulls last -- change the default sorting order to either desc or asc pragma default_order desc text is sorted using the binary comparison collation by default which means values are sorted on their binary utf8 values while this works well for ascii text e g for english language data the sorting order can be incorrect for other languages for this purpose duckdb provides collations for more information on collations see the collation page examples -- select the addresses ordered by city name using the default null order and default order select from addresses order by city -- select the addresses ordered by city name in descending order with nulls at the end select from addresses order by city desc nulls last -- order by city and then by zip code both using the default orderings select from addresses order by city zip -- order by city using german collation rules select from addresses order by city collate de syntax",
			"category": "docs",
			"url": "../docs/sql/query_syntax/orderby",
			"blurb": "ORDER BY  is an output modifier. Logically it is applied at the very end of the query. The  ORDER BY  clause sorts the..."
		},
		{
			"title": "VALUES Clause",
			"text": "the values clause is used to specify a fixed number of rows the values clause can be used as a stand-alone statement as part of the from clause or as input to an insert into statement examples -- generate two rows and directly return them values amsterdam 1 london 2 -- generate two rows as part of a from clause and rename the columns select from values amsterdam 1 london 2 cities name id -- generate two rows and insert them into a table insert into cities values amsterdam 1 london 2 -- create a table directly from a values clause create table cities as select from values amsterdam 1 london 2 cities name id syntax",
			"category": "docs",
			"url": "../docs/sql/query_syntax/values",
			"blurb": "The  VALUES  clause is used to specify a fixed number of rows. The  VALUES  clause can be used as a stand-alone..."
		},
		{
			"title": "GROUPING SETS",
			"text": "grouping sets rollup and cube can be used in the group by clause to perform a grouping over multiple dimensions within the same query examples -- compute the average income along the provided four different dimensions -- signifies the empty set i e computing an ungrouped aggregate select city street_name avg income from addresses group by grouping sets city street_name city street_name -- compute the average income along the same dimensions select city street_name avg income from addresses group by cube city street_name -- compute the average income along the dimensions city street_name city and select city street_name avg income from addresses group by rollup city street_name description grouping sets perform the same aggregate across different group by clauses in a single query create table students course varchar type varchar insert into students course type values cs bachelor cs bachelor cs phd math masters cs null cs null math null select course type count from students group by grouping sets course type course type course type count_star cs bachelor 2 cs phd 1 math masters 1 cs null 2 math null 1 cs null 5 math null 2 null bachelor 2 null phd 1 null masters 1 null null 3 null null 7 in the above query we group across four different sets course type course type and the empty group the result contains null for a group which is not in the grouping set for the result i e the above query is equivalent to the following union statement -- group by course type select course type count from students group by course type union all -- group by type select null as course type count from students group by type union all -- group by course select course null as type count from students group by course union all -- group by nothing select null as course null as type count from students cube and rollup are syntactic sugar to easily produce commonly used grouping sets the rollup clause will produce all sub-groups of a grouping set e g rollup country city zip produces the grouping sets country city zip country city country this can be useful for producing different levels of detail of a group by clause this produces n 1 grouping sets where n is the amount of terms in the rollup clause cube produces grouping sets for all combinations of the inputs e g cube country city zip will produce country city zip country city country zip city zip country city zip this produces 2 n grouping sets grouping alias grouping_id is a special aggregate function that can be used in combination with grouping sets the grouping function takes as parameters a group and returns 0 if the group is included in the grouping for that row or 1 otherwise this is primarily useful because the grouping columns by which we do not aggregate return null which is ambiguous with groups that are actually the value null the grouping or grouping_id function can be used to distinguish these two cases syntax",
			"category": "docs",
			"url": "../docs/sql/query_syntax/grouping_sets",
			"blurb": "GROUPING SETS ,  ROLLUP  and  CUBE  can be used in the  GROUP BY  clause to perform a grouping over multiple dimensions..."
		},
		{
			"title": "WINDOW Clause",
			"text": "the window clause allows you to specify named windows that can be used within window functions these are useful when you have multiple window functions as they allow you to avoid repeating the same window clause syntax",
			"category": "docs",
			"url": "../docs/sql/query_syntax/window",
			"blurb": "The  WINDOW  clause allows you to specify named windows that can be used within window functions. These are useful when..."
		},
		{
			"title": "WITH Clause",
			"text": "the with clause allows you to specify common table expressions ctes regular non-recursive common-table-expressions are essentially views that are limited in scope to a particular query ctes can reference each-other examples -- create a cte called cte and use it in the main query with cte as select 42 select from cte -- create two ctes where the second cte references the first cte with cte as select 42 as i cte2 as select i 100 from cte select from cte2 common table expressions",
			"category": "docs",
			"url": "../docs/sql/query_syntax/with",
			"blurb": "The  WITH  clause allows you to specify common table expressions (CTEs). Regular (non-recursive) common-table-expressions..."
		},
		{
			"title": "LIMIT Clause",
			"text": "limit is an output modifier logically it is applied at the very end of the query the limit clause restricts the amount of rows fetched the offset clause indicates at which position to start reading the values i e the first offset values are ignored note that while limit can be used without an order by clause the results might not be deterministic without the order by clause this can still be useful however for example when you want to inspect a quick snapshot of the data examples -- select the first 5 rows from the addresses table select from addresses limit 5 -- select the 5 rows from the addresses table starting at position 5 i e ignoring the first 5 rows select from addresses limit 5 offset 5 -- select the top 5 cities with the highest population select city count as population from addresses group by city order by population desc limit 5 syntax",
			"category": "docs",
			"url": "../docs/sql/query_syntax/limit",
			"blurb": "LIMIT  is an output modifier. Logically it is applied at the very end of the query. The  LIMIT  clause restricts the..."
		},
		{
			"title": "WHERE Clause",
			"text": "the where clause specifies any filters to apply to the data this allows you to select only a subset of the data in which you are interested logically the where clause is applied immediately after the from clause examples -- select all rows that have id equal to 3 select from table_name where id 3 -- select all rows that match the given case-insensitive like expression select from table_name where name ilike mark -- select all rows that match the given composite expression select from table_name where id 3 or id 7 syntax",
			"category": "docs",
			"url": "../docs/sql/query_syntax/where",
			"blurb": "The  WHERE  clause specifies any filters to apply to the data. This allows you to select only a subset of the data in..."
		},
		{
			"title": "Window Functions",
			"text": "window functions can only be used in the select clause to share over specifications between functions use the statement s window clause and use the over window-name syntax general-purpose window functions the table below shows the available general window functions function return type description example --- --- --- --- row_number bigint the number of the current row within the partition counting from 1 row_number rank bigint the rank of the current row with gaps same as row_number of its first peer rank dense_rank bigint the rank of the current row without gaps this function counts peer groups dense_rank percent_rank double the relative rank of the current row rank - 1 total partition rows - 1 percent_rank cume_dist double the cumulative distribution number of partition rows preceding or peer with current row total partition rows cume_dist ntile num_buckets integer bigint an integer ranging from 1 to the argument value dividing the partition as equally as possible ntile 4 lag expr any offset integer default any same type as expr returns expr evaluated at the row that is offset rows before the current row within the partition if there is no such row instead return default which must be of the same type as expr both offset and default are evaluated with respect to the current row if omitted offset defaults to 1 and default to null lag column 3 0 lead expr any offset integer default any same type as expr returns expr evaluated at the row that is offset rows after the current row within the partition if there is no such row instead return default which must be of the same type as expr both offset and default are evaluated with respect to the current row if omitted offset defaults to 1 and default to null lead column 3 0 first_value expr any same type as expr returns expr evaluated at the row that is the first row of the window frame first_value column last_value expr any same type as expr returns expr evaluated at the row that is the last row of the window frame last_value column nth_value expr any nth integer same type as expr returns expr evaluated at the nth row of the window frame counting from 1 null if no such row nth_value column 2 aggregate window functions all aggregate functions can be used in a windowing context evaluation windowing works by breaking a relation up into independent partitions ordering those partitions and then computing a new column for each row as a function the nearby values some window functions depend only on the partition boundary and the ordering but a few including all the aggregates also use a frame frames are specified as a number of rows on either side preceding or following of the current row the distance can either be specified as a number of rows or a range of values using the partition s ordering value and a distance the full syntax is shown in the diagram at the top of the page and this diagram visually illustrates computation environment partition and ordering partitioning breaks the relation up into independent unrelated pieces partitioning is optional and if none is specified then the entire relation is treated as a single partition window functions cannot access values outside of the partition containing the row they are being evaluated at ordering is also optional but without it the results are not well-defined each partition is ordered using the same ordering clause here is a table of power generation data after partitioning by plant and ordering by date it will have this layout plant date mwh --- --- --- boston 2019-01-02 564337 boston 2019-01-03 507405 boston 2019-01-04 528523 boston 2019-01-05 469538 boston 2019-01-06 474163 boston 2019-01-07 507213 boston 2019-01-08 613040 boston 2019-01-09 582588 boston 2019-01-10 499506 boston 2019-01-11 482014 boston 2019-01-12 486134 boston 2019-01-13 531518 worcester 2019-01-02 118860 worcester 2019-01-03 101977 worcester 2019-01-04 106054 worcester 2019-01-05 92182 worcester 2019-01-06 94492 worcester 2019-01-07 99932 worcester 2019-01-08 118854 worcester 2019-01-09 113506 worcester 2019-01-10 96644 worcester 2019-01-11 93806 worcester 2019-01-12 98963 worcester 2019-01-13 107170 in what follows we shall use this table or small sections of it to illustrate various pieces of window function evaluation the simplest window function is row_number this function just computes the 1-based row number within the partition using the query select plant date row_number as row from history order by 1 2 the result will be plant date row --- --- --- boston 2019-01-02 1 boston 2019-01-03 2 boston 2019-01-04 3 worcester 2019-01-02 1 worcester 2019-01-03 2 worcester 2019-01-04 3 note that even though the function is computed with an order by clause the result does not have to be sorted so the select also needs to be explicitly sorted if that is desired framing framing specifies a set of rows relative to each row where the function is evaluated the distance from the current row is given as an expression either preceding or following the current row this distance can either be specified as an integral number of rows or as a range delta expression from the value of the ordering expression for a range specification there must be only one ordering expression and it has to support addition and subtraction i e numbers or interval s the default values for frames are from unbounded preceding to current row it is invalid for a frame to start after it ends row framing here is a simple row frame query using an aggregate function select points sum points over rows between 1 preceding and 1 following we from results this query computes the sum of each point and the points on either side of it notice that at the edge of the partition there are only two values added together this is because frames are cropped to the edge of the partition range framing returning to the power data suppose the data is noisy we might want to compute a 7 day moving average for each plant to smooth out the noise to do this we can use this window query select plant date avg mwh over partition by plant order by date asc range between interval 3 days preceding and interval 3 days following as mwh 7-day moving average from generation history order by 1 2 this query partitions the data by plant to keep the different power plants data separate orders each plant s partition by date to put the energy measurements next to each other and uses a range frame of three days on either side of each day for the avg to handle any missing days this is the result plant date mwh 7-day br moving average --- --- --- boston 2019-01-02 517450 75 boston 2019-01-03 508793 20 boston 2019-01-04 508529 83 boston 2019-01-13 499793 00 worcester 2019-01-02 104768 25 worcester 2019-01-03 102713 00 worcester 2019-01-04 102249 50 window clauses multiple different over clauses can be specified in the same select and each will be computed separately often however we want to use the same layout for multiple window functions the window clause can be used to define a named window that can be shared between multiple window functions select plant date min mwh over seven as mwh 7-day moving minimum avg mwh over seven as mwh 7-day moving average max mwh over seven as mwh 7-day moving maximum from generation history window seven as partition by plant order by date asc range between interval 3 days preceding and interval 3 days following order by 1 2 the three window functions will also share the data layout which will improve performance box and whisker queries all aggregates can be used as windowing functions including the complex statistical functions these function implementations have been optimised for windowing and we can use the window syntax to write queries that generate the data for moving box-and-whisker plots select plant date min mwh over seven as mwh 7-day moving minimum quantile_cont mwh 0 25 0 5 0 75 over seven as mwh 7-day moving iqr max mwh over seven as mwh 7-day moving maximum from generation history window seven as partition by plant order by date asc range between interval 3 days preceding and interval 3 days following order by 1 2",
			"category": "docs",
			"url": "../docs/sql/window_functions",
			"blurb": "Window functions can only be used in the  SELECT  clause. To share  OVER  specifications between functions, use the..."
		}
	]
}
