{
	"data": [
		{
			"title": "!",
			"text": "! - factorial of x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "! - factorial of x"
		},
		{
			"title": "\"ODBC 101: A Duck Themed Guide to ODBC\"",
			"text": "what is odbc general concepts setting up an application sample application what is odbc odbc which stands for open database connectivity is a standard that allows different programs to talk to different databases including of course duckdb this makes it easier to build programs that work with many different databases which saves time as developers don t have to write custom code to connect to each database instead they can use the standardized odbc interface which reduces development time and costs and programs are easier to maintain however odbc can be slower than other methods of connecting to a database such as using a native driver as it adds an extra layer of abstraction between the application and the database furthermore because duckdb is column-based and odbc is row-based there can be some inefficiencies when using odbc with duckdb there are links throughout this page to the official microsoft odbc documentation which is a great resource for learning more about odbc general concepts handles connecting error handling and diagnostics buffers and binding handles a handle is a pointer to a specific odbc object which is used to interact with the database there are several different types of handles each with a different purpose these are the environment handle the connection handle the statement handle and the descriptor handle handles are allocated using the sqlallochandle which takes as input the type of handle to allocate and a pointer to the handle the driver then creates a new handle of the specified type which it returns to the application handle types handle type description use case additional information --- --- ----- ----- ----- environment sql_handle_env manages the environment settings for odbc operations and provides a global context in which to access data initializing odbc managing driver behavior resource allocation must be allocated once per application upon starting and freed at the end connection sql_handle_dbc represents a connection to a data source used to establish manage and terminate connections defines both the driver and the data source to use within the driver establishing a connection to a database managing the connection state multiple connection handles can be created as needed allowing simultaneous connections to multiple data sources note allocating a connection handle does not establish a connection but must be allocated first and then used once the connection has been established statement sql_handle_stmt handles the execution of sql statements as well as the returned result sets executing sql queries fetching result sets managing statement options to facilitate the execution of concurrent queries multiple handles can be allocated per connection descriptor sql_handle_desc describes the attributes of a data structure or parameter and allows the application to specify the structure of data to be bound retrieved describing table structures result sets binding columns to application buffers used in situations where data structures need to be explicitly defined for example during parameter binding or result set fetching they are automatically allocated when a statement is allocated but can also be allocated explicitly connecting the first step is to connect to the data source so that the application can perform database operations first the application must allocate an environment handle and then a connection handle the connection handle is then used to connect to the data source there are two functions which can be used to connect to a data source sqldriverconnect and sqlconnect the former is used to connect to a data source using a connection string while the latter is used to connect to a data source using a dsn connection string a connection string is a string which contains the information needed to connect to a data source it is formatted as a semicolon separated list of key-value pairs however duckdb currently only utilizes the dsn and ignores the rest of the parameters dsn a dsn data source name is a string that identifies a database it can be a file path url or a database name for example c users me duckdb db and duckdb are both valid dsns more information on dsns can be found on the choosing a data source or driver page of the sql server documentation error handling and diagnostics all functions in odbc return a code which represents the success or failure of the function this allows for easy error handling as the application can simply check the return code of each function call to determine if it was successful when unsuccessful the application can then use the sqlgetdiagrec function to retrieve the error information the following table defines the return codes return code description ---------- --------------------- sql_success the function completed successfully sql_success_with_info the function completed successfully but additional information is available including a warning sql_error the function failed sql_invalid_handle the handle provided was invalid indicating a programming error i e when a handle is not allocated before it is used or is the wrong type sql_no_data the function completed successfully but no more data is available sql_need_data more data is needed such as when a parameter data is sent at execution time or additional connection information is required sql_still_executing a function that was asynchronously executed is still executing buffers and binding a buffer is a block of memory used to store data buffers are used to store data retrieved from the database or to send data to the database buffers are allocated by the application and then bound to a column in a result set or a parameter in a query using the sqlbindcol and sqlbindparameter functions when the application fetches a row from the result set or executes a query the data is stored in the buffer when the application sends a query to the database the data in the buffer is sent to the database setting up an application the following is a step-by-step guide to setting up an application that uses odbc to connect to a database execute a query and fetch the results in c to install the driver as well as anything else you will need follow these instructions what is odbc general concepts handles handle types connecting connection string dsn error handling and diagnostics buffers and binding setting up an application 1 include the sql header files 2 define the odbc handles and connect to the database 3 adding a query 4 fetching results 5 go wild 6 free the handles and disconnecting sample application sample cpp file sample cmakelists txt file 1 include the sql header files the first step is to include the sql header files include sql h include sqlext h these files contain the definitions of the odbc functions as well as the data types used by odbc in order to be able to use these header files you have to have the unixodbc package installed brew install unixodbc or sudo apt-get install unixodbc-dev or sudo yum install unixodbc-devel remember to include the header file location in your cflags for makefile cflags -i usr local include or cflags - opt homebrew cellar unixodbc 2 3 11 include for cmake include_directories usr local include or include_directories opt homebrew cellar unixodbc 2 3 11 include you also have to link the library in your cmake or makefile for cmake target_link_libraries odbc_application path to duckdb_odbc libduckdb_odbc dylib for makefile ldlibs -l path to duckdb_odbc libduckdb_odbc dylib 2 define the odbc handles and connect to the database then set up the odbc handles allocate them and connect to the database first the environment handle is allocated then the environment is set to odbc version 3 then the connection handle is allocated and finally the connection is made to the database the following code snippet shows how to do this sqlhandle env sqlhandle dbc sqlallochandle sql_handle_env sql_null_handle env sqlsetenvattr env sql_attr_odbc_version void sql_ov_odbc3 0 sqlallochandle sql_handle_dbc env dbc std string dsn dsn duckdbmemory sqlconnect dbc sqlchar dsn c_str sql_nts null 0 null 0 std cout connected std endl 3 adding a query now that the application is set up we can add a query to it first we need to allocate a statement handle sqlhandle stmt sqlallochandle sql_handle_stmt dbc stmt then we can execute a query sqlexecdirect stmt sqlchar select from integers sql_nts 4 fetching results now that we have executed a query we can fetch the results first we need to bind the columns in the result set to buffers sqllen int_val sqllen null_val sqlbindcol stmt 1 sql_c_slong int_val 0 null_val then we can fetch the results sqlfetch stmt 5 go wild now that we have the results we can do whatever we want with them for example we can print them std cout value int_val std endl or do any other processing we want as well as executing more queries and doing any thing else we want to do with the database such as inserting updating or deleting data 6 free the handles and disconnecting finally we need to free the handles and disconnect from the database first we need to free the statement handle sqlfreehandle sql_handle_stmt stmt then we need to disconnect from the database sqldisconnect dbc and finally we need to free the connection handle and the environment handle sqlfreehandle sql_handle_dbc dbc sqlfreehandle sql_handle_env env freeing the connection and environment handles can only be done after the connection to the database has been closed trying to free them before disconnecting from the database will result in an error sample application the following is a sample application that includes a cpp file that connects to the database executes a query fetches the results and prints them it also disconnects from the database and frees the handles and includes a function to check the return value of odbc functions it also includes a cmakelists txt file that can be used to build the application sample cpp file include iostream include sql h include sqlext h void check_ret sqlreturn ret std string msg if ret sql_success ret sql_success_with_info std cout ret msg failed std endl exit 1 if ret sql_success_with_info std cout ret msg succeeded with info std endl int main sqlhandle env sqlhandle dbc sqlreturn ret ret sqlallochandle sql_handle_env sql_null_handle env check_ret ret sqlallochandle env ret sqlsetenvattr env sql_attr_odbc_version void sql_ov_odbc3 0 check_ret ret sqlsetenvattr ret sqlallochandle sql_handle_dbc env dbc check_ret ret sqlallochandle dbc std string dsn dsn duckdbmemory ret sqlconnect dbc sqlchar dsn c_str sql_nts null 0 null 0 check_ret ret sqlconnect std cout connected std endl sqlhandle stmt ret sqlallochandle sql_handle_stmt dbc stmt check_ret ret sqlallochandle stmt ret sqlexecdirect stmt sqlchar select from integers sql_nts check_ret ret sqlexecdirect select from integers sqllen int_val sqllen null_val ret sqlbindcol stmt 1 sql_c_slong int_val 0 null_val check_ret ret sqlbindcol ret sqlfetch stmt check_ret ret sqlfetch std cout value int_val std endl ret sqlfreehandle sql_handle_stmt stmt check_ret ret sqlfreehandle stmt ret sqldisconnect dbc check_ret ret sqldisconnect ret sqlfreehandle sql_handle_dbc dbc check_ret ret sqlfreehandle dbc ret sqlfreehandle sql_handle_env env check_ret ret sqlfreehandle env sample cmakelists txt file cmake_minimum_required version 3 25 project odbc_tester_app set cmake_cxx_standard 17 include_directories opt homebrew cellar unixodbc 2 3 11 include add_executable odbc_tester_app main cpp target_link_libraries odbc_tester_app duckdb_odbc libduckdb_odbc dylib",
			"category": "Odbc",
			"url": "/docs/guides/odbc/general",
			"blurb": "What is ODBC? General Concepts Setting up an Application Sample Application What is ODBC? ODBC which stands for Open..."
		},
		{
			"title": "%",
			"text": "% - modulo (remainder)",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "% - modulo (remainder)"
		},
		{
			"title": "&",
			"text": "& - bitwise and",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "& - bitwise AND"
		},
		{
			"title": "*",
			"text": "* - multiplication",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "* - multiplication"
		},
		{
			"title": "**",
			"text": "** - exponent",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "** - exponent"
		},
		{
			"title": "+",
			"text": "+ - addition of an interval",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "+ - addition of an INTERVAL"
		},
		{
			"title": "-",
			"text": "- - subtraction of an interval",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "- - subtraction of an INTERVAL"
		},
		{
			"title": "/",
			"text": "/ - float division",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "/ - float division"
		},
		{
			"title": "//",
			"text": "// - division",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "// - division"
		},
		{
			"title": ":-",
			"text": ":- - :--",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": ":- - :--"
		},
		{
			"title": "<<",
			"text": "<< - bitwise shift left",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "<< - bitwise shift left"
		},
		{
			"title": ">>",
			"text": ">> - bitwise shift right",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": ">> - bitwise shift right"
		},
		{
			"title": "@",
			"text": "@ - absolute value (parentheses optional if operating on a column)",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "@ - Absolute value (parentheses optional if operating on a column)"
		},
		{
			"title": "ADBC API",
			"text": "arrow database connectivity adbc similarly to odbc and jdbc is a c-style api that enables code portability between different database systems this allows developers to effortlessly build applications that communicate with database systems without using code specific to that system the main difference between adbc and odbc jdbc is that adbc uses arrow to transfer data between the database system and the application duckdb has an adbc driver which takes advantage of the zero-copy integration between duckdb and arrow to efficiently transfer data duckdb s adbc driver currently supports version 0 5 1 of adbc please refer to the adbc documentation page for a more extensive discussion on adbc and a detailed api explanation implemented functionality the duckdb-adbc driver implements the full adbc specification with the exception of the connectionreadpartition and statementexecutepartitions functions both of these functions exist to support systems that internally partition the query results which does not apply to duckdb in this section we will describe the main functions that exist in adbc along with the arguments they take and provide examples for each function database set of functions that operate on a database function name description arguments example --- - --- ---- databasenew allocate a new but uninitialized database adbcdatabase database adbcerror error adbcdatabasenew adbc_database adbc_error databasesetoption set a char option adbcdatabase database const char key const char value adbcerror error adbcdatabasesetoption adbc_database path test db adbc_error databaseinit finish setting options and initialize the database adbcdatabase database adbcerror error adbcdatabaseinit adbc_database adbc_error databaserelease destroy the database adbcdatabase database adbcerror error adbcdatabaserelease adbc_database adbc_error connection a set of functions that create and destroy a connection to interact with a database function name description arguments example --- - --- ---- connectionnew allocate a new but uninitialized connection adbcconnection adbcerror adbcconnectionnew adbc_connection adbc_error connectionsetoption options may be set before connectioninit adbcconnection const char const char adbcerror adbcconnectionsetoption adbc_connection adbc_connection_option_autocommit adbc_option_value_disabled adbc_error connectioninit finish setting options and initialize the connection adbcconnection adbcdatabase adbcerror adbcconnectioninit adbc_connection adbc_database adbc_error connectionrelease destroy this connection adbcconnection adbcerror adbcconnectionrelease adbc_connection adbc_error a set of functions that retrieve metadata about the database in general these functions will return arrow objects specifically an arrowarraystream function name description arguments example --- - --- ---- connectiongetobjects get a hierarchical view of all catalogs database schemas tables and columns adbcconnection int const char const char const char const char const char arrowarraystream adbcerror adbcdatabaseinit adbc_database adbc_error connectiongettableschema get the arrow schema of a table adbcconnection const char const char const char arrowschema adbcerror adbcdatabaserelease adbc_database adbc_error connectiongettabletypes get a list of table types in the database adbcconnection arrowarraystream adbcerror adbcdatabasenew adbc_database adbc_error a set of functions with transaction semantics for the connection by default all connections start with auto-commit mode on but this can be turned off via the connectionsetoption function function name description arguments example --- - --- ---- connectioncommit commit any pending transactions adbcconnection adbcerror adbcconnectioncommit adbc_connection adbc_error connectionrollback rollback any pending transactions adbcconnection adbcerror adbcconnectionrollback adbc_connection adbc_error statement statements hold state related to query execution they represent both one-off queries and prepared statements they can be reused however doing so will invalidate prior result sets from that statement the functions used to create destroy and set options for a statement function name description arguments example --- - --- ---- statementnew create a new statement for a given connection adbcconnection adbcstatement adbcerror adbcstatementnew adbc_connection adbc_statement adbc_error statementrelease destroy a statement adbcstatement adbcerror adbcstatementrelease adbc_statement adbc_error statementsetoption set a string option on a statement adbcstatement const char const char adbcerror statementsetoption adbc_statement adbc_ingest_option_target_table table_name adbc_error functions related to query execution function name description arguments example --- - --- ---- statementsetsqlquery set the sql query to execute the query can then be executed with statementexecutequery adbcstatement const char adbcerror adbcstatementsetsqlquery adbc_statement select from table adbc_error statementsetsubstraitplan set a substrait plan to execute the query can then be executed with statementexecutequery adbcstatement const uint8_t size_t adbcerror adbcstatementsetsubstraitplan adbc_statement substrait_plan length adbc_error statementexecutequery execute a statement and get the results adbcstatement arrowarraystream int64_t adbcerror adbcstatementexecutequery adbc_statement arrow_stream rows_affected adbc_error statementprepare turn this statement into a prepared statement to be executed multiple times adbcstatement adbcerror adbcstatementprepare adbc_statement adbc_error functions related to binding used for bulk insertion or in prepared statements function name description arguments example --- - --- ---- statementbindstream bind arrow stream this can be used for bulk inserts or prepared statements adbcstatement arrowarraystream adbcerror statementbindstream adbc_statement input_data adbc_error examples regardless of the programming language being used there are two database options which will be required to utilize adbc with duckdb the first one is the driver which takes a path to the duckdb library the second option is the entrypoint which is an exported function from the duckdb-adbc driver that initializes all the adbc functions once we have configured these two options we can optionally set the path option providing a path on disk to store our duckdb database if not set an in-memory database is created after configuring all the necessary options we can proceed to initialize our database below is how you can do so with various different language environments c we begin our c example by declaring the essential variables for querying data through adbc these variables include error database connection statement handling and an arrow stream to transfer data between duckdb and the application adbcerror adbc_error adbcdatabase adbc_database adbcconnection adbc_connection adbcstatement adbc_statement arrowarraystream arrow_stream we can then initialize our database variable before initializing the database we need to set the driver and entrypoint options as mentioned above then we set the path option and initialize the database with the example below the string path to libduckdb dylib should be the path to the dynamic library for duckdb this will be dylib on macos and so on linux adbcdatabasenew adbc_database adbc_error adbcdatabasesetoption adbc_database driver path to libduckdb dylib adbc_error adbcdatabasesetoption adbc_database entrypoint duckdb_adbc_init adbc_error by default we start an in-memory database but you can optionally define a path to store it on disk adbcdatabasesetoption adbc_database path test db adbc_error adbcdatabaseinit adbc_database adbc_error after initializing the database we must create and initialize a connection to it adbcconnectionnew adbc_connection adbc_error adbcconnectioninit adbc_connection adbc_database adbc_error we can now initialize our statement and run queries through our connection after the adbcstatementexecutequery the arrow_stream is populated with the result adbcstatementnew adbc_connection adbc_statement adbc_error adbcstatementsetsqlquery adbc_statement select 42 adbc_error int64_t rows_affected adbcstatementexecutequery adbc_statement arrow_stream rows_affected adbc_error arrow_stream release arrow_stream besides running queries we can also ingest data via arrow_streams for this we need to set an option with the table name we want to insert to bind the stream and then execute the query statementsetoption adbc_statement adbc_ingest_option_target_table answertoeverything adbc_error statementbindstream adbc_statement arrow_stream adbc_error statementexecutequery adbc_statement nullptr nullptr adbc_error python the first thing to do is to use pip and install the adbc driver manager you will also need to install the pyarrow to directly access apache arrow formatted result sets such as using fetch_arrow_table pip install adbc_driver_manager pyarrow for details on the adbc_driver_manager package see the adbc_driver_manager package documentation as with c we need to provide initialization options consisting of the location of the libduckdb shared object and entrypoint function notice that the path argument for duckdb is passed in through the db_kwargs dictionary import adbc_driver_duckdb dbapi with adbc_driver_duckdb dbapi connect test db as conn conn cursor as cur cur execute select 42 fetch a pyarrow table tbl cur fetch_arrow_table print tbl alongside fetch_arrow_table other methods from dbapi are also implemented on the cursor such as fetchone and fetchall data can also be ingested via arrow_streams we just need to set options on the statement to bind the stream of data and execute the query import adbc_driver_duckdb dbapi import pyarrow data pyarrow record_batch 1 2 3 4 a b c d names ints strs with adbc_driver_duckdb dbapi connect test db as conn conn cursor as cur cur adbc_ingest answertoeverything data",
			"category": "Api",
			"url": "/docs/api/adbc",
			"blurb": "Arrow Database Connectivity (ADBC) , similarly to ODBC and JDBC, is a C-style API that enables code portability..."
		},
		{
			"title": "ALTER TABLE Statement",
			"text": "the alter table statement changes the schema of an existing table in the catalog examples -- add a new column with name k to the table integers it will be filled with the default value null alter table integers add column k integer -- add a new column with name l to the table integers it will be filled with the default value 10 alter table integers add column l integer default 10 -- drop the column k from the table integers alter table integers drop k -- change the type of the column i to the type varchar using a standard cast alter table integers alter i type varchar -- change the type of the column i to the type varchar using the specified expression to convert the data for each row alter table integers alter i set data type varchar using concat i _ j -- set the default value of a column alter table integers alter column i set default 10 -- drop the default value of a column alter table integers alter column i drop default -- make a column not nullable alter table t alter column x set not null -- drop the not null constraint alter table t alter column x drop not null -- rename a table alter table integers rename to integers_old -- rename a column of a table alter table integers rename i to j syntax alter table changes the schema of an existing table all the changes made by alter table fully respect the transactional semantics i e they will not be visible to other transactions until committed and can be fully reverted through a rollback rename table -- rename a table alter table integers rename to integers_old the rename to clause renames an entire table changing its name in the schema note that any views that rely on the table are not automatically updated rename column -- rename a column of a table alter table integers rename i to j alter table integers rename column j to k the rename column clause renames a single column within a table any constraints that rely on this name e g check constraints are automatically updated however note that any views that rely on this column name are not automatically updated add column -- add a new column with name k to the table integers it will be filled with the default value null alter table integers add column k integer -- add a new column with name l to the table integers it will be filled with the default value 10 alter table integers add column l integer default 10 the add column clause can be used to add a new column of a specified type to a table the new column will be filled with the specified default value or null if none is specified drop column -- drop the column k from the table integers alter table integers drop k the drop column clause can be used to remove a column from a table note that columns can only be removed if they do not have any indexes that rely on them this includes any indexes created as part of a primary key or unique constraint columns that are part of multi-column check constraints cannot be dropped either alter type -- change the type of the column i to the type varchar using a standard cast alter table integers alter i type varchar -- change the type of the column i to the type varchar using the specified expression to convert the data for each row alter table integers alter i set data type varchar using concat i _ j the set data type clause changes the type of a column in a table any data present in the column is converted according to the provided expression in the using clause or if the using clause is absent cast to the new data type note that columns can only have their type changed if they do not have any indexes that rely on them and are not part of any check constraints set drop default -- set the default value of a column alter table integers alter column i set default 10 -- drop the default value of a column alter table integers alter column i drop default the set drop default clause modifies the default value of an existing column note that this does not modify any existing data in the column dropping the default is equivalent to setting the default value to null at the moment duckdb will not allow you to alter a table if there are any dependencies that means that if you have an index on a column you will first need to drop the index alter the table and then recreate the index otherwise you will get a dependency error add drop constraint the add constraint and drop constraint clauses are not yet supported in duckdb",
			"category": "Statements",
			"url": "/docs/sql/statements/alter_table",
			"blurb": "The ALTER TABLE statement changes the schema of an existing table in the catalog. Examples -- add a new column with..."
		},
		{
			"title": "ALTER VIEW Statement",
			"text": "the alter view statement changes the schema of an existing view in the catalog examples -- rename a view alter view v1 rename to v2 alter view changes the schema of an existing table all the changes made by alter view fully respect the transactional semantics i e they will not be visible to other transactions until committed and can be fully reverted through a rollback note that other views that rely on the table are not automatically updated",
			"category": "Statements",
			"url": "/docs/sql/statements/alter_view",
			"blurb": "The ALTER VIEW statement changes the schema of an existing view in the catalog. Examples -- rename a view ALTER VIEW..."
		},
		{
			"title": "ATTACH/DETACH Statement",
			"text": "the attach statement adds a new database file to the catalog that can be read from and written to examples -- attach the database file db with the alias inferred from the name file attach file db -- attach the database file db with an explicit alias file_db attach file db as file_db -- attach the database file db in read only mode attach file db read_only -- attach a sqlite database for reading and writing see sqlite extension for more information attach sqlite_file db as sqlite type sqlite -- attach the database file db if inferred database alias file_db does not yet exist attach if not exists file db -- attach the database file db if explicit database alias file_db does not yet exist attach if not exists file db as file_db -- create a table in the attached database with alias file create table file new_table i integer -- detach the database with alias file detach file -- show a list of all attached databases show databases -- change the default database that is used to the database file use file syntax attach allows duckdb to operate on multiple database files and allows for transfer of data between different database files detach the detach statement allows previously attached database files to be closed and detached releasing any locks held on the database file it is not possible to detach from the default database if you would like to do so issue the use statement to change the default database to another one closing the connection e g invoking the close function in python does not release the locks held on the database files as the file handles are held by the main duckdb instance in python s case the duckdb module syntax name qualification the fully qualified name of catalog objects contains the catalog the schema and the name of the object for example -- attach the database new_db attach new_db db -- create the schema my_schema in the database new_db create schema new_db my_schema -- create the table my_table in the schema my_schema create table new_db my_schema my_table col integer -- refer to the column col inside the table my_table select new_db my_schema my_table col from new_db my_schema my_table note that often the fully qualified name is not required when a name is not fully qualified the system looks for which entries to reference using the catalog search path the default catalog search path includes the system catalog the temporary catalog and the initially attached database together with the main schema also note the rules on identifiers and database names in particular default database and schema when a table is created without any qualifications the table is created in the default schema of the default database the default database is the database that is launched when the system is created - and the default schema is main -- create the table my_table in the default database create table my_table col integer changing the default database and schema the default database and schema can be changed using the use command -- set the default database schema to new_db main use new_db -- set the default database schema to new_db my_schema use new_db my_schema resolving conflicts when providing only a single qualification the system can interpret this as either a catalog or a schema as long as there are no conflicts for example attach new_db db create schema my_schema -- creates the table new_db main tbl create table new_db tbl i integer -- creates the table default_db my_schema tbl create table my_schema tbl i integer if we create a conflict i e we have both a schema and a catalog with the same name the system requests that a fully qualified path is used instead create schema new_db create table new_db tbl i integer -- error binder error ambiguous reference to catalog or schema new_db - use a fully qualified path like memory new_db changing the catalog search path the catalog search path can be adjusted by setting the search_path configuration option which uses a comma-separated list of values that will be on the search path the following example demonstrates searching in two databases attach memory as db1 attach memory as db2 create table db1 tbl1 i integer create table db2 tbl2 j integer -- reference the tables using their fully qualified name select from db1 tbl1 select from db2 tbl2 -- or set the search path and reference the tables using their name set search_path db1 db2 select from tbl1 select from tbl2 transactional semantics when running queries on multiple databases the system opens separate transactions per database the transactions are started lazily by default - when a given database is referenced for the first time in a query a transaction for that database will be started set immediate_transaction_mode true can be toggled to change this behavior to eagerly start transactions in all attached databases instead while multiple transactions can be active at a time - the system only supports writing to a single attached database in a single transaction if you try to write to multiple attached databases in a single transaction the following error will be thrown attempting to write to database db2 in a transaction that has already modified database db1 - a single transaction can only write to a single attached database the reason for this restriction is that the system does not maintain atomicity for transactions across attached databases transactions are only atomic within each database file by restricting the global transaction to write to only a single database file the atomicity guarantees are maintained",
			"category": "Statements",
			"url": "/docs/sql/statements/attach",
			"blurb": "The ATTACH statement adds a new database file to the catalog that can be read from and written to. Examples -- attach..."
		},
		{
			"title": "AWS Extension",
			"text": "the aws extension adds functionality e g authentication on top of the httpfs extension s s3 capabilities using the aws sdk installing and loading to install and load the aws extension run install aws load aws usage see the httpfs extension s s3 capabilities for instructions github repository span class github github span",
			"category": "Extensions",
			"url": "/docs/extensions/aws",
			"blurb": "The aws extension adds functionality (e.g., authentication) on top of the httpfs extension's S3 capabilities , using..."
		},
		{
			"title": "Aggregate Functions",
			"text": "examples -- produce a single row containing the sum of the amount column select sum amount from sales -- produce one row per unique region containing the sum of amount for each group select region sum amount from sales group by region -- return only the regions that have a sum of amount higher than 100 select region from sales group by region having sum amount 100 -- return the number of unique values in the region column select count distinct region from sales -- return two values the total sum of amount and the sum of amount minus columns where the region is north select sum amount sum amount filter region north from sales -- returns a list of all regions in order of the amount column select list region order by amount desc from sales -- returns the amount of the first sale using the first aggregate function select first amount order by date asc from sales syntax aggregates are functions that combine multiple rows into a single value aggregates are different from scalar functions and window functions because they change the cardinality of the result as such aggregates can only be used in the select and having clauses of a sql query distinct clause in aggregate functions when the distinct clause is provided only distinct values are considered in the computation of the aggregate this is typically used in combination with the count aggregate to get the number of distinct elements but it can be used together with any aggregate function in the system order by clause in aggregate functions an order by clause can be provided after the last argument of the function call note the lack of the comma separator before the clause select aggregate_function arg sep order by ordering_criteria this clause ensures that the values being aggregated are sorted before applying the function most aggregate functions are order-insensitive therefore this clause is parsed and applied which is inefficient but has on effect on the results however there are some order-sensitive aggregates that can have non-deterministic results without ordering e g first last list and string_agg group_concat listagg these can be made deterministic by ordering the arguments for example create table tbl as select s from range 1 4 r s select string_agg s order by s desc as countdown from tbl countdown varchar 3 2 1 general aggregate functions the table below shows the available general aggregate functions function description example alias es -- --- -- -- any_value arg returns the first non-null value from arg this function is affected by ordering any_value a arbitrary a first a arbitrary arg returns the first non-null value from arg this function is affected by ordering arbitrary a any_value a first a arg_max arg val finds the row with the maximum val calculates the arg expression at that row arg_max a b argmax arg val max_by arg val arg_min arg val finds the row with the minimum val calculates the arg expression at that row arg_min a b argmin arg val min_by arg val avg arg calculates the average value for all tuples in arg avg a mean bit_and arg returns the bitwise and of all bits in a given expression bit_and a - bit_or arg returns the bitwise or of all bits in a given expression bit_or a - bit_xor arg returns the bitwise xor of all bits in a given expression bit_xor a - bitstring_agg arg returns a bitstring with bits set for each distinct value bitstring_agg a - bool_and arg returns true if every input value is true otherwise false bool_and a - bool_or arg returns true if any input value is true otherwise false bool_or a - count arg calculates the number of tuples in arg count a - favg arg calculates the average using a more accurate floating point summation kahan sum favg a - first arg returns the first non-null value of a column this function is affected by ordering first a any_value a arbitrary a fsum arg calculates the sum using a more accurate floating point summation kahan sum fsum a sumkahan kahan_sum geomean arg calculates the geometric mean for all tuples in arg geomean a geometric_mean a histogram arg returns a map of key-value pairs representing buckets and counts histogram a - last arg returns the last value of a column this function is affected by ordering last a - list arg returns a list containing all the values of a column this function is affected by ordering list a array_agg max arg returns the maximum value present in arg max a - min arg returns the minimum value present in arg min a - product arg calculates the product of all tuples in arg product a - string_agg arg sep concatenates the column string values with a separator this function is affected by ordering string_agg s group_concat arg sep listagg arg sep sum arg calculates the sum value for all tuples in arg sum a - sum_no_overflow arg calculates the sum value for all tuples in arg without overflow checks unlike sum which works on floating-point values sum_no_overflow only accepts integer and decimal values sum_no_overflow a - approximate aggregates the table below shows the available approximate aggregate functions function description example --- --- --- approx_count_distinct x gives the approximate count of distinct elements using hyperloglog approx_count_distinct a approx_quantile x pos gives the approximate quantile using t-digest approx_quantile a 0 5 reservoir_quantile x quantile sample_size 8192 gives the approximate quantile using reservoir sampling the sample size is optional and uses 8192 as a default size reservoir_quantile a 0 5 1024 statistical aggregates the table below shows the available statistical aggregate functions function description formula alias -- --- -- - corr y x returns the correlation coefficient for non-null pairs in a group covar_pop y x stddev_pop x stddev_pop y - covar_pop y x returns the population covariance of input values sum x y - sum x sum y count count - covar_samp y x returns the sample covariance for non-null pairs in a group sum x y - sum x sum y count count - 1 - entropy x returns the log-2 entropy of count input-values - - kurtosis_pop x returns the excess kurtosis fisher s definition of all input values bias correction is not applied - - kurtosis x returns the excess kurtosis fisher s definition of all input values with a bias correction according to the sample size - - mad x returns the median absolute deviation for the values within x null values are ignored temporal types return a positive interval median abs x - median x - median x returns the middle value of the set null values are ignored for even value counts quantitative values are averaged and ordinal values return the lower value quantile_cont x 0 5 - mode x returns the most frequent value for the values within x null values are ignored - - quantile_cont x pos returns the interpolated quantile number between 0 and 1 if pos is a list of float s then the result is a list of the corresponding interpolated quantiles - - quantile_disc x pos returns the exact quantile number between 0 and 1 if pos is a list of float s then the result is a list of the corresponding exact quantiles - quantile regr_avgx y x returns the average of the independent variable for non-null pairs in a group where x is the independent variable and y is the dependent variable - - regr_avgy y x returns the average of the dependent variable for non-null pairs in a group where x is the independent variable and y is the dependent variable - - regr_count y x returns the number of non-null number pairs in a group sum x y - sum x sum y count count - regr_intercept y x returns the intercept of the univariate linear regression line for non-null pairs in a group avg y - regr_slope y x avg x - regr_r2 y x returns the coefficient of determination for non-null pairs in a group - - regr_slope y x returns the slope of the linear regression line for non-null pairs in a group covar_pop x y var_pop x - regr_sxx y x - regr_count y x var_pop x - regr_sxy y x returns the population covariance of input values regr_count y x covar_pop y x - regr_syy y x - regr_count y x var_pop y - skewness x returns the skewness of all input values - - stddev_pop x returns the population standard deviation sqrt var_pop x - stddev_samp x returns the sample standard deviation sqrt var_samp x stddev x var_pop x returns the population variance - - var_samp x returns the sample variance of all input values sum x 2 - sum x 2 count x count x - 1 variance arg val ordered set aggregate functions the table below shows the available ordered set aggregate functions these functions are specified using the within group order by sort_expression syntax and they are converted to an equivalent aggregate function that takes the ordering expression as the first argument function equivalent --- --- mode within group order by sort_expression mode sort_expression percentile_cont fraction within group order by sort_expression quantile_cont sort_expression fraction percentile_cont fractions within group order by sort_expression quantile_cont sort_expression fractions percentile_disc fraction within group order by sort_expression quantile_disc sort_expression fraction percentile_disc fractions within group order by sort_expression quantile_disc sort_expression fractions miscellaneous aggregate functions function description alias -- --- -- grouping for queries with group by and either rollup or grouping sets returns an integer identifying which of the argument expressions where used to group on to create the current supper-aggregate row grouping_id",
			"category": "SQL",
			"url": "/docs/sql/aggregates",
			"blurb": "Examples -- produce a single row containing the sum of the amount column SELECT sum(amount) FROM sales; -- produce..."
		},
		{
			"title": "Appender",
			"text": "the appender can be used to load bulk data into a duckdb database it is currently available in the c c go java and rust apis the appender is tied to a connection and will use the transaction context of that connection when appending an appender always appends to a single table in the database file in the c api the appender works as follows duckdb db connection con db create the table con query create table people id integer name varchar initialize the appender appender appender con people the appendrow function is the easiest way of appending data it uses recursive templates to allow you to put all the values of a single row within one function call as follows appender appendrow 1 mark rows can also be individually constructed using the beginrow endrow and append methods this is done internally by appendrow and hence has the same performance characteristics appender beginrow appender append int32_t 2 appender append string hannes appender endrow any values added to the appender are cached prior to being inserted into the database system for performance reasons that means that while appending the rows might not be immediately visible in the system the cache is automatically flushed when the appender goes out of scope or when appender close is called the cache can also be manually flushed using the appender flush method after either flush or close is called all the data has been written to the database system date time and timestamps while numbers and strings are rather self-explanatory dates times and timestamps require some explanation they can be directly appended using the methods provided by duckdb date duckdb time or duckdb timestamp they can also be appended using the internal duckdb value type however this adds some additional overheads and should be avoided if possible below is a short example con query create table dates d date t time ts timestamp appender appender con dates construct the values using the date time timestamp types this is the most efficient approach appender appendrow date fromdate 1992 1 1 time fromtime 1 1 1 0 timestamp fromdatetime date fromdate 1992 1 1 time fromtime 1 1 1 0 construct duckdb value objects appender appendrow value date 1992 1 1 value time 1 1 1 0 value timestamp 1992 1 1 1 1 1 0 handling constraint violations if the appender encounters a primary key conflict or a unique constraint violation it fails and returns the following error constraint error primary key or unique constraint violated duplicate key in this case the entire append operation fails and no rows are inserted appender support in other clients the appender is also available in the following client apis c go jdbc java rust",
			"category": "Data",
			"url": "/docs/data/appender",
			"blurb": "The Appender can be used to load bulk data into a DuckDB database. It is currently available in the C, C++, Go, Java,..."
		},
		{
			"title": "Array Type",
			"text": "an array column stores fixed-sized arrays all fields in the column must have the same length and the same underlying type arrays are typically used to store arrays of numbers but can contain any uniform data type including array list and struct types arrays can be used to store vectors such as word embeddings or image embeddings to store variable-length lists use the list type see the data types overview for a comparison between nested data types the array type in postgresql allows variable-length fields duckdb s array type is fixed-length creating arrays arrays can be created using the array_value expr function -- construct with the array_value function select array_value 1 2 3 -- you can always implicitly cast an array to a list and use list functions like list_extract i select array_value 1 2 3 2 -- you can cast from a list to an array but the dimensions have to match up select 3 2 1 int 3 -- arrays can be nested select array_value array_value 1 2 array_value 3 4 array_value 5 6 -- arrays can store structs select array_value a 1 b 2 a 3 b 4 defining an array field arrays can be created using the type_name length syntax for example to create an array field for 3 integers run create table array_table id int arr int 3 insert into array_table values 10 1 2 3 20 4 5 6 retrieving values from arrays retrieving one or more values from an array can be accomplished using brackets and slicing notation or through list functions like list_extract and array_extract using the example in defining an array field the following queries for extracting the second element of an array are equivalent select id arr 1 as element from array_table select id list_extract arr 1 as element from array_table select id array_extract arr 1 as element from array_table id element int32 int32 10 1 20 4 using the slicing notation returns a list select id arr 1 2 as elements from array_table id elements int32 int32 10 1 2 20 4 5 functions all list functions work with the array type additionally several array -native functions are also supported in the following l1 stands for the 3-element list created by array_value 1 0 float 2 0 float 3 0 float and l2 stands for array_value 2 0 float 3 0 float 4 0 float function description example result ---- ----- ------- --- array_value index create an array containing the argument values array_value 1 0 float 2 0 float 3 0 float 1 0 2 0 3 0 array_cross_product array1 array2 compute the cross product of two arrays of size 3 the array elements can not be null array_cross_product l1 l2 -1 0 2 0 -1 0 array_cosine_similarity array1 array2 compute the cosine similarity between two arrays of the same size the array elements can not be null the arrays can have any size as long as the size is the same for both arguments array_cosine_similarity l1 l2 0 9925833 array_distance array1 array2 compute the distance between two arrays of the same size the array elements can not be null the arrays can have any size as long as the size is the same for both arguments array_distance l1 l2 1 7320508 array_inner_product array1 array2 compute the inner product between two arrays of the same size the array elements can not be null the arrays can have any size as long as the size is the same for both arguments array_inner_product l1 l2 20 0 array_dot_product array1 array2 alias for array_inner_product array1 array2 array_dot_product l1 l2 20 0 examples -- create sample data create table x i int v float 3 create table y i int v float 3 insert into x values 1 array_value 1 0 float 2 0 float 3 0 float insert into y values 1 array_value 2 0 float 3 0 float 4 0 float -- compute cross product select array_cross_product x v y v from x y where x i y i -- compute cosine similarity select array_cosine_similarity x v y v from x y where x i y i ordering the ordering of array instances is defined using a lexicographical order null values compare greater than all other values and are considered equal to each other functions see nested functions",
			"category": "Data Types",
			"url": "/docs/sql/data_types/array",
			"blurb": "An ARRAY column stores fixed-sized arrays. All fields in the column must have the same length and the same underlying..."
		},
		{
			"title": "Arrow Extension",
			"text": "the arrow extension implements provides features for using apache arrow a cross-language development platform for in-memory analytics installing and loading the arrow extension will be transparently autoloaded on first use from the official extension repository if you would like to install and load it manually run install arrow load arrow functions function type description -- ---- ------- to_arrow_ipc table in-out-function serializes a table into a stream of blobs containing arrow ipc buffers scan_arrow_ipc table function scan a list of pointers pointing to arrow ipc buffers github repository span class github github span",
			"category": "Extensions",
			"url": "/docs/extensions/arrow",
			"blurb": "The arrow extension implements provides features for using Apache Arrow , a cross-language development platform for..."
		},
		{
			"title": "AsOf Join",
			"text": "what is an asof join time series data is not always perfectly aligned clocks may be slightly off or there may be a delay between cause and effect this can make connecting two sets of ordered data challenging asof joins are a tool for solving this and other similar problems one of the problems that asof joins are used to solve is finding the value of a varying property at a specific point in time this use case is so common that it is where the name came from give me the value of the property as of this time more generally however asof joins embody some common temporal analytic semantics which can be cumbersome and slow to implement in standard sql portfolio example data set let s start with a concrete example suppose we have a table of stock prices with timestamps ticker when price ----- --- ---- appl 2001-01-01 00 00 00 1 appl 2001-01-01 00 01 00 2 appl 2001-01-01 00 02 00 3 msft 2001-01-01 00 00 00 1 msft 2001-01-01 00 01 00 2 msft 2001-01-01 00 02 00 3 goog 2001-01-01 00 00 00 1 goog 2001-01-01 00 01 00 2 goog 2001-01-01 00 02 00 3 we have another table containing portfolio holdings at various points in time ticker when shares ----- --- ----- appl 2000-12-31 23 59 30 5 16 appl 2001-01-01 00 00 30 2 94 appl 2001-01-01 00 01 30 24 13 goog 2000-12-31 23 59 30 9 33 goog 2001-01-01 00 00 30 23 45 goog 2001-01-01 00 01 30 10 58 data 2000-12-31 23 59 30 6 65 data 2001-01-01 00 00 30 17 95 data 2001-01-01 00 01 30 18 37 to load these tables to duckdb run create table prices as from https duckdb org data prices csv create table holdings as from https duckdb org data holdings csv inner asof joins we can compute the value of each holding at that point in time by finding the most recent price before the holding s timestamp by using an asof join select h ticker h when price shares as value from holdings h asof join prices p on h ticker p ticker and h when p when this attaches the value of the holding at that time to each row ticker when value ----- --- ---- appl 2001-01-01 00 00 30 2 94 appl 2001-01-01 00 01 30 48 26 goog 2001-01-01 00 00 30 23 45 goog 2001-01-01 00 01 30 21 16 it essentially executes a function defined by looking up nearby values in the prices table note also that missing ticker values do not have a match and don t appear in the output outer asof joins because asof produces at most one match from the right hand side the left side table will not grow as a result of the join but it could shrink if there are missing times on the right to handle this situation you can use an outer asof join select h ticker h when price shares as value from holdings h asof left join prices p on h ticker p ticker and h when p when order by all as you might expect this will produce null prices and values instead of dropping left side rows when there is no ticker or the time is before the prices begin ticker when value ----- --- ---- appl 2000-12-31 23 59 30 appl 2001-01-01 00 00 30 2 94 appl 2001-01-01 00 01 30 48 26 goog 2000-12-31 23 59 30 goog 2001-01-01 00 00 30 23 45 goog 2001-01-01 00 01 30 21 16 data 2000-12-31 23 59 30 data 2001-01-01 00 00 30 data 2001-01-01 00 01 30 asof joins with the using keyword so far we have been explicit about specifying the conditions for asof but sql also has a simplified join condition syntax for the common case where the column names are the same in both tables this syntax uses the using keyword to list the fields that should be compared for equality asof also supports this syntax but with two restrictions the last field is the inequality the inequality is the most common case our first query can then be written as select ticker h when price shares as value from holdings h asof join prices p using ticker when be aware that if you don t explicitly list the columns in the select the ordering field value will be the probe value not the build value for a natural join this is not an issue because all the conditions are equalities but for asof one side has to be chosen since asof can be viewed as a lookup function it is more natural to return the function arguments than the function internals see also for implementation details see the blog post duckdb s asof joins fuzzy temporal lookups",
			"category": "Sql Features",
			"url": "/docs/guides/sql_features/asof_join",
			"blurb": "What is an AsOf Join? Time series data is not always perfectly aligned. Clocks may be slightly off, or there may be a..."
		},
		{
			"title": "AutoComplete Extension",
			"text": "the autocomplete extension adds supports for autocomplete in the cli client the extension is shipped by default with the cli client behavior for the behavior of the autocomplete extension see the documentation of the cli client functions function description ---------------------------------------- ------------------------------------------------------- sql_auto_complete query_string attempts autocompletion on the given query_string example select from sql_auto_complete sel returns suggestion suggestion_start ------------- ------------------ select 0 delete 0 insert 0 call 0 load 0 call 0 alter 0 begin 0 export 0 create 0 prepare 0 execute 0 explain 0 rollback 0 describe 0 summarize 0 checkpoint 0 deallocate 0 update 0 drop 0 github the autocomplete extension is part of the main duckdb repository",
			"category": "Extensions",
			"url": "/docs/extensions/autocomplete",
			"blurb": "The autocomplete extension adds supports for autocomplete in the CLI client . The extension is shipped by default..."
		},
		{
			"title": "Autocomplete",
			"text": "the shell offers context-aware autocomplete of sql queries through the autocomplete extension autocomplete is triggered by pressing tab multiple autocomplete suggestions can be present you can cycle forwards through the suggestions by repeatedly pressing tab or shift tab to cycle backwards autocompletion can be reverted by pressing esc twice the shell autocompletes four different groups keywords table names table functions column names scalar functions file names the shell looks at the position in the sql statement to determine which of these autocompletions to trigger for example select s - student_id select student_id f - from select student_id from g - grades select student_id from d - data select student_id from data - data grades csv",
			"category": "Cli",
			"url": "/docs/api/cli/autocomplete",
			"blurb": "The shell offers context-aware autocomplete of SQL queries through the autocomplete extension . autocomplete is..."
		},
		{
			"title": "Azure Extension",
			"text": "the azure extension is a loadable extension that adds a filesystem abstraction for the azure blob storage to duckdb installing and loading to install and load the azure extension run install azure load azure usage once the authentication is set up the azure blob storage can be queried as follows select count from az my_container my_file parquet_or_csv globs are also supported select from az my_container csv configuration use the following configuration options how the extension reads remote files name description type default --- --- --- --- azure_http_stats include http info from azure storage in the explain analyze statement notice that the result may be incorrect for more than one active duckdb connection and the calculation of total received and sent bytes is not yet implemented boolean false azure_read_transfer_concurrency maximum number of threads the azure client can use for a single parallel read if azure_read_transfer_chunk_size is less than azure_read_buffer_size then setting this 1 will allow the azure client to do concurrent requests to fill the buffer bigint 5 azure_read_transfer_chunk_size maximum size in bytes that the azure client will read in a single request it is recommended that this is a factor of azure_read_buffer_size bigint 1024 1024 azure_read_buffer_size size of the read buffer it is recommended that this is evenly divisible by azure_read_transfer_chunk_size ubigint 1024 1024 example set azure_http_stats false set azure_read_transfer_concurrency 5 set azure_read_transfer_chunk_size 1048576 set azure_read_buffer_size 1048576 authentication the azure extension has two ways to configure the authentication the preferred way is to use secrets authentication with secret multiple secret providers are available for the azure extension config provider the default provider config i e user-configured allows access to the storage account using a connection string or anonymously for example create secret secret1 type azure connection_string value if you do not use authentication you still need to specify the storage account name for example -- note that provider config is optional as it is the default one create secret secret2 type azure provider config account_name storage account name credential_chain provider the credential_chain provider allows connecting using credentials automatically fetched by the azure sdk via the azure credential chain by default the defaultazurecredential chain used which tries credentials according to the order specified by the azure documentation for example create secret secret3 type azure provider credential_chain account_name storage account name duckdb also allows specifying a specific chain using the chain keyword this takes a separated list of providers that will be tried in order for example create secret secret4 type azure provider credential_chain chain cli env account_name storage account name the possible values are the following cli managed_identity env default none the latest will result in an exception invalid input configuring a proxy to configure proxy information when using secrets you can add http_proxy proxy_user_name and proxy_password in the secret definition for example create secret secret5 type azure connection_string value http_proxy http localhost 3128 proxy_user_name john proxy_password doe when using secrets the http_proxy environment variable will still be honored except is you provide an explicit value for it when using secrets the set variable of the authentication with variables session will be ignore the azure credential_chain provider the actual token is fetched at query time not at the time of creating the secret authentication with variables deprecated set variable_name variable_value where variable_name can be one of the following name description type default --- --- --- --- azure_storage_connection_string azure connection string used for authenticating and configuring azure requests string - azure_account_name azure account name when set the extension will attempt to automatically detect credentials not used if you pass the connection string string - azure_endpoint override the azure endpoint for when the azure credential providers are used string blob core windows net azure_credential_chain ordered list of azure credential providers in string format separated by for example cli managed_identity env see the list of possible values in the credential_chain provider section not used if you pass the connection string string - azure_http_proxy proxy to use when login performing request to azure string http_proxy environment variable if set azure_proxy_user_name http proxy username if needed string - azure_proxy_password http proxy password if needed string - github repository span class github github span",
			"category": "Extensions",
			"url": "/docs/extensions/azure",
			"blurb": "The azure extension is a loadable extension that adds a filesystem abstraction for the Azure Blob storage to DuckDB...."
		},
		{
			"title": "Benchmarks",
			"text": "for several of the recommendations in our performance guide we use microbenchmarks to back up our claims for these benchmarks we use data sets from the tpc-h benchmark and the ldbc social network benchmark s bi workload data sets we use the ldbc bi sf300 data set s comment table 20gb tar zst archive 21gb when decompressed into csv gz files while others use the same table s creationdate column 4gb parquet file the tpc data sets used in the benchmark are generated with the duckdb tpch extension a note on benchmarks running fair benchmarks is difficult especially when performing system-to-system comparison when running benchmarks on duckdb please make sure you are using the latest version preferably the nightly build if in doubt about your benchmark results feel free to contact us at gabor duckdb org disclaimer on benchmarks note that the benchmark results presented in this guide do not constitute official tpc or ldbc benchmark results instead they merely use the data sets of and some queries provided by the tpc-h and the ldbc bi benchmark frameworks and omit other parts of the workloads such as updates",
			"category": "Performance",
			"url": "/docs/guides/performance/benchmarks",
			"blurb": "For several of the recommendations in our performance guide, we use microbenchmarks to back up our claims. For these..."
		},
		{
			"title": "Bitstring Functions",
			"text": "this section describes functions and operators for examining and manipulating bit values bitstrings must be of equal length when performing the bitwise operands and or and xor when bit shifting the original length of the string is preserved bitstring operators the table below shows the available mathematical operators for bit type operator description example result --- --- --- --- bitwise and 10101 bit 10001 bit 10001 bitwise or 1011 bit 0001 bit 1011 xor bitwise xor xor 101 bit 001 bit 100 bitwise not 101 bit 010 bitwise shift left 1001011 bit 3 1011000 bitwise shift right 1001011 bit 3 0001001 bitstring functions the table below shows the available scalar functions for bit type function description example result -- ---- ---- - bit_count bitstring returns the number of set bits in the bitstring bit_count 1101011 bit 5 bit_length bitstring returns the number of bits in the bitstring bit_length 1101011 bit 7 bit_position substring bitstring returns first starting index of the specified substring within bits or zero if it s not present the first leftmost bit is indexed 1 bit_position 010 bit 1110101 bit 4 bitstring bitstring length returns a bitstring of determined length bitstring 1010 bit 7 0001010 get_bit bitstring index extracts the nth bit from bitstring the first leftmost bit is indexed 0 get_bit 0110010 bit 2 1 length bitstring alias for bit_length length 1101011 bit 7 octet_length bitstring returns the number of bytes in the bitstring octet_length 1101011 bit 1 set_bit bitstring index new_value sets the nth bit in bitstring to newvalue the first leftmost bit is indexed 0 returns a new bitstring set_bit 0110010 bit 2 0 0100010 bitstring aggregate functions these aggregate functions are available for bit type function description example --- ---- -- bit_and arg returns the bitwise and operation performed on all bitstrings in a given expression bit_and a bit_or arg returns the bitwise or operation performed on all bitstrings in a given expression bit_or a bit_xor arg returns the bitwise xor operation performed on all bitstrings in a given expression bit_xor a bitstring_agg arg min max returns a bitstring with bits set for each distinct value bitstring_agg a 1 42 bitstring_agg arg returns a bitstring with bits set for each distinct value bitstring_agg a bitstring aggregation the bitstring_agg function takes any integer type as input and returns a bitstring with bits set for each distinct value the left-most bit represents the smallest value in the column and the right-most bit the maximum value if possible the min and max are retrieved from the column statistics otherwise it is also possible to provide the min and max values the combination of bit_count and bitstring_agg could be used as an alternative to count distinct with possible performance improvements in cases of low cardinality and dense values",
			"category": "Functions",
			"url": "/docs/sql/functions/bitstring",
			"blurb": "This section describes functions and operators for examining and manipulating bit values. Bitstrings must be of equal..."
		},
		{
			"title": "Bitstring Type",
			"text": "name aliases description --- --- --- bit bitstring variable-length strings of 1s and 0s bitstrings are strings of 1s and 0s the bit type data is of variable length a bitstring value requires 1 byte for each group of 8 bits plus a fixed amount to store some metadata by default bitstrings will not be padded with zeroes bitstrings can be very large having the same size restrictions as blob s -- create a bitstring select 101010 bit -- create a bitstring with predefined length -- the resulting bitstring will be left-padded with zeroes -- this returns 000000101011 select bitstring 0101011 12 functions see bitstring functions",
			"category": "Data Types",
			"url": "/docs/sql/data_types/bitstring",
			"blurb": "The bitstring type are strings of 1s and 0s."
		},
		{
			"title": "Blob Functions",
			"text": "this section describes functions and operators for examining and manipulating blob values function description example result - -- --- - blob blob blob concatenation xaa blob xbb blob xaa xbb decode blob convert blob to varchar fails if blob is not valid utf-8 decode xc3 xbc blob \u00fc encode string convert varchar to blob converts utf-8 characters into literal encoding encode my_string_with_\u00fc my_string_with_ xc3 xbc octet_length blob number of bytes in blob octet_length xaa xbb blob 2",
			"category": "Functions",
			"url": "/docs/sql/functions/blob",
			"blurb": "This section describes functions and operators for examining and manipulating blob values. | Function | Description |..."
		},
		{
			"title": "Blob Type",
			"text": "name aliases description --- --- --- blob bytea binary varbinary variable-length binary data the blob b inary l arge ob ject type represents an arbitrary binary object stored in the database system the blob type can contain any type of binary data with no restrictions what the actual bytes represent is opaque to the database system -- create a blob value with a single byte 170 select xaa blob -- create a blob value with three bytes 170 171 172 select xaa xab xac blob -- create a blob value with two bytes 65 66 select ab blob blobs are typically used to store non-textual objects that the database does not provide explicit support for such as images while blobs can hold objects up to 4gb in size typically it is not recommended to store very large objects within the database system in many situations it is better to store the large file on the file system and store the path to the file in the database system in a varchar field functions see blob functions",
			"category": "Data Types",
			"url": "/docs/sql/data_types/blob",
			"blurb": "The blob (Binary Large OBject) type represents an arbitrary binary object stored in the database system."
		},
		{
			"title": "Boolean Type",
			"text": "name aliases description --- --- --- boolean bool logical boolean true false the boolean type represents a statement of truth true or false in sql the boolean field can also have a third state unknown which is represented by the sql null value -- select the three possible values of a boolean column select true false null boolean boolean values can be explicitly created using the literals true and false however they are most often created as a result of comparisons or conjunctions for example the comparison i 10 results in a boolean value boolean values can be used in the where and having clauses of a sql statement to filter out tuples from the result in this case tuples for which the predicate evaluates to true will pass the filter and tuples for which the predicate evaluates to false or null will be filtered out consider the following example -- create a table with the value 5 15 and null create table integers i integer insert into integers values 5 15 null -- select all entries where i 10 select from integers where i 10 -- in this case 5 and null are filtered out -- 5 10 false -- null 10 null -- the result is 15 conjunctions the and or conjunctions can be used to combine boolean values below is the truth table for the and conjunction i e x and y x x and true x and false x and null ------- ------- ------- ------- true true false null false false false false null null false null below is the truth table for the or conjunction i e x or y x x or true x or false x or null ------- ------ ------- ------ true true true true false true false null null true null null expressions see logical operators and comparison operators",
			"category": "Data Types",
			"url": "/docs/sql/data_types/boolean",
			"blurb": "The BOOLEAN type represents a statement of truth (true or false)."
		},
		{
			"title": "C API - Appender",
			"text": "appenders are the most efficient way of loading data into duckdb from within the c interface and are recommended for fast data loading the appender is much faster than using prepared statements or individual insert into statements appends are made in row-wise format for every column a duckdb_append_ type call should be made after which the row should be finished by calling duckdb_appender_end_row after all rows have been appended duckdb_appender_destroy should be used to finalize the appender and clean up the resulting memory note that duckdb_appender_destroy should always be called on the resulting appender even if the function returns duckdberror example duckdb_query con create table people id integer name varchar null duckdb_appender appender if duckdb_appender_create con null people appender duckdberror handle error append the first row 1 mark duckdb_append_int32 appender 1 duckdb_append_varchar appender mark duckdb_appender_end_row appender append the second row 2 hannes duckdb_append_int32 appender 2 duckdb_append_varchar appender hannes duckdb_appender_end_row appender finish appending and flush all the rows to the table duckdb_appender_destroy appender api reference duckdb_appender_create creates an appender object note that the object must be destroyed with duckdb_appender_destroy syntax parameters connection the connection context to create the appender in schema the schema of the table to append to or nullptr for the default schema table the table name to append to out_appender the resulting appender object returns duckdbsuccess on success or duckdberror on failure duckdb_appender_column_count returns the number of columns in the table that belongs to the appender appender the appender to get the column count from syntax parameters returns the number of columns in the table duckdb_appender_column_type returns the type of the column at the specified index note the resulting type should be destroyed with duckdb_destroy_logical_type appender the appender to get the column type from col_idx the index of the column to get the type of syntax parameters returns the duckdb_logical_type of the column duckdb_appender_error returns the error message associated with the given appender if the appender has no error message this returns nullptr instead the error message should not be freed it will be de-allocated when duckdb_appender_destroy is called syntax parameters appender the appender to get the error from returns the error message or nullptr if there is none duckdb_appender_flush flush the appender to the table forcing the cache of the appender to be cleared and the data to be appended to the base table this should generally not be used unless you know what you are doing instead call duckdb_appender_destroy when you are done with the appender syntax parameters appender the appender to flush returns duckdbsuccess on success or duckdberror on failure duckdb_appender_close close the appender flushing all intermediate state in the appender to the table and closing it for further appends this is generally not necessary call duckdb_appender_destroy instead syntax parameters appender the appender to flush and close returns duckdbsuccess on success or duckdberror on failure duckdb_appender_destroy close the appender and destroy it flushing all intermediate state in the appender to the table and de-allocating all memory associated with the appender syntax parameters appender the appender to flush close and destroy returns duckdbsuccess on success or duckdberror on failure duckdb_appender_begin_row a nop function provided for backwards compatibility reasons does nothing only duckdb_appender_end_row is required syntax duckdb_appender_end_row finish the current row of appends after end_row is called the next row can be appended syntax parameters appender the appender returns duckdbsuccess on success or duckdberror on failure duckdb_append_bool append a bool value to the appender syntax duckdb_append_int8 append an int8_t value to the appender syntax duckdb_append_int16 append an int16_t value to the appender syntax duckdb_append_int32 append an int32_t value to the appender syntax duckdb_append_int64 append an int64_t value to the appender syntax duckdb_append_hugeint append a duckdb_hugeint value to the appender syntax duckdb_append_uint8 append a uint8_t value to the appender syntax duckdb_append_uint16 append a uint16_t value to the appender syntax duckdb_append_uint32 append a uint32_t value to the appender syntax duckdb_append_uint64 append a uint64_t value to the appender syntax duckdb_append_uhugeint append a duckdb_uhugeint value to the appender syntax duckdb_append_float append a float value to the appender syntax duckdb_append_double append a double value to the appender syntax duckdb_append_date append a duckdb_date value to the appender syntax duckdb_append_time append a duckdb_time value to the appender syntax duckdb_append_timestamp append a duckdb_timestamp value to the appender syntax duckdb_append_interval append a duckdb_interval value to the appender syntax duckdb_append_varchar append a varchar value to the appender syntax duckdb_append_varchar_length append a varchar value to the appender syntax duckdb_append_blob append a blob value to the appender syntax duckdb_append_null append a null value to the appender of any type syntax duckdb_append_data_chunk appends a pre-filled data chunk to the specified appender the types of the data chunk must exactly match the types of the table no casting is performed if the types do not match or the appender is in an invalid state duckdberror is returned if the append is successful duckdbsuccess is returned syntax parameters appender the appender to append to chunk the data chunk to append returns the return state",
			"category": "C",
			"url": "/docs/api/c/appender",
			"blurb": "Appenders are the most efficient way of loading data into DuckDB from within the C interface, and are recommended for..."
		},
		{
			"title": "C API - Complete API",
			"text": "api reference open connect configuration query execution result functions safe fetch functions helpers date time timestamp helpers hugeint helpers unsigned hugeint helpers decimal helpers prepared statements bind values to prepared statements execute prepared statements extract statements pending result interface value interface logical type interface data chunk interface vector interface validity mask functions table functions table function bind table function init table function replacement scans appender arrow interface threading information streaming result interface duckdb_open creates a new database or opens an existing database file stored at the given path if no path is given a new in-memory database is created instead the instantiated database should be closed with duckdb_close syntax parameters path path to the database file on disk or nullptr or memory to open an in-memory database out_database the result database object returns duckdbsuccess on success or duckdberror on failure duckdb_open_ext extended version of duckdb_open creates a new database or opens an existing database file stored at the given path the instantiated database should be closed with duckdb_close syntax parameters path path to the database file on disk or nullptr or memory to open an in-memory database out_database the result database object config optional configuration used to start up the database system out_error if set and the function returns duckdberror this will contain the reason why the start-up failed note that the error must be freed using duckdb_free returns duckdbsuccess on success or duckdberror on failure duckdb_close closes the specified database and de-allocates all memory allocated for that database this should be called after you are done with any database allocated through duckdb_open or duckdb_open_ext note that failing to call duckdb_close in case of e g a program crash will not cause data corruption still it is recommended to always correctly close a database object after you are done with it syntax parameters database the database object to shut down duckdb_connect opens a connection to a database connections are required to query the database and store transactional state associated with the connection the instantiated connection should be closed using duckdb_disconnect syntax parameters database the database file to connect to out_connection the result connection object returns duckdbsuccess on success or duckdberror on failure duckdb_interrupt interrupt running query syntax parameters connection the connection to interrupt duckdb_query_progress get progress of the running query syntax parameters connection the working connection returns -1 if no progress or a percentage of the progress duckdb_disconnect closes the specified connection and de-allocates all memory allocated for that connection syntax parameters connection the connection to close duckdb_library_version returns the version of the linked duckdb with a version postfix for dev versions usually used for developing c extensions that must return this for a compatibility check syntax duckdb_create_config initializes an empty configuration object that can be used to provide start-up options for the duckdb instance through duckdb_open_ext the duckdb_config must be destroyed using duckdb_destroy_config this will always succeed unless there is a malloc failure syntax parameters out_config the result configuration object returns duckdbsuccess on success or duckdberror on failure duckdb_config_count this returns the total amount of configuration options available for usage with duckdb_get_config_flag this should not be called in a loop as it internally loops over all the options syntax parameters returns the amount of config options available duckdb_get_config_flag obtains a human-readable name and description of a specific configuration option this can be used to e g display configuration options this will succeed unless index is out of range i e duckdb_config_count the result name or description must not be freed syntax parameters index the index of the configuration option between 0 and duckdb_config_count out_name a name of the configuration flag out_description a description of the configuration flag returns duckdbsuccess on success or duckdberror on failure duckdb_set_config sets the specified option for the specified configuration the configuration option is indicated by name to obtain a list of config options see duckdb_get_config_flag in the source code configuration options are defined in config cpp this can fail if either the name is invalid or if the value provided for the option is invalid syntax parameters duckdb_config the configuration object to set the option on name the name of the configuration flag to set option the value to set the configuration flag to returns duckdbsuccess on success or duckdberror on failure duckdb_destroy_config destroys the specified configuration object and de-allocates all memory allocated for the object syntax parameters config the configuration object to destroy duckdb_query executes a sql query within a connection and stores the full materialized result in the out_result pointer if the query fails to execute duckdberror is returned and the error message can be retrieved by calling duckdb_result_error note that after running duckdb_query duckdb_destroy_result must be called on the result object even if the query fails otherwise the error stored within the result will not be freed correctly syntax parameters connection the connection to perform the query in query the sql query to run out_result the query result returns duckdbsuccess on success or duckdberror on failure duckdb_destroy_result closes the result and de-allocates all memory allocated for that connection syntax parameters result the result to destroy duckdb_column_name returns the column name of the specified column the result should not need to be freed the column names will automatically be destroyed when the result is destroyed returns null if the column is out of range syntax parameters result the result object to fetch the column name from col the column index returns the column name of the specified column duckdb_column_type returns the column type of the specified column returns duckdb_type_invalid if the column is out of range syntax parameters result the result object to fetch the column type from col the column index returns the column type of the specified column duckdb_result_statement_type returns the statement type of the statement that was executed syntax parameters result the result object to fetch the statement type from returns duckdb_statement_type value or duckdb_statement_type_invalid duckdb_column_logical_type returns the logical column type of the specified column the return type of this call should be destroyed with duckdb_destroy_logical_type returns null if the column is out of range syntax parameters result the result object to fetch the column type from col the column index returns the logical column type of the specified column duckdb_column_count returns the number of columns present in a the result object syntax parameters result the result object returns the number of columns present in the result object duckdb_row_count returns the number of rows present in the result object syntax parameters result the result object returns the number of rows present in the result object duckdb_rows_changed returns the number of rows changed by the query stored in the result this is relevant only for insert update delete queries for other queries the rows_changed will be 0 syntax parameters result the result object returns the number of rows changed duckdb_column_data deprecated prefer using duckdb_result_get_chunk instead returns the data of a specific column of a result in columnar format the function returns a dense array which contains the result data the exact type stored in the array depends on the corresponding duckdb_type as provided by duckdb_column_type for the exact type by which the data should be accessed see the comments in the types section or the duckdb_type enum for example for a column of type duckdb_type_integer rows can be accessed in the following manner int32_t data int32_t duckdb_column_data result 0 printf data for row d d n row data row syntax parameters result the result object to fetch the column data from col the column index returns the column data of the specified column duckdb_nullmask_data deprecated prefer using duckdb_result_get_chunk instead returns the nullmask of a specific column of a result in columnar format the nullmask indicates for every row whether or not the corresponding row is null if a row is null the values present in the array provided by duckdb_column_data are undefined int32_t data int32_t duckdb_column_data result 0 bool nullmask duckdb_nullmask_data result 0 if nullmask row printf data for row d null n row else printf data for row d d n row data row syntax parameters result the result object to fetch the nullmask from col the column index returns the nullmask of the specified column duckdb_result_error returns the error message contained within the result the error is only set if duckdb_query returns duckdberror the result of this function must not be freed it will be cleaned up when duckdb_destroy_result is called syntax parameters result the result object to fetch the error from returns the error of the result duckdb_result_get_chunk fetches a data chunk from the duckdb_result this function should be called repeatedly until the result is exhausted the result must be destroyed with duckdb_destroy_data_chunk this function supersedes all duckdb_value functions as well as the duckdb_column_data and duckdb_nullmask_data functions it results in significantly better performance and should be preferred in newer code-bases if this function is used none of the other result functions can be used and vice versa i e this function cannot be mixed with the legacy result functions use duckdb_result_chunk_count to figure out how many chunks there are in the result syntax parameters result the result object to fetch the data chunk from chunk_index the chunk index to fetch from returns the resulting data chunk returns null if the chunk index is out of bounds duckdb_result_is_streaming checks if the type of the internal result is streamqueryresult syntax parameters result the result object to check returns whether or not the result object is of the type streamqueryresult duckdb_result_chunk_count returns the number of data chunks present in the result syntax parameters result the result object returns number of data chunks present in the result duckdb_result_return_type returns the return_type of the given result or duckdb_return_type_invalid on error syntax parameters result the result object returns the return_type duckdb_value_boolean syntax parameters returns the boolean value at the specified location or false if the value cannot be converted duckdb_value_int8 syntax parameters returns the int8_t value at the specified location or 0 if the value cannot be converted duckdb_value_int16 syntax parameters returns the int16_t value at the specified location or 0 if the value cannot be converted duckdb_value_int32 syntax parameters returns the int32_t value at the specified location or 0 if the value cannot be converted duckdb_value_int64 syntax parameters returns the int64_t value at the specified location or 0 if the value cannot be converted duckdb_value_hugeint syntax parameters returns the duckdb_hugeint value at the specified location or 0 if the value cannot be converted duckdb_value_uhugeint syntax parameters returns the duckdb_uhugeint value at the specified location or 0 if the value cannot be converted duckdb_value_decimal syntax parameters returns the duckdb_decimal value at the specified location or 0 if the value cannot be converted duckdb_value_uint8 syntax parameters returns the uint8_t value at the specified location or 0 if the value cannot be converted duckdb_value_uint16 syntax parameters returns the uint16_t value at the specified location or 0 if the value cannot be converted duckdb_value_uint32 syntax parameters returns the uint32_t value at the specified location or 0 if the value cannot be converted duckdb_value_uint64 syntax parameters returns the uint64_t value at the specified location or 0 if the value cannot be converted duckdb_value_float syntax parameters returns the float value at the specified location or 0 if the value cannot be converted duckdb_value_double syntax parameters returns the double value at the specified location or 0 if the value cannot be converted duckdb_value_date syntax parameters returns the duckdb_date value at the specified location or 0 if the value cannot be converted duckdb_value_time syntax parameters returns the duckdb_time value at the specified location or 0 if the value cannot be converted duckdb_value_timestamp syntax parameters returns the duckdb_timestamp value at the specified location or 0 if the value cannot be converted duckdb_value_interval syntax parameters returns the duckdb_interval value at the specified location or 0 if the value cannot be converted duckdb_value_varchar syntax parameters deprecated use duckdb_value_string instead this function does not work correctly if the string contains null bytes returns the text value at the specified location as a null-terminated string or nullptr if the value cannot be converted the result must be freed with duckdb_free duckdb_value_string syntax parameters returns the string value at the specified location the resulting field string data must be freed with duckdb_free duckdb_value_varchar_internal syntax parameters deprecated use duckdb_value_string_internal instead this function does not work correctly if the string contains null bytes returns the char value at the specified location only works on varchar columns and does not auto-cast if the column is not a varchar column this function will return null the result must not be freed duckdb_value_string_internal syntax parameters deprecated use duckdb_value_string_internal instead this function does not work correctly if the string contains null bytes returns the char value at the specified location only works on varchar columns and does not auto-cast if the column is not a varchar column this function will return null the result must not be freed duckdb_value_blob syntax parameters returns the duckdb_blob value at the specified location returns a blob with blob data set to nullptr if the value cannot be converted the resulting field blob data must be freed with duckdb_free duckdb_value_is_null syntax parameters returns returns true if the value at the specified index is null and false otherwise duckdb_malloc allocate size bytes of memory using the duckdb internal malloc function any memory allocated in this manner should be freed using duckdb_free syntax parameters size the number of bytes to allocate returns a pointer to the allocated memory region duckdb_free free a value returned from duckdb_malloc duckdb_value_varchar duckdb_value_blob or duckdb_value_string syntax parameters ptr the memory region to de-allocate duckdb_vector_size the internal vector size used by duckdb this is the amount of tuples that will fit into a data chunk created by duckdb_create_data_chunk syntax parameters returns the vector size duckdb_string_is_inlined whether or not the duckdb_string_t value is inlined this means that the data of the string does not have a separate allocation syntax duckdb_from_date decompose a duckdb_date object into year month and date stored as duckdb_date_struct syntax parameters date the date object as obtained from a duckdb_type_date column returns the duckdb_date_struct with the decomposed elements duckdb_to_date re-compose a duckdb_date from year month and date duckdb_date_struct syntax parameters date the year month and date stored in a duckdb_date_struct returns the duckdb_date element duckdb_is_finite_date test a duckdb_date to see if it is a finite value syntax parameters date the date object as obtained from a duckdb_type_date column returns true if the date is finite false if it is infinity duckdb_from_time decompose a duckdb_time object into hour minute second and microsecond stored as duckdb_time_struct syntax parameters time the time object as obtained from a duckdb_type_time column returns the duckdb_time_struct with the decomposed elements duckdb_create_time_tz create a duckdb_time_tz object from micros and a timezone offset syntax parameters micros the microsecond component of the time offset the timezone offset component of the time returns the duckdb_time_tz element duckdb_from_time_tz decompose a time_tz objects into micros and a timezone offset use duckdb_from_time to further decompose the micros into hour minute second and microsecond syntax parameters micros the time object as obtained from a duckdb_type_time_tz column out_micros the microsecond component of the time out_offset the timezone offset component of the time duckdb_to_time re-compose a duckdb_time from hour minute second and microsecond duckdb_time_struct syntax parameters time the hour minute second and microsecond in a duckdb_time_struct returns the duckdb_time element duckdb_from_timestamp decompose a duckdb_timestamp object into a duckdb_timestamp_struct syntax parameters ts the ts object as obtained from a duckdb_type_timestamp column returns the duckdb_timestamp_struct with the decomposed elements duckdb_to_timestamp re-compose a duckdb_timestamp from a duckdb_timestamp_struct syntax parameters ts the de-composed elements in a duckdb_timestamp_struct returns the duckdb_timestamp element duckdb_is_finite_timestamp test a duckdb_timestamp to see if it is a finite value syntax parameters ts the timestamp object as obtained from a duckdb_type_timestamp column returns true if the timestamp is finite false if it is infinity duckdb_hugeint_to_double converts a duckdb_hugeint object as obtained from a duckdb_type_hugeint column into a double syntax parameters val the hugeint value returns the converted double element duckdb_double_to_hugeint converts a double value to a duckdb_hugeint object if the conversion fails because the double value is too big the result will be 0 syntax parameters val the double value returns the converted duckdb_hugeint element duckdb_uhugeint_to_double converts a duckdb_uhugeint object as obtained from a duckdb_type_uhugeint column into a double syntax parameters val the uhugeint value returns the converted double element duckdb_double_to_uhugeint converts a double value to a duckdb_uhugeint object if the conversion fails because the double value is too big the result will be 0 syntax parameters val the double value returns the converted duckdb_uhugeint element duckdb_double_to_decimal converts a double value to a duckdb_decimal object if the conversion fails because the double value is too big or the width scale are invalid the result will be 0 syntax parameters val the double value returns the converted duckdb_decimal element duckdb_decimal_to_double converts a duckdb_decimal object as obtained from a duckdb_type_decimal column into a double syntax parameters val the decimal value returns the converted double element duckdb_prepare create a prepared statement object from a query note that after calling duckdb_prepare the prepared statement should always be destroyed using duckdb_destroy_prepare even if the prepare fails if the prepare fails duckdb_prepare_error can be called to obtain the reason why the prepare failed syntax parameters connection the connection object query the sql query to prepare out_prepared_statement the resulting prepared statement object returns duckdbsuccess on success or duckdberror on failure duckdb_destroy_prepare closes the prepared statement and de-allocates all memory allocated for the statement syntax parameters prepared_statement the prepared statement to destroy duckdb_prepare_error returns the error message associated with the given prepared statement if the prepared statement has no error message this returns nullptr instead the error message should not be freed it will be de-allocated when duckdb_destroy_prepare is called syntax parameters prepared_statement the prepared statement to obtain the error from returns the error message or nullptr if there is none duckdb_nparams returns the number of parameters that can be provided to the given prepared statement returns 0 if the query was not successfully prepared syntax parameters prepared_statement the prepared statement to obtain the number of parameters for duckdb_parameter_name returns the name used to identify the parameter the returned string should be freed using duckdb_free returns null if the index is out of range for the provided prepared statement syntax parameters prepared_statement the prepared statement for which to get the parameter name from duckdb_param_type returns the parameter type for the parameter at the given index returns duckdb_type_invalid if the parameter index is out of range or the statement was not successfully prepared syntax parameters prepared_statement the prepared statement param_idx the parameter index returns the parameter type duckdb_clear_bindings clear the params bind to the prepared statement syntax duckdb_prepared_statement_type returns the statement type of the statement to be executed syntax parameters statement the prepared statement returns duckdb_statement_type value or duckdb_statement_type_invalid duckdb_bind_value binds a value to the prepared statement at the specified index syntax duckdb_bind_parameter_index retrieve the index of the parameter for the prepared statement identified by name syntax duckdb_bind_boolean binds a bool value to the prepared statement at the specified index syntax duckdb_bind_int8 binds an int8_t value to the prepared statement at the specified index syntax duckdb_bind_int16 binds an int16_t value to the prepared statement at the specified index syntax duckdb_bind_int32 binds an int32_t value to the prepared statement at the specified index syntax duckdb_bind_int64 binds an int64_t value to the prepared statement at the specified index syntax duckdb_bind_hugeint binds a duckdb_hugeint value to the prepared statement at the specified index syntax duckdb_bind_uhugeint binds an duckdb_uhugeint value to the prepared statement at the specified index syntax duckdb_bind_decimal binds a duckdb_decimal value to the prepared statement at the specified index syntax duckdb_bind_uint8 binds an uint8_t value to the prepared statement at the specified index syntax duckdb_bind_uint16 binds an uint16_t value to the prepared statement at the specified index syntax duckdb_bind_uint32 binds an uint32_t value to the prepared statement at the specified index syntax duckdb_bind_uint64 binds an uint64_t value to the prepared statement at the specified index syntax duckdb_bind_float binds a float value to the prepared statement at the specified index syntax duckdb_bind_double binds a double value to the prepared statement at the specified index syntax duckdb_bind_date binds a duckdb_date value to the prepared statement at the specified index syntax duckdb_bind_time binds a duckdb_time value to the prepared statement at the specified index syntax duckdb_bind_timestamp binds a duckdb_timestamp value to the prepared statement at the specified index syntax duckdb_bind_interval binds a duckdb_interval value to the prepared statement at the specified index syntax duckdb_bind_varchar binds a null-terminated varchar value to the prepared statement at the specified index syntax duckdb_bind_varchar_length binds a varchar value to the prepared statement at the specified index syntax duckdb_bind_blob binds a blob value to the prepared statement at the specified index syntax duckdb_bind_null binds a null value to the prepared statement at the specified index syntax duckdb_execute_prepared executes the prepared statement with the given bound parameters and returns a materialized query result this method can be called multiple times for each prepared statement and the parameters can be modified between calls to this function note that the result must be freed with duckdb_destroy_result syntax parameters prepared_statement the prepared statement to execute out_result the query result returns duckdbsuccess on success or duckdberror on failure duckdb_execute_prepared_streaming executes the prepared statement with the given bound parameters and returns an optionally-streaming query result to determine if the resulting query was in fact streamed use duckdb_result_is_streaming this method can be called multiple times for each prepared statement and the parameters can be modified between calls to this function note that the result must be freed with duckdb_destroy_result syntax parameters prepared_statement the prepared statement to execute out_result the query result returns duckdbsuccess on success or duckdberror on failure duckdb_extract_statements extract all statements from a query note that after calling duckdb_extract_statements the extracted statements should always be destroyed using duckdb_destroy_extracted even if no statements were extracted if the extract fails duckdb_extract_statements_error can be called to obtain the reason why the extract failed syntax parameters connection the connection object query the sql query to extract out_extracted_statements the resulting extracted statements object returns the number of extracted statements or 0 on failure duckdb_prepare_extracted_statement prepare an extracted statement note that after calling duckdb_prepare_extracted_statement the prepared statement should always be destroyed using duckdb_destroy_prepare even if the prepare fails if the prepare fails duckdb_prepare_error can be called to obtain the reason why the prepare failed syntax parameters connection the connection object extracted_statements the extracted statements object index the index of the extracted statement to prepare out_prepared_statement the resulting prepared statement object returns duckdbsuccess on success or duckdberror on failure duckdb_extract_statements_error returns the error message contained within the extracted statements the result of this function must not be freed it will be cleaned up when duckdb_destroy_extracted is called syntax parameters result the extracted statements to fetch the error from returns the error of the extracted statements duckdb_destroy_extracted de-allocates all memory allocated for the extracted statements syntax parameters extracted_statements the extracted statements to destroy duckdb_pending_prepared executes the prepared statement with the given bound parameters and returns a pending result the pending result represents an intermediate structure for a query that is not yet fully executed the pending result can be used to incrementally execute a query returning control to the client between tasks note that after calling duckdb_pending_prepared the pending result should always be destroyed using duckdb_destroy_pending even if this function returns duckdberror syntax parameters prepared_statement the prepared statement to execute out_result the pending query result returns duckdbsuccess on success or duckdberror on failure duckdb_pending_prepared_streaming executes the prepared statement with the given bound parameters and returns a pending result this pending result will create a streaming duckdb_result when executed the pending result represents an intermediate structure for a query that is not yet fully executed note that after calling duckdb_pending_prepared_streaming the pending result should always be destroyed using duckdb_destroy_pending even if this function returns duckdberror syntax parameters prepared_statement the prepared statement to execute out_result the pending query result returns duckdbsuccess on success or duckdberror on failure duckdb_destroy_pending closes the pending result and de-allocates all memory allocated for the result syntax parameters pending_result the pending result to destroy duckdb_pending_error returns the error message contained within the pending result the result of this function must not be freed it will be cleaned up when duckdb_destroy_pending is called syntax parameters result the pending result to fetch the error from returns the error of the pending result duckdb_pending_execute_task executes a single task within the query returning whether or not the query is ready if this returns duckdb_pending_result_ready the duckdb_execute_pending function can be called to obtain the result if this returns duckdb_pending_result_not_ready the duckdb_pending_execute_task function should be called again if this returns duckdb_pending_error an error occurred during execution the error message can be obtained by calling duckdb_pending_error on the pending_result syntax parameters pending_result the pending result to execute a task within returns the state of the pending result after the execution duckdb_pending_execute_check_state if this returns duckdb_pending_result_ready the duckdb_execute_pending function can be called to obtain the result if this returns duckdb_pending_result_not_ready the duckdb_pending_execute_check_state function should be called again if this returns duckdb_pending_error an error occurred during execution the error message can be obtained by calling duckdb_pending_error on the pending_result syntax parameters pending_result the pending result returns the state of the pending result duckdb_execute_pending fully execute a pending query result returning the final query result if duckdb_pending_execute_task has been called until duckdb_pending_result_ready was returned this will return fast otherwise all remaining tasks must be executed first note that the result must be freed with duckdb_destroy_result syntax parameters pending_result the pending result to execute out_result the result object returns duckdbsuccess on success or duckdberror on failure duckdb_pending_execution_is_finished returns whether a duckdb_pending_state is finished executing for example if pending_state is duckdb_pending_result_ready this function will return true syntax parameters pending_state the pending state on which to decide whether to finish execution returns boolean indicating pending execution should be considered finished duckdb_destroy_value destroys the value and de-allocates all memory allocated for that type syntax parameters value the value to destroy duckdb_create_varchar creates a value from a null-terminated string syntax parameters value the null-terminated string returns the value this must be destroyed with duckdb_destroy_value duckdb_create_varchar_length creates a value from a string syntax parameters value the text length the length of the text returns the value this must be destroyed with duckdb_destroy_value duckdb_create_int64 creates a value from an int64 syntax parameters value the bigint value returns the value this must be destroyed with duckdb_destroy_value duckdb_create_struct_value creates a struct value from a type and an array of values syntax parameters type the type of the struct values the values for the struct fields returns the value this must be destroyed with duckdb_destroy_value duckdb_create_list_value creates a list value from a type and an array of values of length value_count syntax parameters type the type of the list values the values for the list value_count the number of values in the list returns the value this must be destroyed with duckdb_destroy_value duckdb_get_varchar obtains a string representation of the given value the result must be destroyed with duckdb_free syntax parameters value the value returns the string value this must be destroyed with duckdb_free duckdb_get_int64 obtains an int64 of the given value syntax parameters value the value returns the int64 value or 0 if no conversion is possible duckdb_create_logical_type creates a duckdb_logical_type from a standard primitive type the resulting type should be destroyed with duckdb_destroy_logical_type this should not be used with duckdb_type_decimal syntax parameters type the primitive type to create returns the logical type duckdb_logical_type_get_alias returns the alias of a duckdb_logical_type if one is set else null the result must be destroyed with duckdb_free syntax parameters type the logical type to return the alias of returns the alias or null duckdb_create_list_type creates a list type from its child type the resulting type should be destroyed with duckdb_destroy_logical_type syntax parameters type the child type of list type to create returns the logical type duckdb_create_map_type creates a map type from its key type and value type the resulting type should be destroyed with duckdb_destroy_logical_type syntax parameters type the key type and value type of map type to create returns the logical type duckdb_create_union_type creates a union type from the passed types array the resulting type should be destroyed with duckdb_destroy_logical_type syntax parameters types the array of types that the union should consist of type_amount the size of the types array returns the logical type duckdb_create_struct_type creates a struct type from the passed member name and type arrays the resulting type should be destroyed with duckdb_destroy_logical_type syntax parameters member_types the array of types that the struct should consist of member_names the array of names that the struct should consist of member_count the number of members that were specified for both arrays returns the logical type duckdb_create_enum_type creates an enum type from the passed member name array the resulting type should be destroyed with duckdb_destroy_logical_type syntax parameters enum_name the name of the enum member_names the array of names that the enum should consist of member_count the number of elements that were specified in the array returns the logical type duckdb_create_decimal_type creates a duckdb_logical_type of type decimal with the specified width and scale the resulting type should be destroyed with duckdb_destroy_logical_type syntax parameters width the width of the decimal type scale the scale of the decimal type returns the logical type duckdb_get_type_id retrieves the enum type class of a duckdb_logical_type syntax parameters type the logical type object returns the type id duckdb_decimal_width retrieves the width of a decimal type syntax parameters type the logical type object returns the width of the decimal type duckdb_decimal_scale retrieves the scale of a decimal type syntax parameters type the logical type object returns the scale of the decimal type duckdb_decimal_internal_type retrieves the internal storage type of a decimal type syntax parameters type the logical type object returns the internal type of the decimal type duckdb_enum_internal_type retrieves the internal storage type of an enum type syntax parameters type the logical type object returns the internal type of the enum type duckdb_enum_dictionary_size retrieves the dictionary size of the enum type syntax parameters type the logical type object returns the dictionary size of the enum type duckdb_enum_dictionary_value retrieves the dictionary value at the specified position from the enum the result must be freed with duckdb_free syntax parameters type the logical type object index the index in the dictionary returns the string value of the enum type must be freed with duckdb_free duckdb_list_type_child_type retrieves the child type of the given list type the result must be freed with duckdb_destroy_logical_type syntax parameters type the logical type object returns the child type of the list type must be destroyed with duckdb_destroy_logical_type duckdb_map_type_key_type retrieves the key type of the given map type the result must be freed with duckdb_destroy_logical_type syntax parameters type the logical type object returns the key type of the map type must be destroyed with duckdb_destroy_logical_type duckdb_map_type_value_type retrieves the value type of the given map type the result must be freed with duckdb_destroy_logical_type syntax parameters type the logical type object returns the value type of the map type must be destroyed with duckdb_destroy_logical_type duckdb_struct_type_child_count returns the number of children of a struct type syntax parameters type the logical type object returns the number of children of a struct type duckdb_struct_type_child_name retrieves the name of the struct child the result must be freed with duckdb_free syntax parameters type the logical type object index the child index returns the name of the struct type must be freed with duckdb_free duckdb_struct_type_child_type retrieves the child type of the given struct type at the specified index the result must be freed with duckdb_destroy_logical_type syntax parameters type the logical type object index the child index returns the child type of the struct type must be destroyed with duckdb_destroy_logical_type duckdb_union_type_member_count returns the number of members that the union type has syntax parameters type the logical type union object returns the number of members of a union type duckdb_union_type_member_name retrieves the name of the union member the result must be freed with duckdb_free syntax parameters type the logical type object index the child index returns the name of the union member must be freed with duckdb_free duckdb_union_type_member_type retrieves the child type of the given union member at the specified index the result must be freed with duckdb_destroy_logical_type syntax parameters type the logical type object index the child index returns the child type of the union member must be destroyed with duckdb_destroy_logical_type duckdb_destroy_logical_type destroys the logical type and de-allocates all memory allocated for that type syntax parameters type the logical type to destroy duckdb_create_data_chunk creates an empty datachunk with the specified set of types note that the result must be destroyed with duckdb_destroy_data_chunk syntax parameters types an array of types of the data chunk column_count the number of columns returns the data chunk duckdb_destroy_data_chunk destroys the data chunk and de-allocates all memory allocated for that chunk syntax parameters chunk the data chunk to destroy duckdb_data_chunk_reset resets a data chunk clearing the validity masks and setting the cardinality of the data chunk to 0 syntax parameters chunk the data chunk to reset duckdb_data_chunk_get_column_count retrieves the number of columns in a data chunk syntax parameters chunk the data chunk to get the data from returns the number of columns in the data chunk duckdb_data_chunk_get_vector retrieves the vector at the specified column index in the data chunk the pointer to the vector is valid for as long as the chunk is alive it does not need to be destroyed syntax parameters chunk the data chunk to get the data from returns the vector duckdb_data_chunk_get_size retrieves the current number of tuples in a data chunk syntax parameters chunk the data chunk to get the data from returns the number of tuples in the data chunk duckdb_data_chunk_set_size sets the current number of tuples in a data chunk syntax parameters chunk the data chunk to set the size in size the number of tuples in the data chunk duckdb_vector_get_column_type retrieves the column type of the specified vector the result must be destroyed with duckdb_destroy_logical_type syntax parameters vector the vector get the data from returns the type of the vector duckdb_vector_get_data retrieves the data pointer of the vector the data pointer can be used to read or write values from the vector how to read or write values depends on the type of the vector syntax parameters vector the vector to get the data from returns the data pointer duckdb_vector_get_validity retrieves the validity mask pointer of the specified vector if all values are valid this function might return null the validity mask is a bitset that signifies null-ness within the data chunk it is a series of uint64_t values where each uint64_t value contains validity for 64 tuples the bit is set to 1 if the value is valid i e not null or 0 if the value is invalid i e null validity of a specific value can be obtained like this idx_t entry_idx row_idx 64 idx_t idx_in_entry row_idx 64 bool is_valid validity_mask entry_idx 1 idx_in_entry alternatively the slower duckdb_validity_row_is_valid function can be used syntax parameters vector the vector to get the data from returns the pointer to the validity mask or null if no validity mask is present duckdb_vector_ensure_validity_writable ensures the validity mask is writable by allocating it after this function is called duckdb_vector_get_validity will always return non-null this allows null values to be written to the vector regardless of whether a validity mask was present before syntax parameters vector the vector to alter duckdb_vector_assign_string_element assigns a string element in the vector at the specified location syntax parameters vector the vector to alter index the row position in the vector to assign the string to str the null-terminated string duckdb_vector_assign_string_element_len assigns a string element in the vector at the specified location syntax parameters vector the vector to alter index the row position in the vector to assign the string to str the string str_len the length of the string in bytes duckdb_list_vector_get_child retrieves the child vector of a list vector the resulting vector is valid as long as the parent vector is valid syntax parameters vector the vector returns the child vector duckdb_list_vector_get_size returns the size of the child vector of the list syntax parameters vector the vector returns the size of the child list duckdb_list_vector_set_size sets the total size of the underlying child-vector of a list vector syntax parameters vector the list vector size the size of the child list returns the duckdb state returns duckdberror if the vector is nullptr duckdb_list_vector_reserve sets the total capacity of the underlying child-vector of a list syntax parameters vector the list vector required_capacity the total capacity to reserve return the duckdb state returns duckdberror if the vector is nullptr duckdb_struct_vector_get_child retrieves the child vector of a struct vector the resulting vector is valid as long as the parent vector is valid syntax parameters vector the vector index the child index returns the child vector duckdb_validity_row_is_valid returns whether or not a row is valid i e not null in the given validity mask syntax parameters validity the validity mask as obtained through duckdb_vector_get_validity row the row index returns true if the row is valid false otherwise duckdb_validity_set_row_validity in a validity mask sets a specific row to either valid or invalid note that duckdb_vector_ensure_validity_writable should be called before calling duckdb_vector_get_validity to ensure that there is a validity mask to write to syntax parameters validity the validity mask as obtained through duckdb_vector_get_validity row the row index valid whether or not to set the row to valid or invalid duckdb_validity_set_row_invalid in a validity mask sets a specific row to invalid equivalent to duckdb_validity_set_row_validity with valid set to false syntax parameters validity the validity mask row the row index duckdb_validity_set_row_valid in a validity mask sets a specific row to valid equivalent to duckdb_validity_set_row_validity with valid set to true syntax parameters validity the validity mask row the row index duckdb_create_table_function creates a new empty table function the return value should be destroyed with duckdb_destroy_table_function syntax parameters returns the table function object duckdb_destroy_table_function destroys the given table function object syntax parameters table_function the table function to destroy duckdb_table_function_set_name sets the name of the given table function syntax parameters table_function the table function name the name of the table function duckdb_table_function_add_parameter adds a parameter to the table function syntax parameters table_function the table function type the type of the parameter to add duckdb_table_function_add_named_parameter adds a named parameter to the table function syntax parameters table_function the table function name the name of the parameter type the type of the parameter to add duckdb_table_function_set_extra_info assigns extra information to the table function that can be fetched during binding etc syntax parameters table_function the table function extra_info the extra information destroy the callback that will be called to destroy the bind data if any duckdb_table_function_set_bind sets the bind function of the table function syntax parameters table_function the table function bind the bind function duckdb_table_function_set_init sets the init function of the table function syntax parameters table_function the table function init the init function duckdb_table_function_set_local_init sets the thread-local init function of the table function syntax parameters table_function the table function init the init function duckdb_table_function_set_function sets the main function of the table function syntax parameters table_function the table function function the function duckdb_table_function_supports_projection_pushdown sets whether or not the given table function supports projection pushdown if this is set to true the system will provide a list of all required columns in the init stage through the duckdb_init_get_column_count and duckdb_init_get_column_index functions if this is set to false the default the system will expect all columns to be projected syntax parameters table_function the table function pushdown true if the table function supports projection pushdown false otherwise duckdb_register_table_function register the table function object within the given connection the function requires at least a name a bind function an init function and a main function if the function is incomplete or a function with this name already exists duckdberror is returned syntax parameters con the connection to register it in function the function pointer returns whether or not the registration was successful duckdb_bind_get_extra_info retrieves the extra info of the function as set in duckdb_table_function_set_extra_info syntax parameters info the info object returns the extra info duckdb_bind_add_result_column adds a result column to the output of the table function syntax parameters info the info object name the name of the column type the logical type of the column duckdb_bind_get_parameter_count retrieves the number of regular non-named parameters to the function syntax parameters info the info object returns the number of parameters duckdb_bind_get_parameter retrieves the parameter at the given index the result must be destroyed with duckdb_destroy_value syntax parameters info the info object index the index of the parameter to get returns the value of the parameter must be destroyed with duckdb_destroy_value duckdb_bind_get_named_parameter retrieves a named parameter with the given name the result must be destroyed with duckdb_destroy_value syntax parameters info the info object name the name of the parameter returns the value of the parameter must be destroyed with duckdb_destroy_value duckdb_bind_set_bind_data sets the user-provided bind data in the bind object this object can be retrieved again during execution syntax parameters info the info object extra_data the bind data object destroy the callback that will be called to destroy the bind data if any duckdb_bind_set_cardinality sets the cardinality estimate for the table function used for optimization syntax parameters info the bind data object is_exact whether or not the cardinality estimate is exact or an approximation duckdb_bind_set_error report that an error has occurred while calling bind syntax parameters info the info object error the error message duckdb_init_get_extra_info retrieves the extra info of the function as set in duckdb_table_function_set_extra_info syntax parameters info the info object returns the extra info duckdb_init_get_bind_data gets the bind data set by duckdb_bind_set_bind_data during the bind note that the bind data should be considered as read-only for tracking state use the init data instead syntax parameters info the info object returns the bind data object duckdb_init_set_init_data sets the user-provided init data in the init object this object can be retrieved again during execution syntax parameters info the info object extra_data the init data object destroy the callback that will be called to destroy the init data if any duckdb_init_get_column_count returns the number of projected columns this function must be used if projection pushdown is enabled to figure out which columns to emit syntax parameters info the info object returns the number of projected columns duckdb_init_get_column_index returns the column index of the projected column at the specified position this function must be used if projection pushdown is enabled to figure out which columns to emit syntax parameters info the info object column_index the index at which to get the projected column index from 0 duckdb_init_get_column_count info returns the column index of the projected column duckdb_init_set_max_threads sets how many threads can process this table function in parallel default 1 syntax parameters info the info object max_threads the maximum amount of threads that can process this table function duckdb_init_set_error report that an error has occurred while calling init syntax parameters info the info object error the error message duckdb_function_get_extra_info retrieves the extra info of the function as set in duckdb_table_function_set_extra_info syntax parameters info the info object returns the extra info duckdb_function_get_bind_data gets the bind data set by duckdb_bind_set_bind_data during the bind note that the bind data should be considered as read-only for tracking state use the init data instead syntax parameters info the info object returns the bind data object duckdb_function_get_init_data gets the init data set by duckdb_init_set_init_data during the init syntax parameters info the info object returns the init data object duckdb_function_get_local_init_data gets the thread-local init data set by duckdb_init_set_init_data during the local_init syntax parameters info the info object returns the init data object duckdb_function_set_error report that an error has occurred while executing the function syntax parameters info the info object error the error message duckdb_add_replacement_scan add a replacement scan definition to the specified database syntax parameters db the database object to add the replacement scan to replacement the replacement scan callback extra_data extra data that is passed back into the specified callback delete_callback the delete callback to call on the extra data if any duckdb_replacement_scan_set_function_name sets the replacement function name if this function is called in the replacement callback the replacement scan is performed if it is not called the replacement callback is not performed syntax parameters info the info object function_name the function name to substitute duckdb_replacement_scan_add_parameter adds a parameter to the replacement scan function syntax parameters info the info object parameter the parameter to add duckdb_replacement_scan_set_error report that an error has occurred while executing the replacement scan syntax parameters info the info object error the error message duckdb_appender_create creates an appender object note that the object must be destroyed with duckdb_appender_destroy syntax parameters connection the connection context to create the appender in schema the schema of the table to append to or nullptr for the default schema table the table name to append to out_appender the resulting appender object returns duckdbsuccess on success or duckdberror on failure duckdb_appender_column_count returns the number of columns in the table that belongs to the appender appender the appender to get the column count from syntax parameters returns the number of columns in the table duckdb_appender_column_type returns the type of the column at the specified index note the resulting type should be destroyed with duckdb_destroy_logical_type appender the appender to get the column type from col_idx the index of the column to get the type of syntax parameters returns the duckdb_logical_type of the column duckdb_appender_error returns the error message associated with the given appender if the appender has no error message this returns nullptr instead the error message should not be freed it will be de-allocated when duckdb_appender_destroy is called syntax parameters appender the appender to get the error from returns the error message or nullptr if there is none duckdb_appender_flush flush the appender to the table forcing the cache of the appender to be cleared and the data to be appended to the base table this should generally not be used unless you know what you are doing instead call duckdb_appender_destroy when you are done with the appender syntax parameters appender the appender to flush returns duckdbsuccess on success or duckdberror on failure duckdb_appender_close close the appender flushing all intermediate state in the appender to the table and closing it for further appends this is generally not necessary call duckdb_appender_destroy instead syntax parameters appender the appender to flush and close returns duckdbsuccess on success or duckdberror on failure duckdb_appender_destroy close the appender and destroy it flushing all intermediate state in the appender to the table and de-allocating all memory associated with the appender syntax parameters appender the appender to flush close and destroy returns duckdbsuccess on success or duckdberror on failure duckdb_appender_begin_row a nop function provided for backwards compatibility reasons does nothing only duckdb_appender_end_row is required syntax duckdb_appender_end_row finish the current row of appends after end_row is called the next row can be appended syntax parameters appender the appender returns duckdbsuccess on success or duckdberror on failure duckdb_append_bool append a bool value to the appender syntax duckdb_append_int8 append an int8_t value to the appender syntax duckdb_append_int16 append an int16_t value to the appender syntax duckdb_append_int32 append an int32_t value to the appender syntax duckdb_append_int64 append an int64_t value to the appender syntax duckdb_append_hugeint append a duckdb_hugeint value to the appender syntax duckdb_append_uint8 append a uint8_t value to the appender syntax duckdb_append_uint16 append a uint16_t value to the appender syntax duckdb_append_uint32 append a uint32_t value to the appender syntax duckdb_append_uint64 append a uint64_t value to the appender syntax duckdb_append_uhugeint append a duckdb_uhugeint value to the appender syntax duckdb_append_float append a float value to the appender syntax duckdb_append_double append a double value to the appender syntax duckdb_append_date append a duckdb_date value to the appender syntax duckdb_append_time append a duckdb_time value to the appender syntax duckdb_append_timestamp append a duckdb_timestamp value to the appender syntax duckdb_append_interval append a duckdb_interval value to the appender syntax duckdb_append_varchar append a varchar value to the appender syntax duckdb_append_varchar_length append a varchar value to the appender syntax duckdb_append_blob append a blob value to the appender syntax duckdb_append_null append a null value to the appender of any type syntax duckdb_append_data_chunk appends a pre-filled data chunk to the specified appender the types of the data chunk must exactly match the types of the table no casting is performed if the types do not match or the appender is in an invalid state duckdberror is returned if the append is successful duckdbsuccess is returned syntax parameters appender the appender to append to chunk the data chunk to append returns the return state duckdb_query_arrow executes a sql query within a connection and stores the full materialized result in an arrow structure if the query fails to execute duckdberror is returned and the error message can be retrieved by calling duckdb_query_arrow_error note that after running duckdb_query_arrow duckdb_destroy_arrow must be called on the result object even if the query fails otherwise the error stored within the result will not be freed correctly syntax parameters connection the connection to perform the query in query the sql query to run out_result the query result returns duckdbsuccess on success or duckdberror on failure duckdb_query_arrow_schema fetch the internal arrow schema from the arrow result remember to call release on the respective arrowschema object syntax parameters result the result to fetch the schema from out_schema the output schema returns duckdbsuccess on success or duckdberror on failure duckdb_prepared_arrow_schema fetch the internal arrow schema from the prepared statement remember to call release on the respective arrowschema object syntax parameters result the prepared statement to fetch the schema from out_schema the output schema returns duckdbsuccess on success or duckdberror on failure duckdb_result_arrow_array convert a data chunk into an arrow struct array remember to call release on the respective arrowarray object syntax parameters result the result object the data chunk have been fetched from chunk the data chunk to convert out_array the output array duckdb_query_arrow_array fetch an internal arrow struct array from the arrow result remember to call release on the respective arrowarray object this function can be called multiple time to get next chunks which will free the previous out_array so consume the out_array before calling this function again syntax parameters result the result to fetch the array from out_array the output array returns duckdbsuccess on success or duckdberror on failure duckdb_arrow_column_count returns the number of columns present in the arrow result object syntax parameters result the result object returns the number of columns present in the result object duckdb_arrow_row_count returns the number of rows present in the arrow result object syntax parameters result the result object returns the number of rows present in the result object duckdb_arrow_rows_changed returns the number of rows changed by the query stored in the arrow result this is relevant only for insert update delete queries for other queries the rows_changed will be 0 syntax parameters result the result object returns the number of rows changed duckdb_query_arrow_error returns the error message contained within the result the error is only set if duckdb_query_arrow returns duckdberror the error message should not be freed it will be de-allocated when duckdb_destroy_arrow is called syntax parameters result the result object to fetch the error from returns the error of the result duckdb_destroy_arrow closes the result and de-allocates all memory allocated for the arrow result syntax parameters result the result to destroy duckdb_destroy_arrow_stream releases the arrow array stream and de-allocates its memory syntax parameters stream the arrow array stream to destroy duckdb_execute_prepared_arrow executes the prepared statement with the given bound parameters and returns an arrow query result note that after running duckdb_execute_prepared_arrow duckdb_destroy_arrow must be called on the result object syntax parameters prepared_statement the prepared statement to execute out_result the query result returns duckdbsuccess on success or duckdberror on failure duckdb_arrow_scan scans the arrow stream and creates a view with the given name syntax parameters connection the connection on which to execute the scan table_name name of the temporary view to create arrow arrow stream wrapper returns duckdbsuccess on success or duckdberror on failure duckdb_arrow_array_scan scans the arrow array and creates a view with the given name note that after running duckdb_arrow_array_scan duckdb_destroy_arrow_stream must be called on the out stream syntax parameters connection the connection on which to execute the scan table_name name of the temporary view to create arrow_schema arrow schema wrapper arrow_array arrow array wrapper out_stream output array stream that wraps around the passed schema for releasing deleting once done returns duckdbsuccess on success or duckdberror on failure duckdb_execute_tasks execute duckdb tasks on this thread will return after max_tasks have been executed or if there are no more tasks present syntax parameters database the database object to execute tasks for max_tasks the maximum amount of tasks to execute duckdb_create_task_state creates a task state that can be used with duckdb_execute_tasks_state to execute tasks until duckdb_finish_execution is called on the state duckdb_destroy_state must be called on the result syntax parameters database the database object to create the task state for returns the task state that can be used with duckdb_execute_tasks_state duckdb_execute_tasks_state execute duckdb tasks on this thread the thread will keep on executing tasks forever until duckdb_finish_execution is called on the state multiple threads can share the same duckdb_task_state syntax parameters state the task state of the executor duckdb_execute_n_tasks_state execute duckdb tasks on this thread the thread will keep on executing tasks until either duckdb_finish_execution is called on the state max_tasks tasks have been executed or there are no more tasks to be executed multiple threads can share the same duckdb_task_state syntax parameters state the task state of the executor max_tasks the maximum amount of tasks to execute returns the amount of tasks that have actually been executed duckdb_finish_execution finish execution on a specific task syntax parameters state the task state to finish execution duckdb_task_state_is_finished check if the provided duckdb_task_state has finished execution syntax parameters state the task state to inspect returns whether or not duckdb_finish_execution has been called on the task state duckdb_destroy_task_state destroys the task state returned from duckdb_create_task_state note that this should not be called while there is an active duckdb_execute_tasks_state running on the task state syntax parameters state the task state to clean up duckdb_execution_is_finished returns true if the execution of the current query is finished syntax parameters con the connection on which to check duckdb_stream_fetch_chunk fetches a data chunk from the streaming duckdb_result this function should be called repeatedly until the result is exhausted the result must be destroyed with duckdb_destroy_data_chunk this function can only be used on duckdb_results created with duckdb_pending_prepared_streaming if this function is used none of the other result functions can be used and vice versa i e this function cannot be mixed with the legacy result functions or the materialized result functions it is not known beforehand how many chunks will be returned by this result syntax parameters result the result object to fetch the data chunk from returns the resulting data chunk returns null if the result has an error",
			"category": "C",
			"url": "/docs/api/c/api",
			"blurb": "API Reference Open/Connect Configuration Query Execution Result Functions Safe fetch functions Helpers..."
		},
		{
			"title": "C API - Configuration",
			"text": "configuration options can be provided to change different settings of the database system note that many of these settings can be changed later on using pragma statements as well the configuration object should be created filled with values and passed to duckdb_open_ext example duckdb_database db duckdb_config config create the configuration object if duckdb_create_config config duckdberror handle error set some configuration options duckdb_set_config config access_mode read_write or read_only duckdb_set_config config threads 8 duckdb_set_config config max_memory 8gb duckdb_set_config config default_order desc open the database using the configuration if duckdb_open_ext null db config null duckdberror handle error cleanup the configuration object duckdb_destroy_config config run queries cleanup duckdb_close db api reference duckdb_create_config initializes an empty configuration object that can be used to provide start-up options for the duckdb instance through duckdb_open_ext the duckdb_config must be destroyed using duckdb_destroy_config this will always succeed unless there is a malloc failure syntax parameters out_config the result configuration object returns duckdbsuccess on success or duckdberror on failure duckdb_config_count this returns the total amount of configuration options available for usage with duckdb_get_config_flag this should not be called in a loop as it internally loops over all the options syntax parameters returns the amount of config options available duckdb_get_config_flag obtains a human-readable name and description of a specific configuration option this can be used to e g display configuration options this will succeed unless index is out of range i e duckdb_config_count the result name or description must not be freed syntax parameters index the index of the configuration option between 0 and duckdb_config_count out_name a name of the configuration flag out_description a description of the configuration flag returns duckdbsuccess on success or duckdberror on failure duckdb_set_config sets the specified option for the specified configuration the configuration option is indicated by name to obtain a list of config options see duckdb_get_config_flag in the source code configuration options are defined in config cpp this can fail if either the name is invalid or if the value provided for the option is invalid syntax parameters duckdb_config the configuration object to set the option on name the name of the configuration flag to set option the value to set the configuration flag to returns duckdbsuccess on success or duckdberror on failure duckdb_destroy_config destroys the specified configuration object and de-allocates all memory allocated for the object syntax parameters config the configuration object to destroy",
			"category": "C",
			"url": "/docs/api/c/config",
			"blurb": "Configuration options can be provided to change different settings of the database system. Note that many of these..."
		},
		{
			"title": "C API - Data Chunks",
			"text": "data chunks represent a horizontal slice of a table they hold a number of vectors that can each hold up to the vector_size rows the vector size can be obtained through the duckdb_vector_size function and is configurable but is usually set to 2048 data chunks and vectors are what duckdb uses natively to store and represent data for this reason the data chunk interface is the most efficient way of interfacing with duckdb be aware however that correctly interfacing with duckdb using the data chunk api does require knowledge of duckdb s internal vector format the primary manner of interfacing with data chunks is by obtaining the internal vectors of the data chunk using the duckdb_data_chunk_get_vector method and subsequently using the duckdb_vector_get_data and duckdb_vector_get_validity methods to read the internal data and the validity mask of the vector for composite types list and struct vectors duckdb_list_vector_get_child and duckdb_struct_vector_get_child should be used to read child vectors api reference vector interface validity mask functions duckdb_create_data_chunk creates an empty datachunk with the specified set of types note that the result must be destroyed with duckdb_destroy_data_chunk syntax parameters types an array of types of the data chunk column_count the number of columns returns the data chunk duckdb_destroy_data_chunk destroys the data chunk and de-allocates all memory allocated for that chunk syntax parameters chunk the data chunk to destroy duckdb_data_chunk_reset resets a data chunk clearing the validity masks and setting the cardinality of the data chunk to 0 syntax parameters chunk the data chunk to reset duckdb_data_chunk_get_column_count retrieves the number of columns in a data chunk syntax parameters chunk the data chunk to get the data from returns the number of columns in the data chunk duckdb_data_chunk_get_vector retrieves the vector at the specified column index in the data chunk the pointer to the vector is valid for as long as the chunk is alive it does not need to be destroyed syntax parameters chunk the data chunk to get the data from returns the vector duckdb_data_chunk_get_size retrieves the current number of tuples in a data chunk syntax parameters chunk the data chunk to get the data from returns the number of tuples in the data chunk duckdb_data_chunk_set_size sets the current number of tuples in a data chunk syntax parameters chunk the data chunk to set the size in size the number of tuples in the data chunk duckdb_vector_get_column_type retrieves the column type of the specified vector the result must be destroyed with duckdb_destroy_logical_type syntax parameters vector the vector get the data from returns the type of the vector duckdb_vector_get_data retrieves the data pointer of the vector the data pointer can be used to read or write values from the vector how to read or write values depends on the type of the vector syntax parameters vector the vector to get the data from returns the data pointer duckdb_vector_get_validity retrieves the validity mask pointer of the specified vector if all values are valid this function might return null the validity mask is a bitset that signifies null-ness within the data chunk it is a series of uint64_t values where each uint64_t value contains validity for 64 tuples the bit is set to 1 if the value is valid i e not null or 0 if the value is invalid i e null validity of a specific value can be obtained like this idx_t entry_idx row_idx 64 idx_t idx_in_entry row_idx 64 bool is_valid validity_mask entry_idx 1 idx_in_entry alternatively the slower duckdb_validity_row_is_valid function can be used syntax parameters vector the vector to get the data from returns the pointer to the validity mask or null if no validity mask is present duckdb_vector_ensure_validity_writable ensures the validity mask is writable by allocating it after this function is called duckdb_vector_get_validity will always return non-null this allows null values to be written to the vector regardless of whether a validity mask was present before syntax parameters vector the vector to alter duckdb_vector_assign_string_element assigns a string element in the vector at the specified location syntax parameters vector the vector to alter index the row position in the vector to assign the string to str the null-terminated string duckdb_vector_assign_string_element_len assigns a string element in the vector at the specified location syntax parameters vector the vector to alter index the row position in the vector to assign the string to str the string str_len the length of the string in bytes duckdb_list_vector_get_child retrieves the child vector of a list vector the resulting vector is valid as long as the parent vector is valid syntax parameters vector the vector returns the child vector duckdb_list_vector_get_size returns the size of the child vector of the list syntax parameters vector the vector returns the size of the child list duckdb_list_vector_set_size sets the total size of the underlying child-vector of a list vector syntax parameters vector the list vector size the size of the child list returns the duckdb state returns duckdberror if the vector is nullptr duckdb_list_vector_reserve sets the total capacity of the underlying child-vector of a list syntax parameters vector the list vector required_capacity the total capacity to reserve return the duckdb state returns duckdberror if the vector is nullptr duckdb_struct_vector_get_child retrieves the child vector of a struct vector the resulting vector is valid as long as the parent vector is valid syntax parameters vector the vector index the child index returns the child vector duckdb_validity_row_is_valid returns whether or not a row is valid i e not null in the given validity mask syntax parameters validity the validity mask as obtained through duckdb_vector_get_validity row the row index returns true if the row is valid false otherwise duckdb_validity_set_row_validity in a validity mask sets a specific row to either valid or invalid note that duckdb_vector_ensure_validity_writable should be called before calling duckdb_vector_get_validity to ensure that there is a validity mask to write to syntax parameters validity the validity mask as obtained through duckdb_vector_get_validity row the row index valid whether or not to set the row to valid or invalid duckdb_validity_set_row_invalid in a validity mask sets a specific row to invalid equivalent to duckdb_validity_set_row_validity with valid set to false syntax parameters validity the validity mask row the row index duckdb_validity_set_row_valid in a validity mask sets a specific row to valid equivalent to duckdb_validity_set_row_validity with valid set to true syntax parameters validity the validity mask row the row index",
			"category": "C",
			"url": "/docs/api/c/data_chunk",
			"blurb": "Data chunks represent a horizontal slice of a table. They hold a number of vectors, that can each hold up to the..."
		},
		{
			"title": "C API - Overview",
			"text": "duckdb implements a custom c api modelled somewhat following the sqlite c api the api is contained in the duckdb h header continue to startup shutdown to get started or check out the full api overview we also provide a sqlite api wrapper which means that if your applications is programmed against the sqlite c api you can re-link to duckdb and it should continue working see the sqlite_api_wrapper folder in our source repository for more information installation the duckdb c api can be installed as part of the libduckdb packages please see the installation page for details pages in this section",
			"category": "C",
			"url": "/docs/api/c/overview",
			"blurb": "DuckDB implements a custom C API modelled somewhat following the SQLite C API. The API is contained in the duckdb.h..."
		},
		{
			"title": "C API - Prepared Statements",
			"text": "a prepared statement is a parameterized query the query is prepared with question marks or dollar symbols 1 indicating the parameters of the query values can then be bound to these parameters after which the prepared statement can be executed using those parameters a single query can be prepared once and executed many times prepared statements are useful to easily supply parameters to functions while avoiding string concatenation sql injection attacks speeding up queries that will be executed many times with different parameters duckdb supports prepared statements in the c api with the duckdb_prepare method the duckdb_bind family of functions is used to supply values for subsequent execution of the prepared statement using duckdb_execute_prepared after we are done with the prepared statement it can be cleaned up using the duckdb_destroy_prepare method example duckdb_prepared_statement stmt duckdb_result result if duckdb_prepare con insert into integers values 1 2 stmt duckdberror handle error duckdb_bind_int32 stmt 1 42 the parameter index starts counting at 1 duckdb_bind_int32 stmt 2 43 null as second parameter means no result set is requested duckdb_execute_prepared stmt null duckdb_destroy_prepare stmt we can also query result sets using prepared statements if duckdb_prepare con select from integers where i stmt duckdberror handle error duckdb_bind_int32 stmt 1 42 duckdb_execute_prepared stmt result do something with result clean up duckdb_destroy_result result duckdb_destroy_prepare stmt after calling duckdb_prepare the prepared statement parameters can be inspected using duckdb_nparams and duckdb_param_type in case the prepare fails the error can be obtained through duckdb_prepare_error it is not required that the duckdb_bind family of functions matches the prepared statement parameter type exactly the values will be auto-cast to the required value as required for example calling duckdb_bind_int8 on a parameter type of duckdb_type_integer will work as expected do not use prepared statements to insert large amounts of data into duckdb instead it is recommended to use the appender api reference duckdb_prepare create a prepared statement object from a query note that after calling duckdb_prepare the prepared statement should always be destroyed using duckdb_destroy_prepare even if the prepare fails if the prepare fails duckdb_prepare_error can be called to obtain the reason why the prepare failed syntax parameters connection the connection object query the sql query to prepare out_prepared_statement the resulting prepared statement object returns duckdbsuccess on success or duckdberror on failure duckdb_destroy_prepare closes the prepared statement and de-allocates all memory allocated for the statement syntax parameters prepared_statement the prepared statement to destroy duckdb_prepare_error returns the error message associated with the given prepared statement if the prepared statement has no error message this returns nullptr instead the error message should not be freed it will be de-allocated when duckdb_destroy_prepare is called syntax parameters prepared_statement the prepared statement to obtain the error from returns the error message or nullptr if there is none duckdb_nparams returns the number of parameters that can be provided to the given prepared statement returns 0 if the query was not successfully prepared syntax parameters prepared_statement the prepared statement to obtain the number of parameters for duckdb_parameter_name returns the name used to identify the parameter the returned string should be freed using duckdb_free returns null if the index is out of range for the provided prepared statement syntax parameters prepared_statement the prepared statement for which to get the parameter name from duckdb_param_type returns the parameter type for the parameter at the given index returns duckdb_type_invalid if the parameter index is out of range or the statement was not successfully prepared syntax parameters prepared_statement the prepared statement param_idx the parameter index returns the parameter type duckdb_clear_bindings clear the params bind to the prepared statement syntax duckdb_prepared_statement_type returns the statement type of the statement to be executed syntax parameters statement the prepared statement returns duckdb_statement_type value or duckdb_statement_type_invalid",
			"category": "C",
			"url": "/docs/api/c/prepared",
			"blurb": "A prepared statement is a parameterized query. The query is prepared with question marks ( ? ) or dollar symbols ( $1..."
		},
		{
			"title": "C API - Query",
			"text": "the duckdb_query method allows sql queries to be run in duckdb from c this method takes two parameters a null-terminated sql query string and a duckdb_result result pointer the result pointer may be null if the application is not interested in the result set or if the query produces no result after the result is consumed the duckdb_destroy_result method should be used to clean up the result elements can be extracted from the duckdb_result object using a variety of methods the duckdb_column_count and duckdb_row_count methods can be used to extract the number of columns and the number of rows respectively duckdb_column_name and duckdb_column_type can be used to extract the names and types of individual columns example duckdb_state state duckdb_result result create a table state duckdb_query con create table integers i integer j integer null if state duckdberror handle error insert three rows into the table state duckdb_query con insert into integers values 3 4 5 6 7 null null if state duckdberror handle error query rows again state duckdb_query con select from integers result if state duckdberror handle error handle the result destroy the result after we are done with it duckdb_destroy_result result value extraction values can be extracted using either the duckdb_column_data duckdb_nullmask_data functions or using the duckdb_value convenience functions the duckdb_column_data duckdb_nullmask_data functions directly hand you a pointer to the result arrays in columnar format and can therefore be very fast the duckdb_value functions perform bounds- and type-checking and will automatically cast values to the desired type this makes them more convenient and easier to use at the expense of being slower see the types page for more information for optimal performance use duckdb_column_data and duckdb_nullmask_data to extract data from the query result the duckdb_value functions perform internal type-checking bounds-checking and casting which makes them slower duckdb_value below is an example that prints the above result to csv format using the duckdb_value_varchar function note that the function is generic we do not need to know about the types of the individual result columns print the above result to csv format using duckdb_value_varchar idx_t row_count duckdb_row_count result idx_t column_count duckdb_column_count result for idx_t row 0 row row_count row for idx_t col 0 col column_count col if col 0 printf auto str_val duckdb_value_varchar result col row printf s str_val duckdb_free str_val printf n duckdb_column_data below is an example that prints the above result to csv format using the duckdb_column_data function note that the function is not generic we do need to know exactly what the types of the result columns are int32_t i_data int32_t duckdb_column_data result 0 int32_t j_data int32_t duckdb_column_data result 1 bool i_mask duckdb_nullmask_data result 0 bool j_mask duckdb_nullmask_data result 1 idx_t row_count duckdb_row_count result for idx_t row 0 row row_count row if i_mask row printf null else printf d i_data row printf if j_mask row printf null else printf d j_data row printf n when using duckdb_column_data be careful that the type matches exactly what you expect it to be as the code directly accesses an internal array there is no type-checking accessing a duckdb_type_integer column as if it was a duckdb_type_bigint column will provide unpredictable results api reference duckdb_query executes a sql query within a connection and stores the full materialized result in the out_result pointer if the query fails to execute duckdberror is returned and the error message can be retrieved by calling duckdb_result_error note that after running duckdb_query duckdb_destroy_result must be called on the result object even if the query fails otherwise the error stored within the result will not be freed correctly syntax parameters connection the connection to perform the query in query the sql query to run out_result the query result returns duckdbsuccess on success or duckdberror on failure duckdb_destroy_result closes the result and de-allocates all memory allocated for that connection syntax parameters result the result to destroy duckdb_column_name returns the column name of the specified column the result should not need to be freed the column names will automatically be destroyed when the result is destroyed returns null if the column is out of range syntax parameters result the result object to fetch the column name from col the column index returns the column name of the specified column duckdb_column_type returns the column type of the specified column returns duckdb_type_invalid if the column is out of range syntax parameters result the result object to fetch the column type from col the column index returns the column type of the specified column duckdb_result_statement_type returns the statement type of the statement that was executed syntax parameters result the result object to fetch the statement type from returns duckdb_statement_type value or duckdb_statement_type_invalid duckdb_column_logical_type returns the logical column type of the specified column the return type of this call should be destroyed with duckdb_destroy_logical_type returns null if the column is out of range syntax parameters result the result object to fetch the column type from col the column index returns the logical column type of the specified column duckdb_column_count returns the number of columns present in a the result object syntax parameters result the result object returns the number of columns present in the result object duckdb_row_count returns the number of rows present in the result object syntax parameters result the result object returns the number of rows present in the result object duckdb_rows_changed returns the number of rows changed by the query stored in the result this is relevant only for insert update delete queries for other queries the rows_changed will be 0 syntax parameters result the result object returns the number of rows changed duckdb_column_data deprecated prefer using duckdb_result_get_chunk instead returns the data of a specific column of a result in columnar format the function returns a dense array which contains the result data the exact type stored in the array depends on the corresponding duckdb_type as provided by duckdb_column_type for the exact type by which the data should be accessed see the comments in the types section or the duckdb_type enum for example for a column of type duckdb_type_integer rows can be accessed in the following manner int32_t data int32_t duckdb_column_data result 0 printf data for row d d n row data row syntax parameters result the result object to fetch the column data from col the column index returns the column data of the specified column duckdb_nullmask_data deprecated prefer using duckdb_result_get_chunk instead returns the nullmask of a specific column of a result in columnar format the nullmask indicates for every row whether or not the corresponding row is null if a row is null the values present in the array provided by duckdb_column_data are undefined int32_t data int32_t duckdb_column_data result 0 bool nullmask duckdb_nullmask_data result 0 if nullmask row printf data for row d null n row else printf data for row d d n row data row syntax parameters result the result object to fetch the nullmask from col the column index returns the nullmask of the specified column duckdb_result_error returns the error message contained within the result the error is only set if duckdb_query returns duckdberror the result of this function must not be freed it will be cleaned up when duckdb_destroy_result is called syntax parameters result the result object to fetch the error from returns the error of the result",
			"category": "C",
			"url": "/docs/api/c/query",
			"blurb": "The duckdb_query method allows SQL queries to be run in DuckDB from C. This method takes two parameters, a (null-..."
		},
		{
			"title": "C API - Replacement Scans",
			"text": "the replacement scan api can be used to register a callback that is called when a table is read that does not exist in the catalog for example when a query such as select from my_table is executed and my_table does not exist the replacement scan callback will be called with my_table as parameter the replacement scan can then insert a table function with a specific parameter to replace the read of the table api reference duckdb_add_replacement_scan add a replacement scan definition to the specified database syntax parameters db the database object to add the replacement scan to replacement the replacement scan callback extra_data extra data that is passed back into the specified callback delete_callback the delete callback to call on the extra data if any duckdb_replacement_scan_set_function_name sets the replacement function name if this function is called in the replacement callback the replacement scan is performed if it is not called the replacement callback is not performed syntax parameters info the info object function_name the function name to substitute duckdb_replacement_scan_add_parameter adds a parameter to the replacement scan function syntax parameters info the info object parameter the parameter to add duckdb_replacement_scan_set_error report that an error has occurred while executing the replacement scan syntax parameters info the info object error the error message",
			"category": "C",
			"url": "/docs/api/c/replacement_scans",
			"blurb": "The replacement scan API can be used to register a callback that is called when a table is read that does not exist..."
		},
		{
			"title": "C API - Startup & Shutdown",
			"text": "to use duckdb you must first initialize a duckdb_database handle using duckdb_open duckdb_open takes as parameter the database file to read and write from the special value null nullptr can be used to create an in-memory database note that for an in-memory database no data is persisted to disk i e all data is lost when you exit the process with the duckdb_database handle you can create one or many duckdb_connection using duckdb_connect while individual connections are thread-safe they will be locked during querying it is therefore recommended that each thread uses its own connection to allow for the best parallel performance all duckdb_connection s have to explicitly be disconnected with duckdb_disconnect and the duckdb_database has to be explicitly closed with duckdb_close to avoid memory and file handle leaking example duckdb_database db duckdb_connection con if duckdb_open null db duckdberror handle error if duckdb_connect db con duckdberror handle error run queries cleanup duckdb_disconnect con duckdb_close db api reference duckdb_open creates a new database or opens an existing database file stored at the given path if no path is given a new in-memory database is created instead the instantiated database should be closed with duckdb_close syntax parameters path path to the database file on disk or nullptr or memory to open an in-memory database out_database the result database object returns duckdbsuccess on success or duckdberror on failure duckdb_open_ext extended version of duckdb_open creates a new database or opens an existing database file stored at the given path the instantiated database should be closed with duckdb_close syntax parameters path path to the database file on disk or nullptr or memory to open an in-memory database out_database the result database object config optional configuration used to start up the database system out_error if set and the function returns duckdberror this will contain the reason why the start-up failed note that the error must be freed using duckdb_free returns duckdbsuccess on success or duckdberror on failure duckdb_close closes the specified database and de-allocates all memory allocated for that database this should be called after you are done with any database allocated through duckdb_open or duckdb_open_ext note that failing to call duckdb_close in case of e g a program crash will not cause data corruption still it is recommended to always correctly close a database object after you are done with it syntax parameters database the database object to shut down duckdb_connect opens a connection to a database connections are required to query the database and store transactional state associated with the connection the instantiated connection should be closed using duckdb_disconnect syntax parameters database the database file to connect to out_connection the result connection object returns duckdbsuccess on success or duckdberror on failure duckdb_interrupt interrupt running query syntax parameters connection the connection to interrupt duckdb_query_progress get progress of the running query syntax parameters connection the working connection returns -1 if no progress or a percentage of the progress duckdb_disconnect closes the specified connection and de-allocates all memory allocated for that connection syntax parameters connection the connection to close duckdb_library_version returns the version of the linked duckdb with a version postfix for dev versions usually used for developing c extensions that must return this for a compatibility check syntax",
			"category": "C",
			"url": "/docs/api/c/connect",
			"blurb": "To use DuckDB, you must first initialize a duckdb_database handle using duckdb_open() . duckdb_open() takes as..."
		},
		{
			"title": "C API - Table Functions",
			"text": "the table function api can be used to define a table function that can then be called from within duckdb in the from clause of a query api reference table function bind table function init table function duckdb_create_table_function creates a new empty table function the return value should be destroyed with duckdb_destroy_table_function syntax parameters returns the table function object duckdb_destroy_table_function destroys the given table function object syntax parameters table_function the table function to destroy duckdb_table_function_set_name sets the name of the given table function syntax parameters table_function the table function name the name of the table function duckdb_table_function_add_parameter adds a parameter to the table function syntax parameters table_function the table function type the type of the parameter to add duckdb_table_function_add_named_parameter adds a named parameter to the table function syntax parameters table_function the table function name the name of the parameter type the type of the parameter to add duckdb_table_function_set_extra_info assigns extra information to the table function that can be fetched during binding etc syntax parameters table_function the table function extra_info the extra information destroy the callback that will be called to destroy the bind data if any duckdb_table_function_set_bind sets the bind function of the table function syntax parameters table_function the table function bind the bind function duckdb_table_function_set_init sets the init function of the table function syntax parameters table_function the table function init the init function duckdb_table_function_set_local_init sets the thread-local init function of the table function syntax parameters table_function the table function init the init function duckdb_table_function_set_function sets the main function of the table function syntax parameters table_function the table function function the function duckdb_table_function_supports_projection_pushdown sets whether or not the given table function supports projection pushdown if this is set to true the system will provide a list of all required columns in the init stage through the duckdb_init_get_column_count and duckdb_init_get_column_index functions if this is set to false the default the system will expect all columns to be projected syntax parameters table_function the table function pushdown true if the table function supports projection pushdown false otherwise duckdb_register_table_function register the table function object within the given connection the function requires at least a name a bind function an init function and a main function if the function is incomplete or a function with this name already exists duckdberror is returned syntax parameters con the connection to register it in function the function pointer returns whether or not the registration was successful duckdb_bind_get_extra_info retrieves the extra info of the function as set in duckdb_table_function_set_extra_info syntax parameters info the info object returns the extra info duckdb_bind_add_result_column adds a result column to the output of the table function syntax parameters info the info object name the name of the column type the logical type of the column duckdb_bind_get_parameter_count retrieves the number of regular non-named parameters to the function syntax parameters info the info object returns the number of parameters duckdb_bind_get_parameter retrieves the parameter at the given index the result must be destroyed with duckdb_destroy_value syntax parameters info the info object index the index of the parameter to get returns the value of the parameter must be destroyed with duckdb_destroy_value duckdb_bind_get_named_parameter retrieves a named parameter with the given name the result must be destroyed with duckdb_destroy_value syntax parameters info the info object name the name of the parameter returns the value of the parameter must be destroyed with duckdb_destroy_value duckdb_bind_set_bind_data sets the user-provided bind data in the bind object this object can be retrieved again during execution syntax parameters info the info object extra_data the bind data object destroy the callback that will be called to destroy the bind data if any duckdb_bind_set_cardinality sets the cardinality estimate for the table function used for optimization syntax parameters info the bind data object is_exact whether or not the cardinality estimate is exact or an approximation duckdb_bind_set_error report that an error has occurred while calling bind syntax parameters info the info object error the error message duckdb_init_get_extra_info retrieves the extra info of the function as set in duckdb_table_function_set_extra_info syntax parameters info the info object returns the extra info duckdb_init_get_bind_data gets the bind data set by duckdb_bind_set_bind_data during the bind note that the bind data should be considered as read-only for tracking state use the init data instead syntax parameters info the info object returns the bind data object duckdb_init_set_init_data sets the user-provided init data in the init object this object can be retrieved again during execution syntax parameters info the info object extra_data the init data object destroy the callback that will be called to destroy the init data if any duckdb_init_get_column_count returns the number of projected columns this function must be used if projection pushdown is enabled to figure out which columns to emit syntax parameters info the info object returns the number of projected columns duckdb_init_get_column_index returns the column index of the projected column at the specified position this function must be used if projection pushdown is enabled to figure out which columns to emit syntax parameters info the info object column_index the index at which to get the projected column index from 0 duckdb_init_get_column_count info returns the column index of the projected column duckdb_init_set_max_threads sets how many threads can process this table function in parallel default 1 syntax parameters info the info object max_threads the maximum amount of threads that can process this table function duckdb_init_set_error report that an error has occurred while calling init syntax parameters info the info object error the error message duckdb_function_get_extra_info retrieves the extra info of the function as set in duckdb_table_function_set_extra_info syntax parameters info the info object returns the extra info duckdb_function_get_bind_data gets the bind data set by duckdb_bind_set_bind_data during the bind note that the bind data should be considered as read-only for tracking state use the init data instead syntax parameters info the info object returns the bind data object duckdb_function_get_init_data gets the init data set by duckdb_init_set_init_data during the init syntax parameters info the info object returns the init data object duckdb_function_get_local_init_data gets the thread-local init data set by duckdb_init_set_init_data during the local_init syntax parameters info the info object returns the init data object duckdb_function_set_error report that an error has occurred while executing the function syntax parameters info the info object error the error message",
			"category": "C",
			"url": "/docs/api/c/table_functions",
			"blurb": "The table function API can be used to define a table function that can then be called from within DuckDB in the FROM..."
		},
		{
			"title": "C API - Types",
			"text": "duckdb is a strongly typed database system as such every column has a single type specified this type is constant over the entire column that is to say a column that is labeled as an integer column will only contain integer values duckdb also supports columns of composite types for example it is possible to define an array of integers int it is also possible to define types as arbitrary structs row i integer j varchar for that reason native duckdb type objects are not mere enums but a class that can potentially be nested types in the c api are modeled using an enum duckdb_type and a complex class duckdb_logical_type for most primitive types e g integers or varchars the enum is sufficient for more complex types such as lists structs or decimals the logical type must be used typedef enum duckdb_type duckdb_type_invalid duckdb_type_boolean duckdb_type_tinyint duckdb_type_smallint duckdb_type_integer duckdb_type_bigint duckdb_type_utinyint duckdb_type_usmallint duckdb_type_uinteger duckdb_type_ubigint duckdb_type_float duckdb_type_double duckdb_type_timestamp duckdb_type_date duckdb_type_time duckdb_type_interval duckdb_type_hugeint duckdb_type_varchar duckdb_type_blob duckdb_type_decimal duckdb_type_timestamp_s duckdb_type_timestamp_ms duckdb_type_timestamp_ns duckdb_type_enum duckdb_type_list duckdb_type_struct duckdb_type_map duckdb_type_uuid duckdb_type_union duckdb_type_bit duckdb_type functions the enum type of a column in the result can be obtained using the duckdb_column_type function the logical type of a column can be obtained using the duckdb_column_logical_type function duckdb_value the duckdb_value functions will auto-cast values as required for example it is no problem to use duckdb_value_double on a column of type duckdb_value_int32 the value will be auto-cast and returned as a double note that in certain cases the cast may fail for example this can happen if we request a duckdb_value_int8 and the value does not fit within an int8 value in this case a default value will be returned usually 0 or nullptr the same default value will also be returned if the corresponding value is null the duckdb_value_is_null function can be used to check if a specific value is null or not the exception to the auto-cast rule is the duckdb_value_varchar_internal function this function does not auto-cast and only works for varchar columns the reason this function exists is that the result does not need to be freed note that duckdb_value_varchar and duckdb_value_blob require the result to be de-allocated using duckdb_free duckdb_result_get_chunk the duckdb_result_get_chunk function can be used to read data chunks from a duckdb result set and is the most efficient way of reading data from a duckdb result using the c api it is also the only way of reading data of certain types from a duckdb result for example the duckdb_value functions do not support structural reading of composite types lists or structs or more complex types like enums and decimals for more information about data chunks see the documentation on data chunks api reference date time timestamp helpers hugeint helpers decimal helpers logical type interface duckdb_result_get_chunk fetches a data chunk from the duckdb_result this function should be called repeatedly until the result is exhausted the result must be destroyed with duckdb_destroy_data_chunk this function supersedes all duckdb_value functions as well as the duckdb_column_data and duckdb_nullmask_data functions it results in significantly better performance and should be preferred in newer code-bases if this function is used none of the other result functions can be used and vice versa i e this function cannot be mixed with the legacy result functions use duckdb_result_chunk_count to figure out how many chunks there are in the result syntax parameters result the result object to fetch the data chunk from chunk_index the chunk index to fetch from returns the resulting data chunk returns null if the chunk index is out of bounds duckdb_result_is_streaming checks if the type of the internal result is streamqueryresult syntax parameters result the result object to check returns whether or not the result object is of the type streamqueryresult duckdb_result_chunk_count returns the number of data chunks present in the result syntax parameters result the result object returns number of data chunks present in the result duckdb_result_return_type returns the return_type of the given result or duckdb_return_type_invalid on error syntax parameters result the result object returns the return_type duckdb_from_date decompose a duckdb_date object into year month and date stored as duckdb_date_struct syntax parameters date the date object as obtained from a duckdb_type_date column returns the duckdb_date_struct with the decomposed elements duckdb_to_date re-compose a duckdb_date from year month and date duckdb_date_struct syntax parameters date the year month and date stored in a duckdb_date_struct returns the duckdb_date element duckdb_is_finite_date test a duckdb_date to see if it is a finite value syntax parameters date the date object as obtained from a duckdb_type_date column returns true if the date is finite false if it is infinity duckdb_from_time decompose a duckdb_time object into hour minute second and microsecond stored as duckdb_time_struct syntax parameters time the time object as obtained from a duckdb_type_time column returns the duckdb_time_struct with the decomposed elements duckdb_create_time_tz create a duckdb_time_tz object from micros and a timezone offset syntax parameters micros the microsecond component of the time offset the timezone offset component of the time returns the duckdb_time_tz element duckdb_from_time_tz decompose a time_tz objects into micros and a timezone offset use duckdb_from_time to further decompose the micros into hour minute second and microsecond syntax parameters micros the time object as obtained from a duckdb_type_time_tz column out_micros the microsecond component of the time out_offset the timezone offset component of the time duckdb_to_time re-compose a duckdb_time from hour minute second and microsecond duckdb_time_struct syntax parameters time the hour minute second and microsecond in a duckdb_time_struct returns the duckdb_time element duckdb_from_timestamp decompose a duckdb_timestamp object into a duckdb_timestamp_struct syntax parameters ts the ts object as obtained from a duckdb_type_timestamp column returns the duckdb_timestamp_struct with the decomposed elements duckdb_to_timestamp re-compose a duckdb_timestamp from a duckdb_timestamp_struct syntax parameters ts the de-composed elements in a duckdb_timestamp_struct returns the duckdb_timestamp element duckdb_is_finite_timestamp test a duckdb_timestamp to see if it is a finite value syntax parameters ts the timestamp object as obtained from a duckdb_type_timestamp column returns true if the timestamp is finite false if it is infinity duckdb_hugeint_to_double converts a duckdb_hugeint object as obtained from a duckdb_type_hugeint column into a double syntax parameters val the hugeint value returns the converted double element duckdb_double_to_hugeint converts a double value to a duckdb_hugeint object if the conversion fails because the double value is too big the result will be 0 syntax parameters val the double value returns the converted duckdb_hugeint element duckdb_double_to_decimal converts a double value to a duckdb_decimal object if the conversion fails because the double value is too big or the width scale are invalid the result will be 0 syntax parameters val the double value returns the converted duckdb_decimal element duckdb_decimal_to_double converts a duckdb_decimal object as obtained from a duckdb_type_decimal column into a double syntax parameters val the decimal value returns the converted double element duckdb_create_logical_type creates a duckdb_logical_type from a standard primitive type the resulting type should be destroyed with duckdb_destroy_logical_type this should not be used with duckdb_type_decimal syntax parameters type the primitive type to create returns the logical type duckdb_logical_type_get_alias returns the alias of a duckdb_logical_type if one is set else null the result must be destroyed with duckdb_free syntax parameters type the logical type to return the alias of returns the alias or null duckdb_create_list_type creates a list type from its child type the resulting type should be destroyed with duckdb_destroy_logical_type syntax parameters type the child type of list type to create returns the logical type duckdb_create_map_type creates a map type from its key type and value type the resulting type should be destroyed with duckdb_destroy_logical_type syntax parameters type the key type and value type of map type to create returns the logical type duckdb_create_union_type creates a union type from the passed types array the resulting type should be destroyed with duckdb_destroy_logical_type syntax parameters types the array of types that the union should consist of type_amount the size of the types array returns the logical type duckdb_create_struct_type creates a struct type from the passed member name and type arrays the resulting type should be destroyed with duckdb_destroy_logical_type syntax parameters member_types the array of types that the struct should consist of member_names the array of names that the struct should consist of member_count the number of members that were specified for both arrays returns the logical type duckdb_create_enum_type creates an enum type from the passed member name array the resulting type should be destroyed with duckdb_destroy_logical_type syntax parameters enum_name the name of the enum member_names the array of names that the enum should consist of member_count the number of elements that were specified in the array returns the logical type duckdb_create_decimal_type creates a duckdb_logical_type of type decimal with the specified width and scale the resulting type should be destroyed with duckdb_destroy_logical_type syntax parameters width the width of the decimal type scale the scale of the decimal type returns the logical type duckdb_get_type_id retrieves the enum type class of a duckdb_logical_type syntax parameters type the logical type object returns the type id duckdb_decimal_width retrieves the width of a decimal type syntax parameters type the logical type object returns the width of the decimal type duckdb_decimal_scale retrieves the scale of a decimal type syntax parameters type the logical type object returns the scale of the decimal type duckdb_decimal_internal_type retrieves the internal storage type of a decimal type syntax parameters type the logical type object returns the internal type of the decimal type duckdb_enum_internal_type retrieves the internal storage type of an enum type syntax parameters type the logical type object returns the internal type of the enum type duckdb_enum_dictionary_size retrieves the dictionary size of the enum type syntax parameters type the logical type object returns the dictionary size of the enum type duckdb_enum_dictionary_value retrieves the dictionary value at the specified position from the enum the result must be freed with duckdb_free syntax parameters type the logical type object index the index in the dictionary returns the string value of the enum type must be freed with duckdb_free duckdb_list_type_child_type retrieves the child type of the given list type the result must be freed with duckdb_destroy_logical_type syntax parameters type the logical type object returns the child type of the list type must be destroyed with duckdb_destroy_logical_type duckdb_map_type_key_type retrieves the key type of the given map type the result must be freed with duckdb_destroy_logical_type syntax parameters type the logical type object returns the key type of the map type must be destroyed with duckdb_destroy_logical_type duckdb_map_type_value_type retrieves the value type of the given map type the result must be freed with duckdb_destroy_logical_type syntax parameters type the logical type object returns the value type of the map type must be destroyed with duckdb_destroy_logical_type duckdb_struct_type_child_count returns the number of children of a struct type syntax parameters type the logical type object returns the number of children of a struct type duckdb_struct_type_child_name retrieves the name of the struct child the result must be freed with duckdb_free syntax parameters type the logical type object index the child index returns the name of the struct type must be freed with duckdb_free duckdb_struct_type_child_type retrieves the child type of the given struct type at the specified index the result must be freed with duckdb_destroy_logical_type syntax parameters type the logical type object index the child index returns the child type of the struct type must be destroyed with duckdb_destroy_logical_type duckdb_union_type_member_count returns the number of members that the union type has syntax parameters type the logical type union object returns the number of members of a union type duckdb_union_type_member_name retrieves the name of the union member the result must be freed with duckdb_free syntax parameters type the logical type object index the child index returns the name of the union member must be freed with duckdb_free duckdb_union_type_member_type retrieves the child type of the given union member at the specified index the result must be freed with duckdb_destroy_logical_type syntax parameters type the logical type object index the child index returns the child type of the union member must be destroyed with duckdb_destroy_logical_type duckdb_destroy_logical_type destroys the logical type and de-allocates all memory allocated for that type syntax parameters type the logical type to destroy",
			"category": "C",
			"url": "/docs/api/c/types",
			"blurb": "DuckDB is a strongly typed database system. As such, every column has a single type specified. This type is constant..."
		},
		{
			"title": "C API - Values",
			"text": "the value class represents a single value of any type api reference duckdb_destroy_value destroys the value and de-allocates all memory allocated for that type syntax parameters value the value to destroy duckdb_create_varchar creates a value from a null-terminated string syntax parameters value the null-terminated string returns the value this must be destroyed with duckdb_destroy_value duckdb_create_varchar_length creates a value from a string syntax parameters value the text length the length of the text returns the value this must be destroyed with duckdb_destroy_value duckdb_create_int64 creates a value from an int64 syntax parameters value the bigint value returns the value this must be destroyed with duckdb_destroy_value duckdb_create_struct_value creates a struct value from a type and an array of values syntax parameters type the type of the struct values the values for the struct fields returns the value this must be destroyed with duckdb_destroy_value duckdb_create_list_value creates a list value from a type and an array of values of length value_count syntax parameters type the type of the list values the values for the list value_count the number of values in the list returns the value this must be destroyed with duckdb_destroy_value duckdb_get_varchar obtains a string representation of the given value the result must be destroyed with duckdb_free syntax parameters value the value returns the string value this must be destroyed with duckdb_free duckdb_get_int64 obtains an int64 of the given value syntax parameters value the value returns the int64 value or 0 if no conversion is possible",
			"category": "C",
			"url": "/docs/api/c/value",
			"blurb": "The value class represents a single value of any type. API Reference duckdb_destroy_value Destroys the value and de-..."
		},
		{
			"title": "C++ API",
			"text": "installation the duckdb c api can be installed as part of the libduckdb packages please see the installation page for details basic api usage duckdb implements a custom c api this is built around the abstractions of a database instance duckdb class multiple connection s to the database instance and queryresult instances as the result of queries the header file for the c api is duckdb hpp the standard source distribution of libduckdb contains an amalgamation of the duckdb sources which combine all sources into two files duckdb hpp and duckdb cpp the duckdb hpp header is much larger in this case regardless of whether you are using the amalgamation or not just include duckdb hpp startup shutdown to use duckdb you must first initialize a duckdb instance using its constructor duckdb takes as parameter the database file to read and write from the special value nullptr can be used to create an in-memory database note that for an in-memory database no data is persisted to disk i e all data is lost when you exit the process the second parameter to the duckdb constructor is an optional dbconfig object in dbconfig you can set various database parameters for example the read write mode or memory limits the duckdb constructor may throw exceptions for example if the database file is not usable with the duckdb instance you can create one or many connection instances using the connection constructor while connections should be thread-safe they will be locked during querying it is therefore recommended that each thread uses its own connection if you are in a multithreaded environment duckdb db nullptr connection con db querying connections expose the query method to send a sql query string to duckdb from c query fully materializes the query result as a materializedqueryresult in memory before returning at which point the query result can be consumed there is also a streaming api for queries see further below create a table con query create table integers i integer j integer insert three rows into the table con query insert into integers values 3 4 5 6 7 null materializedqueryresult result con query select from integers if result- success cerr result- error the materializedqueryresult instance contains firstly two fields that indicate whether the query was successful query will not throw exceptions under normal circumstances instead invalid queries or other issues will lead to the success boolean field in the query result instance to be set to false in this case an error message may be available in error as a string if successful other fields are set the type of statement that was just executed e g statementtype insert_statement is contained in statement_type the high-level logical type sql type types of the result set columns are in types the names of the result columns are in the names string vector in case multiple result sets are returned for example because the result set contained multiple statements the result set can be chained using the next field duckdb also supports prepared statements in the c api with the prepare method this returns an instance of preparedstatement this instance can be used to execute the prepared statement with parameters below is an example std unique_ptr preparedstatement prepare con prepare select count from a where i 1 std unique_ptr queryresult result prepare- execute 12 do not use prepared statements to insert large amounts of data into duckdb see the data import documentation for better options udf api the udf api allows the definition of user-defined functions it is exposed in duckdb connection through the methods createscalarfunction createvectorizedfunction and variants these methods created udfs into the temporary schema temp_schema of the owner connection that is the only one allowed to use and change them createscalarfunction the user can code an ordinary scalar function and invoke the createscalarfunction to register and afterward use the udf in a select statement for instance bool bigger_than_four int value return value 4 connection createscalarfunction bool int bigger_than_four bigger_than_four connection query select bigger_than_four i from values 3 5 tbl i - print the createscalarfunction methods automatically creates vectorized scalar udfs so they are as efficient as built-in functions we have two variants of this method interface as follows 1 template typename tr typename args void createscalarfunction string name tr udf_func args template parameters tr is the return type of the udf function args are the arguments up to 3 for the udf function this method only supports until ternary functions name is the name to register the udf function udf_func is a pointer to the udf function this method automatically discovers from the template typenames the corresponding logicaltypes bool logicaltype boolean int8_t logicaltype tinyint int16_t logicaltype smallint int32_t logicaltype integer int64_t logicaltype bigint float logicaltype float double logicaltype double string_t logicaltype varchar in duckdb some primitive types e g int32_t are mapped to the same logicaltype integer time and date then for disambiguation the users can use the following overloaded method 2 template typename tr typename args void createscalarfunction string name vector logicaltype args logicaltype ret_type tr udf_func args an example of use would be int32_t udf_date int32_t a return a con query create table dates d date con query insert into dates values 1992-01-01 con createscalarfunction int32_t int32_t udf_date logicaltype date logicaltype date udf_date con query select udf_date d from dates - print template parameters tr is the return type of the udf function args are the arguments up to 3 for the udf function this method only supports until ternary functions name is the name to register the udf function args are the logicaltype arguments that the function uses which should match with the template args types ret_type is the logicaltype of return of the function which should match with the template tr type udf_func is a pointer to the udf function this function checks the template types against the logicaltypes passed as arguments and they must match as follow logicaltypeid boolean bool logicaltypeid tinyint int8_t logicaltypeid smallint int16_t logicaltypeid date logicaltypeid time logicaltypeid integer int32_t logicaltypeid bigint logicaltypeid timestamp int64_t logicaltypeid float logicaltypeid double logicaltypeid decimal double logicaltypeid varchar logicaltypeid char logicaltypeid blob string_t logicaltypeid varbinary blob_t createvectorizedfunction the createvectorizedfunction methods register a vectorized udf such as this vectorized function copies the input values to the result vector template typename type static void udf_vectorized datachunk args expressionstate state vector result set the result vector type result vector_type vectortype flat_vector get a raw array from the result auto result_data flatvector getdata type result get the solely input vector auto input args data 0 now get an orrified vector vectordata vdata input orrify args size vdata get a raw array from the orrified input auto input_data type vdata data handling the data for idx_t i 0 i args size i auto idx vdata sel- get_index i if vdata nullmask idx continue result_data i input_data idx con query create table integers i integer con query insert into integers values 1 2 3 999 con createvectorizedfunction int int udf_vectorized_int udf_vectorized int con query select udf_vectorized_int i from integers - print the vectorized udf is a pointer of the type scalar_function_t typedef std function void datachunk args expressionstate expr vector result scalar_function_t args is a datachunk that holds a set of input vectors for the udf that all have the same length expr is an expressionstate that provides information to the query s expression state result is a vector to store the result values there are different vector types to handle in a vectorized udf constantvector dictionaryvector flatvector listvector stringvector structvector sequencevector the general api of the createvectorizedfunction method is as follows 1 template typename tr typename args void createvectorizedfunction string name scalar_function_t udf_func logicaltype varargs logicaltype invalid template parameters tr is the return type of the udf function args are the arguments up to 3 for the udf function name is the name to register the udf function udf_func is a vectorized udf function varargs the type of varargs to support or logicaltypeid invalid default value if the function does not accept variable length arguments this method automatically discovers from the template typenames the corresponding logicaltypes bool logicaltype boolean int8_t logicaltype tinyint int16_t logicaltype smallint int32_t logicaltype integer int64_t logicaltype bigint float logicaltype float double logicaltype double string_t logicaltype varchar 2 template typename tr typename args void createvectorizedfunction string name vector logicaltype args logicaltype ret_type scalar_function_t udf_func logicaltype varargs logicaltype invalid",
			"category": "Api",
			"url": "/docs/api/cpp",
			"blurb": "Installation The DuckDB C++ API can be installed as part of the libduckdb packages. Please see the installation page..."
		},
		{
			"title": "CALL Statement",
			"text": "the call statement invokes the given table function and returns the results examples -- invoke the duckdb_functions table function call duckdb_functions -- invoke the pragma_table_info table function call pragma_table_info pg_am syntax",
			"category": "Statements",
			"url": "/docs/sql/statements/call",
			"blurb": "The CALL statement invokes the given table function and returns the results. Examples -- Invoke the..."
		},
		{
			"title": "CASE Statement",
			"text": "the case statement performs a switch based on a condition the basic form is identical to the ternary condition used in many programming languages case when cond then a else b end is equivalent to cond a b with a single condition this can be expressed with if cond a b create or replace table integers as select unnest 1 2 3 as i select i case when i 2 then 1 else 0 end as test from integers -- 1 2 3 -- 0 0 1 -- this is equivalent to select i if i 2 1 0 as test from integers -- 1 2 3 -- 0 0 1 the when cond then expr part of the case statement can be chained whenever any of the conditions returns true for a single tuple the corresponding expression is evaluated and returned create or replace table integers as select unnest 1 2 3 as i select i case when i 1 then 10 when i 2 then 20 else 0 end as test from integers -- 1 2 3 -- 10 20 0 the else part of the case statement is optional if no else statement is provided and none of the conditions match the case statement will return null create or replace table integers as select unnest 1 2 3 as i select i case when i 1 then 10 end as test from integers -- 1 2 3 -- 10 null null after the case but before the when an individual expression can also be provided when this is done the case statement is essentially transformed into a switch statement create or replace table integers as select unnest 1 2 3 as i select i case i when 1 then 10 when 2 then 20 when 3 then 30 end as test from integers -- 1 2 3 -- 10 20 30 -- this is equivalent to select i case when i 1 then 10 when i 2 then 20 when i 3 then 30 end as test from integers",
			"category": "Expressions",
			"url": "/docs/sql/expressions/case",
			"blurb": "The CASE statement performs a switch based on a condition. The basic form is identical to the ternary condition used..."
		},
		{
			"title": "CHECKPOINT Statement",
			"text": "the checkpoint statement synchronizes data in the write-ahead log wal to the database data file for in-memory databases this statement will succeed with no effect examples -- synchronize data in the default database checkpoint -- synchronize data in the specified database checkpoint file_db -- abort any in-progress transactions to synchronize the data force checkpoint syntax checkpoint operations happen automatically based on the wal size see configuration this statement is for manual checkpoint actions behavior the default checkpoint command will fail if there are any running transactions including force will abort any transactions and execute the checkpoint operation also see the related pragma option for further behavior modification reclaiming space when performing a checkpoint automatic or otherwise the space occupied by deleted rows is partially reclaimed note that this does not remove all deleted rows but rather merges row groups that have a significant amount of deletes together in the current implementation this requires 25 of rows to be deleted in adjacent row groups when running in in-memory mode checkpointing has no effect hence it does not reclaim space after deletes in in-memory databases the vacuum statement does not trigger vacuuming deletes and hence does not reclaim space",
			"category": "Statements",
			"url": "/docs/sql/statements/checkpoint",
			"blurb": "The CHECKPOINT statement synchronizes data in the write-ahead log (WAL) to the database data file. For in-memory..."
		},
		{
			"title": "CLI API",
			"text": "installation the duckdb cli command line interface is a single dependency-free executable it is precompiled for windows mac and linux for both the stable version and for nightly builds produced by github actions please see the installation page under the cli tab for download links the duckdb cli is based on the sqlite command line shell so cli-client-specific functionality is similar to what is described in the sqlite documentation although duckdb s sql syntax follows postgresql conventions duckdb has a tldr page that summarizes the most common uses of the cli client if you have tldr installed you can display it by running tldr duckdb getting started once the cli executable has been downloaded unzip it and save it to any directory navigate to that directory in a terminal and enter the command duckdb to run the executable if in a powershell or posix shell environment use the command duckdb instead usage the typical usage of the duckdb command is the following duckdb options filename options the options part encodes arguments for the cli client common options include -csv sets the output mode to csv -json sets the output mode to json -readonly open the database in read-only mode see concurrency in duckdb for a full list of options see the command line arguments page in-memory vs persistent database when no filename argument is provided the duckdb cli will open a temporary in-memory database you will see duckdb s version number the information on the connection and a prompt starting with a d duckdb v0 10 0 20b1486d11 enter help for usage hints connected to a transient in-memory database use open filename to reopen on a persistent database d to open or create a persistent database simply include a path as a command line argument like duckdb path to my_database duckdb this path can point to an existing database or to a file that does not yet exist and duckdb will open or create a database at that location as needed the file may have any arbitrary extension but db or duckdb are two common choices running on a persistent database allows spilling to disk thus facilitating larger-than-memory workloads i e out-of-core-processing running sql statements in the cli once the cli has been opened enter a sql statement followed by a semicolon then hit enter and it will be executed results will be displayed in a table in the terminal if a semicolon is omitted hitting enter will allow for multi-line sql statements to be entered select quack as my_column my_column varchar quack the cli supports all of duckdb s rich sql syntax including select create and alter statements editor features the cli supports autocompletion and has sophisticated editor features and syntax highlighting on certain platforms exiting the cli to exit the cli press ctrl - d if your platform supports it otherwise press ctrl - c or use the exit command if used a persistent database duckdb will automatically checkpoint save the latest edits to disk and close this will remove the wal file the write-ahead-log and consolidate all of your data into the single-file database dot commands in addition to sql syntax special dot commands may be entered into the cli client to use one of these commands begin the line with a period immediately followed by the name of the command you wish to execute additional arguments to the command are entered space separated after the command if an argument must contain a space either single or double quotes may be used to wrap that parameter dot commands must be entered on a single line and no whitespace may occur before the period no semicolon is required at the end of the line frequently-used configurations can be stored in the file duckdbrc which will be loaded when starting the cli client see the configuring the cli section below for further information on these options below we summarize a few important dot commands to see all available commands see the dot commands page or use the help command opening database files in addition to connecting to a database when opening the cli a new database connection can be made by using the open command if no additional parameters are supplied a new in-memory database connection is created this database will not be persisted when the cli connection is closed open the open command optionally accepts several options but the final parameter can be used to indicate a path to a persistent database or where one should be created the special string memory can also be used to open a temporary in-memory database open persistent duckdb one important option accepted by open is the --readonly flag this disallows any editing of the database to open in read only mode the database must already exist this also means that a new in-memory database can t be opened in read only mode since in-memory databases are created upon connection open --readonly preexisting duckdb output formats the mode dot command may be used to change the appearance of the tables returned in the terminal output these include the default duckbox mode csv and json mode for ingestion by other tools markdown and latex for documents and insert mode for generating sql statements writing results to a file by default the duckdb cli sends results to the terminal s standard output however this can be modified using either the output or once commands for details see the documentation for the output dot command reading sql from a file the duckdb cli can read both sql commands and dot commands from an external file instead of the terminal using the read command this allows for a number of commands to be run in sequence and allows command sequences to be saved and reused the read command requires only one argument the path to the file containing the sql and or commands to execute after running the commands in the file control will revert back to the terminal output from the execution of that file is governed by the same output and once commands that have been discussed previously this allows the output to be displayed back to the terminal as in the first example below or out to another file as in the second example in this example the file select_example sql is located in the same directory as duckdb exe and contains the following sql statement select from generate_series 5 to execute it from the cli the read command is used read select_example sql the output below is returned to the terminal by default the formatting of the table can be adjusted using the output or once commands generate_series ----------------- 0 1 2 3 4 5 multiple commands including both sql and dot commands can also be run in a single read command in this example the file write_markdown_to_file sql is located in the same directory as duckdb exe and contains the following commands mode markdown output series md select from generate_series 5 to execute it from the cli the read command is used as before read write_markdown_to_file sql in this case no output is returned to the terminal instead the file series md is created or replaced if it already existed with the markdown-formatted results shown here generate_series ----------------- 0 1 2 3 4 5 configuring the cli several dot commands can be used to configure the cli on startup the cli reads and executes all commands in the file duckdbrc including dot commands and sql statements this allows you to store the configuration state of the cli you may also point to a different initialization file using the -init setting a custom prompt as an example a file in the same directory as the duckdb cli named prompt sql will change the duckdb prompt to be a duck head and run a sql statement note that the duck head is built with unicode characters and does not work in all terminal environments e g in windows unless running with wsl and using the windows terminal prompt to invoke that file on initialization use this command duckdb -init prompt sql this outputs -- loading resources from prompt sql v version git hash enter help for usage hints connected to a transient in-memory database use open filename to reopen on a persistent database non-interactive usage to read process a file and exit immediately pipe the file contents in to duckdb duckdb select_example sql to execute a command with sql text passed in directly from the command line call duckdb with two arguments the database location or memory and a string with the sql statement to execute duckdb memory select 42 as the_answer loading extensions to load extensions use duckdb s sql install and load commands as you would other sql statements install fts load fts for details see the extension docs reading from stdin and writing to stdout when in a unix environment it can be useful to pipe data between multiple commands duckdb is able to read data from stdin as well as write to stdout using the file location of stdin dev stdin and stdout dev stdout within sql commands as pipes act very similarly to file handles this command will create an example csv copy select 42 as woot union all select 43 as woot to test csv header first read a file and pipe it to the duckdb cli executable as arguments to the duckdb cli pass in the location of the database to open in this case an in-memory database and a sql command that utilizes dev stdin as a file location cat test csv duckdb memory select from read_csv dev stdin woot int32 42 43 to write back to stdout the copy command can be used with the dev stdout file location cat test csv duckdb memory copy select from read_csv dev stdin to dev stdout with format csv header woot 42 43 reading environment variables the getenv function can read environment variables examples to retrieve the home directory s path from the home environment variable use select getenv home as home home varchar users user_name the output of the getenv function can be used to set configuration options for example to set the null order based on the environment variable default_null_order use set default_null_order getenv default_null_order restrictions for reading environment variables the getenv function can only be run when the enable_external_access is set to true the default setting it is only available in the cli client and is not supported in other duckdb clients prepared statements the duckdb cli supports executing prepared statements in addition to regular select statements to create and execute a prepared statement in the cli client use the prepare clause and the execute statement pages in this section",
			"category": "Cli",
			"url": "/docs/api/cli/overview",
			"blurb": "Installation The DuckDB CLI (Command Line Interface) is a single, dependency-free executable. It is precompiled for..."
		},
		{
			"title": "CLI Charting with YouPlot",
			"text": "duckdb can be used with cli graphing tools to quickly pipe input to stdout to graph your data in one line youplot is a ruby-based cli tool for drawing visually pleasing plots on the terminal it can accept input from other programs by piping data from stdin it takes tab-separated or delimiter of your choice data and can easily generate various types of plots including bar line histogram and scatter with duckdb you can write to the console stdout by using the to dev stdout command and you can also write comma-separated values by using with format csv header installing youplot installation instructions for youplot can be found on the main youplot repository if you re on a mac you can use brew install youplot run uplot --help to ensure you ve installed it successfully piping duckdb queries to stdout by combining the copy to function with a csv output file data can be read from any format supported by duckdb and piped to youplot there are three important steps to doing this as an example this is how to read all data from input json duckdb -s select from read_json_auto input json to prepare the data for youplot write a simple aggregate duckdb -s select date sum purchases as total_purchases from read_json_auto input json group by 1 order by 2 desc limit 10 finally wrap the select in the copy to function with an output location of dev stdout the syntax looks like this copy your_select_query to dev stdout with format csv header the full duckdb command below outputs the query in csv format with a header duckdb -s copy select date sum purchases as total_purchases from read_json_auto input json group by 1 order by 2 desc limit 10 to dev stdout with format csv header connecting duckdb to youplot finally the data can now be piped to youplot let s assume we have an input json file with dates and number of purchases made by somebody on that date using the query above we ll pipe the data to the uplot command to draw a plot of the top 10 purchase dates duckdb -s copy select date sum purchases as total_purchases from read_json_auto input json group by 1 order by 2 desc limit 10 to dev stdout with format csv header uplot bar -d -h -t top 10 purchase dates this tells uplot to draw a bar plot use a comma-seperated delimiter -d that the data has a header -h and give the plot a title -t youplot-top-10 bonus round stdin stdout maybe you re piping some data through jq maybe you re downloading a json file from somewhere you can also tell duckdb to read the data from another process by changing the filename to dev stdin let s combine this with a quick curl from github to see what a certain user has been up to lately curl -sl https api github com users dacort events per_page 100 duckdb -s copy select type count as event_count from read_json_auto dev stdin group by 1 order by 2 desc limit 10 to dev stdout with format csv header uplot bar -d -h -t github events for dacort github-events",
			"category": "Data Viewers",
			"url": "/docs/guides/data_viewers/youplot",
			"blurb": "DuckDB can be used with CLI graphing tools to quickly pipe input to stdout to graph your data in one line. YouPlot is..."
		},
		{
			"title": "COMMENT ON Statement",
			"text": "the comment on statement allows adding metadata to catalog entries tables columns etc it follows the postgresql syntax examples comment on table test_table is very nice table comment on column test_table test_table_column is very nice column comment on view test_view is very nice view comment on index test_index is very nice index comment on sequence test_sequence is very nice sequence comment on type test_type is very nice type comment on macro test_macro is very nice macro comment on macro table test_table_macro is very nice table macro -- to unset a comment set it to null e g comment on table test_table is null reading comments comments can be read by querying the comment column of the respective metadata functions select comment from duckdb_tables -- table select comment from duckdb_columns -- column select comment from duckdb_views -- view select comment from duckdb_indexes -- index select comment from duckdb_sequences -- sequence select comment from duckdb_types -- type select comment from duckdb_functions -- macro select comment from duckdb_functions -- macro table limitations the comment on statement currently has the following limitations it is not possible to comment on schemas or databases it is not possible to comment on things that have a dependency e g a table with an index syntax",
			"category": "Statements",
			"url": "/docs/sql/statements/comment_on",
			"blurb": "The COMMENT ON statement allows adding metadata to catalog entries (tables, columns, etc.). It follows the PostgreSQL..."
		},
		{
			"title": "COPY Statement",
			"text": "examples -- read a csv file into the lineitem table using auto-detected csv options copy lineitem from lineitem csv -- read a csv file into the lineitem table using manually specified csv options copy lineitem from lineitem csv delimiter -- read a parquet file into the lineitem table copy lineitem from lineitem pq format parquet -- read a json file into the lineitem table using auto-detected options copy lineitem from lineitem json format json auto_detect true -- read a csv file into the lineitem table using double quotes copy lineitem from lineitem csv -- read a csv file into the lineitem table omitting quotes copy lineitem from lineitem csv -- write a table to a csv file copy lineitem to lineitem csv format csv delimiter header -- write a table to a csv file using double quotes copy lineitem to lineitem csv -- write a table to a csv file omitting quotes copy lineitem to lineitem csv -- write the result of a query to a parquet file copy select l_orderkey l_partkey from lineitem to lineitem parquet compression zstd -- copy the entire content of database db1 to database db2 copy from database db1 to db2 -- copy only the schema catalog elements but not any data copy from database db1 to db2 schema overview copy moves data between duckdb and external files copy from imports data into duckdb from an external file copy to writes data from duckdb to an external file the copy command can be used for csv parquet and json files copy from copy from imports data from an external file into an existing table the data is appended to whatever data is in the table already the amount of columns inside the file must match the amount of columns in the table table_name and the contents of the columns must be convertible to the column types of the table in case this is not possible an error will be thrown if a list of columns is specified copy will only copy the data in the specified columns from the file if there are any columns in the table that are not in the column list copy from will insert the default values for those columns -- copy the contents of a comma-separated file test csv without a header into the table test copy test from test csv -- copy the contents of a comma-separated file with a header into the category table copy category from categories csv header -- copy the contents of lineitem tbl into the lineitem table where the contents are delimited by a pipe character copy lineitem from lineitem tbl delimiter -- copy the contents of lineitem tbl into the lineitem table where the delimiter quote character and presence of a header are automatically detected copy lineitem from lineitem tbl auto_detect true -- read the contents of a comma-separated file names csv into the name column of the category table any other columns of this table are filled with their default value copy category name from names csv -- read the contents of a parquet file lineitem parquet into the lineitem table copy lineitem from lineitem parquet format parquet -- read the contents of a newline-delimited json file lineitem ndjson into the lineitem table copy lineitem from lineitem ndjson format json -- read the contents of a json file lineitem json into the lineitem table copy lineitem from lineitem json format json array true syntax copy to copy to exports data from duckdb to an external csv or parquet file it has mostly the same set of options as copy from however in the case of copy to the options specify how the file should be written to disk any file created by copy to can be copied back into the database by using copy from with a similar set of options the copy to function can be called specifying either a table name or a query when a table name is specified the contents of the entire table will be written into the resulting file when a query is specified the query is executed and the result of the query is written to the resulting file -- copy the contents of the lineitem table to a csv file with a header copy lineitem to lineitem csv -- copy the contents of the lineitem table to the file lineitem tbl -- where the columns are delimited by a pipe character including a header line copy lineitem to lineitem tbl delimiter -- use tab separators to create a tsv file without a header copy lineitem to lineitem tsv delimiter t header false -- copy the l_orderkey column of the lineitem table to the file orderkey tbl copy lineitem l_orderkey to orderkey tbl delimiter -- copy the result of a query to the file query csv including a header with column names copy select 42 as a hello as b to query csv delimiter -- copy the result of a query to the parquet file query parquet copy select 42 as a hello as b to query parquet format parquet -- copy the result of a query to the newline-delimited json file query ndjson copy select 42 as a hello as b to query ndjson format json -- copy the result of a query to the json file query json copy select 42 as a hello as b to query json format json array true copy to options zero or more copy options may be provided as a part of the copy operation the with specifier is optional but if any options are specified the parentheses are required parameter values can be passed in with or without wrapping in single quotes any option that is a boolean can be enabled or disabled in multiple ways you can write true on or 1 to enable the option and false off or 0 to disable it the boolean value can also be omitted e g by only passing header in which case true is assumed the below options are applicable to all formats written with copy name description type default -- ----- - - overwrite_or_ignore whether or not to allow overwriting a directory if one already exists only has an effect when used with partition_by bool false file_size_bytes if this parameter is set the copy process creates a directory which will contain the exported files if a file exceeds the set limit specified as bytes such as 1000 or in human-readable format such as 1k the process creates a new file in the directory this parameter works in combination with per_thread_output note that the size is used as an approximation and files can be occasionally slightly over the limit varchar or bigint empty format specifies the copy function to use the default is selected from the file extension e g parquet results in a parquet file being written read if the file extension is unknown csv is selected available options are csv parquet and json varchar auto partition_by the columns to partition by using a hive partitioning scheme see the partitioned writes section varchar empty per_thread_output generate one file per thread rather than one file in total this allows for faster parallel writing bool false use_tmp_file whether or not to write to a temporary file first if the original file exists target csv tmp this prevents overwriting an existing file with a broken file in case the writing is cancelled bool auto syntax copy from database to this statement is currently only available in nightly builds duckdb 0 9 3-dev and will be released in the upcoming v0 10 0 version the copy from database to statement copies the entire content from one attached database to another attached database this includes the schema including constraints indexes sequences macros and the data itself attach db1 db as db1 create table db1 tbl as select 42 as x 3 as y create macro db1 two_x_plus_y x y as 2 x y attach db2 db as db2 copy from database db1 to db2 select db2 two_x_plus_y x y as z from db2 tbl z int32 87 to only copy the schema of db1 to db2 but omit copying the data add schema to the statement copy from database db1 to db2 schema syntax format-specific options csv options the below options are applicable when writing csv files name description type default -- ----- - - compression the compression type for the file by default this will be detected automatically from the file extension e g file csv gz will use gzip file csv will use none options are none gzip zstd varchar auto force_quote the list of columns to always add quotes to even if not required varchar dateformat specifies the date format to use when writing dates see date format varchar empty delim or sep the character that is written to separate columns within each row varchar escape the character that should appear before a character that matches the quote value varchar header whether or not to write a header for the csv file bool true nullstr the string that is written to represent a null value varchar empty quote the quoting character to be used when a data value is quoted varchar timestampformat specifies the date format to use when writing timestamps see date format varchar empty parquet options the below options are applicable when writing parquet files name description type default -- ----- - - compression the compression format to use uncompressed snappy gzip or zstd varchar snappy row_group_size the target size i e number of rows of each row group bigint 122880 row_group_size_bytes the target size of each row group you can pass either a human-readable string e g 2mb or an integer i e the number of bytes this option is only used when you have issued set preserve_insertion_order false otherwise it is ignored bigint row_group_size 1024 field_ids the field_id for each column pass auto to attempt to infer automatically struct empty some examples of field_ids are -- assign field_ids automatically copy select 128 as i to my parquet field_ids auto -- sets the field_id of column i to 42 copy select 128 as i to my parquet field_ids i 42 -- sets the field_id of column i to 42 and column j to 43 copy select 128 as i 256 as j to my parquet field_ids i 42 j 43 -- sets the field_id of column my_struct to 43 -- and column i nested inside my_struct to 43 copy select i 128 as my_struct to my parquet field_ids my_struct __duckdb_field_id 42 i 43 -- sets the field_id of column my_list to 42 -- and column element default name of list child to 43 copy select 128 256 as my_list to my parquet field_ids my_list __duckdb_field_id 42 element 43 -- sets the field_id of colum my_map to 42 -- and columns key and value default names of map children to 43 and 44 copy select map key1 128 key2 256 my_map to my parquet field_ids my_map __duckdb_field_id 42 key 43 value 44 json options the below options are applicable when writing json files name description type default -- ----- - - compression the compression type for the file by default this will be detected automatically from the file extension e g file csv gz will use gzip file csv will use none options are none gzip zstd varchar auto dateformat specifies the date format to use when writing dates see date format varchar empty timestampformat specifies the date format to use when writing timestamps see date format varchar empty array whether to write a json array if true a json array of records is written if false newline-delimited json is written bool false",
			"category": "Statements",
			"url": "/docs/sql/statements/copy",
			"blurb": "Examples -- read a CSV file into the lineitem table, using auto-detected CSV options COPY lineitem FROM..."
		},
		{
			"title": "CREATE INDEX Statement",
			"text": "create index the create index statement constructs an index on the specified column s of the specified table compound indexes on multiple columns expressions are supported unidimensional indexes are supported while multidimensional indexes are not yet supported examples -- create a unique index films_id_idx on the column id of table films create unique index films_id_idx on films id -- create index s_idx that allows for duplicate values on column revenue of table films create index s_idx on films revenue -- create compound index gy_idx on genre and year columns create index gy_idx on films genre year -- create index i_index on the expression of the sum of columns j and k from table integers create index i_index on integers j k parameters name description - ----- unique causes the system to check for duplicate values in the table when the index is created if data already exist and each time data is added attempts to insert or update data that would result in duplicate entries will generate an error name the name of the index to be created table the name of the table to be indexed column the name of the column to be indexed expression an expression based on one or more columns of the table the expression usually must be written with surrounding parentheses as shown in the syntax however the parentheses can be omitted if the expression has the form of a function call syntax drop index drop index drops an existing index from the database system examples -- remove the index title_idx drop index title_idx parameters name description --- --- if exists do not throw an error if the index does not exist name the name of an index to remove syntax",
			"category": "Statements",
			"url": "/docs/sql/statements/create_index",
			"blurb": "CREATE INDEX The CREATE INDEX statement constructs an index on the specified column(s) of the specified table...."
		},
		{
			"title": "CREATE MACRO Statement",
			"text": "the create macro statement can create a scalar or table macro function in the catalog a macro may only be a single select statement similar to a view but it has the benefit of accepting parameters for a scalar macro create macro is followed by the name of the macro and optionally parameters within a set of parentheses the keyword as is next followed by the text of the macro by design a scalar macro may only return a single value for a table macro the syntax is similar to a scalar macro except as is replaced with as table a table macro may return a table of arbitrary size and shape if a macro is temporary it is only usable within the same database connection and is deleted when the connection is closed examples scalar macros -- create a macro that adds two expressions a and b create macro add a b as a b -- create a macro for a case expression create macro ifelse a b c as case when a then b else c end -- create a macro that does a subquery create macro one as select 1 -- create a macro with a common table expression -- parameter names get priority over column names disambiguate using the table name create macro plus_one a as with cte as select 1 as a select cte a a from cte -- macros are schema-dependent and have an alias function create function main myavg x as sum x count x -- create a macro with default constant parameters create macro add_default a b 5 as a b -- create a macro arr_append with a functionality equivalent to array_append create macro arr_append l e as list_concat l list_value e table macros -- create a table macro without parameters create macro static_table as table select hello as column1 world as column2 -- create a table macro with parameters that can be of any type create macro dynamic_table col1_value col2_value as table select col1_value as column1 col2_value as column2 -- create a table macro that returns multiple rows -- it will be replaced if it already exists and it is temporary will be automatically deleted when the connection ends create or replace temp macro dynamic_table col1_value col2_value as table select col1_value as column1 col2_value as column2 union all select hello as col1_value 456 as col2_value -- pass an argument as a list select from get_users 1 5 create macro get_users i as table select from users where uid in select unnest i syntax macros allow you to create shortcuts for combinations of expressions -- failure cannot find column b create macro add a as a b -- this works create macro add a b as a b -- error cannot bind varchar integer select add hello 3 -- success select add 1 2 -- 3 macros can have default parameters unlike some languages default parameters must be named when the macro is invoked -- b is a default parameter create macro add_default a b 5 as a b -- the following will result in 42 select add_default 37 -- error add_default only has one positional parameter select add_default 40 2 -- success default parameters are used by assigning them like so select add_default 40 b 2 -- error default parameters must come after positional parameters select add_default b 2 40 -- the order of default parameters does not matter create macro triple_add a b 5 c 10 as a b c -- success select triple_add 40 c 1 b 1 -- 42 when macros are used they are expanded i e replaced with the original expression and the parameters within the expanded expression are replaced with the supplied arguments step by step -- the add macro we defined above is used in a query select add 40 2 -- internally add is replaced with its definition of a b select a b -- then the parameters are replaced by the supplied arguments select 40 2 -- 42 limitations if a macro is defined as a subquery it cannot be invoked in a table function duckdb will return the following error binder error table function cannot contain subqueries",
			"category": "Statements",
			"url": "/docs/sql/statements/create_macro",
			"blurb": "The CREATE MACRO statement can create a scalar or table macro (function) in the catalog. A macro may only be a single..."
		},
		{
			"title": "CREATE SCHEMA Statement",
			"text": "the create schema statement creates a schema in the catalog the default schema is main examples -- create a schema create schema s1 -- create a schema if it does not exist yet create schema if not exists s2 -- create table in the schemas create table s1 t id integer primary key other_id integer create table s2 t id integer primary key j varchar -- compute a join between tables from two schemas select from s1 t s1t s2 t s2t where s1t other_id s2t id syntax",
			"category": "Statements",
			"url": "/docs/sql/statements/create_schema",
			"blurb": "The CREATE SCHEMA statement creates a schema in the catalog. The default schema is main . Examples -- create a schema..."
		},
		{
			"title": "CREATE SECRET Statement",
			"text": "the create secret statement creates a new secret in the secrets manager which provides unified user interface for secrets across all backends that use them secrets can be scoped so different storage prefixes can have different secrets allowing for example to join data across organizations in a single query secrets can also be persisted so that they do not need to be specified every time duckdb is launched secrets were introduced with duckdb version 0 10 persistent secrets are stored in unencrypted binary format on the disk secrets types of secrets secrets are typed their type identifies which service they are for currently the following cloud services are available aws s3 s3 through the httpfs extension google cloud storage gcs through the httpfs extension cloudflare r2 r2 through the httpfs extension azure blob storage azure through the azure extension for each type there are one or more secret providers that specify how the secret is created secrets can also have an optional scope which is a file path prefix that the secret applies to when fetching a secret for a path the secret scopes are compared to the path returning the matching secret for the path in the case of multiple matching secrets the longest prefix is chosen creating a secret secrets can be temporary or persistent temporary secrets are used by default and are stored in-memory for the life span of the duckdb instance similar to how settings worked previously persistent secrets are stored in unencrypted binary format in the duckdb stored_secrets directory on startup of duckdb persistent secrets are read from this directory and automatically loaded secret providers to create a secret a secret provider needs to be used a secret provider is a mechanism through which a secret is generated to illustrate this for the s3 gcs r2 and azure secret types duckdb currently supports two providers config and credential_chain the config provider requires the user to pass all configuration information into the create secret whereas the credential_chain provider will automatically try to fetch credentials when no secret provider is specified the config provider is used for more details on how to create secrets using different providers checkout the respective pages on httpfs and azure temporary secrets to create a temporary unscoped secret to access s3 we can now use the following create secret type s3 key_id mykey secret mysecret region myregion note that we implicitly use the default config secret provider here persistent secrets in order to persist secrets between duckdb database instances we can now use the create persistent secret command e g create persistent secret my_persistent_secret type s3 key_id key secret secret this will write the secret unencrypted to the duckdb stored_secrets directory deleting secrets secrets can be deleted using the drop secret statement e g drop persistent secret my_persistent_secret creating multiple secrets for the same service type if two secrets exist for a service type the scope can be used to decide which one should be used for example create secret secret1 type s3 key_id my_key1 secret my_secret1 scope s3 my-bucket create secret secret2 type s3 key_id my_key2 secret my_secret2 scope s3 my-other-bucket now if the user queries something from s3 my-other-bucket something secret secret2 will be chosen automatically for that request to see which secret is being used the which_secret scalar function can be used which takes a path and a secret type as parameters select which_secret s3 my-other-bucket file parquet s3 listing secrets secrets can be listed using the built-in table-producing function e g by using the duckdb_secrets table function from duckdb_secrets sensitive information will be redacted syntax for create secret syntax for drop secret",
			"category": "Statements",
			"url": "/docs/sql/statements/create_secret",
			"blurb": "The CREATE SECRET statement creates a new secret in the Secrets manager , which provides unified user interface for..."
		},
		{
			"title": "CREATE SEQUENCE Statement",
			"text": "the create sequence statement creates a new sequence number generator examples -- generate an ascending sequence starting from 1 create sequence serial -- generate sequence from a given start number create sequence serial start 101 -- generate odd numbers using increment by create sequence serial start with 1 increment by 2 -- generate a descending sequqnce starting from 99 create sequence serial start with 99 increment by -1 maxvalue 99 -- by default cycles are not allowed and will result in a serialization error e g -- reached maximum value of sequence serial 10 create sequence serial start with 1 maxvalue 10 -- cycle allows cycling through the same sequence repeatedly create sequence serial start with 1 maxvalue 10 cycle creating and dropping sequences sequences can be created and dropped similarly to other catalogue items -- overwrite an existing sequence create or replace sequence serial -- only create sequence if no such sequence exists yet create sequence if not exists serial -- remove sequence drop sequence serial -- remove sequence if exists drop sequence if exists serial using sequences for primary keys sequences can provide an integer primary key for a table for example create sequence id_sequence start 1 create table tbl id int default nextval id_sequence s varchar insert into tbl s values hello world select from tbl the script results in the following table id s int32 varchar 1 hello 2 world sequences can also be added using the alter table statement the following example adds an id column and fills it with values generated by the sequence create table tbl s varchar insert into tbl values hello world create sequence id_sequence start 1 alter table tbl add column id int default nextval id_sequence select from tbl this script results in the same table as the previous example selecting the next value select the next number from a sequence select nextval serial as nextval nextval int64 1 using this sequence in an insert command insert into distributors values nextval serial nothing selecting the current value you may also view the current number from the sequence note that the nextval function must have already been called before calling currval otherwise a serialization error sequence is not yet defined in this session will be thrown select currval serial as currval currval int64 1 syntax create sequence creates a new sequence number generator if a schema name is given then the sequence is created in the specified schema otherwise it is created in the current schema temporary sequences exist in a special schema so a schema name may not be given when creating a temporary sequence the sequence name must be distinct from the name of any other sequence in the same schema after a sequence is created you use the function nextval to operate on the sequence parameters name description -- ----- cycle or no cycle the cycle option allows the sequence to wrap around when the maxvalue or minvalue has been reached by an ascending or descending sequence respectively if the limit is reached the next number generated will be the minvalue or maxvalue respectively increment the optional clause increment by increment specifies which value is added to the current sequence value to create a new value a positive value will make an ascending sequence a negative one a descending sequence the default value is 1 maxvalue the optional clause maxvalue maxvalue determines the maximum value for the sequence if this clause is not supplied or no maxvalue is specified then default values will be used the defaults are 2 63 - 1 and -1 for ascending and descending sequences respectively minvalue the optional clause minvalue minvalue determines the minimum value a sequence can generate if this clause is not supplied or no minvalue is specified then defaults will be used the defaults are 1 and - 2 63 - 1 for ascending and descending sequences respectively start the optional clause start with start allows the sequence to begin anywhere the default starting value is minvalue for ascending sequences and maxvalue for descending ones temporary or temp if specified the sequence object is created only for this session and is automatically dropped on session exit existing permanent sequences with the same name are not visible in this session while the temporary sequence exists unless they are referenced with schema-qualified names name the name optionally schema-qualified of the sequence to be created if no cycle is specified any calls to nextval after the sequence has reached its maximum value will return an error if neither cycle or no cycle are specified no cycle is the default sequences are based on bigint arithmetic so the range cannot exceed the range of an eight-byte integer -9223372036854775808 to 9223372036854775807",
			"category": "Statements",
			"url": "/docs/sql/statements/create_sequence",
			"blurb": "The CREATE SEQUENCE statement creates a new sequence number generator. Examples -- generate an ascending sequence..."
		},
		{
			"title": "CREATE TABLE Statement",
			"text": "the create table statement creates a table in the catalog examples -- create a table with two integer columns i and j create table t1 i integer j integer -- create a table with a primary key create table t1 id integer primary key j varchar -- create a table with a composite primary key create table t1 id integer j varchar primary key id j -- create a table with various different types and constraints create table t1 i integer not null decimalnr double check decimalnr 10 date date unique time timestamp -- create table as select ctas create table t1 as select 42 as i 84 as j -- create a table from a csv file automatically detecting column names and types create table t1 as select from read_csv path file csv -- we can use the from-first syntax to omit select create table t1 as from read_csv path file csv -- copy the schema of t2 to t1 create table t1 as from t2 limit 0 temporary tables temporary tables can be created using the create temp table or the create temporary table statement see diagram below temporary tables are session scoped similar to postgresql for example meaning that only the specific connection that created them can access them and once the connection to duckdb is closed they will be automatically dropped temporary tables reside in memory rather than on disk even when connecting to a persistent duckdb but if the temp_directory configuration is set when connecting or with a set command data will be spilled to disk if memory becomes constrained -- create a temporary table from a csv file automatically detecting column names and types create temp table t1 as select from read_csv path file csv -- allow temporary tables to off-load excess memory to disk set temp_directory path to directory temporary tables are part of the temp main schema while discouraged their names can overlap with the names of the regular database tables in these cases use their fully qualified name e g temp main t1 for disambiguation create or replace the create or replace syntax allows a new table to be created or for an existing table to be overwritten by the new table this is shorthand for dropping the existing table and then creating the new one -- create a table with two integer columns i and j even if t1 already exists create or replace table t1 i integer j integer if not exists the if not exists syntax will only proceed with the creation of the table if it does not already exist if the table already exists no action will be taken and the existing table will remain in the database -- create a table with two integer columns i and j only if t1 does not exist yet create table if not exists t1 i integer j integer check constraints a check constraint is an expression that must be satisfied by the values of every row in the table create table t1 id integer primary key percentage integer check 0 percentage and percentage 100 insert into t1 values 1 5 insert into t1 values 2 -1 -- error constraint error check constraint failed t1 insert into t1 values 3 101 -- error constraint error check constraint failed t1 create table t2 id integer primary key x integer y integer check x y insert into t2 values 1 5 10 insert into t2 values 2 5 3 -- error constraint error check constraint failed t2 check constraints can also be added as part of the constraints clause create table t3 id integer primary key x integer y integer constraint x_smaller_than_y check x y insert into t3 values 1 5 10 insert into t3 values 2 5 3 -- error constraint error check constraint failed t3 foreign key constraints a foreign key is a column or set of columns that references another table s primary key foreign keys check referential integrity i e the referred primary key must exist in the other table upon insertion create table t1 id integer primary key j varchar create table t2 id integer primary key t1_id integer foreign key t1_id references t1 id -- example insert into t1 values 1 a insert into t2 values 1 1 insert into t2 values 2 2 -- error constraint error violates foreign key constraint because key id 2 does not exist in the referenced table foreign keys can be defined on composite primary keys create table t3 id integer j varchar primary key id j create table t4 id integer primary key t3_id integer t3_j varchar foreign key t3_id t3_j references t3 id j -- example insert into t3 values 1 a insert into t4 values 1 1 a insert into t4 values 2 1 b -- error constraint error violates foreign key constraint because key id 1 j b does not exist in the referenced table foreign keys can also be defined on unique columns create table t5 id integer unique j varchar create table t6 id integer primary key t5_id integer foreign key t5_id references t5 id foreign keys with cascading deletes foreign key references on delete cascade are not supported generated columns the type generated always as expr virtual stored syntax will create a generated column the data in this kind of column is generated from its expression which can reference other regular or generated columns of the table since they are produced by calculations these columns can not be inserted into directly duckdb can infer the type of the generated column based on the expression s return type this allows you to leave out the type when declaring a generated column it is possible to explicitly set a type but insertions into the referenced columns might fail if the type can not be cast to the type of the generated column generated columns come in two varieties virtual and stored the data of virtual generated columns is not stored on disk instead it is computed from the expression every time the column is referenced through a select statement the data of stored generated columns is stored on disk and is computed every time the data of their dependencies change through an insert update drop statement currently only the virtual kind is supported and it is also the default option if the last field is left blank -- the simplest syntax for a generated column -- the type is derived from the expression and the variant defaults to virtual create table t1 x float two_x as 2 x -- fully specifying the same generated column for completeness create table t1 x float two_x float generated always as 2 x virtual syntax",
			"category": "Statements",
			"url": "/docs/sql/statements/create_table",
			"blurb": "The CREATE TABLE statement creates a table in the catalog. Examples -- create a table with two integer columns (i and..."
		},
		{
			"title": "CREATE TYPE Statement",
			"text": "the create type statement defines a new type in the catalog examples -- create a simple enum type create type mood as enum happy sad curious -- create a simple struct type create type many_things as struct k integer l varchar -- create a simple union type create type one_thing as union number integer string varchar -- create a type alias create type x_index as integer syntax create type defines a new data type available to this duckdb instance these new types can then be inspected in the duckdb_types table extending these custom types to support custom operators such as the postgresql operator would require c development to do this create an extension",
			"category": "Statements",
			"url": "/docs/sql/statements/create_type",
			"blurb": "The CREATE TYPE statement defines a new type in the catalog. Examples -- create a simple enum type CREATE TYPE mood..."
		},
		{
			"title": "CREATE VIEW Statement",
			"text": "the create view statement defines a new view in the catalog examples -- create a simple view create view v1 as select from tbl -- create a view or replace it if a view with that name already exists create or replace view v1 as select 42 -- create a view and replace the column names create view v1 a as select 42 the sql query behind an existing view can be read using the duckdb_views function like this select sql from duckdb_views where view_name v1 syntax create view defines a view of a query the view is not physically materialized instead the query is run every time the view is referenced in a query create or replace view is similar but if a view of the same name already exists it is replaced if a schema name is given then the view is created in the specified schema otherwise it is created in the current schema temporary views exist in a special schema so a schema name cannot be given when creating a temporary view the name of the view must be distinct from the name of any other view or table in the same schema",
			"category": "Statements",
			"url": "/docs/sql/statements/create_view",
			"blurb": "The CREATE VIEW statement defines a new view in the catalog. Examples -- create a simple view CREATE VIEW v1 AS..."
		},
		{
			"title": "CSV Auto Detection",
			"text": "when using read_csv the system tries to automatically infer how to read the csv file using the csv sniffer this step is necessary because csv files are not self-describing and come in many different dialects the auto-detection works roughly as follows detect the dialect of the csv file delimiter quoting rule escape detect the types of each of the columns detect whether or not the file has a header row by default the system will try to auto-detect all options however options can be individually overridden by the user this can be useful in case the system makes a mistake for example if the delimiter is chosen incorrectly we can override it by calling the read_csv with an explicit delimiter e g read_csv file csv delim the detection works by operating on a sample of the file the size of the sample can be modified by setting the sample_size parameter the default sample size is 20480 rows setting the sample_size parameter to -1 means the entire file is read for sampling the way sampling is performed depends on the type of file if we are reading from a regular file on disk we will jump into the file and try to sample from different locations in the file if we are reading from a file in which we cannot jump - such as a gz compressed csv file or stdin - samples are taken only from the beginning of the file sniff_csv function it is possible to run the csv sniffer as a separate step using the sniff_csv filename function which returns the detected csv properties as a table with a single row the sniff_csv function accepts an optional sample_size parameter to configure the number of rows sampled from sniff_csv my_file csv from sniff_csv my_file csv sample_size 1000 column name description example ---- ----- ------- delimiter delimiter quote quote character escape escape newlinedelimiter new-line delimiter r n skiprow number of rows skipped 1 hasheader whether the csv has a header true columns column types encoded as a list of struct s name varchar age bigint dateformat date format d m y timestampformat timestamp format y- m- dt h m s f userarguments arguments used to invoke sniff_csv sample_size 1000 prompt prompt ready to be used to read the csv from read_csv my_file csv auto_detect false delim prompt the prompt column contains a sql command with the configurations detected by the sniffer -- use line mode in cli to get the full command mode line select prompt from sniff_csv my_file csv prompt from read_csv my_file csv auto_detect false delim quote escape new_line n skip 0 header true columns detection steps dialect detection dialect detection works by attempting to parse the samples using the set of considered values the detected dialect is the dialect that has 1 a consistent number of columns for each row and 2 the highest number of columns for each row the following dialects are considered for automatic dialect detection parameters considered values ------------ ----------------------- delim t quote empty escape empty consider the example file flights csv flightdate uniquecarrier origincityname destcityname 1988-01-01 aa new york ny los angeles ca 1988-01-02 aa new york ny los angeles ca 1988-01-03 aa new york ny los angeles ca in this file the dialect detection works as follows if we split by a every row is split into 4 columns if we split by a rows 2-4 are split into 3 columns while the first row is split into 1 column if we split by every row is split into 1 column if we split by t every row is split into 1 column in this example - the system selects the as the delimiter all rows are split into the same amount of columns and there is more than one column per row meaning the delimiter was actually found in the csv file type detection after detecting the dialect the system will attempt to figure out the types of each of the columns note that this step is only performed if we are calling read_csv in case of the copy statement the types of the table that we are copying into will be used instead the type detection works by attempting to convert the values in each column to the candidate types if the conversion is unsuccessful the candidate type is removed from the set of candidate types for that column after all samples have been handled - the remaining candidate type with the highest priority is chosen the set of considered candidate types in order of priority is given below types ------------- boolean bigint double time date timestamp varchar note everything can be cast to varchar this type has the lowest priority - i e columns are converted to varchar if they cannot be cast to anything else in flights csv the flightdate column will be cast to a date while the other columns will be cast to varchar the detected types can be individually overridden using the types option this option takes either a list of types e g types int varchar date which overrides the types of the columns in-order of occurrence in the csv file alternatively types takes a name - type map which overrides options of individual columns e g types quarter int the type detection can be entirely disabled by using the all_varchar option if this is set all columns will remain as varchar as they originally occur in the csv file header detection header detection works by checking if the candidate header row deviates from the other rows in the file in terms of types for example in flights csv we can see that the header row consists of only varchar columns - whereas the values contain a date value for the flightdate column as such - the system defines the first row as the header row and extracts the column names from the header row in files that do not have a header row the column names are generated as column0 column1 etc note that headers cannot be detected correctly if all columns are of type varchar - as in this case the system cannot distinguish the header row from the other rows in the file in this case the system assumes the file has no header this can be overridden using the header option dates and timestamps duckdb supports the iso 8601 format format by default for timestamps dates and times unfortunately not all dates and times are formatted using this standard for that reason the csv reader also supports the dateformat and timestampformat options using this format the user can specify a format string that specifies how the date or timestamp should be read as part of the auto-detection the system tries to figure out if dates and times are stored in a different representation this is not always possible - as there are ambiguities in the representation for example the date 01-02-2000 can be parsed as either january 2nd or february 1st often these ambiguities can be resolved for example if we later encounter the date 21-02-2000 then we know that the format must have been dd-mm-yyyy mm-dd-yyyy is no longer possible as there is no 21nd month if the ambiguities cannot be resolved by looking at the data the system has a list of preferences for which date format to use if the system choses incorrectly the user can specify the dateformat and timestampformat options manually the system considers the following formats for dates dateformat higher entries are chosen over lower entries in case of ambiguities i e iso 8601 is preferred over mm-dd-yyyy dateformat ------------ iso 8601 y- m- d y- m- d d- m- y d- m- y m- d- y m- d- y the system considers the following formats for timestamps timestampformat higher entries are chosen over lower entries in case of ambiguities timestampformat ------------------------ iso 8601 y- m- d h m s y- m- d h m s d- m- y h m s d- m- y h m s m- d- y i m s p m- d- y i m s p y- m- d h m s f",
			"category": "Csv",
			"url": "/docs/data/csv/auto_detection",
			"blurb": "When using read_csv , the system tries to automatically infer how to read the CSV file using the CSV sniffer . This..."
		},
		{
			"title": "CSV Export",
			"text": "to export the data from a table to a csv file use the copy statement copy tbl to output csv header delimiter the result of queries can also be directly exported to a csv file copy select from tbl to output csv header delimiter for additional options see the copy statement documentation",
			"category": "Import",
			"url": "/docs/guides/import/csv_export",
			"blurb": "To export the data from a table to a CSV file, use the COPY statement. COPY tbl TO 'output.csv' (HEADER, DELIMITER..."
		},
		{
			"title": "CSV Import",
			"text": "examples the following examples use the flights csv file -- read a csv file from disk auto-infer options select from flights csv -- read_csv with custom options select from read_csv flights csv delim header true columns flightdate date uniquecarrier varchar origincityname varchar destcityname varchar read a csv from stdin auto-infer options cat flights csv duckdb -c select from read_csv dev stdin -- read a csv file into a table create table ontime flightdate date uniquecarrier varchar origincityname varchar destcityname varchar copy ontime from flights csv -- alternatively create a table without specifying the schema manually create table ontime as select from flights csv -- we can use the from-first syntax to omit select create table ontime as from flights csv -- write the result of a query to a csv file copy select from ontime to flights csv with header true delimiter -- if we serialize the entire table we can simply refer to it with its name copy ontime to flights csv with header true delimiter csv loading csv loading i e importing csv files to the database is a very common and yet surprisingly tricky task while csvs seem simple on the surface there are a lot of inconsistencies found within csv files that can make loading them a challenge csv files come in many different varieties are often corrupt and do not have a schema the csv reader needs to cope with all of these different situations the duckdb csv reader can automatically infer which configuration flags to use by analyzing the csv file using the csv sniffer this will work correctly in most situations and should be the first option attempted in rare situations where the csv reader cannot figure out the correct configuration it is possible to manually configure the csv reader to correctly parse the csv file see the auto detection page for more information parameters below are parameters that can be passed to the csv reader these parameters are accepted by both the copy statement and the read_csv function name description type default -- ----- - - all_varchar option to skip type detection for csv parsing and assume all columns to be of type varchar bool false allow_quoted_nulls option to allow the conversion of quoted values to null values bool true auto_detect enables auto detection of csv parameters bool true auto_type_candidates this option allows you to specify the types that the sniffer will use when detecting csv column types e g select from read_csv csv_file csv auto_type_candidates bigint date the varchar type is always included in the detected types as a fallback option type sqlnull boolean bigint double time date timestamp varchar columns a struct that specifies the column names and column types contained within the csv file e g col1 integer col2 varchar using this option implies that auto detection is not used struct empty compression the compression type for the file by default this will be detected automatically from the file extension e g t csv gz will use gzip t csv will use none options are none gzip zstd varchar auto dateformat specifies the date format to use when parsing dates see date format varchar empty decimal_separator the decimal separator of numbers varchar delim or sep specifies the string that separates columns within each row line of the file varchar escape specifies the string that should appear before a data character sequence that matches the quote value varchar filename whether or not an extra filename column should be included in the result bool false force_not_null do not match the specified columns values against the null string in the default case where the null string is empty this means that empty values will be read as zero-length strings rather than null s varchar header specifies that the file contains a header line with the names of each column in the file bool false hive_partitioning whether or not to interpret the path as a hive partitioned path bool false ignore_errors option to ignore any parsing errors encountered - and instead ignore rows with errors bool false max_line_size the maximum line size in bytes bigint 2097152 names the column names as a list see example varchar empty new_line set the new line character s in the file options are r n or r n varchar empty normalize_names boolean value that specifies whether or not column names should be normalized removing any non-alphanumeric characters from them bool false null_padding if this option is enabled when a row lacks columns it will pad the remaining columns on the right with null values bool false nullstr specifies the string that represents a null value varchar empty parallel whether or not the parallel csv reader is used bool true quote specifies the quoting string to be used when a data value is quoted varchar sample_size the number of sample rows for auto detection of parameters bigint 20480 skip the number of lines at the top of the file to skip bigint 0 timestampformat specifies the date format to use when parsing timestamps see date format varchar empty types or dtypes the column types as either a list by position or a struct by name example here varchar or struct empty union_by_name whether the columns of multiple schemas should be unified by name rather than by position bool false csv functions duckdb 0 9 3-dev and the upcoming v0 10 0 versions introduce breaking changes to the read_csv function namely the read_csv function now attempts auto-detecting the csv parameters making its behavior identical to the old read_csv_auto function if you would like to use read_csv with its old behavior turn off the auto-detection manually by using read_csv auto_detect false the read_csv automatically attempts to figure out the correct configuration of the csv reader using the csv sniffer it also automatically deduces types of columns if the csv file has a header it will use the names found in that header to name the columns otherwise the columns will be named column0 column1 column2 an example with the flights csv file select from read_csv flights csv flightdate uniquecarrier origincityname destcityname ---------- ------------- ----------------- --------------- 1988-01-01 aa new york ny los angeles ca 1988-01-02 aa new york ny los angeles ca 1988-01-03 aa new york ny los angeles ca the path can either be a relative path relative to the current working directory or an absolute path we can use read_csv to create a persistent table as well create table ontime as select from read_csv flights csv describe ontime field type null key default extra -------------- ------- ---- ---- ------- ----- flightdate date yes null null null uniquecarrier varchar yes null null null origincityname varchar yes null null null destcityname varchar yes null null null select from read_csv flights csv sample_size 20000 if we set delim sep quote escape or header explicitly we can bypass the automatic detection of this particular parameter select from read_csv flights csv header true multiple files can be read at once by providing a glob or a list of files refer to the multiple files section for more information writing using the copy statement the copy statement can be used to load data from a csv file into a table this statement has the same syntax as the one used in postgresql to load the data using the copy statement we must first create a table with the correct schema which matches the order of the columns in the csv file and uses types that fit the values in the csv file copy detects the csv s configuration options automatically create table ontime flightdate date uniquecarrier varchar origincityname varchar destcityname varchar copy ontime from flights csv select from ontime flightdate uniquecarrier origincityname destcityname ---------- ------------- ----------------- --------------- 1988-01-01 aa new york ny los angeles ca 1988-01-02 aa new york ny los angeles ca 1988-01-03 aa new york ny los angeles ca if we want to manually specify the csv format we can do so using the configuration options of copy create table ontime flightdate date uniquecarrier varchar origincityname varchar destcityname varchar copy ontime from flights csv delimiter header select from ontime reading faulty csv files duckdb supports reading erroneous csv files for details see the reading faulty csv files page limitations the csv reader only supports input files using utf-8 character encoding for csv files using different encodings use e g the iconv command-line tool to convert them to utf-8 pages in this section",
			"category": "Csv",
			"url": "/docs/data/csv/overview",
			"blurb": "Examples The following examples use the flights.csv file. -- read a CSV file from disk, auto-infer options SELECT *..."
		},
		{
			"title": "CSV Import",
			"text": "to read data from a csv file use the read_csv function in the from clause of a query select from read_csv input csv to create a new table using the result from a query use create table as from a select statement create table new_tbl as select from read_csv input csv we can use duckdb s optional from -first syntax to omit select create table new_tbl as from read_csv input csv to load data into an existing table from a query use insert into from a select statement insert into tbl select from read_csv input csv alternatively the copy statement can also be used to load data from a csv file into an existing table copy tbl from input csv for additional options see the csv import reference and the copy statement documentation",
			"category": "Import",
			"url": "/docs/guides/import/csv_import",
			"blurb": "To read data from a CSV file, use the read_csv function in the FROM clause of a query. SELECT * FROM..."
		},
		{
			"title": "CSV Import Tips",
			"text": "below is a collection of tips to help when attempting to import complex csv files in the examples we use the flights csv file override the header flag if the header is not correctly detected if a file contains only string columns the header auto-detection might fail provide the header option to override this behavior select from read_csv flights csv header true provide names if the file does not contain a header if the file does not contain a header names will be auto-generated by default you can provide your own names with the names option select from read_csv flights csv names dateofflight carriername override the types of specific columns the types flag can be used to override types of only certain columns by providing a struct of name - type mappings select from read_csv flights csv types flightdate date use copy when loading data into a table the copy statement copies data directly into a table the csv reader uses the schema of the table instead of auto-detecting types from the file this speeds up the auto-detection and prevents mistakes from being made during auto-detection copy tbl from test csv use union_by_name when loading files with different schemas the union_by_name option can be used to unify the schema of files that have different or missing columns for files that do not have certain columns null values are filled in select from read_csv flights csv union_by_name true",
			"category": "Csv",
			"url": "/docs/data/csv/tips",
			"blurb": "Below is a collection of tips to help when attempting to import complex CSV files. In the examples, we use the..."
		},
		{
			"title": "Casting",
			"text": "casting refers to the operation of converting a value in a particular data type to the corresponding value in another data type casting can occur either implicitly or explicitly explicit casting the standard sql syntax for explicit casting is cast expr as typename where typename is a name or alias of one of duckdb s data types duckdb also supports the shorthand expr typename which is also present in postgresql select cast i as varchar from generate_series 1 3 tbl i -- 1 2 3 select i double from generate_series 1 3 tbl i -- 1 0 2 0 3 0 select cast hello as integer -- conversion error could not convert string hello to int32 select try_cast hello as integer -- null the exact behavior of the cast depends on the source and destination types for example when casting from varchar to any other type the string will be attempted to be converted not all casts are possible for example it is not possible to convert an integer to a date casts may also throw errors when the cast could not be successfully performed for example trying to cast the string hello to an integer will result in an error being thrown try_cast can be used when the preferred behavior is not to throw an error but instead to return a null value try_cast will never throw an error and will instead return null if a cast is not possible implicit casting in many situations the system will add casts by itself this is called implicit casting this happens for example when a function is called with an argument that does not match the type of the function but can be casted to the desired type consider the function sin double this function takes as input argument a column of type double however it can be called with an integer as well sin 1 the integer is converted into a double before being passed to the sin function generally implicit casts only cast upwards that is to say we can implicitly cast an integer to a bigint but not the other way around allowed casting operations values of a particular data type can typically not be casted to any arbitrary target data type the supported cast operations are described in the typecasting page as part of the data types documentation",
			"category": "Expressions",
			"url": "/docs/sql/expressions/cast",
			"blurb": "Casting refers to the operation of converting a value in a particular data type to the corresponding value in another..."
		},
		{
			"title": "Client APIs Overview",
			"text": "there are various client apis for duckdb c c go by marcboeker java julia node js python r rust webassembly wasm adbc api odbc api additionally there is a standalone command line interface cli client there are also contributed third-party duckdb wrappers which currently do not have an official documentation page c by giorgi common lisp by ak-coram crystal by amauryt ruby by suketa zig by karlseguin pages in this section",
			"category": "Api",
			"url": "/docs/api/overview",
			"blurb": "There are various client APIs for DuckDB: C C++ Go by marcboeker Java Julia Node.js Python R Rust WebAssembly/Wasm..."
		},
		{
			"title": "Cloudflare R2 Import",
			"text": "prerequisites for cloudflare r2 the s3 compatibility api allows you to use duckdb s s3 support to read and write from r2 buckets this requires the httpfs extension which can be installed use the install sql command this only needs to be run once credentials and configuration you will need to generate an s3 auth token and create an r2 secret in duckdb create secret type r2 key_id akiaiosfodnn7example secret wjalrxutnfemi k7mdeng bpxrficyexamplekey account_id my_account_id querying after setting up the r2 credentials you can query the r2 data using select from read_parquet r2 r2_bucket_name file",
			"category": "Import",
			"url": "/docs/guides/import/cloudflare_r2_import",
			"blurb": "Prerequisites For Cloudflare R2, the S3 Compatibility API allows you to use DuckDB's S3 support to read and write..."
		},
		{
			"title": "Collations",
			"text": "collations provide rules for how text should be sorted or compared in the execution engine collations are useful for localization as the rules for how text should be ordered are different for different languages or for different countries these orderings are often incompatible with one another for example in english the letter y comes between x and z however in lithuanian the letter y comes between the i and j for that reason different collations are supported the user must choose which collation they want to use when performing sorting and comparison operations by default the binary collation is used that means that strings are ordered and compared based only on their binary contents this makes sense for standard ascii characters i e the letters a-z and numbers 0-9 but generally does not make much sense for special unicode characters it is however by far the fastest method of performing ordering and comparisons hence it is recommended to stick with the binary collation unless required otherwise using collations in the stand-alone installation of duckdb three collations are included nocase noaccent and nfc the nocase collation compares characters as equal regardless of their casing the noaccent collation compares characters as equal regardless of their accents the nfc collation performs nfc-normalized comparisons see unicode normalization for more information select hello hello -- false select hello collate nocase hello -- true select hello h\u00ebllo -- false select hello collate noaccent h\u00ebllo -- true collations can be combined by chaining them using the dot operator note however that not all collations can be combined together in general the nocase collation can be combined with any other collator but most other collations cannot be combined select hello collate nocase hell\u00f6 -- false select hello collate noaccent hell\u00f6 -- false select hello collate nocase noaccent hell\u00f6 -- true default collations the collations we have seen so far have all been specified per expression it is also possible to specify a default collator either on the global database level or on a base table column the pragma default_collation can be used to specify the global default collator this is the collator that will be used if no other one is specified set default_collation nocase select hello hello -- true collations can also be specified per-column when creating a table when that column is then used in a comparison the per-column collation is used to perform that comparison create table names name varchar collate noaccent insert into names values h\u00e4nnes select name from names where name hannes -- h\u00e4nnes be careful here however as different collations cannot be combined this can be problematic when you want to compare columns that have a different collation specified select name from names where name hannes collate nocase -- error cannot combine types with different collation create table other_names name varchar collate nocase insert into other_names values h\u00e4nnes select from names other_names where names name other_names name -- error cannot combine types with different collation -- need to manually overwrite the collation select from names other_names where names name collate noaccent nocase other_names name collate noaccent nocase -- h\u00e4nnes h\u00e4nnes icu collations the collations we have seen so far are not region-dependent and do not follow any specific regional rules if you wish to follow the rules of a specific region or language you will need to use one of the icu collations for that you need to load the icu extension if you are using the c api you may find the extension in the extension icu folder of the duckdb project using the c api the extension can be loaded as follows duckdb db db loadextension icuextension loading this extension will add a number of language and region specific collations to your database these can be queried using pragma collations command or by querying the pragma_collations function pragma collations select from pragma_collations -- af am ar as az be bg bn bo bs bs ca ceb chr cs cy da de de_at dsb dz ee el en en_us en_us eo es et fa fa_af fi fil fo fr fr_ca ga gl gu ha haw he he_il hi hr hsb hu hy id id_id ig is it ja ka kk kl km kn ko kok ku ky lb lkt ln lo lt lv mk ml mn mr ms mt my nb nb_no ne nl nn om or pa pa pa_in pl ps pt ro ru se si sk sl smn sq sr sr sr_ba sr_me sr_rs sr sr_ba sr_rs sv sw ta te th tk to tr ug uk ur uz vi wae wo xh yi yo zh zh zh_cn zh_sg zh zh_hk zh_mo zh_tw zu these collations can then be used as the other collations would be used before they can also be combined with the nocase collation for example to use the german collation rules you could use the following code snippet create table strings s varchar collate de insert into strings values gabel g\u00f6bel goethe goldmann g\u00f6the g\u00f6tz select from strings order by s -- gabel g\u00f6bel goethe goldmann g\u00f6the g\u00f6tz",
			"category": "Expressions",
			"url": "/docs/sql/expressions/collations",
			"blurb": "Collations provide rules for how text should be sorted or compared in the execution engine. Collations are useful for..."
		},
		{
			"title": "Combining Schemas",
			"text": "examples -- read a set of csv files combining columns by position select from read_csv flights csv -- read a set of csv files combining columns by name select from read_csv flights csv union_by_name true combining schemas when reading from multiple files we have to combine schemas from those files that is because each file has its own schema that can differ from the other files duckdb offers two ways of unifying schemas of multiple files by column position and by column name by default duckdb reads the schema of the first file provided and then unifies columns in subsequent files by column position this works correctly as long as all files have the same schema if the schema of the files differs you might want to use the union_by_name option to allow duckdb to construct the schema by reading all of the names instead below is an example of how both methods work union by position by default duckdb unifies the columns of these different files by position this means that the first column in each file is combined together as well as the second column in each file etc for example consider the following two files flights1 csv flightdate uniquecarrier origincityname destcityname 1988-01-01 aa new york ny los angeles ca 1988-01-02 aa new york ny los angeles ca flights2 csv flightdate uniquecarrier origincityname destcityname 1988-01-03 aa new york ny los angeles ca reading the two files at the same time will produce the following result set flightdate uniquecarrier origincityname destcityname ------------ --------------- ---------------- ----------------- 1988-01-01 aa new york ny los angeles ca 1988-01-02 aa new york ny los angeles ca 1988-01-03 aa new york ny los angeles ca this is equivalent to the sql construct union all union by name if you are processing multiple files that have different schemas perhaps because columns have been added or renamed it might be desirable to unify the columns of different files by name instead this can be done by providing the union_by_name option for example consider the following two files where flights4 csv has an extra column uniquecarrier flights3 csv flightdate origincityname destcityname 1988-01-01 new york ny los angeles ca 1988-01-02 new york ny los angeles ca flights4 csv flightdate uniquecarrier origincityname destcityname 1988-01-03 aa new york ny los angeles ca reading these when unifying column names by position results in an error - as the two files have a different number of columns when specifying the union_by_name option the columns are correctly unified and any missing values are set to null select from read_csv flights3 csv flights4 csv union_by_name true flightdate origincityname destcityname uniquecarrier ------------ ---------------- ----------------- --------------- 1988-01-01 new york ny los angeles ca null 1988-01-02 new york ny los angeles ca null 1988-01-03 new york ny los angeles ca aa this is equivalent to the sql construct union all by name",
			"category": "Multiple Files",
			"url": "/docs/data/multiple_files/combining_schemas",
			"blurb": "Examples -- read a set of CSV files combining columns by position SELECT * FROM read_csv('flights*.csv'); -- read a..."
		},
		{
			"title": "Command Line Arguments",
			"text": "the table below summarizes duckdb s command line options to list all command line options use the command duckdb -help fot a list of dot commands available in the cli shell see the dot commands page argument description --- ------- -append append the database to the end of the file -ascii set output mode to ascii -bail stop after hitting an error -batch force batch i o -box set output mode to box -column set output mode to column -cmd command run command before reading stdin -c command run command and exit -csv set output mode to csv -echo print commands before execution -init filename run the script in filename upon startup instead of duckdbrc -header turn headers on -help show this message -html set output mode to html -interactive force interactive i o -json set output mode to json -line set output mode to line -list set output mode to list -markdown set output mode to markdown -newline sep set output row separator default n -nofollow refuse to open symbolic links to database files -noheader turn headers off -no-stdin exit after processing options instead of reading stdin -nullvalue text set text string for null values default empty string -quote set output mode to quote -readonly open the database read-only -s command run command and exit -separator sep set output column separator to sep default -stats print memory stats before each finalize -table set output mode to table -unsigned allow loading of unsigned extensions -version show duckdb version",
			"category": "Cli",
			"url": "/docs/api/cli/arguments",
			"blurb": "The table below summarizes DuckDB's command line options. To list all command line options, use the command duckdb..."
		},
		{
			"title": "Comparisons",
			"text": "comparison operators the table below shows the standard comparison operators whenever either of the input arguments is null the output of the comparison is null operator description example result --- --- --- --- less than 2 3 true greater than 2 3 false less than or equal to 2 3 true greater than or equal to 4 null null equal null null null or not equal 2 2 false the table below shows the standard distinction operators these operators treat null values as equal operator description example result --- --- --- - is distinct from not equal including null 2 is distinct from null true is not distinct from equal including null null is not distinct from null true between and is not null besides the standard comparison operators there are also the between and is not null operators these behave much like operators but have special syntax mandated by the sql standard they are shown in the table below note that between and not between are only equivalent to the examples below in the cases where both a x and y are of the same type as between will cast all of its inputs to the same type predicate description --- --- a between x and y equivalent to x a and a y a not between x and y equivalent to x a or a y expression is null true if expression is null false otherwise expression isnull alias for is null non-standard expression is not null false if expression is null true otherwise expression notnull alias for is not null non-standard for the expression between x and y x is used as the lower bound and y is used as the upper bound therefore if x y the result will always be false",
			"category": "Expressions",
			"url": "/docs/sql/expressions/comparison_operators",
			"blurb": "Comparison Operators The table below shows the standard comparison operators. Whenever either of the input arguments..."
		},
		{
			"title": "Concurrency",
			"text": "handling concurrency duckdb has two configurable options for concurrency one process can both read and write to the database multiple processes can read from the database but no processes can write access_mode read_only when using option 1 duckdb supports multiple writer threads using a combination of mvcc multi-version concurrency control and optimistic concurrency control see concurrency within a single process but all within that single writer process the reason for this concurrency model is to allow for the caching of data in ram for faster analytical queries rather than going back and forth to disk during each query it also allows the caching of functions pointers the database catalog and other items so that subsequent queries on the same connection are faster duckdb is optimized for bulk operations so executing many small transactions is not a primary design goal concurrency within a single process duckdb supports concurrency within a single process according to the following rules as long as there are no write conflicts multiple concurrent writes will succeed appends will never conflict even on the same table multiple threads can also simultaneously update separate tables or separate subsets of the same table optimistic concurrency control comes into play when two threads attempt to edit update or delete the same row at the same time in that situation the second thread to attempt the edit will fail with a conflict error writing to duckdb from multiple processes writing to duckdb from multiple processes is not supported automatically and is not a primary design goal see handling concurrency if multiple processes must write to the same file several design patterns are possible but would need to be implemented in application logic for example each process could acquire a cross-process mutex lock then open the database in read write mode and close it when the query is complete instead of using a mutex lock each process could instead retry the connection if another process is already connected to the database being sure to close the connection upon query completion another alternative would be to do multi-process transactions on a mysql postgresql or sqlite database and use duckdb s mysql postgresql or sqlite extensions to execute analytical queries on that data periodically additional options include writing data to parquet files and using duckdb s ability to read multiple parquet files taking a similar approach with csv files or creating a web server to receive requests and manage reads and writes to duckdb",
			"category": "Connect",
			"url": "/docs/connect/concurrency",
			"blurb": "Handling Concurrency DuckDB has two configurable options for concurrency: One process can both read and write to the..."
		},
		{
			"title": "Configuration",
			"text": "duckdb has a number of configuration options that can be used to change the behavior of the system the configuration options can be set using either the set statement or the pragma statement they can be reset to their original values using the reset statement the values of configuration options can be queried via the current_setting scalar function or using the duckdb_settings table function examples -- set the memory limit of the system to 10gb set memory_limit 10gb -- configure the system to use 1 thread set threads to 1 -- enable printing of a progress bar during long-running queries set enable_progress_bar true -- set the default null order to nulls last set default_null_order nulls_last -- return the current value of a specific setting select current_setting threads as threads threads int64 10 -- query a specific setting select from duckdb_settings where name threads name value description input_type varchar varchar varchar varchar threads 10 the number of total threads used by the system bigint -- show a list of all available settings select from duckdb_settings -- reset the memory limit of the system back to the default reset memory_limit secrets manager duckdb has a secrets manager which provides a unified user interface for secrets across all backends e g aws s3 that use them configuration reference below is a list of all available settings name description input type default value ---- -------- -- --- calendar the current calendar varchar system locale calendar timezone the current time zone varchar system locale timezone access_mode access mode of the database automatic read_only or read_write varchar automatic allocator_flush_threshold peak allocation threshold at which to flush the allocator after completing a task varchar 128 0 mib allow_persistent_secrets allow the creation of persistent secrets that are stored and loaded on restarts boolean 1 allow_unsigned_extensions allow to load extensions with invalid or missing signatures boolean false arrow_large_buffer_size if arrow buffers for strings blobs uuids and bits should be exported using large buffers boolean false autoinstall_extension_repository overrides the custom endpoint for extension installation on autoloading varchar autoinstall_known_extensions whether known extensions are allowed to be automatically installed when a query depends on them boolean true autoload_known_extensions whether known extensions are allowed to be automatically loaded when a query depends on them boolean true binary_as_string in parquet files interpret binary data as a string boolean checkpoint_threshold wal_autocheckpoint the wal size threshold at which to automatically trigger a checkpoint e g 1gb varchar 16 0 mib custom_extension_repository overrides the custom endpoint for remote extension installation varchar custom_user_agent metadata from duckdb callers varchar default_collation the collation setting used when none is specified varchar default_null_order null_order null ordering used when none is specified nulls_first or nulls_last varchar nulls_last default_order the order type used when none is specified asc or desc varchar asc default_secret_storage allows switching the default storage for secrets varchar local_file disabled_filesystems disable specific file systems preventing access e g localfilesystem varchar duckdb_api duckdb api surface varchar cli enable_external_access allow the database to access external state through e g loading installing modules copy to from csv readers pandas replacement scans etc boolean true enable_fsst_vectors allow scans on fsst compressed segments to emit compressed vectors to utilize late decompression boolean false enable_http_metadata_cache whether or not the global http metadata is used to cache http metadata boolean false enable_object_cache whether or not object cache is used to cache e g parquet metadata boolean false enable_profiling enables profiling and sets the output format json query_tree query_tree_optimizer varchar null enable_progress_bar_print controls the printing of the progress bar when enable_progress_bar is true boolean true enable_progress_bar enables the progress bar printing progress to the terminal for long queries boolean false errors_as_json output error messages as structured json instead of as a raw string boolean false explain_output output of explain statements all optimized_only physical_only varchar physical_only extension_directory set the directory to store extensions in varchar external_threads the number of external threads that work on duckdb tasks bigint 1 file_search_path a comma separated list of directories to search for input files varchar force_download forces upfront download of file boolean 0 home_directory sets the home directory used by the system varchar http_keep_alive keep alive connections setting this to false can help when running into connection failures boolean 1 http_retries http retries on i o error default 3 ubigint 3 http_retry_backoff backoff factor for exponentially increasing retry wait time default 4 float 4 http_retry_wait_ms time between retries default 100ms ubigint 100 http_timeout http timeout read write connection retry default 30000ms ubigint 30000 immediate_transaction_mode whether transactions should be started lazily when needed or immediately when begin transaction is called boolean false integer_division whether or not the operator defaults to integer division or to floating point division boolean 0 lock_configuration whether or not the configuration can be altered boolean false log_query_path specifies the path to which queries should be logged default empty string queries are not logged varchar null max_expression_depth the maximum expression depth limit in the parser warning increasing this setting and using very deep expressions might lead to stack overflow errors ubigint 1000 max_memory memory_limit the maximum memory of the system e g 1gb varchar 80 of ram old_implicit_casting allow implicit casting to from varchar boolean false ordered_aggregate_threshold the number of rows to accumulate before sorting used for tuning ubigint 262144 password the password to use ignored for legacy compatibility varchar null perfect_ht_threshold threshold in bytes for when to use a perfect hash table default 12 bigint 12 pivot_filter_threshold the threshold to switch from using filtered aggregates to list with a dedicated pivot operator bigint 10 pivot_limit the maximum number of pivot columns in a pivot statement default 100000 bigint 100000 prefer_range_joins force use of range joins with mixed predicates boolean false preserve_identifier_case whether or not to preserve the identifier case instead of always lowercasing all non-quoted identifiers boolean true preserve_insertion_order whether or not to preserve insertion order if set to false the system is allowed to re-order any results that do not contain order by clauses boolean true profile_output profiling_output the file to which profile output should be saved or empty to print to the terminal varchar profiling_mode the profiling mode standard or detailed varchar null progress_bar_time sets the time in milliseconds how long a query needs to take before we start printing a progress bar bigint 2000 s3_access_key_id s3 access key id varchar s3_endpoint s3 endpoint empty for default endpoint varchar s3_region s3 region default us-east-1 varchar us-east-1 s3_secret_access_key s3 access key varchar s3_session_token s3 session token varchar s3_uploader_max_filesize s3 uploader max filesize between 50gb and 5tb default 800gb varchar 800gb s3_uploader_max_parts_per_file s3 uploader max parts per file between 1 and 10000 default 10000 ubigint 10000 s3_uploader_thread_limit s3 uploader global thread limit default 50 ubigint 50 s3_url_compatibility_mode disable globs and query parameters on s3 urls boolean 0 s3_url_style s3 url style vhost default or path varchar vhost s3_use_ssl s3 use ssl default true boolean 1 schema sets the default search schema equivalent to setting search_path to a single value varchar main search_path sets the default catalog search path as a comma-separated list of values varchar secret_directory set the directory to which persistent secrets are stored varchar home runner duckdb stored_secrets temp_directory set the directory to which to write temp files varchar threads worker_threads the number of total threads used by the system bigint cores username user the username to use ignored for legacy compatibility varchar null",
			"category": "SQL",
			"url": "/docs/sql/configuration",
			"blurb": "DuckDB has a number of configuration options that can be used to change the behavior of the system. The configuration..."
		},
		{
			"title": "Connect",
			"text": "connect or create a database to use duckdb you must first create a connection to a database the exact process varies by client most clients take a parameter pointing to a database file to read and write from the file extension may be anything e g db duckdb etc if the database file does not exist it will be created the special value memory can be used to create an in-memory database where no data is persisted to disk i e all data is lost when you exit the process see the api docs for client-specific details pages in this section",
			"category": "Connect",
			"url": "/docs/connect/overview",
			"blurb": "Connect or Create a Database To use DuckDB, you must first create a connection to a database. The exact process..."
		},
		{
			"title": "Constraints",
			"text": "in sql constraints can be specified for tables constraints enforce certain properties over data that is inserted into a table constraints can be specified along with the schema of the table as part of the create table statement in certain cases constraints can also be added to a table using the alter table statement but this is not currently supported for all constraints constraints have a strong impact on performance they slow down loading and updates but speed up certain queries please consult the performance guide for details syntax check check constraints allow you to specify an arbitrary boolean expression any columns that do not satisfy this expression violate the constraint for example we could enforce that the name column does not contain spaces using the following check constraint create table students name varchar check not contains name insert into students values this name contains spaces -- constraint error check constraint failed students not null a not-null constraint specifies that the column cannot contain any null values by default all columns in tables are nullable adding not null to a column definition enforces that a column cannot contain null values create table students name varchar not null insert into students values null -- constraint error not null constraint failed students name primary key unique primary key or unique constraints define a column or set of columns that are a unique identifier for a row in the table the constraint enforces that the specified columns are unique within a table i e that at most one row contains the given values for the set of columns create table students id integer primary key name varchar insert into students values 1 student 1 insert into students values 1 student 2 -- constraint error duplicate key id 1 violates primary key constraint in order to enforce this property efficiently an art index is automatically created for every primary key or unique constraint that is defined in the table primary key constraints and unique constraints are identical except for two points a table can only have one primary key constraint defined but many unique constraints a primary key constraint also enforces the keys to not be null indexes have certain limitations that might result in constraints being evaluated too eagerly see the indexes section for more details foreign key foreign keys define a column or set of columns that refer to a primary key or unique constraint from another table the constraint enforces that the key exists in the other table create table students id integer primary key name varchar create table exams exam_id integer references students id grade integer insert into students values 1 student 1 insert into exams values 1 10 insert into exams values 2 10 -- constraint error violates foreign key constraint because key id 2 does not exist in the referenced table in order to enforce this property efficiently an art index is automatically created for every foreign key constraint that is defined in the table indexes have certain limitations that might result in constraints being evaluated too eagerly see the indexes section for more details",
			"category": "SQL",
			"url": "/docs/sql/constraints",
			"blurb": "In SQL, constraints can be specified for tables. Constraints enforce certain properties over data that is inserted..."
		},
		{
			"title": "DBeaver SQL IDE",
			"text": "dbeaver is a powerful and popular desktop sql editor and integrated development environment ide it has both an open source and enterprise version it is useful for visually inspecting the available tables in duckdb and for quickly building complex queries duckdb s jdbc connector allows dbeaver to query duckdb files and by extension any other files that duckdb can access like parquet files install dbeaver using the download links and instructions found at their download page open dbeaver and create a new connection either click on the new database connection button or go to database new database connection in the menu bar search for duckdb select it and click next enter the path or browse to the duckdb database file you wish to query to use an in-memory duckdb useful primarily if just interested in querying parquet files or for testing enter memory as the path click test connection this will then prompt you to install the duckdb jdbc driver if you are not prompted see alternative driver installation instructions below click download to download duckdb s jdbc driver from maven once download is complete click ok then click finish note if you are in a corporate environment or behind a firewall before clicking download click the download configuration link to configure your proxy settings you should now see a database connection to your duckdb database in the left hand database navigator pane expand it to see the tables and views in your database right click on that connection and create a new sql script write some sql and click the execute button now you re ready to fly with duckdb and dbeaver alternative driver installation if not prompted to install the duckdb driver when testing your connection return to the connect to a database dialog and click edit driver settings alternate you may also access the driver settings menu by returning to the main dbeaver window and clicking database driver manager in the menu bar then select duckdb then click edit go to the libraries tab then click on the duckdb driver and click download update if you do not see the duckdb driver first click on reset to defaults click download to download duckdb s jdbc driver from maven once download is complete click ok then return to the main dbeaver window and continue with step 7 above note if you are in a corporate environment or behind a firewall before clicking download click the download configuration link to configure your proxy settings",
			"category": "Sql Editors",
			"url": "/docs/guides/sql_editors/dbeaver",
			"blurb": "DBeaver is a powerful and popular desktop sql editor and integrated development environment (IDE). It has both an..."
		},
		{
			"title": "DELETE Statement",
			"text": "the delete statement removes rows from the table identified by the table-name examples -- remove the rows matching the condition i 2 from the database delete from tbl where i 2 -- delete all rows in the table tbl delete from tbl -- the truncate statement removes all rows from a table -- acting as an alias for delete from without a where clause truncate tbl syntax the delete statement removes rows from the table identified by the table-name if the where clause is not present all records in the table are deleted if a where clause is supplied then only those rows for which the where clause results in true are deleted rows for which the expression is false or null are retained the using clause allows deleting based on the content of other tables or subqueries limitations on reclaiming memory and disk space running delete does not mean space is reclaimed in general rows are only marked as deleted duckdb s support for vacuum is limited to vacuuming entire row groups",
			"category": "Statements",
			"url": "/docs/sql/statements/delete",
			"blurb": "The DELETE statement removes rows from the table identified by the table-name. Examples -- remove the rows matching..."
		},
		{
			"title": "DROP Statement",
			"text": "the drop statement removes a catalog entry added previously with the create command examples -- delete the table with the name tbl drop table tbl -- drop the view with the name v1 do not throw an error if the view does not exist drop view if exists v1 -- drop type drop type type_name syntax the optional if exists clause suppresses the error that would normally result if the table does not exist by default or if the restrict clause is provided the entry will not be dropped if there are any other objects that depend on it if the cascade clause is provided then all the objects that are dependent on the object will be dropped as well create schema myschema create table myschema t1 i integer -- error cannot drop myschema because the table myschema t1 depends on it drop schema myschema -- cascade drops both myschema and myschema t1 drop schema myschema cascade limitations on reclaiming disk space running drop table should free the memory used by the table but not always disk space even if disk space does not decrease the free blocks will be marked as free for example if we have a 2 gb file and we drop a 1 gb table the file might still be 2 gb but it should have 1 gb of free blocks in it to check this use the following pragma and check the number of free_blocks in the output pragma database_size",
			"category": "Statements",
			"url": "/docs/sql/statements/drop",
			"blurb": "The DROP statement removes a catalog entry added previously with the CREATE command. Examples -- delete the table..."
		},
		{
			"title": "Data Ingestion",
			"text": "duckdb-wasm has multiple ways to import data depending on the format of the data there are two steps to import data into duckdb first the data file is imported into a local file system using register functions registeremptyfilebuffer registerfilebuffer registerfilehandle registerfiletext registerfileurl then the data file is imported into duckdb using insert functions insertarrowfromipcstream insertarrowtable insertcsvfrompath insertjsonfrompath or directly using from sql query using extensions like parquet or wasm-flavored httpfs insert statements can also be used to import data data import open close connection create a new connection const c await db connect import data close the connection to release memory await c close apache arrow data can be inserted from an existing arrow table more example https arrow apache org docs js import tablefromarrays from apache-arrow eos signal according to arrorw ipc streaming format see https arrow apache org docs format columnar html ipc-streaming-format const eos new uint8array 255 255 255 255 0 0 0 0 const arrowtable tablefromarrays id 1 2 3 name john jane jack age 20 21 22 await c insertarrowtable arrowtable name arrow_table write eos await c insertarrowtable eos name arrow_table from a raw arrow ipc stream const streamresponse await fetch someapi const streamreader streamresponse body getreader const streaminserts while true const value done await streamreader read if done break streaminserts push c insertarrowfromipcstream value name streamed write eos streaminserts push c insertarrowfromipcstream eos name streamed await promise all streaminserts csv from csv files interchangeable registerfile text buffer url handle const csvcontent 1 foo n2 bar n await db registerfiletext data csv csvcontent with typed insert options await c insertcsvfrompath data csv schema main name foo detect false header false delimiter columns col1 new arrow int32 col2 new arrow utf8 json from json documents in row-major format const jsonrowcontent col1 1 col2 foo col1 2 col2 bar await db registerfiletext rows json json stringify jsonrowcontent await c insertjsonfrompath rows json name rows or column-major format const jsoncolcontent col1 1 2 col2 foo bar await db registerfiletext columns json json stringify jsoncolcontent await c insertjsonfrompath columns json name columns from api const streamresponse await fetch someapi content json await db registerfilebuffer file json new uint8array await streamresponse arraybuffer await c insertjsonfrompath file json name jsoncontent parquet from parquet files local const pickedfile file letuserpickfile await db registerfilehandle local parquet pickedfile duckdbdataprotocol browser_filereader true remote await db registerfileurl remote parquet https origin remote parquet duckdbdataprotocol http false using fetch const res await fetch https origin remote parquet await db registerfilebuffer buffer parquet new uint8array await res arraybuffer by specifying urls in the sql text await c query create table direct as select from https origin remote parquet or by executing raw insert statements await c query insert into existing_table values 1 foo 2 bar httpfs wasm-flavored by specifying urls in the sql text await c query create table direct as select from https origin remote parquet insert statement or by executing raw insert statements await c query insert into existing_table values 1 foo 2 bar",
			"category": "Wasm",
			"url": "/docs/api/wasm/data_ingestion",
			"blurb": "DuckDB-Wasm has multiple ways to import data, depending on the format of the data. There are two steps to import data..."
		},
		{
			"title": "Data Ingestion",
			"text": "csv files csv files can be read using the read_csv function called either from within python or directly from within sql by default the read_csv function attempts to auto-detect the csv settings by sampling from the provided file import duckdb read from a file using fully auto-detected settings duckdb read_csv example csv read multiple csv files from a folder duckdb read_csv folder csv specify options on how the csv is formatted internally duckdb read_csv example csv header false sep override types of the first two columns duckdb read_csv example csv dtype int varchar use the experimental parallel csv reader duckdb read_csv example csv parallel true directly read a csv file from within sql duckdb sql select from example csv call read_csv from within sql duckdb sql select from read_csv example csv see the csv import page for more information parquet files parquet files can be read using the read_parquet function called either from within python or directly from within sql import duckdb read from a single parquet file duckdb read_parquet example parquet read multiple parquet files from a folder duckdb read_parquet folder parquet read a parquet over https duckdb read_parquet https some url some_file parquet read a list of parquet files duckdb read_parquet file1 parquet file2 parquet file3 parquet directly read a parquet file from within sql duckdb sql select from example parquet call read_parquet from within sql duckdb sql select from read_parquet example parquet see the parquet loading page for more information json files json files can be read using the read_json function called either from within python or directly from within sql by default the read_json function will automatically detect if a file contains newline-delimited json or regular json and will detect the schema of the objects stored within the json file import duckdb read from a single json file duckdb read_json example json read multiple json files from a folder duckdb read_json folder json directly read a json file from within sql duckdb sql select from example json call read_json from within sql duckdb sql select from read_json_auto example json dataframes arrow tables duckdb is automatically able to query a pandas dataframe polars dataframe or arrow object that is stored in a python variable by name accessing these is made possible by replacement scans duckdb supports querying multiple types of apache arrow objects including tables datasets recordbatchreaders and scanners see the python guides for more examples import duckdb import pandas as pd test_df pd dataframe from_dict i 1 2 3 4 j one two three four duckdb sql select from test_df fetchall 1 one 2 two 3 three 4 four duckdb also supports registering a dataframe or arrow object as a virtual table comparable to a sql view this is useful when querying a dataframe arrow object that is stored in another way as a class variable or a value in a dictionary below is a pandas example if your pandas dataframe is stored in another location here is an example of manually registering it import duckdb import pandas as pd my_dictionary my_dictionary test_df pd dataframe from_dict i 1 2 3 4 j one two three four duckdb register test_df_view my_dictionary test_df duckdb sql select from test_df_view fetchall 1 one 2 two 3 three 4 four you can also create a persistent table in duckdb from the contents of the dataframe or the view create a new table from the contents of a dataframe con execute create table test_df_table as select from test_df insert into an existing table from the contents of a dataframe con execute insert into test_df_table select from test_df pandas dataframes object columns pandas dataframe columns of an object dtype require some special care since this stores values of arbitrary type to convert these columns to duckdb we first go through an analyze phase before converting the values in this analyze phase a sample of all the rows of the column are analyzed to determine the target type this sample size is by default set to 1000 if the type picked during the analyze step is incorrect this will result in a failed to cast value error in which case you will need to increase the sample size the sample size can be changed by setting the pandas_analyze_sample config option example setting the sample size to 100000 duckdb default_connection execute set global pandas_analyze_sample 100000 object conversion this is a mapping of python object types to duckdb logical types none - null bool - boolean datetime timedelta - interval str - varchar bytearray - blob memoryview - blob decimal decimal - decimal double uuid uuid - uuid the rest of the conversion rules are as follows int since integers can be of arbitrary size in python there is not a one-to-one conversion possible for ints intead we perform these casts in order until one succeeds bigint integer ubigint uinteger double when using the duckdb value class it s possible to set a target type which will influence the conversion float these casts are tried in order until one succeeds double float datetime datetime for datetime we will check pandas isnull if it s available and return null if it returns true we check against datetime datetime min and datetime datetime max to convert to -inf and inf respectively if the datetime has tzinfo we will use timestamptz otherwise it becomes timestamp datetime time if the time has tzinfo we will use timetz otherwise it becomes time datetime date date converts to the date type we check against datetime date min and datetime date max to convert to -inf and inf respectively bytes bytes converts to blob by default when it s used to construct a value object of type bitstring it maps to bitstring instead list list becomes a list type of the most permissive type of its children for example my_list_value 12345 test will become varchar because 12345 can convert to varchar but test can not convert to integer 12345 test dict the dict object can convert to either struct or map depending on its structure if the dict has a structure similar to my_map_dict key 1 2 3 value one two three then we ll convert it to a map of key-value pairs of the two lists zipped together the example above becomes a map integer varchar 1 one 2 two 3 three the name of the fields matters and the two lists need to have the same size otherwise we ll try to convert it to a struct my_struct_dict 1 one 2 2 three 1 2 3 false true becomes 1 one 2 2 three 1 2 3 false true every key of the dictionary is converted to string tuple tuple converts to list by default when it s used to construct a value object of type struct it will convert to struct instead numpy ndarray and numpy datetime64 ndarray and datetime64 are converted by calling tolist and converting the result of that",
			"category": "Python",
			"url": "/docs/api/python/data_ingestion",
			"blurb": "CSV Files CSV files can be read using the read_csv function, called either from within Python or directly from within..."
		},
		{
			"title": "Data Types",
			"text": "general-purpose data types the table below shows all the built-in general-purpose data types the alternatives listed in the aliases column can be used to refer to these types as well however note that the aliases are not part of the sql standard and hence might not be accepted by other database engines name aliases description -- -- ---- bigint int8 long signed eight-byte integer bit bitstring string of 1s and 0s blob bytea binary varbinary variable-length binary data boolean bool logical logical boolean true false date calendar date year month day decimal prec scale numeric prec scale fixed-precision number with the given width precision and scale defaults to prec 18 and scale 3 double float8 double precision floating-point number 8 bytes hugeint signed sixteen-byte integer integer int4 int signed signed four-byte integer interval date time delta real float4 float single precision floating-point number 4 bytes smallint int2 short signed two-byte integer time time of day no time zone timestamp with time zone timestamptz combination of time and date that uses the current time zone timestamp datetime combination of time and date tinyint int1 signed one-byte integer ubigint unsigned eight-byte integer uhugeint unsigned sixteen-byte integer uinteger unsigned four-byte integer usmallint unsigned two-byte integer utinyint unsigned one-byte integer uuid uuid data type varchar char bpchar text string variable-length character string implicit and explicit typecasting is possible between numerous types see the typecasting page for details nested composite types duckdb supports five nested data types array list struct map and union each supports different use cases and has a different structure name description rules when used in a column build from values define in ddl create - --- --- -- -- array an ordered fixed-length sequence of data values of the same type each row must have the same data type within each instance of the array and the same number of elements 1 2 3 int 3 list an ordered sequence of data values of the same type each row must have the same data type within each instance of the list but can have any number of elements 1 2 3 int map a dictionary of multiple named values each key having the same type and each value having the same type keys and values can be any type and can be different types from one another rows may have different keys map 1 2 a b map int varchar struct a dictionary of multiple named values where each key is a string but the value can be a different type for each key each row must have the same keys i 42 j a struct i int j varchar union a union of multiple alternative data types storing one of them in each value at a time a union also contains a discriminator tag value to inspect and access the currently set member type rows may be set to different member types of the union union_value num 2 union num int text varchar nesting array list map struct and union types can be arbitrarily nested to any depth so long as the type rules are observed -- struct with lists select birds duck goose heron aliens null amphibians frog toad -- struct with list of maps select test map 1 5 42 1 45 map 1 5 42 1 45 -- a list of unions select union_value num 2 union_value str abc union str varchar num integer performance implications the choice of data types can have a strong effect on performance please consult the performance guide for details pages in this section",
			"category": "Data Types",
			"url": "/docs/sql/data_types/overview",
			"blurb": "The table below shows all the built-in general-purpose data types."
		},
		{
			"title": "Date Format Functions",
			"text": "the strftime and strptime functions can be used to convert between dates timestamps and strings this is often required when parsing csv files displaying output to the user or transferring information between programs because there are many possible date representations these functions accept a format string that describes how the date or timestamp should be structured strftime examples strftime timestamp format converts timestamps or dates to strings according to the specified pattern select strftime date 1992-03-02 d m y -- 02 03 1992 select strftime timestamp 1992-03-02 20 32 45 a -d b y - i m s p -- monday 2 march 1992 - 08 32 45 pm strptime examples strptime string format converts strings to timestamps according to the specified pattern select strptime 02 03 1992 d m y -- 1992-03-02 00 00 00 select strptime monday 2 march 1992 - 08 32 45 pm a -d b y - i m s p -- 1992-03-02 20 32 45 csv parsing the date formats can also be specified during csv parsing either in the copy statement or in the read_csv function this can be done by either specifying a dateformat or a timestampformat or both dateformat will be used for converting dates and timestampformat will be used for converting timestamps below are some examples for how to use this -- in copy statement copy dates from test csv dateformat d m y timestampformat a -d b y - i m s p -- in read_csv function select from read_csv test csv dateformat m d y format specifiers below is a full list of all available format specifiers specifier description example - ------ --- a abbreviated weekday name sun mon a full weekday name sunday monday b abbreviated month name jan feb dec b full month name january february c iso date and time representation 1992-03-02 10 30 20 d day of the month as a zero-padded decimal 01 02 31 -d day of the month as a decimal number 1 2 30 f microsecond as a decimal number zero-padded on the left 000000 - 999999 g millisecond as a decimal number zero-padded on the left 000 - 999 g iso 8601 year with century representing the year that contains the greater part of the iso week see v 0001 0002 2013 2014 9998 9999 h hour 24-hour clock as a zero-padded decimal number 00 01 23 -h hour 24-hour clock as a decimal number 0 1 23 i hour 12-hour clock as a zero-padded decimal number 01 02 12 -i hour 12-hour clock as a decimal number 1 2 12 j day of the year as a zero-padded decimal number 001 002 366 -j day of the year as a decimal number 1 2 366 m month as a zero-padded decimal number 01 02 12 -m month as a decimal number 1 2 12 m minute as a zero-padded decimal number 00 01 59 -m minute as a decimal number 0 1 59 n nanosecond as a decimal number zero-padded on the left 000000000 - 999999999 p locale s am or pm am pm s second as a zero-padded decimal number 00 01 59 -s second as a decimal number 0 1 59 u iso 8601 weekday as a decimal number where 1 is monday 1 2 7 u week number of the year week 01 starts on the first sunday of the year so there can be week 00 note that this is not compliant with the week date standard in iso-8601 00 01 53 v iso 8601 week as a decimal number with monday as the first day of the week week 01 is the week containing jan 4 01 53 w weekday as a decimal number 0 1 6 w week number of the year week 01 starts on the first monday of the year so there can be week 00 note that this is not compliant with the week date standard in iso-8601 00 01 53 x iso date representation 1992-03-02 x iso time representation 10 30 20 y year without century as a zero-padded decimal number 00 01 99 -y year without century as a decimal number 0 1 99 y year with century as a decimal number 2013 2019 etc z time offset from utc in the form hh mm hhmm or hh -0700 z time zone name europe amsterdam a literal character",
			"category": "Functions",
			"url": "/docs/sql/functions/dateformat",
			"blurb": "The strftime and strptime functions can be used to convert between dates/timestamps and strings. This is often..."
		},
		{
			"title": "Date Functions",
			"text": "this section describes functions and operators for examining and manipulating date values date operators the table below shows the available mathematical operators for date types operator description example result - -- --- -- addition of days integers date 1992-03-22 5 1992-03-27 addition of an interval date 1992-03-22 interval 5 day 1992-03-27 addition of a variable interval select date 1992-03-22 interval 1 day d days from values 5 11 as d days 1992-03-27 and 1992-04-02 - subtraction of date s date 1992-03-27 - date 1992-03-22 5 - subtraction of an interval date 1992-03-27 - interval 5 day 1992-03-22 - subtraction of a variable interval select date 1992-03-27 - interval 1 day d days from values 5 11 as d days 1992-03-22 and 1992-03-16 adding to or subtracting from infinite values produces the same infinite value date functions the table below shows the available functions for date types dates can also be manipulated with the timestamp functions through type promotion function description example result -- -- --- - current_date current date at start of current transaction current_date 2022-10-08 date_add date interval add the interval to the date date_add date 1992-09-15 interval 2 month 1992-11-15 date_diff part startdate enddate the number of partition boundaries between the dates date_diff month date 1992-09-15 date 1992-11-14 2 date_part part date get the subfield equivalent to extract date_part year date 1992-09-20 1992 date_sub part startdate enddate the number of complete partitions between the dates date_sub month date 1992-09-15 date 1992-11-14 1 date_trunc part date truncate to specified precision date_trunc month date 1992-03-07 1992-03-01 datediff part startdate enddate alias of date_diff the number of partition boundaries between the dates datediff month date 1992-09-15 date 1992-11-14 2 datepart part date alias of date_part get the subfield equivalent to extract datepart year date 1992-09-20 1992 datesub part startdate enddate alias of date_sub the number of complete partitions between the dates datesub month date 1992-09-15 date 1992-11-14 1 datetrunc part date alias of date_trunc truncate to specified precision datetrunc month date 1992-03-07 1992-03-01 dayname date the english name of the weekday dayname date 1992-09-20 sunday extract part from date get subfield from a date extract year from date 1992-09-20 1992 greatest date date the later of two dates greatest date 1992-09-20 date 1992-03-07 1992-09-20 isfinite date returns true if the date is finite false otherwise isfinite date 1992-03-07 true isinf date returns true if the date is infinite false otherwise isinf date -infinity true last_day date the last day of the corresponding month in the date last_day date 1992-09-20 1992-09-30 least date date the earlier of two dates least date 1992-09-20 date 1992-03-07 1992-03-07 make_date bigint bigint bigint the date for the given parts make_date 1992 9 20 1992-09-20 monthname date the english name of the month monthname date 1992-09-20 september strftime date format converts a date to a string according to the format string strftime date 1992-01-01 a -d b y wed 1 january 1992 time_bucket bucket_width date offset truncate date by the specified interval bucket_width buckets are offset by offset interval time_bucket interval 2 months date 1992-04-20 interval 1 month 1992-04-01 time_bucket bucket_width date origin truncate date by the specified interval bucket_width buckets are aligned relative to origin date origin defaults to 2000-01-03 for buckets that don t include a month or year interval and to 2000-01-01 for month and year buckets time_bucket interval 2 weeks date 1992-04-20 date 1992-04-01 1992-04-15 today current date start of current transaction today 2022-10-08 there are also dedicated extraction functions to get the subfields a few examples include extracting the day from a date or the day of the week from a date functions applied to infinite dates will either return the same infinite dates e g greatest or null e g date_part depending on what makes sense in general if the function needs to examine the parts of the infinite date the result will be null",
			"category": "Functions",
			"url": "/docs/sql/functions/date",
			"blurb": "This section describes functions and operators for examining and manipulating date values. Date Operators The table..."
		},
		{
			"title": "Date Part Functions",
			"text": "the date_part and date_diff and date_trunc functions can be used to manipulate the fields of temporal types the fields are specified as strings that contain the part name of the field part specifiers below is a full list of all available date part specifiers the examples are the corresponding parts of the timestamp 2021-08-03 11 59 44 123456 usable as date part specifiers and in intervals specifier description synonyms example -- -- --- - century gregorian century cent centuries c 21 day gregorian day days d dayofmonth 3 decade gregorian decade dec decades decs 202 hour hours hr hours hrs h 11 microseconds sub-minute microseconds microsecond us usec usecs usecond useconds 44123456 millennium gregorian millennium mil millenniums millenia mils millenium 3 milliseconds sub-minute milliseconds millisecond ms msec msecs msecond mseconds 44123 minute minutes min minutes mins m 59 month gregorian month mon months mons 8 quarter quarter of the year 1-4 quarters 3 second seconds sec seconds secs s 44 year gregorian year yr y years yrs 2021 usable in date part specifiers only specifier description synonyms example -- -- --- - dayofweek day of the week sunday 0 saturday 6 weekday dow 2 dayofyear day of the year 1-365 366 doy 215 epoch seconds since 1970-01-01 1627991984 era gregorian era ce ad bce bc 1 isodow iso day of the week monday 1 sunday 7 2 isoyear iso year number starts on monday of week containing jan 4th 2021 timezone_hour time zone offset hour portion 0 timezone_minute time zone offset minute portion 0 timezone time zone offset in seconds 0 week week number weeks w 31 yearweek iso year and week number in yyyyww format 202131 note that the time zone parts are all zero unless a time zone plugin such as icu has been installed to support timestamp with time zone part functions there are dedicated extraction functions to get certain subfields function description example result -- -- --- - century date century century date 1992-02-15 20 day date day day date 1992-02-15 15 dayofmonth date day synonym dayofmonth date 1992-02-15 15 dayofweek date numeric weekday sunday 0 saturday 6 dayofweek date 1992-02-15 6 dayofyear date day of the year starts from 1 i e january 1 1 dayofyear date 1992-02-15 46 decade date decade year 10 decade date 1992-02-15 199 epoch date seconds since 1970-01-01 epoch date 1992-02-15 698112000 era date calendar era era date 0044-03-15 bc 0 hour date hours hour timestamp 2021-08-03 11 59 44 123456 11 isodow date numeric iso weekday monday 1 sunday 7 isodow date 1992-02-15 6 isoyear date iso year number starts on monday of week containing jan 4th isoyear date 2022-01-01 2021 microsecond date sub-minute microseconds microsecond timestamp 2021-08-03 11 59 44 123456 44123456 millennium date millennium millennium date 1992-02-15 2 millisecond date sub-minute milliseconds millisecond timestamp 2021-08-03 11 59 44 123456 44123 minute date minutes minute timestamp 2021-08-03 11 59 44 123456 59 month date month month date 1992-02-15 2 quarter date quarter quarter date 1992-02-15 1 second date seconds second timestamp 2021-08-03 11 59 44 123456 44 timezone_hour date time zone offset hour portion timezone_hour date 1992-02-15 0 timezone_minute date time zone offset minutes portion timezone_minute date 1992-02-15 0 timezone date time zone offset in minutes timezone date 1992-02-15 0 week date iso week week date 1992-02-15 7 weekday date numeric weekday synonym sunday 0 saturday 6 weekday date 1992-02-15 6 weekofyear date iso week synonym weekofyear date 1992-02-15 7 year date year year date 1992-02-15 1992 yearweek date bigint of combined iso year number and 2-digit version of iso week number yearweek date 1992-02-15 199207",
			"category": "Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "The date_part and date_diff and date_trunc functions can be used to manipulate the fields of temporal types. The..."
		},
		{
			"title": "Date Types",
			"text": "name aliases description ------- -------- -------------------------------- date calendar date year month day a date specifies a combination of year month and day duckdb follows the sql standard s lead by counting dates exclusively in the gregorian calendar even for years before that calendar was in use dates can be created using the date keyword where the data must be formatted according to the iso 8601 format yyyy-mm-dd -- 20 september 1992 select date 1992-09-20 special values there are also three special date values that can be used on input input string description ------------- ---------------------------------- epoch 1970-01-01 unix system day zero infinity later than all other dates -infinity earlier than all other dates the values infinity and -infinity are specially represented inside the system and will be displayed unchanged but epoch is simply a notational shorthand that will be converted to the date value when read select -infinity date epoch date infinity date negative epoch positive ---------- ----------- --------- -infinity 1970-01-01 infinity functions see date functions",
			"category": "Data Types",
			"url": "/docs/sql/data_types/date",
			"blurb": "A date specifies a combination of year, month and day."
		},
		{
			"title": "Describe",
			"text": "describing a table in order to view the schema of a table use describe or show followed by the table name create table tbl i integer primary key j varchar describe tbl show tbl -- equivalent to describe tbl column_name column_type null key default extra ------------- ------------- ------ ------ --------- ------- i integer no pri null null j varchar yes null null null describing a query in order to view the schema of the result of a query prepend describe to a query describe select from tbl column_name column_type null key default extra ------------- ------------- ------ ------ --------- ------- i integer yes null null null j varchar yes null null null note that there are subtle differences compared to the result when describing a table nullability null and key information key are lost using describe in a subquery describe can be used a subquery this allows creating a table from the description for example create table tbl_description as select from describe tbl describing remote tables it is possible to describe remote tables via the httpfs extension using the describe table statement for example describe table https blobs duckdb org data star_trek-season_1 csv column_name column_type null key default extra varchar varchar varchar varchar varchar varchar season_num bigint yes episode_num bigint yes aired_date date yes 18 rows 6 columns",
			"category": "Meta",
			"url": "/docs/guides/meta/describe",
			"blurb": "Describing a Table In order to view the schema of a table, use DESCRIBE or SHOW followed by the table name. CREATE..."
		},
		{
			"title": "Directly Reading Files",
			"text": "duckdb allows directly reading files via the read_text and read_blob functions these functions accept a filename a list of filenames or a glob pattern and output the content of each file as a varchar or blob respectively as well as additional metadata such as the file size and last modified time read_text the read_text table function reads from the selected source s to a varchar select size parse_path filename content from read_text test sql table_function files txt size parse_path filename content int64 varchar varchar 12 test sql table_function files one txt hello world 2 test sql table_function files three txt 42 10 test sql table_function files two txt f\u00f6\u00f6 b\u00e4r the file content is first validated to be valid utf-8 if read_text attempts to read a file with invalid utf-8 an error is thrown suggesting to use read_blob instead read_blob the read_blob table function reads from the selected source s to a blob select size content filename from read_blob test sql table_function files size content filename int64 blob varchar 178 pk x03 x04 x0a x00 x00 x00 x00 x00 xaci x x14t xce xc7 x0a x00 x00 x00 x0a x00 x00 x00 x09 x00 test sql table_function files four blob 12 hello world test sql table_function files one txt 2 42 test sql table_function files three txt 10 f xc3 xb6 xc3 xb6 b xc3 xa4r test sql table_function files two txt schema the schemas of the tables returned by read_text and read_blob are identical column_name column_type null key default extra varchar varchar varchar varchar varchar varchar filename varchar yes content varchar yes size bigint yes last_modified timestamp yes handling missing metadata in cases where the underlying filesystem is unable to provide some of this data due e g because httpfs can t always return a valid timestamp the cell is set to null instead support for projection pushdown the table functions also utilize projection pushdown to avoid computing properties unnecessarily so you could e g use this to glob a directory full of huge files to get the file size in the size column as long as you omit the content column the data wont be read into duckdb",
			"category": "Import",
			"url": "/docs/guides/import/read_file",
			"blurb": "DuckDB allows directly reading files via the read_text and read_blob functions. These functions accept a filename, a..."
		},
		{
			"title": "Documentation",
			"text": "welcome to the duckdb documentation feel free to grab a a href site baseurl duckdb-docs pdf pdf copy a of the documentation for offline reference",
			"category": "Docs",
			"url": "/docs/index",
			"blurb": "Welcome to the DuckDB Documentation! Feel free to grab a <a href={{ site.baseurl }}/duckdb-docs.pdf> PDF copy </a> of..."
		},
		{
			"title": "Dot Commands",
			"text": "dot commands are available in the duckdb cli client to use one of these commands begin the line with a period immediately followed by the name of the command you wish to execute additional arguments to the command are entered space separated after the command if an argument must contain a space either single or double quotes may be used to wrap that parameter dot commands must be entered on a single line and no whitespace may occur before the period no semicolon is required at the end of the line to see available commands use the help command dot commands command description --- ------ bail on off stop after hitting an error default off binary on off turn binary output on or off default off cd directory change the working directory to directory changes on off show number of rows changed by sql check glob fail if output since testcase does not match columns column-wise rendering of query results constant color sets the syntax highlighting color used for constant values constantcode code sets the syntax highlighting terminal code used for constant values databases list names and files of attached databases echo on off turn command echo on or off excel display the output of next command in spreadsheet exit code exit this program with return-code code explain on off auto change the explain formatting mode default auto fullschema --indent show schema and the content of sqlite_stat tables headers on off turn display of headers on or off help -all pattern show help text for pattern highlight on off toggle syntax highlighting in the shell on off import file table import data from file into table indexes table show names of indexes keyword color sets the syntax highlighting color used for keywords keywordcode code sets the syntax highlighting terminal code used for keywords lint options report potential schema issues log file off turn logging on or off file can be stderr stdout maxrows count sets the maximum number of rows for display only for duckbox mode maxwidth count sets the maximum width in characters 0 defaults to terminal width only for duckbox mode mode mode table set output mode nullvalue string use string in place of null values once options file output for the next sql command only to file open options file close existing database and reopen file output file send output to file or stdout if file is omitted parameter cmd manage sql parameter bindings print string print literal string prompt main continue replace the standard prompts quit exit this program read file read input from file rows row-wise rendering of query results default schema pattern show the create statements matching pattern separator col row change the column and row separators sha3sum compute a sha3 hash of database content shell cmd args run cmd args in a system shell show show the current values for various settings system cmd args run cmd args in a system shell tables table list names of tables matching like pattern table testcase name begin redirecting output to name timer on off turn sql timer on or off width num1 num2 set minimum column widths for columnar output using the help commmand the help text may be filtered by passing in a text string as the second argument help m maxrows count sets the maximum number of rows for display default 40 only for duckbox mode maxwidth count sets the maximum width in characters 0 defaults to terminal width only for duckbox mode mode mode table set output mode output writing results to a file by default the duckdb cli sends results to the terminal s standard output however this can be modified using either the output or once commands pass in the desired output file location as a parameter the once command will only output the next set of results and then revert to standard out but output will redirect all subsequent output to that file location note that each result will overwrite the entire file at that destination to revert back to standard output enter output with no file parameter in this example the output format is changed to markdown the destination is identified as a markdown file and then duckdb will write the output of the sql statement to that file output is then reverted to standard output using output with no parameter mode markdown output my_results md select taking flight as output_column output select back to the terminal as displayed_column the file my_results md will then contain output_column --------------- taking flight the terminal will then display displayed_column ---------------------- back to the terminal a common output format is csv or comma separated values duckdb supports sql syntax to export data as csv or parquet but the cli-specific commands may be used to write a csv instead if desired mode csv once my_output_file csv select 1 as col_1 2 as col_2 union all select 10 as col1 20 as col_2 the file my_output_file csv will then contain col_1 col_2 1 2 10 20 by passing special options flags to the once command query results can also be sent to a temporary file and automatically opened in the user s default program use either the -e flag for a text file opened in the default text editor or the -x flag for a csv file opened in the default spreadsheet editor this is useful for more detailed inspection of query results especially if there is a relatively large result set the excel command is equivalent to once -x once -e select quack as hello the results then open in the default text file editor of the system for example querying the database schema all duckdb clients support querying the database schema with sql but the cli has additional dot commands that can make it easier to understand the contents of a database the tables command will return a list of tables in the database it has an optional argument that will filter the results according to a like pattern create table swimmers as select duck as animal create table fliers as select duck as animal create table walkers as select duck as animal tables fliers swimmers walkers for example to filter to only tables that contain an l use the like pattern l tables l fliers walkers the schema command will show all of the sql statements used to define the schema of the database schema create table fliers animal varchar create table swimmers animal varchar create table walkers animal varchar configuring the syntax highlighter by default the shell includes support for syntax highlighting the cli s syntax highlighter can be configured using the following commands to turn off the highlighter highlight on to turn on the highlighter highlight off to configure the color used to highlight constants constant red green yellow blue magenta cyan white brightblack brightred brightgreen brightyellow brightblue brightmagenta brightcyan brightwhite constantcode terminal_code to configure the color used to highlight keywords keyword red green yellow blue magenta cyan white brightblack brightred brightgreen brightyellow brightblue brightmagenta brightcyan brightwhite keywordcode terminal_code importing data from csv this feature is only included for compatibility reasons and may be removed in the future use the read_csv function or the copy statement to load csv files duckdb supports sql syntax to directly query or import csv files but the cli-specific commands may be used to import a csv instead if desired the import command takes two arguments and also supports several options the first argument is the path to the csv file and the second is the name of the duckdb table to create since duckdb requires stricter typing than sqlite upon which the duckdb cli is based the destination table must be created before using the import command to automatically detect the schema and create a table from a csv see the read_csv examples in the import docs in this example a csv file is generated by changing to csv mode and setting an output file location mode csv output import_example csv select 1 as col_1 2 as col_2 union all select 10 as col1 20 as col_2 now that the csv has been written a table can be created with the desired schema and the csv can be imported the output is reset to the terminal to avoid continuing to edit the output file specified above the --skip n option is used to ignore the first row of data since it is a header row and the table has already been created with the correct column names mode csv output create table test_table col_1 int col_2 int import import_example csv test_table --skip 1 note that the import command utilizes the current mode and separator settings when identifying the structure of the data to import the --csv option can be used to override that behavior import import_example csv test_table --skip 1 --csv",
			"category": "Cli",
			"url": "/docs/api/cli/dot_commands",
			"blurb": "Dot commands are available in the DuckDB CLI client. To use one of these commands, begin the line with a period ( . )..."
		},
		{
			"title": "DuckDB Environment",
			"text": "duckdb provides a number of functions and pragma options to retrieve information on the running duckdb instance and its environment version the version function returns the version number of duckdb select version version varchar v0 10 0 using a pragma pragma version library_version source_id varchar varchar v0 10 0 20b1486d11 platform the platform information consists of the operating system compiler and optionally the compiler the platform is used when installing extensions to retrieve the platform use the following pragma pragma platform on macos running on apple silicon architecture the result is platform varchar osx_arm64 on windows running on an amd64 architecture the platform is windows_amd64 on centos 7 running on the amd64 architecture the platform is linux_amd64_gcc4 on ubuntu 22 04 running on the arm64 architecture the platform is linux_arm64 extensions to get a list of duckdb extension and their status e g loaded installed use the duckdb_extensions function select from duckdb_extensions meta table functions duckdb has the following built-in table functions to obtain metadata about available catalog objects duckdb_columns columns duckdb_constraints constraints duckdb_databases lists the databases that are accessible from within the current duckdb process duckdb_dependencies dependencies between objects duckdb_extensions extensions duckdb_functions functions duckdb_indexes secondary indexes duckdb_keywords duckdb s keywords and reserved words duckdb_optimizers the available optimization rules in the duckdb instance duckdb_schemas schemas duckdb_sequences sequences duckdb_settings settings duckdb_tables base tables duckdb_types data types duckdb_views views duckdb_temporary_files the temporary files duckdb has written to disk to offload data from memory",
			"category": "Meta",
			"url": "/docs/guides/meta/duckdb_environment",
			"blurb": "DuckDB provides a number of functions and PRAGMA options to retrieve information on the running DuckDB instance and..."
		},
		{
			"title": "DuckDB Wasm",
			"text": "duckdb has been compiled to webassembly so it can run inside any browser on any device include iframe html src https shell duckdb org duckdb-wasm offers a layered api it can be embedded as a javascript webassembly library as a web shell or built from source according to your needs getting started with duckdb-wasm a great starting point is to read the duckdb-wasm launch blog post another great resource is the github repository for details see the full duckdb-wasm api documentation pages in this section",
			"category": "Wasm",
			"url": "/docs/api/wasm/overview",
			"blurb": "DuckDB has been compiled to WebAssembly, so it can run inside any browser on any device. {% include iframe.html..."
		},
		{
			"title": "DuckDB_% Metadata Functions",
			"text": "duckdb offers a collection of table functions that provide metadata about the current database these functions reside in the main schema and their names are prefixed with duckdb_ the resultset returned by a duckdb_ table function may be used just like an ordinary table or view for example you can use a duckdb_ function call in the from clause of a select statement and you may refer to the columns of its returned resultset elsewhere in the statement for example in the where clause table functions are still functions and you should write parenthesis after the function name to call it to obtain its returned resultset select from duckdb_settings alternatively you may execute table functions also using the call -syntax call duckdb_settings in this case too the parentheses are mandatory for some of the duckdb_ functions there is also an identically named view available which also resides in the main schema typically these views do a select on the duckdb_ table function with the same name while filtering out those objects that are marked as internal we mention it here because if you accidentally omit the parentheses in your duckdb_ table function call you might still get a result but from the identically named view example -- duckdb_views table function returns all views including those marked internal select from duckdb_views -- duckdb_views view returns views that are not marked as internal select from duckdb_views duckdb_columns the duckdb_columns function provides metadata about the columns available in the duckdb instance column description type - --- - database_name the name of the database that contains the column object varchar database_oid internal identifier of the database that contains the column object bigint schema_name the sql name of the schema that contains the table object that defines this column varchar schema_oid internal identifier of the schema object that contains the table of the column bigint table_name the sql name of the table that defines the column varchar table_oid internal identifier name of the table object that defines the column bigint column_name the sql name of the column varchar column_index the unique position of the column within its table integer internal true if this column built-in false if it is user-defined boolean column_default the default value of the column expressed in sql varchar is_nullable true if the column can hold null values false if the column cannot hold null -values boolean data_type the name of the column datatype varchar data_type_id the internal identifier of the column data type bigint character_maximum_length always null duckdb text types do not enforce a value length restriction based on a length type parameter integer numeric_precision the number of units in the base indicated by numeric_precision_radix used for storing column values for integral and approximate numeric types this is the number of bits for decimal types this is the number of digits positions integer numeric_precision_radix the number-base of the units in the numeric_precision column for integral and approximate numeric types this is 2 indicating the precision is expressed as a number of bits for the decimal type this is 10 indicating the precision is expressed as a number of decimal positions integer numeric_scale applicable to decimal type indicates the maximum number of fractional digits i e the number of digits that may appear after the decimal separator integer the information_schema columns system view provides a more standardized way to obtain metadata about database columns but the duckdb_columns function also returns metadata about duckdb internal objects in fact information_schema columns is implemented as a query on top of duckdb_columns duckdb_constraints the duckdb_constraints function provides metadata about the constraints available in the duckdb instance column description type - --- - database_name the name of the database that contains the constraint varchar database_oid internal identifier of the database that contains the constraint bigint schema_name the sql name of the schema that contains the table on which the constraint is defined varchar schema_oid internal identifier of the schema object that contains the table on which the constraint is defined bigint table_name the sql name of the table on which the constraint is defined varchar table_oid internal identifier name of the table object on which the constraint is defined bigint constraint_index indicates the position of the constraint as it appears in its table definition bigint constraint_type indicates the type of constraint applicable values are check foreign key primary key not null unique varchar constraint_text the definition of the constraint expressed as a sql-phrase not necessarily a complete or syntactically valid ddl-statement varchar expression if constraint is a check constraint the definition of the condition being checked otherwise null varchar constraint_column_indexes an array of table column indexes referring to the columns that appear in the constraint definition bigint constraint_column_names an array of table column names appearing in the constraint definition varchar duckdb_databases the duckdb_databases function lists the databases that are accessible from within the current duckdb process apart from the database associated at startup the list also includes databases that were attached later on to the duckdb process column description type - --- - database_name the name of the database or the alias if the database was attached using an alias-clause varchar database_oid the internal identifier of the database varchar path the file path associated with the database varchar internal true indicates a system or built-in database false indicates a user-defined database boolean type the type indicates the type of rdbms implemented by the attached database for duckdb databases that value is duckdb varchar duckdb_dependencies the duckdb_dependencies function provides metadata about the dependencies available in the duckdb instance column description type -- ------ - classid always 0 bigint objid the internal id of the object bigint objsubid always 0 integer refclassid always 0 bigint refobjid the internal id of the dependent object bigint refobjsubid always 0 integer deptype the type of dependency either regular n or automatic a varchar duckdb_extensions the duckdb_extensions function provides metadata about the extensions available in the duckdb instance column description type -- ------ - extension_name the name of the extension varchar loaded true if the extension is loaded false if it s not loaded boolean installed true if the extension is installed false if it s not installed boolean install_path built-in if the extension is built-in otherwise the filesystem path where binary that implements the extension resides varchar description human readable text that describes the extension s functionality varchar aliases list of alternative names for this extension varchar duckdb_functions the duckdb_functions function provides metadata about the functions including macros available in the duckdb instance column description type - --- - database_name the name of the database that contains this function varchar schema_name the sql name of the schema where the function resides varchar function_name the sql name of the function varchar function_type the function kind value is one of table scalar aggregate pragma macro varchar description description of this function always null varchar return_type the logical data type name of the returned value applicable for scalar and aggregate functions varchar parameters if the function has parameters the list of parameter names varchar parameter_types if the function has parameters a list of logical data type names corresponding to the parameter list varchar varargs the name of the data type in case the function has a variable number of arguments or null if the function does not have a variable number of arguments varchar macro_definition if this is a macro the sql expression that defines it varchar has_side_effects false if this is a pure function true if this function changes the database state like sequence functions nextval and curval boolean function_oid the internal identifier for this function bigint duckdb_indexes the duckdb_indexes function provides metadata about secondary indexes available in the duckdb instance column description type - --- - database_name the name of the database that contains this index varchar database_oid internal identifier of the database containing the index bigint schema_name the sql name of the schema that contains the table with the secondary index varchar schema_oid internal identifier of the schema object bigint index_name the sql name of this secondary index varchar index_oid the object identifier of this index bigint table_name the name of the table with the index varchar table_oid internal identifier name of the table object bigint is_unique true if the index was created with the unique modifier false if it was not boolean is_primary always false boolean expressions always null varchar sql the definition of the index expressed as a create index sql statement varchar note that duckdb_indexes only provides metadata about secondary indexes - i e those indexes created by explicit create index statements primary keys foreign keys and unique constraints are maintained using indexes but their details are included in the duckdb_constraints function duckdb_keywords the duckdb_keywords function provides metadata about duckdb s keywords and reserved words column description type - --- - keyword_name the keyword varchar keyword_category indicates the category of the keyword values are column_name reserved type_function and unreserved varchar duckdb_memory the duckdb_memory function provides metadata about duckdb s buffer manager column description type - --- - tag the memory tag it has one of the following values base_table hash_table parquet_reader csv_reader order_by art_index column_data metadata overflow_strings in_memory_table allocator extension varchar memory_usage_bytes the memory used in bytes bigint temporary_storage_bytes the disk storage used in bytes bigint duckdb_optimizers the duckdb_optimizers function provides metadata about the optimization rules e g expression_rewriter filter_pushdown available in the duckdb instance these can be selectively turned off using pragma disabled_optimizers column description type - --- - name the name of the optimization rule varchar duckdb_schemas the duckdb_schemas function provides metadata about the schemas available in the duckdb instance column description type - --- - oid internal identifier of the schema object bigint database_name the name of the database that contains this schema varchar database_oid internal identifier of the database containing the schema bigint schema_name the sql name of the schema varchar internal true if this is an internal built-in schema false if this is a user-defined schema boolean sql always null varchar the information_schema schemata system view provides a more standardized way to obtain metadata about database schemas duckdb_secrets the duckdb_secrets function provides metadata about the secrets available in the duckdb instance column description type - --- - name the name of the secret varchar type the type of the secret e g s3 gcs r2 azure varchar provider the provider of the secret varchar persistent denotes whether the secret is persisent boolean storage the backend for storing the secret varchar scope the scope of the secret varchar secret_string returns the content of the secret as a string sensitive pieces of information e g they access key are redacted varchar duckdb_sequences the duckdb_sequences function provides metadata about the sequences available in the duckdb instance column description type - --- - database_name the name of the database that contains this sequence varchar database_oid internal identifier of the database containing the sequence bigint schema_name the sql name of the schema that contains the sequence object varchar schema_oid internal identifier of the schema object that contains the sequence object bigint sequence_name the sql name that identifies the sequence within the schema varchar sequence_oid the internal identifier of this sequence object bigint temporary whether this sequence is temporary temporary sequences are transient and only visible within the current connection boolean start_value the initial value of the sequence this value will be returned when nextval is called for the very first time on this sequence bigint min_value the minimum value of the sequence bigint max_value the maximum value of the sequence bigint increment_by the value that is added to the current value of the sequence to draw the next value from the sequence bigint cycle whether the sequence should start over when drawing the next value would result in a value outside the range boolean last_value null if no value was ever drawn from the sequence using nextval 1 if a value was drawn bigint sql the definition of this object expressed as sql ddl-statement varchar attributes like temporary start_value etc correspond to the various options available in the create sequence statement and are documented there in full note that the attributes will always be filled out in the duckdb_sequences resultset even if they were not explicitly specified in the create sequence statement the column name last_value suggests that it contains the last value that was drawn from the sequence but that is not the case it s either null if a value was never drawn from the sequence or 1 when there was a value drawn ever from the sequence if the sequence cycles then the sequence will start over from the boundary of its range not necessarily from the value specified as start value duckdb_settings the duckdb_settings function provides metadata about the settings available in the duckdb instance column description type - --- - name name of the setting varchar value current value of the setting varchar description a description of the setting varchar input_type the logical datatype of the setting s value varchar the various settings are described in the configuration page duckdb_tables the duckdb_tables function provides metadata about the base tables available in the duckdb instance column description type - --- - database_name the name of the database that contains this table varchar database_oid internal identifier of the database containing the table bigint schema_name the sql name of the schema that contains the base table varchar schema_oid internal identifier of the schema object that contains the base table bigint table_name the sql name of the base table varchar table_oid internal identifier of the base table object bigint internal false if this is a user-defined table boolean temporary whether this is a temporary table temporary tables are not persisted and only visible within the current connection boolean has_primary_key true if this table object defines a primary key boolean estimated_size the estimated number of rows in the table bigint column_count the number of columns defined by this object bigint index_count the number of indexes associated with this table this number includes all secondary indexes as well as internal indexes generated to maintain primary key and or unique constraints bigint check_constraint_count the number of check constraints active on columns within the table bigint sql the definition of this object expressed as sql create table -statement varchar the information_schema tables system view provides a more standardized way to obtain metadata about database tables that also includes views but the resultset returned by duckdb_tables contains a few columns that are not included in information_schema tables duckdb_types the duckdb_types function provides metadata about the data types available in the duckdb instance column description type - --- - database_name the name of the database that contains this schema varchar database_oid internal identifier of the database that contains the data type bigint schema_name the sql name of the schema containing the type definition always main varchar schema_oid internal identifier of the schema object bigint type_name the name or alias of this data type varchar type_oid the internal identifier of the data type object if null then this is an alias of the type as identified by the value in the logical_type column bigint type_size the number of bytes required to represent a value of this type in memory bigint logical_type the canonical name of this data type the same logical_type may be referenced by several types having different type_name s varchar type_category the category to which this type belongs data types within the same category generally expose similar behavior when values of this type are used in expression for example the numeric type_category includes integers decimals and floating point numbers varchar internal whether this is an internal built-in or a user object boolean duckdb_views the duckdb_views function provides metadata about the views available in the duckdb instance column description type - --- - database_name the name of the database that contains this view varchar database_oid internal identifier of the database that contains this view bigint schema_name the sql name of the schema where the view resides varchar schema_oid internal identifier of the schema object that contains the view bigint view_name the sql name of the view object varchar view_oid the internal identifier of this view object bigint internal true if this is an internal built-in view false if this is a user-defined view boolean temporary true if this is a temporary view temporary views are not persistent and are only visible within the current connection boolean column_count the number of columns defined by this view object bigint sql the definition of this object expressed as sql ddl-statement varchar the information_schema tables system view provides a more standardized way to obtain metadata about database views that also includes base tables but the resultset returned by duckdb_views contains also definitions of internal view objects as well as a few columns that are not included in information_schema tables duckdb_temporary_files the duckdb_temporary_files function provides metadata about the temporary files duckdb has written to disk to offload data from memory this function mostly exists for debugging and testing purposes column description type - --- - path the name of the temporary file varchar size the size in bytes of the temporary file int64",
			"category": "SQL",
			"url": "/docs/sql/duckdb_table_functions",
			"blurb": "DuckDB offers a collection of table functions that provide metadata about the current database. These functions..."
		},
		{
			"title": "EXPORT/IMPORT DATABASE Statements",
			"text": "the export database command allows you to export the contents of the database to a specific directory the import database command allows you to then read the contents again examples -- export the database to the target directory target_directory as csv files export database target_directory -- export to directory target_directory -- using the given options for the csv serialization export database target_directory format csv delimiter -- export to directory target_directory tables serialized as parquet export database target_directory format parquet -- export to directory target_directory tables serialized as parquet -- compressed with zstd with a row_group_size of 100 000 export database target_directory format parquet compression zstd row_group_size 100_000 -- reload the database again import database source_directory -- alternatively use a pragma pragma import_database source_directory for details regarding the writing of parquet files see the parquet files page in the data import section and the copy statement page export database the export database command exports the full contents of the database - including schema information tables views and sequences - to a specific directory that can then be loaded again the created directory will be structured as follows target_directory schema sql target_directory load sql target_directory t_1 csv target_directory t_n csv the schema sql file contains the schema statements that are found in the database it contains any create schema create table create view and create sequence commands that are necessary to re-construct the database the load sql file contains a set of copy statements that can be used to read the data from the csv files again the file contains a single copy statement for every table found in the schema syntax import database the database can be reloaded by using the import database command again or manually by running schema sql followed by load sql to re-load the data syntax",
			"category": "Statements",
			"url": "/docs/sql/statements/export",
			"blurb": "The EXPORT DATABASE command allows you to export the contents of the database to a specific directory. The IMPORT..."
		},
		{
			"title": "Editing",
			"text": "linenoise is currently only available for macos and linux duckdb s cli uses a line-editing library based on linenoise which has short-cuts that are based on emacs mode of readline below is a list of available commands moving key action --------------- ------------------------------------------------------------------------ left move back a character right move forward a character up move up a line when on the first line move to previous history entry down move down a line when on last line move to next history entry home move to beginning of buffer end move to end of buffer ctrl left move back a word ctrl right move forward a word ctrl a move to beginning of buffer ctrl b move back a character ctrl e move to end of buffer ctrl f move forward a character alt left move back a word alt right move forward a word history key action -------- -------------------------------- ctrl p move to previous history entry ctrl n move to next history entry ctrl r search the history ctrl s search the history alt move to first history entry alt move to last history entry alt n search the history alt p search the history changing text key action --------------- ---------------------------------------------------------- backspace delete previous character delete delete next character ctrl d delete next character when buffer is empty end editing ctrl h delete previous character ctrl k delete everything after the cursor ctrl t swap current and next character ctrl u delete all text ctrl w delete previous word alt c convert next word to titlecase alt d delete next word alt l convert next word to lowercase alt r delete all text alt t swap current and next word alt u convert next word to uppercase alt backspace delete previous word alt delete spaces around cursor completing key action ----------- -------------------------------------------------------- tab autocomplete when autocompleting cycle to next entry shift tab when autocompleting cycle to previous entry esc esc when autocompleting revert autocompletion miscellaneous key action -------- ------------------------------------------------------------------------------------ enter execute query if query is not complete insert a newline at the end of the buffer ctrl j execute query if query is not complete insert a newline at the end of the buffer ctrl c cancel editing of current query ctrl g cancel editing of current query ctrl l clear screen ctrl o cancel editing of current query ctrl x insert a newline after the cursor ctrl z suspend cli and return to shell use fg to re-open using read-line if you prefer you can use rlwrap to use read-line directly with the shell rlwrap --substitute-prompt d duckdb -batch",
			"category": "Cli",
			"url": "/docs/api/cli/editing",
			"blurb": "linenoise is currently only available for macOS and Linux. DuckDB's CLI uses a line-editing library based on..."
		},
		{
			"title": "Enum Data Type",
			"text": "name description -- ----- enum dictionary encoding representing all possible string values of a column the enum type represents a dictionary data structure with all possible unique values of a column for example a column storing the days of the week can be an enum holding all possible days enums are particularly interesting for string columns with low cardinality i e fewer distinct values this is because the column only stores a numerical reference to the string in the enum dictionary resulting in immense savings in disk storage and faster query performance enum definition enum types are created from either a hardcoded set of values or from a select statement that returns a single column of varchars the set of values in the select statement will be deduplicated but if the enum is created from a hardcoded set there may not be any duplicates -- create enum using hardcoded values create type enum_name as enum value_1 value_2 -- create enum using a select statement that returns a single column of varchars create type enum_name as enum select expression for example -- creates new user defined type mood as an enum create type mood as enum sad ok happy -- this will fail since the mood type already exists create type mood as enum sad ok happy anxious -- this will fail since enums cannot hold null values create type breed as enum maltese null -- this will fail since enum values must be unique create type breed as enum maltese maltese -- create an enum from a select statement -- first create an example table of values create table my_inputs as select duck as my_varchar union all select duck as my_varchar union all select goose as my_varchar -- create an enum using the unique string values in the my_varchar column create type birds as enum select my_varchar from my_inputs -- show the available values in the birds enum using the enum_range function select enum_range null birds as my_enum_range my_enum_range ----------------- duck goose enum usage after an enum has been created it can be used anywhere a standard built-in type is used for example we can create a table with a column that references the enum creates a table person with attributes name string type and current_mood mood type create table person name text current_mood mood inserts tuples in the person table insert into person values pedro happy mark null pagliacci sad mr mackey ok the following query will fail since the mood type does not have a quackity-quack value insert into person values hannes quackity-quack the string sad is cast to the type mood returning a numerical reference value this makes the comparison a numerical comparison instead of a string comparison select from person where current_mood sad name current_mood varchar enum sad ok happy anxious pagliacci sad if you are importing data from a file you can create an enum for a varchar column before importing given this the following subquery selects automatically selects only distinct values create type mood as enum select mood from path to file csv then you can create a table with the enum type and import using any data import statement create table person name text current_mood mood copy person from path to file csv enums vs strings duckdb enums are automatically cast to varchar types whenever necessary this characteristic allows for enum columns to be used in any varchar function in addition it also allows for comparisons between different enum columns or an enum and a varchar column for example -- regexp_matches is a function that takes a varchar hence current_mood is cast to varchar select regexp_matches current_mood a as contains_a from person contains_a boolean true null true false create a new mood and table create type new_mood as enum happy anxious create table person_2 name text current_mood mood future_mood new_mood past_mood varchar since the current_mood and future_mood columns are constructed on different enum types duckdb will cast both enum s to strings and perform a string comparison select from person_2 where current_mood future_mood when comparing the past_mood column string duckdb will cast the current_mood enum to varchar and perform a string comparison select from person_2 where current_mood past_mood enum removal enum types are stored in the catalog and a catalog dependency is added to each table that uses them it is possible to drop an enum from the catalog using the following command drop type enum_name currently it is possible to drop enums that are used in tables without affecting the tables this behavior of the enum removal feature is subject to change in future releases it is expected that any dependent columns must be removed before dropping the enum or the enum must be dropped with the additional cascade parameter comparison of enums enum values are compared according to their order in the enum s definition for example create type mood as enum sad ok happy select sad mood ok mood as comp comp boolean true select unnest ok mood happy mood sad mood as m order by m m enum sad ok happy sad ok happy",
			"category": "Data Types",
			"url": "/docs/sql/data_types/enum",
			"blurb": "The Enum type represents a dictionary data structure with all possible unique values of a column."
		},
		{
			"title": "Enum Functions",
			"text": "this section describes functions and operators for examining and manipulating enum values the examples assume an enum type created as create type mood as enum sad ok happy anxious these functions can take null or a specific value of the type as argument s with the exception of enum_range_boundary the result depends only on the type of the argument and not on its value function description example result -- -- --- - enum_code enum_value returns the numeric value backing the given enum value enum_code happy mood 2 enum_first enum returns the first value of the input enum type enum_first null mood sad enum_last enum returns the last value of the input enum type enum_last null mood anxious enum_range enum returns all values of the input enum type as an array enum_range null mood sad ok happy anxious enum_range_boundary enum enum returns the range between the two given enum values as an array the values must be of the same enum type when the first parameter is null the result starts with the first value of the enum type when the second parameter is null the result ends with the last value of the enum type enum_range_boundary null happy mood sad ok happy",
			"category": "Functions",
			"url": "/docs/sql/functions/enum",
			"blurb": "This section describes functions and operators for examining and manipulating ENUM values. The examples assume an..."
		},
		{
			"title": "Environment",
			"text": "the environment where duckdb is run has an obvious impact on performance this page focuses on the effects of the hardware configuration and the operating system used hardware configuration cpu and memory as a rule of thumb aggregation-heavy workloads require approx 5 gb memory per cpu core and join-heavy workloads require approximately 10 gb memory per core for best performance in aws ec2 the former are available as general-purpose instances e g m7g and the latter as memory-optimized instances e g r7g bestpractice aim for 5-10 gb memory per cpu core disk duckdb is capable of operating both as an in-memory and as a disk-based database system in the latter case it can spill to disk to process larger-than-memory workloads a k a out-of-core processing in these cases a fast disk is highly beneficial however if the workload fits in memory the disk speed only has a limited effect on performance in general network-based storage will result in slower duckdb workloads than using local disks this includes network disks such as nfs network drives such as smb and samba and network-backed cloud disks such as aws ebs however different network disks can have vastly varying io performance ranging from very slow to almost as fast as local therefore for optimal performance only use network disks that can provide high io performance bestpractice fast disks are important if your workload is larger than memory and or fast data loading is important only use network-backed disks if they guarantee high io operating system we recommend using the latest stable version of operating systems macos windows and linux are all well-tested and duckdb can run on them with high performance among linux distributions we recommended using ubuntu linux lts due to its stability and the fact that most of duckdb s linux test suite jobs run on ubuntu workers",
			"category": "Performance",
			"url": "/docs/guides/performance/environment",
			"blurb": "The environment where DuckDB is run has an obvious impact on performance. This page focuses on the effects of the..."
		},
		{
			"title": "Excel Export",
			"text": "installing the extension to export the data from a table to an excel file install and load the spatial extension this is only needed once per duckdb connection install spatial load spatial exporting excel sheets then use the copy statement the file will contain one worksheet with the same name as the file but without the xlsx extension copy tbl to output xlsx with format gdal driver xlsx the result of a query can also be directly exported to an excel file copy select from tbl to output xlsx with format gdal driver xlsx dates and timestamps are currently not supported by the xlsx writer cast columns of those types to varchar prior to creating the xlsx file the output file must not already exist see also duckdb can also import export files for additional details see the spatial extension page and the gdal xlsx driver page",
			"category": "Import",
			"url": "/docs/guides/import/excel_export",
			"blurb": "Installing the Extension To export the data from a table to an Excel file, install and load the spatial extension ...."
		},
		{
			"title": "Excel Extension",
			"text": "this extension contrary to its name does not provide support for reading excel files it instead provides a function that wraps the number formatting functionality of the i18npool library which formats numbers per excel s formatting rules excel files can be handled through the spatial extension see the excel import and excel export pages for instructions installing and loading the excel extension will be transparently autoloaded on first use from the official extension repository if you would like to install and load it manually run install excel load excel usage select excel_text 1234567 897 h mm am pm as timestamp timestamp varchar 9 31 pm functions function description example result -- --- -- - excel_text number format_string alias for text excel_text 1234567 897 h mm am pm 9 31 pm text number format_string format the given number per the rules given in the format_string text 1234567 897 h am pm 9 pm github the excel extension is part of the main duckdb repository",
			"category": "Extensions",
			"url": "/docs/extensions/excel",
			"blurb": "This extension, contrary to its name, does not provide support for reading Excel files. It instead provides a..."
		},
		{
			"title": "Excel Import",
			"text": "installing the extension to read data from an excel file install and load the spatial extension this is only needed once per duckdb connection install spatial load spatial importing excel sheets use the st_read function in the from clause of a query select from st_read test_excel xlsx the layer parameter allows specifying the name of the excel worksheet select from st_read test_excel xlsx layer sheet1 creating a new table to create a new table using the result from a query use create table as from a select statement create table new_tbl as select from st_read test_excel xlsx layer sheet1 loading to an existing table to load data into an existing table from a query use insert into from a select statement insert into tbl select from st_read test_excel xlsx layer sheet1 options several configuration options are also available for the underlying gdal library that is doing the xlsx parsing you can pass them via the open_options parameter of the st_read function as a list of key value strings importing a sheet with without a header the option headers has three possible values force treat the first row as a header disable treat the first row as a row of data auto attempt auto-detection default for example to treat the first row as a header run select from st_read test_excel xlsx layer sheet1 open_options headers force detecting types the option field_type defines how field types should be treated string all fields should be loaded as strings varchar type auto field types should be auto-detected default for example to treat the first row as a header and use auto-detection for types run select from st_read test_excel xlsx layer sheet1 open_options headers force field_types auto to treat the fields as strings select from st_read test_excel xlsx layer sheet1 open_options field_types string see also duckdb can also export export files for additional details on excel support see the spatial extension page the gdal xlsx driver page and the gdal configuration options page",
			"category": "Import",
			"url": "/docs/guides/import/excel_import",
			"blurb": "Installing the Extension To read data from an Excel file, install and load the spatial extension . This is only..."
		},
		{
			"title": "Executing SQL in Python",
			"text": "sql queries can be executed using the duckdb sql function import duckdb duckdb sql select 42 show by default this will create a relation object the result can be converted to various formats using the result conversion functions for example the fetchall method can be used to convert the result to python objects results duckdb sql select 42 fetchall print results 42 several other result objects exist for example you can use df to convert the result to a pandas dataframe results duckdb sql select 42 df print results 42 0 42 by default a global in-memory connection will be used any data stored in files will be lost after shutting down the program a connection to a persistent database can be created using the connect function after connecting sql queries can be executed using the sql command con duckdb connect file db con sql create table integers i integer con sql insert into integers values 42 con sql select from integers show",
			"category": "Python",
			"url": "/docs/guides/python/execute_sql",
			"blurb": "SQL queries can be executed using the duckdb.sql function. import duckdb duckdb.sql(SELECT 42).show() By default this..."
		},
		{
			"title": "Export to Apache Arrow",
			"text": "all results of a query can be exported to an apache arrow table using the arrow function alternatively results can be returned as a recordbatchreader using the fetch_record_batch function and results can be read one batch at a time in addition relations built using duckdb s relational api can also be exported export to an arrow table import duckdb import pyarrow as pa my_arrow_table pa table from_pydict i 1 2 3 4 j one two three four query the apache arrow table my_arrow_table and return as an arrow table results duckdb sql select from my_arrow_table arrow export as a recordbatchreader import duckdb import pyarrow as pa my_arrow_table pa table from_pydict i 1 2 3 4 j one two three four query the apache arrow table my_arrow_table and return as an arrow recordbatchreader chunk_size 1_000_000 results duckdb sql select from my_arrow_table fetch_record_batch chunk_size loop through the results a stopiteration exception is thrown when the recordbatchreader is empty while true try process a single chunk here just printing as an example print results read_next_batch to_pandas except stopiteration print already fetched all batches break export from relational api arrow objects can also be exported from the relational api a relation can be converted to an arrow table using the arrow or to_arrow_table functions or a record batch using record_batch a result can be exported to an arrow table with arrow or the alias fetch_arrow_table or to a recordbatchreader using fetch_arrow_reader import duckdb connect to an in-memory database con duckdb connect con execute create table integers i integer con execute insert into integers values 0 1 2 3 4 5 6 7 8 9 null create a relation from the table and export the entire relation as arrow rel con table integers relation_as_arrow rel arrow or to_arrow_table or calculate a result using that relation and export that result to arrow res rel aggregate sum i execute result_as_arrow res arrow or fetch_arrow_table",
			"category": "Python",
			"url": "/docs/guides/python/export_arrow",
			"blurb": "All results of a query can be exported to an Apache Arrow Table using the arrow function. Alternatively, results can..."
		},
		{
			"title": "Export to Pandas",
			"text": "the result of a query can be converted to a pandas dataframe using the df function import duckdb read the result of an arbitrary sql query to a pandas dataframe results duckdb sql select 42 df",
			"category": "Python",
			"url": "/docs/guides/python/export_pandas",
			"blurb": "The result of a query can be converted to a Pandas DataFrame using the df() function. import duckdb # read the result..."
		},
		{
			"title": "Expression API",
			"text": "the expression class represents an instance of an expression why would i use the expression api using this api makes it possible to dynamically build up expressions which are typically created by the parser from the query string this allows you to skip that and have more fine-grained control over the used expressions below is a list of currently supported expressions that can be created through the api column expression this expression references a column by name import duckdb import pandas as pd df pd dataframe a 1 2 3 4 b true none false true c 42 21 13 14 selecting a single column col duckdb columnexpression a res duckdb df df select col fetchall print res 1 2 3 4 selecting multiple columns col_list duckdb columnexpression a duckdb columnexpression c res duckdb df df select col_list fetchall print res 1 42 2 21 3 13 4 14 star expression this expression selects all columns of the input source optionally it s possible to provide an exclude list to filter out columns of the table this exclude list can contain either strings or expressions import duckdb import pandas as pd df pd dataframe a 1 2 3 4 b true none false true c 42 21 13 14 star duckdb starexpression exclude b res duckdb df df select star fetchall print res 1 42 2 21 3 13 4 14 constant expression this expression contains a single value import duckdb import pandas as pd df pd dataframe a 1 2 3 4 b true none false true c 42 21 13 14 const duckdb constantexpression hello res duckdb df df select const fetchall print res hello hello hello hello case expression this expression contains a case when then else end expression by default else is null and it can be set using else value additional when then blocks can be added with when condition value import duckdb import pandas as pd from duckdb import constantexpression columnexpression caseexpression df pd dataframe a 1 2 3 4 b true none false true c 42 21 13 14 hello constantexpression hello world constantexpression world case caseexpression condition columnexpression b false value world otherwise hello res duckdb df df select case fetchall print res hello hello world hello function expression this expression contains a function call it can be constructed by providing the function name and an arbitrary amount of expressions as arguments import duckdb import pandas as pd from duckdb import constantexpression columnexpression functionexpression df pd dataframe a test pest text rest ends_with functionexpression ends_with columnexpression a constantexpression est res duckdb df df select ends_with fetchall print res true true false true common operations the expression class also contains many operations that can be applied to any expression type cast type duckdbpytype applies a cast to the provided type on the expression alias name str apply an alias to the expression isin exprs expression create a in expression against the provided expressions as the list isnotin exprs expression create a not in expression against the provided expressions as the list order operations when expressions are provided to duckdbpyrelation order these take effect asc indicates that this expression should be sorted in ascending order desc indicates that this expression should be sorted in descending order nulls_first indicates that the nulls in this expression should preceed the non-null values nulls_last indicates that the nulls in this expression should come after the non-null values",
			"category": "Python",
			"url": "/docs/api/python/expression",
			"blurb": "The Expression class represents an instance of an expression . Why Would I Use the Expression API? Using this API..."
		},
		{
			"title": "Expressions",
			"text": "an expression is a combination of values operators and functions expressions are highly composable and range from very simple to arbitrarily complex they can be found in many different parts of sql statements in this section we provide the different types of operators and functions that can be used within expressions pages in this section",
			"category": "Expressions",
			"url": "/docs/sql/expressions/overview",
			"blurb": "An expression is a combination of values, operators and functions. Expressions are highly composable, and range from..."
		},
		{
			"title": "Extensions",
			"text": "overview duckdb has a flexible extension mechanism that allows for dynamically loading extensions these may extend duckdb s functionality by providing support for additional file formats introducing new types and domain-specific functionality extensions are loadable on all clients e g python and r extensions distributed via the official repository are built and tested on macos amd64 and arm64 windows amd64 and linux amd64 and arm64 we maintain a list of official extensions using extensions listing extensions to get a list of extensions run from duckdb_extensions extension_name loaded installed install_path description aliases ------------------ -------- ----------- -------------- ------------------------------------------------------------------------------------ ------------------- arrow false false a zero-copy data integration between apache arrow and duckdb autocomplete false false adds support for autocomplete in the shell extension types duckdb has three types of extensions built-in extensions built-in extensions are loaded at startup and are immediately available for use select from test json this will use the json extension to read the json file to make the duckdb distribution lightweight it only contains a few fundamental built-in extensions e g autocomplete json parquet which are loaded automatically autoloadable extensions autoloadable extensions are loaded on first use select from https raw githubusercontent com duckdb duckdb-web main data weather csv to access files via the https protocol duckdb will automatically load the httpfs extension similarly other autoloadable extensions aws fts will be loaded on-demand if an extension is not already available locally it will be installed from the official extension repository extensions duckdb org explicitly loadable extensions some extensions make several changes to the running duckdb instance hence autoloading them may not be possible these extensions have to be installed and loaded using the following sql statements install spatial load spatial extension handling through the python api if you are using the python api client you can install and load them with the install_extension name str and load_extension name str methods autoloadable extensions can also be installed explicitly ensuring the integrity of extensions extensions are signed with a cryptographic key which also simplifies distribution this is why they are served over http and not https by default duckdb uses its built-in public keys to verify the integrity of extension before loading them all extensions provided by the duckdb core team are signed if you wish to load your own extensions or extensions from third-parties you will need to enable the allow_unsigned_extensions flag to load unsigned extensions using the cli client pass the -unsigned flag to it on startup for the python client see the loading and installing extensions section in the python api documentation sharing extensions between clients the shared installation location allows extensions to be shared between the client apis of the same duckdb version as long as they share the same platfrom or abi for example if an extension is installed with version 0 10 0 of the cli client on macos it is available from the python r etc client libraries provided that they have access to the user s home directory and use duckdb version 0 10 0 see the working with extensions page for details on available platforms installation location extensions are by default installed under the user s home directory duckdb extensions v duckdb_version platform_name for example the extensions for duckdb version 0 10 0 on macos arm64 apple silicon are installed to duckdb extensions v0 10 0 osx_arm64 for development builds the directory of the extensions corresponds to the git hash of the build e g duckdb extensions fc2e4b26a6 linux_amd64_gcc4 changing the extension directory to specify a different extension directory use the extension_directory configuration option set extension_directory path to your extension directory developing extensions the same api that the official extensions use is available for developing extensions this allows users to extend the functionality of duckdb such that it suits their domain the best a template for creating extensions is available in the extension-template repository working with extensions for advanced installation instructions and more details on extensions see the working with extensions page pages in this section",
			"category": "Extensions",
			"url": "/docs/extensions/overview",
			"blurb": "Overview DuckDB has a flexible extension mechanism that allows for dynamically loading extensions. These may extend..."
		},
		{
			"title": "Extensions",
			"text": "duckdb-wasm s dynamic extension loading is modeled after the regular duckdb s extension loading with a few relevant differences due to the difference in platform format extensions in duckdb are binaries to be dynamically loaded via dlopen a cryptographical signature is appended to the binary an extension in duckdb-wasm is a regular wasm file to be dynamically loaded via emscripten s dlopen a cryptographical signature is appended to the wasm file as a webassembly custom section called duckdb_signature this ensures the file remains a valid webassembly file currently we require this custom section to be the last one but this can be potentially relaxed in the future install and load the install semantic in native embeddings of duckdb is to fetch decompress from gzip and store data in local disk the load semantic in native embeddings of duckdb is to optionally perform signature checks and dynamic load the binary with the main duckdb binary in duckdb-wasm install is a no-op given there is no durable cross-session storage the load operation will fetch and decompress on the fly perform signature checks and dynamically load via the emscripten implementation of dlopen autoloading autoloading i e the possibility for duckdb to add extension functionality on-the-fly is enabled by default in duckdb-wasm list of officially available extensions extension name description aliases --- ----- -- autocomplete adds support for autocomplete in the shell excel adds support for excel-like format strings fts adds support for full-text search indexes icu adds support for time zones and collations using the icu library inet adds support for ip-related data types and functions json adds support for json operations parquet adds support for reading and writing parquet files sqlite span class github github span adds support for reading sqlite database files sqlite sqlite3 sqlsmith substrait span class github github span adds support for the substrait integration tpcds adds tpc-ds data generation and query support tpch adds tpc-h data generation and query support webassembly is basically an additional platform and there might be platform-specific limitations that make some extensions not able to match their native capabilities or to perform them in a different way we will document here relevant differences for duckdb-hosted extensions httpfs the httpfs extension is at the moment not available in duckdb-wasm https protocol capabilities needs to go through an additional layer the browser which adds both differences and some restrictions to what is doable from native instead duckdb-wasm has a separate implementation that for most purposes is interchangable but does not support all use cases as it must follow security rules imposed by the browser such as cors due to this cors restriction any requests for data made using the httpfs extension must be to websites that allow using cors headers the website hosting the duckdb-wasm instance to access that data the mdn website is a great resource for more information regarding cors extension signing as with regular duckdb extensions duckdb-wasm extension are by default checked on load to verify the signature confirm the extension has not been tampered with extension signature verification can be disabled via a configuration option signing is a property of the binary itself so copying a duckdb extension say to serve it from a different location will still keep a valid signature e g for local development fetching duckdb-wasm extensions official duckdb extensions are served at extensions duckdb org and this is also the default value for the default_extension_repository option when installing extensions a relevant url will be built that will look like extensions duckdb org duckdb_version_hash duckdb_platform name duckdb_extension gz duckdb-wasm extension are fetched only on load and the url will look like extensions duckdb org duckdb-wasm duckdb_version_hash duckdb_platform name duckdb_extension wasm note that an additional duckdb-wasm is added to the folder structure and the file is served as a wasm file duckdb-wasm extensions are served pre-compressed using brotli compression while fetched from a browser extensions will be transparently uncompressed if you want to fetch the duckdb-wasm extension manually you can use curl --compress extensions duckdb org icu duckdb_extension wasm serving extensions from a third-party repository as with regular duckdb if you use set custom_extension_repository some url com subsequent loads will be attempted at some url com duckdb-wasm duckdb_version_hash duckdb_platform name duckdb_extension wasm note that get requests on the extensions needs to be cors enabled for a browser to allow the connection tooling both duckdb-wasm and its extensions have been compiled using latest packaged emscripten toolchain include iframe html src https shell duckdb org",
			"category": "Wasm",
			"url": "/docs/api/wasm/extensions",
			"blurb": "DuckDB-Wasm's (dynamic) extension loading is modeled after the regular DuckDB's extension loading, with a few..."
		},
		{
			"title": "FILTER Clause",
			"text": "the filter clause may optionally follow an aggregate function in a select statement this will filter the rows of data that are fed into the aggregate function in the same way that a where clause filters rows but localized to the specific aggregate function filter s are not currently able to be used when the aggregate function is in a windowing context there are multiple types of situations where this is useful including when evaluating multiple aggregates with different filters and when creating a pivoted view of a dataset filter provides a cleaner syntax for pivoting data when compared with the more traditional case when approach discussed below some aggregate functions also do not filter out null values so using a filter clause will return valid results when at times the case when approach will not this occurs with the functions first and last which are desirable in a non-aggregating pivot operation where the goal is to simply re-orient the data into columns rather than re-aggregate it filter also improves null handling when using the list and array_agg functions as the case when approach will include null values in the list result while the filter clause will remove them examples -- compare total row count to -- the number of rows where i 5 -- the number of rows where i is odd select count as total_rows count filter i 5 as lte_five count filter i 2 1 as odds from generate_series 1 10 tbl i total_rows lte_five odds --- --- --- 10 5 5 -- different aggregate functions may be used and multiple where expressions are also permitted -- the sum of i for rows where i 5 -- the median of i where i is odd select sum i filter i 5 as lte_five_sum median i filter i 2 1 as odds_median median i filter i 2 1 and i 5 as odds_lte_five_median from generate_series 1 10 tbl i lte_five_sum odds_median odds_lte_five_median --- --- --- 15 5 0 3 0 the filter clause can also be used to pivot data from rows into columns this is a static pivot as columns must be defined prior to runtime in sql however this kind of statement can be dynamically generated in a host programming language to leverage duckdb s sql engine for rapid larger than memory pivoting -- first generate an example dataset create temp table stacked_data as select i case when i rows 0 25 then 2022 when i rows 0 5 then 2023 when i rows 0 75 then 2024 when i rows 0 875 then 2025 else null end as year from select i count over as rows from generate_series 1 100_000_000 tbl i tbl -- pivot the data out by year move each year out to a separate column select count i filter year 2022 as 2022 count i filter year 2023 as 2023 count i filter year 2024 as 2024 count i filter year 2025 as 2025 count i filter year is null as nulls from stacked_data -- this syntax produces the same results as the filter clauses above select count case when year 2022 then i end as 2022 count case when year 2023 then i end as 2023 count case when year 2024 then i end as 2024 count case when year 2025 then i end as 2025 count case when year is null then i end as nulls from stacked_data 2022 2023 2024 2025 nulls --- --- --- --- --- 25000000 25000000 25000000 12500000 12500000 however the case when approach will not work as expected when using an aggregate function that does not ignore null values the first function falls into this category so filter is preferred in this case -- pivot the data out by year move each year out to a separate column select first i filter year 2022 as 2022 first i filter year 2023 as 2023 first i filter year 2024 as 2024 first i filter year 2025 as 2025 first i filter year is null as nulls from stacked_data 2022 2023 2024 2025 nulls --- --- --- --- --- 1474561 25804801 50749441 76431361 87500001 -- this will produce null values whenever the first evaluation of the case when clause returns a null select first case when year 2022 then i end as 2022 first case when year 2023 then i end as 2023 first case when year 2024 then i end as 2024 first case when year 2025 then i end as 2025 first case when year is null then i end as nulls from stacked_data 2022 2023 2024 2025 nulls --- --- --- --- --- 1228801 null null null null aggregate function syntax including filter clause",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/filter",
			"blurb": "The FILTER clause may optionally follow an aggregate function in a SELECT statement. This will filter the rows of..."
		},
		{
			"title": "FROM & JOIN Clauses",
			"text": "the from clause specifies the source of the data on which the remainder of the query should operate logically the from clause is where the query starts execution the from clause can contain a single table a combination of multiple tables that are joined together using join clauses or another select query inside a subquery node duckdb also has an optional from -first syntax which enables you to also query without a select statement examples -- select all columns from the table called table_name select from table_name -- select all columns from the table using the from-first syntax from table_name select -- select all columns using the from-first syntax and omitting the select clause from table_name -- select all columns from the table table_name in the schema schema_name select from schema_name table_name -- select the column i from the table function range -- where the first column of the range function is renamed to i select t i from range 100 as t i -- select all columns from the csv file called test csv select from test csv -- select all columns from a subquery select from select from table_name -- select the entire row of the table as a struct select t from t -- select the entire row of the subquery as a struct i e a single column select t from select unnest generate_series 41 43 as x hello as y t -- join two tables together select from table_name join other_table on table_name key other_table key -- select a 10 sample from a table select from table_name tablesample 10 -- select a sample of 10 rows from a table select from table_name tablesample 10 rows -- use the from-first syntax with where clause and aggregation from range 100 as t i select sum t i where i 2 0 joins joins are a fundamental relational operation used to connect two tables or relations horizontally the relations are referred to as the left and right sides of the join based on how they are written in the join clause each result row has the columns from both relations a join uses a rule to match pairs of rows from each relation often this is a predicate but there are other implied rules that may be specified outer joins rows that do not have any matches can still be returned if an outer join is specified outer joins can be one of left all rows from the left relation appear at least once right all rows from the right relation appear at least once full all rows from both relations appear at least once a join that is not outer is inner only rows that get paired are returned when an unpaired row is returned the attributes from the other table are set to null cross product joins the simplest type of join is a cross join there are no conditions for this type of join and it just returns all the possible pairs -- return all pairs of rows select a b from a cross join b conditional joins most joins are specified by a predicate that connects attributes from one side to attributes from the other side the conditions can be explicitly specified using an on clause with the join clearer or implied by the where clause old-fashioned we use the l_regions and the l_nations tables from the tpc-h schema create table l_regions r_regionkey integer not null primary key r_name char 25 not null r_comment varchar 152 create table l_nations n_nationkey integer not null primary key n_name char 25 not null n_regionkey integer not null n_comment varchar 152 foreign key n_regionkey references l_regions r_regionkey -- return the regions for the nations select n r from l_nations n join l_regions r on n_regionkey r_regionkey if the column names are the same and are required to be equal then the simpler using syntax can be used create table l_regions regionkey integer not null primary key name char 25 not null comment varchar 152 create table l_nations nationkey integer not null primary key name char 25 not null regionkey integer not null comment varchar 152 foreign key regionkey references l_regions regionkey -- return the regions for the nations select n r from l_nations n join l_regions r using regionkey the expressions to not have to be equalities - any predicate can be used -- return the pairs of jobs where one ran longer but cost less select s1 t_id s2 t_id from west s1 west s2 where s1 time s2 time and s1 cost s2 cost semi and anti joins semi joins return rows from the left table that have at least one match in the right table anti joins return rows from the left table that have no matches in the right table when using a semi or anti join the result will never have more rows than the left hand side table semi and anti joins provide the same logic as not in statements -- return a list of cars that have a valid region select cars name cars manufacturer from cars semi join region on cars region region id -- return a list of cars with no recorded safety data select cars name cars manufacturer from cars anti join safety_data on cars safety_report_id safety_data report_id lateral joins the lateral keyword allows subqueries in the from clause to refer to previous subqueries this feature is also known as a lateral join select from range 3 t i lateral select i 1 t2 j i j int64 int64 0 1 1 2 2 3 lateral joins are a generalization of correlated subqueries as they can return multiple values per input value rather than only a single value select from generate_series 0 1 t i lateral select i 10 union all select i 100 t2 j i j int64 int64 0 10 1 11 0 100 1 101 it may be helpful to think about lateral as a loop where we iterate through the rows of the first subquery and use it as input to the second lateral subquery in the examples above we iterate through table t and refer to its column i from the definition of table t2 the rows of t2 form column j in the result it is possible to refer to multiple attributes from the lateral subquery using the table from the first example create table t1 as select from range 3 t i lateral select i 1 t2 j select from t1 lateral select i j t2 k i j k int64 int64 int64 0 1 1 1 2 3 2 3 5 duckdb detects when lateral joins should be used making the use of the lateral keyword optional positional joins when working with data frames or other embedded tables of the same size the rows may have a natural correspondence based on their physical order in scripting languages this is easily expressed using a loop for i 0 i n i f t1 a i t2 b i it is difficult to express this in standard sql because relational tables are not ordered but imported tables like data frames or disk files like csvs or parquet files do have a natural ordering connecting them using this ordering is called a positional join -- treat two data frames as a single table select df1 df2 from df1 positional join df2 positional joins are always full outer joins as-of joins a common operation when working with temporal or similarly-ordered data is to find the nearest first event in a reference table such as prices this is called an as-of join -- attach prices to stock trades select t p price from trades t asof join prices p on t symbol p symbol and t when p when the asof join requires at least one inequality condition on the ordering field the inequality can be any inequality condition on any data type but the most common form is on a temporal type any other conditions must be equalities or not distinct this means that the left right order of the tables is significant asof joins each left side row with at most one right side row it can be specified as an outer join to find unpaired rows e g trades without prices or prices which have no trades -- attach prices or nulls to stock trades select from trades t asof left join prices p on t symbol p symbol and t when p when asof joins can also specify join conditions on matching column names with the using syntax but the last attribute in the list must be the inequality which will be greater than or equal to select from trades t asof join prices p using symbol when -- returns symbol trades when price but not prices when if you combine using with a select like this the query will return the left side probe column values for the matches not the right side build column values to get the prices times in the example you will need to list the columns explicitly select t symbol t when as trade_when p when as price_when price from trades t asof left join prices p using symbol when syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/from",
			"blurb": "The FROM clause can contain a single table, a combination of multiple tables that are joined together, or another..."
		},
		{
			"title": "File Formats",
			"text": "handling parquet files duckdb has advanced support for parquet files which includes directly querying parquet files when deciding on whether to query these files directly or to first load them to the database you need to consider several factors reasons for querying parquet files availability of basic statistics parquet files use a columnar storage format and contain basic statistics such as zonemaps thanks to these features duckdb can leverage optimizations such as projection and filter pushdown on parquet files therefore workloads that combine projection filtering and aggregation tend to perform quite well when run on parquet files storage considerations loading the data from parquet files will require approximately the same amount of space for the duckdb database file therefore if the available disk space is constrained it is worth running the queries directly on parquet files reasons against querying parquet files lack of advanced statistics the duckdb database format has the hyperloglog statistics that parquet files do not have these improve the accuracy of cardinality estimates and are especially important if the queries contain a large number of join operators tip if you find that duckdb produces a suboptimal join order on parquet files try loading the parquet files to duckdb tables the improved statistics likely help obtain a better join order repeated queries if you plan to run multiple queries on the same data set it is worth loading the data into duckdb the queries will always be somewhat faster which over time amortizes the initial load time high decompression times some parquet files are compressed using heavyweight compression algorithms such as gzip in these cases querying the parquet files will necessitate an expensive decompression time every time the file is accessed meanwhile lightweight compression methods like snappy lz4 zstd are faster to decompress you may use the parquet_metadata function to find out the compression algorithm used microbenchmark running tpc-h on a duckdb database vs parquet the queries on the tpc-h benchmark run approximately 1 1-5 0x slower on parquet files than on a duckdb database bestpractice if you have the storage space available and have a join-heavy workload and or plan to run many queries on the same dataset load the parquet files into the database first the compression algorithm and the row group sizes in the parquet files have a large effect on performance study these using the parquet_metadata function the effect of row group sizes duckdb works best on parquet files with row groups of 100k-1m rows each the reason for this is that duckdb can only parallelize over row groups so if a parquet file has a single giant row group it can only be processed by a single thread you can use the parquet_metadata function to figure out how many row groups a parquet file has when writing parquet files use the row_group_size option microbenchmark running aggregation query at different row group sizes we run a simple aggregation query over parquet files using different row group sizes selected between 960 and 1 966 080 the results are as follows row group size execution time ---------------- ---------------- 960 8 77s 1920 8 95s 3840 4 33s 7680 2 35s 15360 1 58s 30720 1 17s 61440 0 94s 122880 0 87s 245760 0 93s 491520 0 95s 983040 0 97s 1966080 0 88s the results show that row group sizes 5 000 have a strongly detrimental effect making runtimes more than 5-10x larger than ideally-sized row groups while row group sizes between 5 000 and 20 000 are still 1 5-2 5x off from best performance above row group size of 100 000 the differences are small the gap is about 10 between the best and the worst runtime parquet file sizes duckdb can also parallelize across multiple parquet files it is advisable to have at least as many total row groups across all files as there are cpu threads for example with a machine having 10 threads both 10 files with 1 row group or 1 file with 10 row groups will achieve full parallelism it is also beneficial to keep the size of individual parquet files moderate bestpractice the ideal range is between 100mb and 10gb per individual parquet file hive partitioning for filter pushdown when querying many files with filter conditions performance can be improved by using a hive-format folder structure to partition the data along the columns used in the filter condition duckdb will only need to read the folders and files that meet the filter criteria this can be especially helpful when querying remote files more tips on reading and writing parquet files for tips on reading and writing parquet files see the parquet tips page loading csv files csv files are often distributed in compressed format such as gzip archives csv gz duckdb can decompress these files on the fly in fact this is typically faster than decompressing the files first and loading them due to reduced io schema load time --- --- load from gzip-compressed csv files csv gz 107 1s decompressing using parallel gunzip and loading from decompressed csv files 121 3s loading many small csv files the csv reader runs the csv sniffer on all files for many small files this may cause an unnecessarily high overhead a potential optimization to speed this up is to turn the sniffer off assuming that all files have the same csv dialect and colum names types get the sniffer options as follows mode line select prompt from sniff_csv part-0001 csv prompt from read_csv file_path csv auto_detect false delim quote escape new_line n skip 0 header true columns hello bigint world varchar then you can adjust read_csv command by e g applying filename expansion globbing and run with the rest of the options detected by the sniffer from read_csv part- csv auto_detect false delim quote escape new_line n skip 0 header true columns hello bigint world varchar",
			"category": "Performance",
			"url": "/docs/guides/performance/file_formats",
			"blurb": "Handling Parquet Files DuckDB has advanced support for Parquet files, which includes directly querying Parquet files..."
		},
		{
			"title": "Friendly SQL",
			"text": "duckdb offers several advanced sql features as well as extensions to the sql syntax we call these colloquially as friendly sql several of these features are also supported in other systems while some are currently exclusive to duckdb clauses create or replace table create table as ctas describe from -first syntax group by all insert into by name order by all pivot unpivot select exclude select replace summarize union by name query features column aliases in where group by and having columns expression with regular expressions with exclude and replace with lambda functions literals and identifiers case-insensitivity while maintaining case of entities in the catalog deduplicating identifiers underscores as digit separators data types map data type union data type functions and expressions formatters format function with the fmt syntax and the printf function list comprehensions string slicing and list slicing struct notation simple list and struct creation join types asof joins lateral joins positional joins trailing commas duckdb allows trailing commas both when listing entities e g column and table names and when constructing list items for example the following query works select 42 as x a b c as y hello world as z see also friendlier sql with duckdb blog post even friendlier sql with duckdb blog post",
			"category": "Sql Features",
			"url": "/docs/guides/sql_features/friendly_sql",
			"blurb": "DuckDB offers several advanced SQL features as well as extensions to the SQL syntax. We call these colloquially as..."
		},
		{
			"title": "Full-Text Search",
			"text": "duckdb supports full-text search via the fts extension a full-text index allows for a query to quickly search for all occurrences of individual words within longer text strings example shakespeare corpus here s an example of building a full-text index of shakespeare s plays create table corpus as select from https blobs duckdb org data shakespeare parquet describe corpus column_name column_type null line_id varchar yes play_name varchar yes line_number varchar yes speaker varchar yes text_entry varchar yes the text of each line is in text_entry and a unique key for each line is in line_id creating a full-text search index first we create the index specifying the table name the unique id column and the column s to index we will just index the single column text_entry which contains the text of the lines in the play pragma create_fts_index corpus line_id text_entry the table is now ready to query using the okapi bm25 ranking function rows with no match return a null score what does shakespeare say about butter select fts_main_corpus match_bm25 line_id butter as score line_id play_name speaker text_entry from corpus where score is not null order by score score line_id play_name speaker text_entry double varchar varchar varchar varchar 2 683490686835495 h4 2 4 115 henry iv prince henry didst thou never see titan kiss a dish of 3 781282331450016 h4 1 2 21 henry iv falstaff prologue to an egg and butter 3 781282331450016 h4 2 1 55 henry iv chamberlain they are up already and call for eggs and 3 781282331450016 h4 4 2 21 henry iv falstaff toasts-and-butter with hearts in their be 3 781282331450016 h4 4 2 62 henry iv prince henry already made thee butter but tell me jac 3 781282331450016 aww 4 1 40 alls well that end parolles butter-womans mouth and buy myself another 3 781282331450016 aww 5 2 9 alls well that end clown henceforth eat no fish of fortunes butteri 3 781282331450016 ayli 3 2 93 as you like it touchstone right butter-womens rank to market 3 781282331450016 kl 2 4 132 king lear fool kindness to his horse buttered his hay 3 781282331450016 mww 2 2 260 merry wives of win falstaff hang him mechanical salt-butter rogue i 3 781282331450016 mww 2 2 284 merry wives of win ford rather trust a fleming with my butter par 3 781282331450016 mww 3 5 7 merry wives of win falstaff ill have my brains taen out and buttered 3 781282331450016 mww 3 5 102 merry wives of win falstaff to heat as butter a man of continual diss 6 399093176300027 h4 2 4 494 henry iv carrier as fat as butter 14 rows 5 columns unlike standard indexes full-text indexes don t auto-update as the underlying data is changed so you need to pragma drop_fts_index my_fts_index and recreate it when appropriate note on generating the corpus table for more details see the generating a shakespeare corpus for full-text searching from json blog post the columns are line_id play_name line_number speaker text_entry we need a unique key for each row in order for full-text searching to work the line_id kl 2 4 132 means king lear act 2 scene 4 line 132",
			"category": "Sql Features",
			"url": "/docs/guides/sql_features/full_text_search",
			"blurb": "DuckDB supports full-text search via the fts extension . A full-text index allows for a query to quickly search for..."
		},
		{
			"title": "Full-Text Search Extension",
			"text": "full-text search is an extension to duckdb that allows for search through strings similar to sqlite s fts5 extension installing and loading the fts extension will be transparently autoloaded on first use from the official extension repository if you would like to install and load it manually run install fts load fts usage the extension adds two pragma statements to duckdb one to create and one to drop an index additionally a scalar macro stem is added which is used internally by the extension pragma create_fts_index create_fts_index input_table input_id input_values stemmer porter stopwords english ignore a-z strip_accents 1 lower 1 overwrite 0 pragma that creates a fts index for the specified table name type description -- -- ---------- input_table varchar qualified name of specified table e g table_name or main table_name input_id varchar column name of document identifier e g document_identifier input_values varchar column names of the text fields to be indexed vararg e g text_field_1 text_field_2 text_field_n or for all columns in input_table of type varchar stemmer varchar the type of stemmer to be used one of arabic basque catalan danish dutch english finnish french german greek hindi hungarian indonesian irish italian lithuanian nepali norwegian porter portuguese romanian russian serbian spanish swedish tamil turkish or none if no stemming is to be used defaults to porter stopwords varchar qualified name of table containing a single varchar column containing the desired stopwords or none if no stopwords are to be used defaults to english for a pre-defined list of 571 english stopwords ignore varchar regular expression of patterns to be ignored defaults to a-z ignoring all escaped and non-alphabetic lowercase characters strip_accents boolean whether to remove accents e g convert \u00e1 to a defaults to 1 lower boolean whether to convert all text to lowercase defaults to 1 overwrite boolean whether to overwrite an existing index on a table defaults to 0 \u00df this pragma builds the index under a newly created schema the schema will be named after the input table if an index is created on table main table_name then the schema will be named fts_main_table_name pragma drop_fts_index drop_fts_index input_table drops a fts index for the specified table name type description -- -- ----------- input_table varchar qualified name of input table e g table_name or main table_name match_bm25 function match_bm25 input_id query_string fields null k 1 2 b 0 75 conjunctive 0 when an index is built this retrieval macro is created that can be used to search the index name type description -- -- ---------- input_id varchar column name of document identifier e g document_identifier query_string varchar the string to search the index for fields varchar comma-separarated list of fields to search in e g text_field_2 text_field_n defaults to null to search all indexed fields k double parameter k sub 1 sub in the okapi bm25 retrieval model defaults to 1 2 b double parameter b in the okapi bm25 retrieval model defaults to 0 75 conjunctive boolean whether to make the query conjunctive i e all terms in the query string must be present in order for a document to be retrieved stem function stem input_string stemmer reduces words to their base used internally by the extension name type description -- -- ---------- input_string varchar the column or constant to be stemmed stemmer varchar the type of stemmer to be used one of arabic basque catalan danish dutch english finnish french german greek hindi hungarian indonesian irish italian lithuanian nepali norwegian porter portuguese romanian russian serbian spanish swedish tamil turkish or none if no stemming is to be used example usage create a table and fill it with text data create table documents document_identifier varchar text_content varchar author varchar doc_version integer insert into documents values doc1 the mallard is a dabbling duck that breeds throughout the temperate hannes m\u00fchleisen 3 doc2 the cat is a domestic species of small carnivorous mammal laurens kuiper 2 build the index and make both the text_content and author columns searchable pragma create_fts_index documents document_identifier text_content author search the author field index for documents that are authored by muhleisen this retrieves doc1 select document_identifier text_content score from select fts_main_documents match_bm25 document_identifier muhleisen fields author as score from documents sq where score is not null and doc_version 2 order by score desc search for documents about small cats this retrieves doc2 select document_identifier text_content score from select fts_main_documents match_bm25 document_identifier small cats as score from documents sq where score is not null order by score desc the fts index will not update automatically when input table changes a workaround of this limitation can be recreating the index to refresh github the fts extension is part of the main duckdb repository",
			"category": "Extensions",
			"url": "/docs/extensions/full_text_search",
			"blurb": "Full-Text Search is an extension to DuckDB that allows for search through strings, similar to SQLite's FTS5 extension..."
		},
		{
			"title": "Functions",
			"text": "function syntax query functions duckdb_functions table function shows the list of functions currently built into the system select distinct on function_name function_name function_type return_type parameters parameter_types description from duckdb_functions where function_type scalar and function_name like b order by function_name function_name function_type return_type parameters parameter_types description varchar varchar varchar varchar varchar varchar bar scalar varchar x min max width double double d draws a band whose width is proportion base64 scalar varchar blob blob converts a blob to a base64 encoded st bin scalar varchar value varchar converts the value to binary represent bit_count scalar tinyint x tinyint returns the number of bits that are set bit_length scalar bigint col0 varchar bit_position scalar integer substring bitstr bit bit returns first starting index of the sp bitstring scalar bit bitstring length varchar integer pads the bitstring until the specified currently the description and parameter names of functions are still missing pages in this section",
			"category": "Functions",
			"url": "/docs/sql/functions/overview",
			"blurb": "Function Syntax Query Functions duckdb_functions table function shows the list of functions currently built into the..."
		},
		{
			"title": "GCS Import",
			"text": "prerequisites for google cloud storage gcs the interoperability api enables you to have access to it like an s3 connection using the httpfs extension this can be installed use the install sql command this only needs to be run once credentials and configuration you need to create hmac keys and declare them create secret type gcs key_id akiaiosfodnn7example secret wjalrxutnfemi k7mdeng bpxrficyexamplekey querying after setting up the gcs credentials you can query the gcs data using select from read_parquet gs gcs_bucket file",
			"category": "Import",
			"url": "/docs/guides/import/gcs_import",
			"blurb": "Prerequisites For Google Cloud Storage (GCS), the Interoperability API enables you to have access to it like an S3..."
		},
		{
			"title": "GROUP BY Clause",
			"text": "the group by clause specifies which grouping columns should be used to perform any aggregations in the select clause if the group by clause is specified the query is always an aggregate query even if no aggregations are present in the select clause when a group by clause is specified all tuples that have matching data in the grouping columns i e all tuples that belong to the same group will be combined the values of the grouping columns themselves are unchanged and any other columns can be combined using an aggregate function such as count sum avg etc group by all use group by all to group by all columns in the select statement that are not wrapped in aggregate functions this simplifies the syntax by allowing the columns list to be maintained in a single location and prevents bugs by keeping the select granularity aligned to the group by granularity ex prevents any duplication see examples below and additional examples in the friendlier sql with duckdb blog post multiple dimensions normally the group by clause groups along a single dimension using the grouping sets cube or rollup clauses it is possible to group along multiple dimensions see the grouping sets page for more information examples -- count the number of entries in the addresses table that belong to each different city select city count from addresses group by city -- compute the average income per city per street_name select city street_name avg income from addresses group by city street_name group by all examples -- group by city and street_name to remove any duplicate values select city street_name from addresses group by all -- group by city street_name -- compute the average income per city per street_name -- since income is wrapped in an aggregate function do not include it in the group by select city street_name avg income from addresses group by all -- group by city street_name syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/groupby",
			"blurb": "The GROUP BY clause specifies which grouping columns should be used to perform any aggregations in the SELECT clause...."
		},
		{
			"title": "GROUPING SETS",
			"text": "grouping sets rollup and cube can be used in the group by clause to perform a grouping over multiple dimensions within the same query note that this syntax is not compatible with group by all examples -- compute the average income along the provided four different dimensions -- signifies the empty set i e computing an ungrouped aggregate select city street_name avg income from addresses group by grouping sets city street_name city street_name -- compute the average income along the same dimensions select city street_name avg income from addresses group by cube city street_name -- compute the average income along the dimensions city street_name city and select city street_name avg income from addresses group by rollup city street_name description grouping sets perform the same aggregate across different group by clauses in a single query create table students course varchar type varchar insert into students course type values cs bachelor cs bachelor cs phd math masters cs null cs null math null select course type count from students group by grouping sets course type course type course type count_star cs bachelor 2 cs phd 1 math masters 1 cs null 2 math null 1 cs null 5 math null 2 null bachelor 2 null phd 1 null masters 1 null null 3 null null 7 in the above query we group across four different sets course type course type and the empty group the result contains null for a group which is not in the grouping set for the result i e the above query is equivalent to the following union statement -- group by course type select course type count from students group by course type union all -- group by type select null as course type count from students group by type union all -- group by course select course null as type count from students group by course union all -- group by nothing select null as course null as type count from students cube and rollup are syntactic sugar to easily produce commonly used grouping sets the rollup clause will produce all sub-groups of a grouping set e g rollup country city zip produces the grouping sets country city zip country city country this can be useful for producing different levels of detail of a group by clause this produces n 1 grouping sets where n is the amount of terms in the rollup clause cube produces grouping sets for all combinations of the inputs e g cube country city zip will produce country city zip country city country zip city zip country city zip this produces 2 n grouping sets identifying grouping sets with grouping_id the super-aggregate rows generated by grouping sets rollup and cube can often be identified by null -values returned for the respective column in the grouping but if the columns used in the grouping can themselves contain actual null -values then it can be challenging to distinguish whether the value in the resultset is a real null -value coming out of the data itself or a null -value generated by the grouping construct the grouping_id or grouping function is designed to identify which groups generated the super-aggregate rows in the result grouping_id is an aggregate function that takes the column expressions that make up the grouping s it returns a bigint value the return value is 0 for the rows that are not super-aggregate rows but for the super-aggregate rows it returns an integer value that identifies the combination of expressions that make up the group for which the super-aggregate is generated at this point an example might help consider the following query with days as select year generate_series as y quarter generate_series as q month generate_series as m from generate_series date 2023-01-01 date 2023-12-31 interval 1 day select y q m grouping_id y q m as grouping_id from days group by grouping sets y q m y q y order by y q m these are the results y q m grouping_id int64 int64 int64 int64 2023 1 1 0 2023 1 2 0 2023 1 3 0 2023 1 null 1 2023 2 4 0 2023 2 5 0 2023 2 6 0 2023 2 null 1 2023 3 7 0 2023 3 8 0 2023 3 9 0 2023 3 null 1 2023 4 10 0 2023 4 11 0 2023 4 12 0 2023 4 null 1 2023 null null 3 null null null 7 18 rows 4 columns in this example the lowest level of grouping is at the month level defined by the grouping set y q m result rows corresponding to that level are simply aggregate rows and the grouping_id y q m function returns 0 for those the grouping set y q results in super-aggregate rows over the month level leaving a null -value for the m column and for which grouping_id y q m returns 1 the grouping set y results in super-aggregate rows over the quarter level leaving null -values for the m and q column for which grouping_id y q m returns 3 finally the grouping set results in one super-aggregate row for the entire resultset leaving null -values for y q and m and for which grouping_id y q m returns 7 to understand the relationship between the return value and the grouping set you can think of grouping_id y q m writing to a bitfield where the first bit corresponds to the last expression passed to grouping_id the second bit to the one-but-last expression passed to grouping_id and so on this may become clearer by casting grouping_id to bit with days as select year generate_series as y quarter generate_series as q month generate_series as m from generate_series date 2023-01-01 date 2023-12-31 interval 1 day select y q m grouping_id y q m as grouping_id y q m right grouping_id y q m bit 3 as y_q_m_bits from days group by grouping sets y q m y q y order by y q m which returns these results y q m grouping_id y q m y_q_m_bits int64 int64 int64 int64 varchar 2023 1 1 0 000 2023 1 2 0 000 2023 1 3 0 000 2023 1 null 1 001 2023 2 4 0 000 2023 2 5 0 000 2023 2 6 0 000 2023 2 null 1 001 2023 3 7 0 000 2023 3 8 0 000 2023 3 9 0 000 2023 3 null 1 001 2023 4 10 0 000 2023 4 11 0 000 2023 4 12 0 000 2023 4 null 1 001 2023 null null 3 011 null null null 7 111 18 rows 5 columns note that the number of expressions passed to grouping_id or the order in which they are passed is independent from the actual group definitions appearing in the grouping sets -clause or the groups implied by rollup and cube as long as the expressions passed to grouping_id are expressions that appear some where in the grouping sets -clause grouping_id will set a bit corresponding to the position of the expression whenever that expression is rolled up to a super-aggregate syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/grouping_sets",
			"blurb": "GROUPING SETS , ROLLUP and CUBE can be used in the GROUP BY clause to perform a grouping over multiple dimensions..."
		},
		{
			"title": "Go",
			"text": "the duckdb go driver go-duckdb allows using duckdb via the database sql interface for examples on how to use this interface see the official documentation and tutorial the go client is provided as a third-party library installation to install the go-duckdb client run go get github com marcboeker go-duckdb importing to import the duckdb go package add the following entries to your imports import database sql _ github com marcboeker go-duckdb appender the duckdb go client supports the duckdb appender api for bulk inserts you can obtain a new appender by supplying a duckdb connection to newappenderfromconn for example connector err duckdb newconnector test db nil if err nil conn err connector connect context background if err nil defer conn close retrieve appender from connection note that you have to create the table test beforehand appender err newappenderfromconn conn test if err nil defer appender close err appender appendrow if err nil optional if you want to access the appended rows immediately err appender flush if err nil examples simple example an example for using the go api is as follows package main import database sql errors fmt log _ github com marcboeker go-duckdb func main db err sql open duckdb if err nil log fatal err defer db close _ err db exec create table people id integer name varchar if err nil log fatal err _ err db exec insert into people values 42 john if err nil log fatal err var id int name string row db queryrow select id name from people err row scan id name if errors is err sql errnorows log println no rows else if err nil log fatal err fmt printf id d name s n id name more examples for more examples see the examples in the duckdb-go repository github repository span class github github span",
			"category": "Api",
			"url": "/docs/api/go",
			"blurb": "The DuckDB Go driver, go-duckdb , allows using DuckDB via the database/sql interface. For examples on how to use this..."
		},
		{
			"title": "Guides",
			"text": "the guides section contains compact how-to guides that are focused on achieving a single goal for an in-depth reference see the documentation section note that there are many tools using duckdb which are not covered in the official guides to find a list of these tools check out the awesome duckdb repository data import and export csv files how to load a csv file into a table how to export a table to a csv file parquet files how to load a parquet file into a table how to export a table to a parquet file how to run a query directly on a parquet file http s s3 and gcp how to load a parquet file directly from http s how to load a parquet file directly from s3 how to export a parquet file to s3 how to load a parquet file from s3 express one how to load a parquet file directly from gcs how to load a parquet file directly from cloudflare r2 how to load an iceberg table directly from s3 json files how to load a json file into a table how to export a table to a json file excel files with the spatial extension how to load an excel file into a table how to export a table to an excel file querying other database systems how to directly query a postgresql database how to directly query a sqlite database how to directly query a mysql database directly reading files how to directly read a binary file how to directly read a text file performance my workload is slow troubleshooting guide how to design the schema for optimal performance what is the ideal hardware environment for duckdb what performance implications do parquet files and compressed csv files have how to tune workloads benchmarks meta queries how to list all tables how to view the schema of the result of a query how to quickly get a feel for a dataset using summarize how to view the query plan of a query how to profile a query odbc how to set up an odbc application and more python client how to install the python client how to execute sql queries how to easily query duckdb in jupyter notebooks how to use multiple python threads with duckdb how to use fsspec filesystems with duckdb pandas how to execute sql on a pandas dataframe how to create a table from a pandas dataframe how to export data to a pandas dataframe apache arrow how to execute sql on apache arrow how to create a duckdb table from apache arrow how to export data to apache arrow relational api how to query pandas dataframes with the relational api python library integrations how to use ibis to query duckdb with or without sql how to use duckdb with polars dataframes via apache arrow sql features friendly sql as-of join full-text search sql editors and ides how to set up the dbeaver sql ide data viewers how to visualize duckdb databases with tableau how to draw command-line plots with duckdb and youplot",
			"category": "Guides",
			"url": "/docs/guides/index",
			"blurb": "The guides section contains compact how-to guides that are focused on achieving a single goal. For an in-depth..."
		},
		{
			"title": "HAVING Clause",
			"text": "the having clause can be used after the group by clause to provide filter criteria after the grouping has been completed in terms of syntax the having clause is identical to the where clause but while the where clause occurs before the grouping the having clause occurs after the grouping examples -- count the number of entries in the addresses table that belong to each different city -- filtering out cities with a count below 50 select city count from addresses group by city having count 50 -- compute the average income per city per street_name -- filtering out cities with an average income bigger than twice the median income select city street_name avg income from addresses group by city street_name having avg income 2 median income syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/having",
			"blurb": "The HAVING clause can be used after the GROUP BY clause to provide filter criteria after the grouping has been..."
		},
		{
			"title": "HTTP Parquet Import",
			"text": "to load a parquet file over http s the httpfs extension is required this can be installed use the install sql command this only needs to be run once install httpfs to load the httpfs extension for usage use the load sql command load httpfs after the httpfs extension is set up parquet files can be read over http s select from read_parquet https domain path to file parquet for example select from read_parquet https duckdb org data prices parquet the function read_parquet can be omitted if the url ends with parquet select from read_parquet https duckdb org data holdings parquet moreover the read_parquet function itself can also be omitted thanks to duckdb s replacement scan mechanism select from https duckdb org data holdings parquet",
			"category": "Import",
			"url": "/docs/guides/import/http_import",
			"blurb": "To load a Parquet file over HTTP(S), the httpfs extension is required. This can be installed use the INSTALL SQL..."
		},
		{
			"title": "HTTP(S) Support",
			"text": "with the httpfs extension it is possible to directly query files over the http s protocol this works for all files supported by duckdb or its various extensions and provides read-only access select from https domain tld file extension for csv files files will be downloaded entirely in most cases due to the row-based nature of the format for parquet files duckdb can use a combination of the parquet metadata and http range requests to only download the parts of the file that are actually required by the query for example the following query will only read the parquet metadata and the data for the column_a column select column_a from https domain tld file parquet in some cases even no actual data needs to be read at all as they only require reading the metadata select count from https domain tld file parquet scanning multiple files over http s is also supported select from read_parquet https domain tld file1 parquet https domain tld file2 parquet",
			"category": "Httpfs",
			"url": "/docs/extensions/httpfs/https",
			"blurb": "With the httpfs extension, it is possible to directly query files over the HTTP(S) protocol. This works for all files..."
		},
		{
			"title": "Hive Partitioning",
			"text": "examples -- read data from a hive partitioned data set select from read_parquet orders parquet hive_partitioning 1 -- write a table to a hive partitioned data set copy orders to orders format parquet partition_by year month hive partitioning hive partitioning is a partitioning strategy that is used to split a table into multiple files based on partition keys the files are organized into folders within each folder the partition key has a value that is determined by the name of the folder below is an example of a hive partitioned file hierarchy the files are partitioned on two keys year and month orders year 2021 month 1 file1 parquet file2 parquet month 2 file3 parquet year 2022 month 11 file4 parquet file5 parquet month 12 file6 parquet files stored in this hierarchy can be read using the hive_partitioning flag select from read_parquet orders parquet hive_partitioning 1 when we specify the hive_partitioning flag the values of the columns will be read from the directories filter pushdown filters on the partition keys are automatically pushed down into the files this way the system skips reading files that are not necessary to answer a query for example consider the following query on the above dataset select from read_parquet orders parquet hive_partitioning 1 where year 2022 and month 11 when executing this query only the following files will be read orders year 2022 month 11 file4 parquet file5 parquet autodetection by default the system tries to infer if the provided files are in a hive partitioned hierarchy and if so the hive_partitioning flag is enabled automatically the autodetection will look at the names of the folders and search for a key value pattern this behaviour can be overridden by setting the hive_partitioning flag manually hive types hive_types is a way to specify the logical types of the hive partitions in a struct from read_parquet dir parquet hive_partitioning 1 hive_types release date orders bigint hive_types will be autodetected for the following types date timestamp and bigint to switch off the autodetection the flag hive_types_autocast 0 can be set writing partitioned files see the partitioned writes section",
			"category": "Partitioning",
			"url": "/docs/data/partitioning/hive_partitioning",
			"blurb": "Examples -- read data from a hive partitioned data set SELECT * FROM read_parquet('orders/*/*/*.parquet',..."
		},
		{
			"title": "ICU Extension",
			"text": "the icu extension contains an easy-to-use version of the collation timezone part of the icu library installing and loading to install and load the icu extension run install icu load icu features the icu extension introduces the following features region-dependent collations time zones used for timestamp data types and timestamp functions github the icu extension is part of the main duckdb repository",
			"category": "Extensions",
			"url": "/docs/extensions/icu",
			"blurb": "The icu extension contains an easy-to-use version of the collation/timezone part of the ICU library . Installing and..."
		},
		{
			"title": "IN Operator",
			"text": "the in operator checks containment of the left expression inside the set of expressions on the right hand side rhs the in operator returns true if the expression is present in the rhs false if the expression is not in the rhs and the rhs has no null values or null if the expression is not in the rhs and the rhs has null values select math in cs math -- true select english in cs math -- false select math in cs math null -- true select english in cs math null -- null not in can be used to check if an element is not present in the set x not in y is equivalent to not x in y the in operator can also be used with a subquery that returns a single column see the subqueries page for more information",
			"category": "Expressions",
			"url": "/docs/sql/expressions/in",
			"blurb": "The IN operator checks containment of the left expression inside the set of expressions on the right hand side (RHS)...."
		},
		{
			"title": "INSERT Statement",
			"text": "the insert statement inserts new data into a table examples -- insert the values 1 2 3 into tbl insert into tbl values 1 2 3 -- insert the result of a query into a table insert into tbl select from other_tbl -- insert values into the i column inserting the default value into other columns insert into tbl i values 1 2 3 -- explicitly insert the default value into a column insert into tbl i values 1 default 3 -- assuming tbl has a primary key unique constraint do nothing on conflict insert or ignore into tbl i values 1 -- or update the table with the new values instead insert or replace into tbl i values 1 syntax insert into inserts new rows into a table one can insert one or more rows specified by value expressions or zero or more rows resulting from a query insert column order it s possible to provide an optional insert column order this can either be by position the default or by name each column not present in the explicit or implicit column list will be filled with a default value either its declared default value or null if there is none if the expression for any column is not of the correct data type automatic type conversion will be attempted insert into by position the order that values are inserted into the columns of the table is determined by the order that the columns were declared in that is the values supplied by the values clause or query are associated with the column list left-to-right this is the default option that can be explicitly specified using the by position option for example create table tbl a integer b integer insert into tbl values 5 42 -- specifying by position is optional and is equivalent to the default behavior insert into tbl by position values 5 42 to use a different order column names can be provided as part of the target for example create table tbl a integer b integer insert into tbl b a values 5 42 -- adding by position results in the same behavior insert into tbl by position b a values 5 42 this will insert 5 into b and 42 into a insert into by name the names of the column list of the select statement are matched against the column names of the table to determine the order that values should be inserted into the table even if the order of the columns in the table differs from the order of the values in the select statement for example create table tbl a integer b integer insert into tbl by name select 42 as b this will insert 42 into b and insert null or its default value into a it s important to note that when using insert into by name the column names specified in the select statement must match the column names in the table if a column name is misspelled or does not exist in the table an error will occur columns that are missing from the select statement will be filled with the default value on conflict clause an on conflict clause can be used to perform a certain action on conflicts that arise from unique or primary key constraints an example for such a conflict is shown in the following example create table tbl i int primary key j int insert into tbl values 1 42 insert into tbl values 1 84 this raises as an error and leaves the table with a single row i 1 j 42 error constraint error duplicate key i 1 violates primary key constraint there are two supported actions do nothing and do update set do nothing clause the do nothing clause causes the error s to be ignored and the values are not inserted or updated for example create table tbl i int primary key j int insert into tbl values 1 42 insert into tbl values 1 84 on conflict do nothing these statements finish successfully and leaves the table with the row i 1 j 42 shorthand the insert or ignore into statement is a shorter syntax alternative to insert into on conflict do nothing for example the following statements are equivalent insert or ignore into tbl values 1 84 insert into tbl values 1 84 on conflict do nothing do update clause the do update clause causes the insert to turn into an update on the conflicting row s instead the set expressions that follow determine how these rows are updated the expressions can use the special virtual table excluded which contains the conflicting values for the row optionally you can provide an additional where clause that can exclude certain rows from the update the conflicts that don t meet this condition are ignored instead because we need a way to refer to both the to-be-inserted tuple and the existing tuple we introduce the special excluded qualifier when the excluded qualifier is provided the reference refers to the to-be-inserted tuple otherwise it refers to the existing tuple this special qualifier can be used within the where clauses and set expressions of the on conflict clause an example using do update is the following create table tbl i int primary key j int insert into tbl values 1 42 insert into tbl values 1 84 on conflict do update set j excluded j -- rearranging columns and by name also work insert into tbl j i values 168 1 on conflict do update set j excluded j insert into tbl by name select 1 as i 336 as j on conflict do update set j excluded j these statements finish successfully and leaves the table with a single row i 1 j 84 shorthand the insert or replace into statement is a shorter syntax alternative to insert into do update set c1 excluded c1 c2 excluded c2 that is it updates every column of the existing row to the new values of the to-be-inserted row for example given the following input table create table tbl i int primary key j int insert into tbl values 1 42 these statements are equivalent insert or replace into tbl values 1 84 insert into tbl values 1 84 on conflict do update set j excluded j insert into tbl j i values 84 1 on conflict do update set j excluded j insert into tbl by name select 84 as j 1 as i on conflict do update set j excluded j defining a conflict target a conflict target may be provided as on conflict confict_target this is a group of columns that an index or uniqueness key constraint is defined on if the conflict target is omitted or primary key constraint s on the table are targeted specifying a conflict target is optional unless using a do update and there are multiple unique primary key constraints on the table create table tbl i int primary key j int unique k int insert into tbl values 1 20 300 insert into tbl values 1 40 700 on conflict i do update set k 2 excluded k -- tbl will contain 1 20 1400 insert into tbl values 1 20 900 on conflict j do update set k 5 excluded k -- tbl will contain 1 20 4500 when a conflict target is provided you can further filter this with a where clause that should be met by all conflicts insert into tbl values 1 40 700 on conflict i do update set k 2 excluded k where k 100 multiple tuples conflicting on the same key having multiple tuples conflicting on the same key is not supported for example create table tbl i int primary key j int insert into tbl values 1 42 insert into tbl values 1 84 1 168 on conflict do nothing running this returns the following message error invalid input error on conflict do update can not update the same row twice in the same command ensure that no rows proposed for insertion within the same command have duplicate constrained values returning clause the returning clause may be used to return the contents of the rows that were inserted this can be useful if some columns are calculated upon insert for example if the table contains an automatically incrementing primary key then the returning clause will include the automatically created primary key this is also useful in the case of generated columns some or all columns can be explicitly chosen to be returned and they may optionally be renamed using aliases arbitrary non-aggregating expressions may also be returned instead of simply returning a column all columns can be returned using the expression and columns or expressions can be returned in addition to all columns returned by the for example create table t1 i int insert into t1 select 42 returning i ---- 42 a more complex example that includes an expression in the returning clause create table t2 i int j int insert into t2 select 2 as i 3 as j returning i j as i_times_j i j i_times_j --- --- --- 2 3 6 the next example shows a situation where the returning clause is more helpful first a table is created with a primary key column then a sequence is created to allow for that primary key to be incremented as new rows are inserted when we insert into the table we do not already know the values generated by the sequence so it is valuable to return them for additional information see the create sequence page create table t3 i int primary key j int create sequence t3_key insert into t3 select nextval t3_key as i 42 as j union all select nextval t3_key as i 43 as j returning i j --- --- 1 42 2 43",
			"category": "Statements",
			"url": "/docs/sql/statements/insert",
			"blurb": "The INSERT statement inserts new data into a table. Examples -- insert the values (1), (2), (3) into tbl INSERT INTO..."
		},
		{
			"title": "INSERT Statements",
			"text": "insert statements are the standard way of loading data into a relational database when using insert statements the values are supplied row-by-row while simple there is significant overhead involved in parsing and processing individual insert statements this makes lots of individual row-by-row insertions very inefficient for bulk insertion as a rule-of-thumb avoid using lots of individual row-by-row insert statements when inserting more than a few rows i e avoid using insert statements as part of a loop when bulk inserting data try to maximize the amount of data that is inserted per statement if you must use insert statements to load data in a loop avoid executing the statements in auto-commit mode after every commit the database is required to sync the changes made to disk to ensure no data is lost in auto-commit mode every single statement will be wrapped in a separate transaction meaning fsync will be called for every statement this is typically unnecessary when bulk loading and will significantly slow down your program if you absolutely must use insert statements in a loop to load data wrap them in calls to begin transaction and commit syntax an example of using insert into to load data in a table is as follows create table people id integer name varchar insert into people values 1 mark 2 hannes for a more detailed description together with syntax diagram can be found see the page on the insert statement",
			"category": "Data",
			"url": "/docs/data/insert",
			"blurb": "INSERT statements are the standard way of loading data into a relational database. When using INSERT statements, the..."
		},
		{
			"title": "Iceberg Extension",
			"text": "the iceberg extension is a loadable extension that implements support for the apache iceberg format installing and loading to install and load the iceberg extension run install iceberg load iceberg usage to test the examples download the iceberg_data zip file and unzip it querying individual tables select count from iceberg_scan data iceberg lineitem_iceberg allow_moved_paths true 51793 the allow_moved_paths option ensures that some path resolution is performed which allows scanning iceberg tables that are moved access iceberg metadata select from iceberg_metadata data iceberg lineitem_iceberg allow_moved_paths true manifest_path manifest_sequence_number manifest_content status content file_path file_format record_count varchar int64 varchar varchar varchar varchar varchar int64 lineitem_iceberg metadata 10eaca8a-1e1c-421e-ad6d-b2 2 data added existing lineitem_iceberg data 00041-414-f3c73457-bbd6-4b92-9c15-17b241171b16-00001 parquet parquet 51793 lineitem_iceberg metadata 10eaca8a-1e1c-421e-ad6d-b2 2 data deleted existing lineitem_iceberg data 00000-411-0792dcfe-4e25-4ca3-8ada-175286069a47-00001 parquet parquet 60175 visualizing snapshots select from iceberg_snapshots data iceberg lineitem_iceberg sequence_number snapshot_id timestamp_ms manifest_list uint64 uint64 timestamp varchar 1 3776207205136740581 2023-02-15 15 07 54 504 lineitem_iceberg metadata snap-3776207205136740581-1-cf3d0be5-cf70-453d-ad8f-48fdc412e608 avro 2 7635660646343998149 2023-02-15 15 08 14 73 lineitem_iceberg metadata snap-7635660646343998149-1-10eaca8a-1e1c-421e-ad6d-b232e5ee23d3 avro limitations writing i e exporting to iceberg files is currently not supported github repository span class github github span",
			"category": "Extensions",
			"url": "/docs/extensions/iceberg",
			"blurb": "The iceberg extension is a loadable extension that implements support for the Apache Iceberg format . Installing and..."
		},
		{
			"title": "Import from Apache Arrow",
			"text": "create table as and insert into can be used to create a table from any query we can then create tables or insert into existing tables by referring to referring to the apache arrow object in the query this example imports from an arrow table but duckdb can query different apache arrow formats as seen in the sql on arrow guide import duckdb import pyarrow as pa connect to an in-memory database my_arrow pa table from_pydict a 42 create the table my_table from the dataframe my_arrow duckdb sql create table my_table as select from my_arrow insert into the table my_table from the dataframe my_arrow duckdb sql insert into my_table select from my_arrow",
			"category": "Python",
			"url": "/docs/guides/python/import_arrow",
			"blurb": "CREATE TABLE AS and INSERT INTO can be used to create a table from any query. We can then create tables or insert..."
		},
		{
			"title": "Import from Pandas",
			"text": "create table as and insert into can be used to create a table from any query we can then create tables or insert into existing tables by referring to referring to the pandas dataframe in the query import duckdb import pandas create a pandas dataframe my_df pandas dataframe from_dict a 42 create the table my_table from the dataframe my_df note duckdb sql connects to the default in-memory database connection duckdb sql create table my_table as select from my_df insert into the table my_table from the dataframe my_df duckdb sql insert into my_table select from my_df",
			"category": "Python",
			"url": "/docs/guides/python/import_pandas",
			"blurb": "CREATE TABLE AS and INSERT INTO can be used to create a table from any query. We can then create tables or insert..."
		},
		{
			"title": "Importing Data",
			"text": "the first step to using a database system is to insert data into that system duckdb provides several data ingestion methods that allow you to easily and efficiently fill up the database in this section we provide an overview of these methods so you can select which one is correct for you insert statements insert statements are the standard way of loading data into a database system they are suitable for quick prototyping but should be avoided for bulk loading as they have significant per-row overhead insert into people values 1 mark for a more detailed description see the page on the insert statement csv loading data can be efficiently loaded from csv files using the read_csv function or the copy statement select from read_csv test csv you can also load data from compressed e g compressed with gzip csv files for example select from read_csv test csv gz for more details see the page on csv loading parquet loading parquet files can be efficiently loaded and queried using the read_parquet function select from read_parquet test parquet for more details see the page on parquet loading json loading json files can be efficiently loaded and queried using the read_json_auto function select from read_json_auto test json for more details see the page on json loading appender c and java in c and java the appender can be used as an alternative for bulk data loading this class can be used to efficiently add rows to the database system without needing to use sql c appender appender con people appender appendrow 1 mark appender close java con createappender main people appender beginrow appender append mark appender endrow appender close for a detailed description see the pages on the c appender and the java appender pages in this section",
			"category": "Data",
			"url": "/docs/data/overview",
			"blurb": "The first step to using a database system is to insert data into that system. DuckDB provides several data ingestion..."
		},
		{
			"title": "Indexes",
			"text": "index types duckdb currently uses two index types a min-max index also known as zonemap and block range index is automatically created for columns of all general-purpose data types an adaptive radix tree art is mainly used to ensure primary key constraints and to speed up point and very highly selective i e 0 1 queries such an index is automatically created for columns with a unique or primary key constraint and can be defined using create index art indexes must currently be able to fit in-memory avoid creating art indexes if the index does not fit in memory persistence both min-max indexes and art indexes are persisted on disk create index and drop index to create an index use the create index statement to drop an index use the drop index statement index limitations art indexes create a secondary copy of the data in a second location - this complicates processing particularly when combined with transactions certain limitations apply when it comes to modifying data that is also stored in secondary indexes as expected indexes have a strong effect on performance slowing down loading and updates but speeding up certain queries please consult the performance guide for details updates become deletes and inserts when an update statement is executed on a column that is present in an index - the statement is transformed into a delete of the original row followed by an insert this has certain performance implications particularly for wide tables as entire rows are rewritten instead of only the affected columns over-eager unique constraint checking due to the presence of transactions data can only be removed from the index after 1 the transaction that performed the delete is committed and 2 no further transactions exist that refer to the old entry still present in the index as a result of this - transactions that perform deletions followed by insertions may trigger unexpected unique constraint violations as the deleted tuple has not actually been removed from the index yet for example create table students id integer primary key name varchar insert into students values 1 student 1 begin delete from students where id 1 insert into students values 1 student 2 -- constraint error duplicate key id 1 violates primary key constraint this combined with the fact that updates are turned into deletions and insertions within the same transaction means that updating rows in the presence of unique or primary key constraints can often lead to unexpected unique constraint violations create table students id integer primary key name varchar insert into students values 1 student 1 update students set name student 2 id 1 where id 1 -- constraint error duplicate key id 1 violates primary key constraint currently this is an expected limitation of the system - although we aim to resolve this in the future",
			"category": "SQL",
			"url": "/docs/sql/indexes",
			"blurb": "Index Types DuckDB currently uses two index types: A min-max index (also known as zonemap and block range index) is..."
		},
		{
			"title": "Indexing",
			"text": "duckdb has two types of indexes zonemaps and art indexes zonemaps duckdb automatically creates zonemaps also known as min-max indexes for the columns of all general-purpose data types these indexes are used for predicate pushdown into scan operators and computing aggregations this means that if a filter criterion like where column1 123 is in use duckdb can skip any row group whose min-max range does not contain that filter value e g a block with a min-max range of 1000 to 2000 will be omitted when comparing for 123 or 400 the effect of ordering on zonemaps the more ordered the data within a column the more useful the zonemap indexes will be for example in the worst case a column could contain a random number on every row duckdb will be unlikely to be able to skip any row groups the best case of ordered data commonly arises with datetime columns if specific columns will be queried with selective filters it is best to pre-order data by those columns when inserting it even an imperfect ordering will still be helpful microbenchmark the effect of ordering for an example let s repeat the microbenchmark for timestamps with a timestamp column that sorted using an ascending order vs an unordered one column type ordered storage size query time --- --- --- --- datetime yes 1 3 gb 0 578 s datetime no 3 3 gb 0 904 s the results show that simply keeping the column order allows for improved compression yielding a 2 5x smaller storage size it also allows the computation to be 1 5x faster ordered integers another practical way to exploit ordering is to use the integer type with automatic increments rather than uuid for columns that will be queried using selective filters uuid s will likely be inserted in a random order so many row groups in the table will need to be scanned to find a specific uuid value while an ordered integer column will allow all row groups to be skipped except the one that contains the value art indexes duckdb allows defining adaptive radix tree art indexes in two ways first such an index is created implicitly for columns with primary key foreign key and unique constraints second explicitly running a the create index statement creates an art index on the target column s the tradeoffs of having an art index on a column are as follows it enables efficient constraint checking upon changes inserts updates and deletes for non-bulky changes having an art index makes changes to the affected column s slower compared to non-indexed performance that is because of index maintenance for these operations regarding query performance an art index has the following effects it speeds up point queries and other highly selective queries using the indexed column s where the filtering condition returns approx 0 1 of all rows or fewer when in doubt use explain to verify that your query plan uses the index scan an art index has no effect on the performance of join aggregation and sorting queries indexes are serialized to disk and deserialized lazily i e when the database is reopened operations using the index will only load the required parts of the index therefore having an index will not cause any slowdowns when opening an existing database bestpractice we recommend following these guidelines only use primary keys foreign keys or unique constraints if these are necessary for enforcing constraints on your data do not define explicit indexes unless you have highly selective queries if you define an art index do so after bulk loading the data to the table adding an index prior to loading either explicitly or via primary foreign keys is detrimental to load performance",
			"category": "Performance",
			"url": "/docs/guides/performance/indexing",
			"blurb": "DuckDB has two types of indexes: zonemaps and ART indexes. Zonemaps DuckDB automatically creates zonemaps (also known..."
		},
		{
			"title": "Information Schema",
			"text": "the views in the information_schema are sql-standard views that describe the catalog entries of the database these views can be filtered to obtain information about a specific column or table database catalog and schema the top level catalog view is information_schema schemata it lists the catalogs and the schemas present in the database and has the following layout column description type example -- --- - - catalog_name name of the database that the schema is contained in varchar null schema_name name of the schema varchar main schema_owner name of the owner of the schema not yet implemented varchar null default_character_set_catalog applies to a feature not available in duckdb varchar null default_character_set_schema applies to a feature not available in duckdb varchar null default_character_set_name applies to a feature not available in duckdb varchar null sql_path the file system location of the database currently unimplemented varchar null tables and views the view that describes the catalog information for tables and views is information_schema tables it lists the tables present in the database and has the following layout column description type example -- --- - - table_catalog the catalog the table or view belongs to varchar null table_schema the schema the table or view belongs to varchar main table_name the name of the table or view varchar widgets table_type the type of table one of base table local temporary view varchar base table self_referencing_column_name applies to a feature not available in duckdb varchar null reference_generation applies to a feature not available in duckdb varchar null user_defined_type_catalog if the table is a typed table the name of the database that contains the underlying data type always the current database else null currently unimplemented varchar null user_defined_type_schema if the table is a typed table the name of the schema that contains the underlying data type else null currently unimplemented varchar null user_defined_type_name if the table is a typed table the name of the underlying data type else null currently unimplemented varchar null is_insertable_into yes if the table is insertable into no if not base tables are always insertable into views not necessarily varchar yes is_typed yes if the table is a typed table no if not varchar no commit_action not yet implemented varchar no columns the view that describes the catalog information for columns is information_schema columns it lists the column present in the database and has the following layout column description type example -- --- - - table_catalog name of the database containing the table varchar null table_schema name of the schema containing the table varchar main table_name name of the table varchar widgets column_name name of the column varchar price ordinal_position ordinal position of the column within the table count starts at 1 integer 5 column_default default expression of the column varchar 1 99 is_nullable yes if the column is possibly nullable no if it is known not nullable varchar yes data_type data type of the column varchar decimal 18 2 character_maximum_length if data_type identifies a character or bit string type the declared maximum length null for all other data types or if no maximum length was declared integer 255 character_octet_length if data_type identifies a character type the maximum possible length in octets bytes of a datum null for all other data types the maximum octet length depends on the declared character maximum length see above and the character encoding integer 1073741824 numeric_precision if data_type identifies a numeric type this column contains the declared or implicit precision of the type for this column the precision indicates the number of significant digits for all other data types this column is null integer 18 numeric_scale if data_type identifies a numeric type this column contains the declared or implicit scale of the type for this column the precision indicates the number of significant digits for all other data types this column is null integer 2 datetime_precision if data_type identifies a date time timestamp or interval type this column contains the declared or implicit fractional seconds precision of the type for this column that is the number of decimal digits maintained following the decimal point in the seconds value no fractional seconds are currently supported in duckdb for all other data types this column is null integer 0 catalog functions several functions are also provided to see details about the catalogs and schemas that are configured in the database function description example result -- --- -- -- current_catalog return the name of the currently active catalog default is memory current_catalog memory current_schema return the name of the currently active schema default is main current_schema main current_schemas boolean return list of schemas pass a parameter of true to include implicit schemas current_schemas true temp main pg_catalog",
			"category": "SQL",
			"url": "/docs/sql/information_schema",
			"blurb": "The views in the information_schema are SQL-standard views that describe the catalog entries of the database. These..."
		},
		{
			"title": "Inspecting Query Plans Using EXPLAIN",
			"text": "in order to view the query plan of a query prepend explain to a query explain select from tbl by default only the final physical plan is shown in order to see the unoptimized and optimized logical plans change the explain_output setting set explain_output all below is an example of running explain on q1 of the tpc-h benchmark order_by lineitem l_returnflag asc lineitem l_linestatus asc hash_group_by 0 1 sum 2 sum 3 sum 4 sum 5 avg 6 avg 7 avg 8 count_star projection l_returnflag l_linestatus l_quantity l_extendedprice 4 4 1 00 l_tax l_quantity l_extendedprice l_discount projection l_returnflag l_linestatus l_quantity l_extendedprice l_extendedprice 1 00 - l_discount l_tax l_discount seq_scan lineitem l_shipdate l_returnflag l_linestatus l_quantity l_extendedprice l_discount l_tax filters l_shipdate 1998 -09-02 and l_shipdate null",
			"category": "Meta",
			"url": "/docs/guides/meta/explain",
			"blurb": "In order to view the query plan of a query, prepend EXPLAIN to a query. EXPLAIN SELECT * FROM tbl; By default only..."
		},
		{
			"title": "Installing the Python Client",
			"text": "installing via pip the latest release of the python client can be installed using pip pip install duckdb the pre-release python client can be installed using --pre pip install duckdb --upgrade --pre installing from source the latest python client can be installed from source from the tools pythonpkg directory in the duckdb github repository build_python 1 gen ninja make cd tools pythonpkg python setup py install",
			"category": "Python",
			"url": "/docs/guides/python/install",
			"blurb": "Installing via Pip The latest release of the Python client can be installed using pip . pip install duckdb The pre-..."
		},
		{
			"title": "Instantiation",
			"text": "instantiation duckdb-wasm has multiple ways to be instantiated depending on the use case cdn jsdelivr import as duckdb from duckdb duckdb-wasm const jsdelivr_bundles duckdb getjsdelivrbundles select a bundle based on browser checks const bundle await duckdb selectbundle jsdelivr_bundles const worker_url url createobjecturl new blob importscripts bundle mainworker type text javascript instantiate the asynchronus version of duckdb-wasm const worker new worker worker_url const logger new duckdb consolelogger const db new duckdb asyncduckdb logger worker await db instantiate bundle mainmodule bundle pthreadworker url revokeobjecturl worker_url webpack import as duckdb from duckdb duckdb-wasm import duckdb_wasm from duckdb duckdb-wasm dist duckdb-mvp wasm import duckdb_wasm_next from duckdb duckdb-wasm dist duckdb-eh wasm const manual_bundles duckdb duckdbbundles mvp mainmodule duckdb_wasm mainworker new url duckdb duckdb-wasm dist duckdb-browser-mvp worker js import meta url tostring eh mainmodule duckdb_wasm_next mainworker new url duckdb duckdb-wasm dist duckdb-browser-eh worker js import meta url tostring select a bundle based on browser checks const bundle await duckdb selectbundle manual_bundles instantiate the asynchronus version of duckdb-wasm const worker new worker bundle mainworker const logger new duckdb consolelogger const db new duckdb asyncduckdb logger worker await db instantiate bundle mainmodule bundle pthreadworker vite import as duckdb from duckdb duckdb-wasm import duckdb_wasm from duckdb duckdb-wasm dist duckdb-mvp wasm url import mvp_worker from duckdb duckdb-wasm dist duckdb-browser-mvp worker js url import duckdb_wasm_eh from duckdb duckdb-wasm dist duckdb-eh wasm url import eh_worker from duckdb duckdb-wasm dist duckdb-browser-eh worker js url const manual_bundles duckdb duckdbbundles mvp mainmodule duckdb_wasm mainworker mvp_worker eh mainmodule duckdb_wasm_eh mainworker eh_worker select a bundle based on browser checks const bundle await duckdb selectbundle manual_bundles instantiate the asynchronus version of duckdb-wasm const worker new worker bundle mainworker const logger new duckdb consolelogger const db new duckdb asyncduckdb logger worker await db instantiate bundle mainmodule bundle pthreadworker statically served it is possible to manually download the files from https cdn jsdelivr net npm duckdb duckdb-wasm dist import as duckdb from duckdb duckdb-wasm const manual_bundles duckdb duckdbbundles mvp mainmodule change me duckdb-mvp wasm mainworker change me duckdb-browser-mvp worker js eh mainmodule change m duckdb-eh wasm mainworker change m duckdb-browser-eh worker js select a bundle based on browser checks const bundle await duckdb selectbundle jsdelivr_bundles instantiate the asynchronous version of duckdb-wasm const worker new worker bundle mainworker const logger new duckdb consolelogger const db new duckdb asyncduckdb logger worker await db instantiate bundle mainmodule bundle pthreadworker",
			"category": "Wasm",
			"url": "/docs/api/wasm/instantiation",
			"blurb": "Instantiation DuckDB-Wasm has multiple ways to be instantiated depending on the use case. cdn(jsdelivr) import * as..."
		},
		{
			"title": "Integration with Ibis",
			"text": "ibis is a python dataframe library that supports 15 backends with duckdb as the default ibis with duckdb provides a pythonic interface for sql with great performance installation you can pip install ibis with the duckdb backend pip install ibis-framework duckdb or use conda conda install ibis-framework or use mamba mamba install ibis-framework create a database file ibis can work with several file types but at its core it connects to existing databases and interacts with the data there you can get started with your own duckdb databases or create a new one with example data import ibis con ibis connect duckdb penguins ddb con create_table penguins ibis examples penguins fetch to_pyarrow overwrite true output databasetable penguins species string island string bill_length_mm float64 bill_depth_mm float64 flipper_length_mm int64 body_mass_g int64 sex string year int64 you can now see the example dataset copied over to the database reconnect to the persisted database dropping temp tables con ibis connect duckdb penguins ddb con list_tables output penguins there s one table called penguins we can ask ibis to give us an object that we can interact with penguins con table penguins penguins output databasetable penguins species string island string bill_length_mm float64 bill_depth_mm float64 flipper_length_mm int64 body_mass_g int64 sex string year int64 ibis is lazily evaluated so instead of seeing the data we see the schema of the table to peek at the data we can call head and then to_pandas to get the first few rows of the table as a pandas dataframe penguins head to_pandas species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year 0 adelie torgersen 39 1 18 7 181 0 3750 0 male 2007 1 adelie torgersen 39 5 17 4 186 0 3800 0 female 2007 2 adelie torgersen 40 3 18 0 195 0 3250 0 female 2007 3 adelie torgersen nan nan nan nan none 2007 4 adelie torgersen 36 7 19 3 193 0 3450 0 female 2007 to_pandas takes the existing lazy table expression and evaluates it if we leave it off you ll see the ibis representation of the table expression that to_pandas will evaluate when you re ready penguins head output r0 databasetable penguins species string island string bill_length_mm float64 bill_depth_mm float64 flipper_length_mm int64 body_mass_g int64 sex string year int64 limit r0 n 5 ibis returns results as a pandas dataframe using to_pandas but isn t using pandas to perform any of the computation the query is executed by duckdb only when to_pandas is called does ibis then pull back the results and convert them into a dataframe interactive mode for the rest of this intro we ll turn on interactive mode which partially executes queries to give users a preview of the results there is a small difference in the way the output is formatted but otherwise this is the same as calling to_pandas on the table expression with a limit of 10 result rows returned ibis options interactive true penguins head species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year string string float64 float64 int64 int64 string int64 adelie torgersen 39 1 18 7 181 3750 male 2007 adelie torgersen 39 5 17 4 186 3800 female 2007 adelie torgersen 40 3 18 0 195 3250 female 2007 adelie torgersen nan nan null null null 2007 adelie torgersen 36 7 19 3 193 3450 female 2007 common operations ibis has a collection of useful table methods to manipulate and query the data in a table filter filter allows you to select rows based on a condition or set of conditions we can filter so we only have penguins of the species adelie penguins filter penguins species gentoo species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year string string float64 float64 int64 int64 string int64 gentoo biscoe 46 1 13 2 211 4500 female 2007 gentoo biscoe 50 0 16 3 230 5700 male 2007 gentoo biscoe 48 7 14 1 210 4450 female 2007 gentoo biscoe 50 0 15 2 218 5700 male 2007 gentoo biscoe 47 6 14 5 215 5400 male 2007 gentoo biscoe 46 5 13 5 210 4550 female 2007 gentoo biscoe 45 4 14 6 211 4800 female 2007 gentoo biscoe 46 7 15 3 219 5200 male 2007 gentoo biscoe 43 3 13 4 209 4400 female 2007 gentoo biscoe 46 8 15 4 215 5150 male 2007 or filter for gentoo penguins that have a body mass larger than 6 kg penguins filter penguins species gentoo penguins body_mass_g 6000 species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year string string float64 float64 int64 int64 string int64 gentoo biscoe 49 2 15 2 221 6300 male 2007 gentoo biscoe 59 6 17 0 230 6050 male 2007 you can use any boolean comparison in a filter although if you try to do something like use on a string ibis will yell at you select your data analysis might not require all the columns present in a given table select lets you pick out only those columns that you want to work with to select a column you can use the name of the column as a string penguins select species island year limit 3 species island year string string int64 adelie torgersen 2007 adelie torgersen 2007 adelie torgersen 2007 or you can use column objects directly this can be convenient when paired with tab-completion penguins select penguins species penguins island penguins year limit 3 species island year string string int64 adelie torgersen 2007 adelie torgersen 2007 adelie torgersen 2007 or you can mix-and-match penguins select species island penguins year limit 3 species island year string string int64 adelie torgersen 2007 adelie torgersen 2007 adelie torgersen 2007 mutate mutate lets you add new columns to your table derived from the values of existing columns penguins mutate bill_length_cm penguins bill_length_mm 10 species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year bill_length_cm string string float64 float64 int64 int64 string int64 float64 adelie torgersen 39 1 18 7 181 3750 male 2007 3 91 adelie torgersen 39 5 17 4 186 3800 female 2007 3 95 adelie torgersen 40 3 18 0 195 3250 female 2007 4 03 adelie torgersen nan nan null null null 2007 nan adelie torgersen 36 7 19 3 193 3450 female 2007 3 67 adelie torgersen 39 3 20 6 190 3650 male 2007 3 93 adelie torgersen 38 9 17 8 181 3625 female 2007 3 89 adelie torgersen 39 2 19 6 195 4675 male 2007 3 92 adelie torgersen 34 1 18 1 193 3475 null 2007 3 41 adelie torgersen 42 0 20 2 190 4250 null 2007 4 20 notice that the table is a little too wide to display all the columns now depending on your screen-size bill_length is now present in millimeters and centimeters use a select to trim down the number of columns we re looking at penguins mutate bill_length_cm penguins bill_length_mm 10 select species island bill_depth_mm flipper_length_mm body_mass_g sex year bill_length_cm species island bill_depth_mm flipper_length_mm body_mass_g sex year bill_length_cm string string float64 int64 int64 string int64 float64 adelie torgersen 18 7 181 3750 male 2007 3 91 adelie torgersen 17 4 186 3800 female 2007 3 95 adelie torgersen 18 0 195 3250 female 2007 4 03 adelie torgersen nan null null null 2007 nan adelie torgersen 19 3 193 3450 female 2007 3 67 adelie torgersen 20 6 190 3650 male 2007 3 93 adelie torgersen 17 8 181 3625 female 2007 3 89 adelie torgersen 19 6 195 4675 male 2007 3 92 adelie torgersen 18 1 193 3475 null 2007 3 41 adelie torgersen 20 2 190 4250 null 2007 4 20 selectors typing out all of the column names except one is a little annoying instead of doing that again we can use a selector to quickly select or deselect groups of columns import ibis selectors as s penguins mutate bill_length_cm penguins bill_length_mm 10 select s matches bill_length_mm match every column except bill_length_mm species island bill_depth_mm flipper_length_mm body_mass_g sex year bill_length_cm string string float64 int64 int64 string int64 float64 adelie torgersen 18 7 181 3750 male 2007 3 91 adelie torgersen 17 4 186 3800 female 2007 3 95 adelie torgersen 18 0 195 3250 female 2007 4 03 adelie torgersen nan null null null 2007 nan adelie torgersen 19 3 193 3450 female 2007 3 67 adelie torgersen 20 6 190 3650 male 2007 3 93 adelie torgersen 17 8 181 3625 female 2007 3 89 adelie torgersen 19 6 195 4675 male 2007 3 92 adelie torgersen 18 1 193 3475 null 2007 3 41 adelie torgersen 20 2 190 4250 null 2007 4 20 you can also use a selector alongside a column name penguins select island s numeric island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g year string float64 float64 int64 int64 int64 torgersen 39 1 18 7 181 3750 2007 torgersen 39 5 17 4 186 3800 2007 torgersen 40 3 18 0 195 3250 2007 torgersen nan nan null null 2007 torgersen 36 7 19 3 193 3450 2007 torgersen 39 3 20 6 190 3650 2007 torgersen 38 9 17 8 181 3625 2007 torgersen 39 2 19 6 195 4675 2007 torgersen 34 1 18 1 193 3475 2007 torgersen 42 0 20 2 190 4250 2007 you can read more about selectors in the docs order_by order_by arranges the values of one or more columns in ascending or descending order by default ibis sorts in ascending order penguins order_by penguins flipper_length_mm select species island flipper_length_mm species island flipper_length_mm string string int64 adelie biscoe 172 adelie biscoe 174 adelie torgersen 176 adelie dream 178 adelie dream 178 adelie dream 178 chinstrap dream 178 adelie dream 179 adelie torgersen 180 adelie biscoe 180 you can sort in descending order using the desc method of a column penguins order_by penguins flipper_length_mm desc select species island flipper_length_mm species island flipper_length_mm string string int64 gentoo biscoe 231 gentoo biscoe 230 gentoo biscoe 230 gentoo biscoe 230 gentoo biscoe 230 gentoo biscoe 230 gentoo biscoe 230 gentoo biscoe 230 gentoo biscoe 229 gentoo biscoe 229 or you can use ibis desc penguins order_by ibis desc flipper_length_mm select species island flipper_length_mm species island flipper_length_mm string string int64 gentoo biscoe 231 gentoo biscoe 230 gentoo biscoe 230 gentoo biscoe 230 gentoo biscoe 230 gentoo biscoe 230 gentoo biscoe 230 gentoo biscoe 230 gentoo biscoe 229 gentoo biscoe 229 aggregate ibis has several aggregate functions available to help summarize data mean max min count sum the list goes on to aggregate an entire column call the corresponding method on that column penguins flipper_length_mm mean output 200 91520467836258 you can compute multiple aggregates at once using the aggregate method penguins aggregate penguins flipper_length_mm mean penguins bill_depth_mm max mean flipper_length_mm max bill_depth_mm float64 float64 200 915205 21 5 but aggregate really shines when it s paired with group_by group_by group_by creates groupings of rows that have the same value for one or more columns but it doesn t do much on its own -- you can pair it with aggregate to get a result penguins group_by species aggregate species string adelie gentoo chinstrap we grouped by the species column and handed it an empty aggregate command the result of that is a column of the unique values in the species column if we add a second column to the group_by we ll get each unique pairing of the values in those columns penguins group_by species island aggregate species island string string adelie torgersen adelie biscoe adelie dream gentoo biscoe chinstrap dream now if we add an aggregation function to that we start to really open things up penguins group_by species island aggregate penguins bill_length_mm mean species island mean bill_length_mm string string float64 adelie torgersen 38 950980 adelie biscoe 38 975000 adelie dream 38 501786 gentoo biscoe 47 504878 chinstrap dream 48 833824 by adding that mean to the aggregate we now have a concise way to calculate aggregates over each of the distinct groups in the group_by and we can calculate as many aggregates as we need penguins group_by species island aggregate penguins bill_length_mm mean penguins flipper_length_mm max species island mean bill_length_mm max flipper_length_mm string string float64 int64 adelie torgersen 38 950980 210 adelie biscoe 38 975000 203 adelie dream 38 501786 208 gentoo biscoe 47 504878 231 chinstrap dream 48 833824 212 if we need more specific groups we can add to the group_by penguins group_by species island sex aggregate penguins bill_length_mm mean penguins flipper_length_mm max species island sex mean bill_length_mm max flipper_length_mm string string string float64 int64 adelie torgersen male 40 586957 210 adelie torgersen female 37 554167 196 adelie torgersen null 37 925000 193 adelie biscoe female 37 359091 199 adelie biscoe male 40 590909 203 adelie dream female 36 911111 202 adelie dream male 40 071429 208 adelie dream null 37 500000 179 gentoo biscoe female 45 563793 222 gentoo biscoe male 49 473770 231 chaining it all together we ve already chained some ibis calls together we used mutate to create a new column and then select to only view a subset of the new table we were just chaining group_by with aggregate there s nothing stopping us from putting all of these concepts together to ask questions of the data how about what was the largest female penguin by body mass on each island in the year 2008 penguins filter penguins sex female penguins year 2008 group_by island aggregate penguins body_mass_g max island max body_mass_g string int64 biscoe 5200 torgersen 3800 dream 3900 what about the largest male penguin by body mass on each island for each year of data collection penguins filter penguins sex male group_by island year aggregate penguins body_mass_g max name max_body_mass order_by year max_body_mass island year max_body_mass string int64 int64 dream 2007 4650 torgersen 2007 4675 biscoe 2007 6300 torgersen 2008 4700 dream 2008 4800 biscoe 2008 6000 torgersen 2009 4300 dream 2009 4475 biscoe 2009 6000 learn more that s all for this quick-start guide if you want to learn more check out the ibis documentation",
			"category": "Python",
			"url": "/docs/guides/python/ibis",
			"blurb": "Ibis is a Python dataframe library that supports 15+ backends, with DuckDB as the default. Ibis with DuckDB provides..."
		},
		{
			"title": "Integration with Polars",
			"text": "polars is a dataframes library built in rust with bindings for python and node js it uses apache arrow s columnar format as its memory model duckdb can read polars dataframes and convert query results to polars dataframes it does this internally using the efficient apache arrow integration note that the pyarrow library must be installed for the integration to work installation pip install duckdb pip install -u polars pyarrow polars to duckdb duckdb can natively query polars dataframes by referring to the name of polars dataframes as they exist in the current scope import duckdb import polars as pl df pl dataframe a 1 2 3 4 5 fruits banana banana apple apple banana b 5 4 3 2 1 cars beetle audi beetle beetle beetle duckdb sql select from df show duckdb to polars duckdb can output results as polars dataframes using the pl result-conversion method df duckdb sql select 1 as id banana as fruit union all select 2 apple union all select 3 mango pl print df shape 3 2 id fruit --- --- i32 str 1 banana 2 apple 3 mango to learn more about polars feel free to explore their python api reference",
			"category": "Python",
			"url": "/docs/guides/python/polars",
			"blurb": "Polars is a DataFrames library built in Rust with bindings for Python and Node.js. It uses Apache Arrow's columnar..."
		},
		{
			"title": "Interval Functions",
			"text": "this section describes functions and operators for examining and manipulating interval values interval operators the table below shows the available mathematical operators for interval types operator description example result - -- ---- -- addition of an interval interval 1 hour interval 5 hour interval 6 hour addition to a date date 1992-03-22 interval 5 day 1992-03-27 addition to a timestamp timestamp 1992-03-22 01 02 03 interval 5 day 1992-03-27 01 02 03 addition to a time time 01 02 03 interval 5 hour 06 02 03 - subtraction of an interval interval 5 hour - interval 1 hour interval 4 hour - subtraction from a date date 1992-03-27 - interval 5 day 1992-03-22 - subtraction from a timestamp timestamp 1992-03-27 01 02 03 - interval 5 day 1992-03-22 01 02 03 - subtraction from a time time 06 02 03 - interval 5 hour 01 02 03 interval functions the table below shows the available scalar functions for interval types function description example result -- -- --- -- date_part part interval get subfield equivalent to extract date_part year interval 14 months 1 datepart part interval alias of date_part get subfield equivalent to extract datepart year interval 14 months 1 extract part from interval get subfield from a date extract month from interval 14 months 2 to_centuries integer construct a century interval to_centuries 5 interval 500 year to_days integer construct a day interval to_days 5 interval 5 day to_decades integer construct a decade interval to_decades 5 interval 50 year to_hours integer construct a hour interval to_hours 5 interval 5 hour to_microseconds integer construct a microsecond interval to_microseconds 5 interval 5 microsecond to_millennia integer construct a millenium interval to_millennia 5 interval 5000 year to_milliseconds integer construct a millisecond interval to_milliseconds 5 interval 5 millisecond to_minutes integer construct a minute interval to_minutes 5 interval 5 minute to_months integer construct a month interval to_months 5 interval 5 month to_seconds integer construct a second interval to_seconds 5 interval 5 second to_weeks integer construct a week interval to_weeks 5 interval 35 day to_years integer construct a year interval to_years 5 interval 5 year only the documented date parts are defined for intervals",
			"category": "Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "This section describes functions and operators for examining and manipulating INTERVAL values. Interval Operators The..."
		},
		{
			"title": "Interval Type",
			"text": "intervals represent a period of time this period can be measured in a specific unit or combination of units for example years days or seconds intervals are generally used to modify timestamps or dates by either adding or subtracting them name description --- --- interval period of time an interval can be constructed by providing an amount together with a unit intervals can be added or subtracted from date or timestamp values -- 1 year select interval 1 year -- add 1 year to a specific date select date 2000-01-01 interval 1 year -- subtract 1 year from a specific date select date 2000-01-01 - interval 1 year -- construct an interval from a column instead of a constant select interval i year from range 1 5 t i -- construct an interval with mixed units select interval 1 month 1 day -- intervals greater than 24 hours 12 months etc are supported select 540 58 47 210 interval select interval 16 months -- warning -- if a decimal value is specified it will be automatically rounded to an integer -- to use more precise values simply use a more granular date part -- in this example use 18 months instead of 1 5 years -- the statement below is equivalent to to_years cast 1 5 as integer select interval 1 5 years -- warning this returns 2 years details the interval class represents a period of time using three distinct components the month day and microsecond these three components are required because there is no direct translation between them for example a month does not correspond to a fixed amount of days that depends on which month is referenced february has fewer days than march the division into components makes the interval class suitable for adding or subtracting specific time units to a date for example we can generate a table with the first day of every month using the following sql query select date 2000-01-01 interval i month from range 12 t i difference between dates if we subtract two timestamps from one another we obtain an interval describing the difference between the timestamps with the days and microseconds components for example select timestamp 2000-02-01 12 00 00 - timestamp 2000-01-01 11 00 00 as diff diff interval 31 days 01 00 00 the datediff function can be used to obtain the difference between two dates for a specific unit select datediff month timestamp 2000-01-01 11 00 00 timestamp 2000-02-01 12 00 00 as diff diff int64 1 functions see the date part functions page for a list of available date parts for use with an interval see the interval operators page for functions that operate on intervals",
			"category": "Data Types",
			"url": "/docs/sql/data_types/interval",
			"blurb": "An interval specifies a period of time measured in units of a specific date part like years, days, seconds, or others."
		},
		{
			"title": "JSON Export",
			"text": "to export the data from a table to a json file use the copy statement copy tbl to output json the result of queries can also be directly exported to a json file copy select from tbl to output json for additional options see the copy statement documentation",
			"category": "Import",
			"url": "/docs/guides/import/json_export",
			"blurb": "To export the data from a table to a JSON file, use the COPY statement. COPY tbl TO 'output.json'; The result of..."
		},
		{
			"title": "JSON Extension",
			"text": "the json extension is a loadable extension that implements sql functions that are useful for reading values from existing json and creating new json data installing and loading the json extension is shipped by default in duckdb builds otherwise it will be transparently autoloaded on first use if you would like to install and load it manually run install json load json example uses -- read a json file from disk auto-infer options select from todos json -- read_json with custom options select from read_json todos json format array columns userid ubigint id ubigint title varchar completed boolean -- write the result of a query to a json file copy select from todos to todos json see more examples on the json data page json type the json extension makes use of the json logical type the json logical type is interpreted as json i e parsed in json functions rather than interpreted as varchar i e a regular string all json creation functions return values of this type we also allow any of our types to be casted to json and json to be casted back to any of our types for example -- cast json to our struct type select duck 42 json struct duck integer -- duck 42 -- and back select duck 42 json -- duck 42 this works for our nested types as shown in the example but also for non-nested types select 2023-05-12 date json -- 2023-05-12 the only exception to this behavior is the cast from varchar to json which does not alter the data but instead parses and validates the contents of the varchar as json json table functions the following table functions are used to read json function description --- --- read_json_objects filename read a json object from filename where filename can also be a list of files or a glob pattern read_ndjson_objects filename alias for read_json_objects with parameter format set to newline_delimited read_json_objects_auto filename alias for read_json_objects with parameter format set to auto these functions have the following parameters name description type default -- ----- - - compression the compression type for the file by default this will be detected automatically from the file extension e g t json gz will use gzip t json will use none options are none gzip zstd and auto varchar auto filename whether or not an extra filename column should be included in the result bool false format can be one of auto unstructured newline_delimited array varchar array hive_partitioning whether or not to interpret the path as a hive partitioned path bool false ignore_errors whether to ignore parse errors only possible when format is newline_delimited bool false maximum_sample_files the maximum number of json files sampled for auto-detection bigint 32 maximum_object_size the maximum size of a json object in bytes uinteger 16777216 the format parameter specifies how to read the json from a file with unstructured the top-level json is read e g duck 42 goose 1 2 3 will result in two objects being read with newline_delimited ndjson is read where each json is separated by a newline n e g duck 42 goose 1 2 3 will also result in two objects being read with array each array element is read e g duck 42 goose 1 2 3 again will result in two objects being read example usage select from read_json_objects my_file1 json -- duck 42 goose 1 2 3 select from read_json_objects my_file1 json my_file2 json -- duck 42 goose 1 2 3 -- duck 43 goose 4 5 6 swan 3 3 select from read_ndjson_objects json gz -- duck 42 goose 1 2 3 -- duck 43 goose 4 5 6 swan 3 3 duckdb also supports reading json as a table using the following functions function description ---- ------- read_json filename read json from filename where filename can also be a list of files or a glob pattern read_json_auto filename alias for read_json with all auto-detection enabled read_ndjson filename alias for read_json with parameter format set to newline_delimited read_ndjson_auto filename alias for read_json_auto with parameter format set to newline_delimited besides the maximum_object_size format ignore_errors and compression these functions have additional parameters name description type default -- ------ - - auto_detect whether to auto-detect detect the names of the keys and data types of the values automatically bool false columns a struct that specifies the key names and value types contained within the json file e g key1 integer key2 varchar if auto_detect is enabled these will be inferred struct empty dateformat specifies the date format to use when parsing dates see date format varchar iso maximum_depth maximum nesting depth to which the automatic schema detection detects types set to -1 to fully detect nested json types bigint -1 records can be one of auto true false varchar records sample_size option to define number of sample objects for automatic json type detection set to -1 to scan the entire input file ubigint 20480 timestampformat specifies the date format to use when parsing timestamps see date format varchar iso union_by_name whether the schema s of multiple json files should be unified bool false example usage select from read_json my_file1 json columns duck integer duck --- 42 duckdb can convert json arrays directly to its internal list type and missing keys become null select from read_json my_file1 json my_file2 json columns duck integer goose integer swan double duck goose swan --- --- --- 42 1 2 3 null 43 4 5 6 3 3 duckdb can automatically detect the types like so select goose duck from read_json_auto json gz select goose duck from json gz -- equivalent goose duck --- --- 1 2 3 42 4 5 6 43 duckdb can read and auto-detect a variety of formats specified with the format parameter querying a json file that contains an array e g duck 42 goose 4 2 duck 43 goose 4 3 can be queried exactly the same as a json file that contains unstructured json e g duck 42 goose 4 2 duck 43 goose 4 3 both can be read as the table duck goose --- --- 42 4 2 43 4 3 if your json file does not contain records i e any other type of json than objects duckdb can still read it this is specified with the records parameter the records parameter specifies whether the json contains records that should be unpacked into individual columns i e reading the following file with records duck 42 goose 1 2 3 duck 43 goose 4 5 6 results in two columns duck goose --- --- 42 1 2 3 42 4 5 6 you can read the same file with records set to false to get a single column which is a struct containing the data json --- duck 42 goose 1 2 3 duck 43 goose 4 5 6 for additional examples reading more complex data please see the shredding deeply nested json one vector at a time blog post json import export when the json extension is installed format json is supported for copy from copy to export database and import database see copy and import export by default copy expects newline-delimited json if you prefer copying data to from a json array you can specify array true e g copy select from range 5 to my json array true will create the following file range 0 range 1 range 2 range 3 range 4 this can be read like so create table test range bigint copy test from my json array true the format can be detected automatically the format like so copy test from my json auto_detect true json scalar functions the following scalar json functions can be used to gain information about the stored json values with the exception of json_valid json all json functions produce an error when invalid json is supplied we support two kinds of notations to describe locations within json json pointer and jsonpath function description --- ---- json_array_length json path return the number of elements in the json array json or 0 if it is not a json array if path is specified return the number of elements in the json array at the given path if path is a list the result will be list of array lengths json_contains json_haystack json_needle returns true if json_needle is contained in json_haystack both parameters are of json type but json_needle can also be a numeric value or a string however the string must be wrapped in double quotes json_keys json path returns the keys of json as a list of varchar if json is a json object if path is specified return the keys of the json object at the given path if path is a list the result will be list of list of varchar json_structure json return the structure of json defaults to json the structure is inconsistent e g incompatible types in an array json_type json path return the type of the supplied json which is one of object array bigint ubigint varchar boolean null if path is specified return the type of the element at the given path if path is a list the result will be list of types json_valid json return whether json is valid json json json parse and minify json the jsonpointer syntax separates each field with a for example to extract the first element of the array with key duck you can do select json_extract duck 1 2 3 duck 0 -- 1 the jsonpath syntax separates fields with a and accesses array elements with i and always starts with using the same example we can do the following select json_extract duck 1 2 3 duck 0 -- 1 note that duckdb s json data type uses 0-based indexing jsonpath is more expressive and can also access from the back of lists select json_extract duck 1 2 3 duck -1 -- 3 jsonpath also allows escaping syntax tokens using double quotes select json_extract duck goose 1 2 3 duck goose 1 -- 2 examples using the anatidae biological family create table example j json insert into example values family anatidae species duck goose swan null select json j from example -- family anatidae species duck goose swan null select j family from example -- anatidae select j species 0 from example -- duck select json_valid j from example -- true select json_valid -- false select json_array_length duck goose swan null -- 4 select json_array_length j species from example -- 4 select json_array_length j species from example -- 4 select json_array_length j species from example -- 4 select json_array_length j species from example -- 4 select json_type j from example -- object select json_keys j from example -- family species select json_structure j from example -- family varchar species varchar select json_structure duck family anatidae -- json select json_contains key value value -- true select json_contains key 1 1 -- true select json_contains top_key key value key value -- true json extraction functions there are two extraction functions which have their respective operators the operators can only be used if the string is stored as the json logical type these functions supports the same two location notations as the previous functions function alias operator description --- --- - json_extract json path json_extract_path - extract json from json at the given path if path is a list the result will be a list of json json_extract_string json path json_extract_path_text - extract varchar from json at the given path if path is a list the result will be a list of varchar note that duckdb s json data type uses 0-based indexing examples create table example j json insert into example values family anatidae species duck goose swan null select json_extract j family from example -- anatidae select j- family from example -- anatidae select j- species 0 from example -- duck select j- species from example -- duck goose swan null select j- species from example -- duck goose swan null select j- species - 0 from example -- duck select j- species - 0 1 from example -- duck goose select json_extract_string j family from example -- anatidae select j- family from example -- anatidae select j- species 0 from example -- duck select j- species - 0 from example -- duck select j- species - 0 1 from example -- duck goose note that duckdb s json data type uses 0-based indexing if multiple values need to be extracted from the same json it is more efficient to extract a list of paths -- the following will cause the json to be parsed twice -- resulting in a slower query that uses more memory select json_extract j family as family json_extract j species as species from example -- the following is faster and more memory efficient with extracted as select json_extract j family species extracted_list from example select extracted_list 1 as family extracted_list 2 as species from extracted json creation functions the following functions are used to create json function description -- ---- to_json any create json from a value of any type our list is converted to a json array and our struct and map are converted to a json object json_quote any alias for to_json array_to_json list alias for to_json that only accepts list row_to_json list alias for to_json that only accepts struct json_array any create a json array from any number of values json_object key value create a json object from any number of key value pairs json_merge_patch json json merge two json documents together examples select to_json duck -- duck select to_json 1 2 3 -- 1 2 3 select to_json duck 42 -- duck 42 select to_json map duck 42 -- duck 42 select json_array 42 duck null -- 42 duck null select json_object duck 42 -- duck 42 select json_merge_patch duck 42 goose 123 -- goose 123 duck 42 json aggregate functions there are three json aggregate functions function description --- ---- json_group_array any return a json array with all values of any in the aggregation json_group_object key value return a json object with all key value pairs in the aggregation json_group_structure json return the combined json_structure of all json in the aggregation examples create table example1 k varchar v integer insert into example1 values duck 42 goose 7 select json_group_array v from example1 -- 42 7 select json_group_object k v from example1 -- duck 42 goose 7 create table example2 j json insert into example2 values family anatidae species duck goose coolness 42 42 family canidae species labrador bulldog hair true select json_group_structure j from example2 -- family varchar species varchar coolness double hair boolean transforming json in many cases it is inefficient to extract values from json one-by-one instead we can extract all values at once transforming json to the nested types list and struct function description --- --- json_transform json structure transform json according to the specified structure from_json json structure alias for json_transform json_transform_strict json structure same as json_transform but throws an error when type casting fails from_json_strict json structure alias for json_transform_strict the structure argument is json of the same form as returned by json_structure the structure argument can be modified to transform the json into the desired structure and types it is possible to extract fewer key value pairs than are present in the json and it is also possible to extract more missing keys become null examples create table example j json insert into example values family anatidae species duck goose coolness 42 42 family canidae species labrador bulldog hair true select json_transform j family varchar coolness double from example -- family anatidae coolness 42 420000 -- family canidae coolness null select json_transform j family tinyint coolness decimal 4 2 from example -- family null coolness 42 42 -- family null coolness null select json_transform_strict j family tinyint coolness double from example -- invalid input error failed to cast value anatidae serializing and deserializing sql to json and vice versa the json extension also provides functions to serialize and deserialize select statements between sql and json as well as executing json serialized statements function type description ------ - --------- json_deserialize_sql json scalar deserialize one or many json serialized statements back to an equivalent sql string json_execute_serialized_sql varchar table execute json serialized statements and return the resulting rows only one statement at a time is supported for now json_serialize_sql varchar skip_empty boolean skip_null boolean format boolean scalar serialize a set of separated select statments to an equivalent list of json serialized statements pragma json_execute_serialized_sql varchar pragma pragma version of the json_execute_serialized_sql function the json_serialize_sql varchar function takes three optional parameters skip_empty skip_null and format that can be used to control the output of the serialized statements if you run the json_execute_serialize_sql varchar table function inside of a transaction the serialized statements will not be able to see any transaction local changes this is because the statements are executed in a separate query context you can use the pragma json_execute_serialize_sql varchar pragma version to execute the statements in the same query context as the pragma although with the limitation that the serialized json must be provided as a constant string i e you cannot do pragma json_execute_serialize_sql json_serialize_sql note that these functions do not preserve syntactic sugar such as from select so a statement round-tripped through json_deserialize_sql json_serialize_sql may not be identical to the original statement but should always be semantically equivalent and produce the same output examples -- simple example select json_serialize_sql select 2 -- error false statements node type select_node modifiers cte_map map select_list class constant type value_constant alias value type id integer type_info null is_null false value 2 from_table type empty alias sample null where_clause null group_expressions group_sets aggregate_handling standard_handling having null sample null qualify null -- example with multiple statements and skip options select json_serialize_sql select 1 2 select a b from tbl1 skip_empty true skip_null true -- error false statements node type select_node select_list class function type function function_name children class constant type value_constant value type id integer is_null false value 1 class constant type value_constant value type id integer is_null false value 2 order_bys type order_modifier distinct false is_operator true export_state false from_table type empty aggregate_handling standard_handling node type select_node select_list class function type function function_name children class column_ref type column_ref column_names a class column_ref type column_ref column_names b order_bys type order_modifier distinct false is_operator true export_state false from_table type base_table table_name tbl1 aggregate_handling standard_handling -- example with a syntax error select json_serialize_sql totally not valid sql -- error true error_type parser error_message syntax error at or near totally nline 1 totally not valid sql n -- example with deserialize select json_deserialize_sql json_serialize_sql select 1 2 -- select 1 2 -- example with deserialize and syntax sugar select json_deserialize_sql json_serialize_sql from x select 1 2 -- select 1 2 from x -- example with execute select from json_execute_serialized_sql json_serialize_sql select 1 2 -- 3 -- example with error select from json_execute_serialized_sql json_serialize_sql totally not valid sql -- error parser error error parsing json parser syntax error at or near totally indexing following postgresql s conventions duckdb uses 1-based indexing for arrays and lists but 0-based indexing for the json data type github the json extension is part of the main duckdb repository",
			"category": "Extensions",
			"url": "/docs/extensions/json",
			"blurb": "The json extension is a loadable extension that implements SQL functions that are useful for reading values from..."
		},
		{
			"title": "JSON Import",
			"text": "to read data from a json file use the read_json_auto function in the from clause of a query select from read_json_auto input json to create a new table using the result from a query use create table as from a select statement create table new_tbl as select from read_json_auto input json to load data into an existing table from a query use insert into from a select statement insert into tbl select from read_json_auto input json alternatively the copy statement can also be used to load data from a json file into an existing table copy tbl from input json for additional options see the json loading reference and the copy statement documentation",
			"category": "Import",
			"url": "/docs/guides/import/json_import",
			"blurb": "To read data from a JSON file, use the read_json_auto function in the FROM clause of a query. SELECT * FROM..."
		},
		{
			"title": "JSON Loading",
			"text": "examples -- read a json file from disk auto-infer options select from todos json -- read_json with custom options select from read_json todos json format array columns userid ubigint id ubigint title varchar completed boolean -- read a json file from stdin auto-infer options cat data json todos json duckdb -c select from read_json_auto dev stdin -- read a json file into a table create table todos userid ubigint id ubigint title varchar completed boolean copy todos from todos json -- alternatively create a table without specifying the schema manually create table todos as select from todos json -- write the result of a query to a json file copy select from todos to todos json json loading json is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute value pairs and arrays or other serializable values while it is not a very efficient format for tabular data it is very commonly used especially as a data interchange format the duckdb json reader can automatically infer which configuration flags to use by analyzing the json file this will work correctly in most situations and should be the first option attempted in rare situations where the json reader cannot figure out the correct configuration it is possible to manually configure the json reader to correctly parse the json file below are parameters that can be passed in to the json reader parameters name description type default -- ----- - - auto_detect whether to auto-detect detect the names of the keys and data types of the values automatically bool false columns a struct that specifies the key names and value types contained within the json file e g key1 integer key2 varchar if auto_detect is enabled these will be inferred struct empty compression the compression type for the file by default this will be detected automatically from the file extension e g t json gz will use gzip t json will use none options are none gzip zstd and auto varchar auto convert_strings_to_integers whether strings representing integer values should be converted to a numerical type bool false dateformat specifies the date format to use when parsing dates see date format varchar iso filename whether or not an extra filename column should be included in the result bool false format can be one of auto unstructured newline_delimited array varchar array hive_partitioning whether or not to interpret the path as a hive partitioned path bool false ignore_errors whether to ignore parse errors only possible when format is newline_delimited bool false maximum_depth maximum nesting depth to which the automatic schema detection detects types set to -1 to fully detect nested json types bigint -1 maximum_object_size the maximum size of a json object in bytes uinteger 16777216 records can be one of auto true false varchar records sample_size option to define number of sample objects for automatic json type detection set to -1 to scan the entire input file ubigint 20480 timestampformat specifies the date format to use when parsing timestamps see date format varchar iso union_by_name whether the schema s of multiple json files should be unified bool false when using read_json_auto every parameter that supports auto-detection is enabled examples of format settings the json extension can attempt to determine the format of a json file when setting format to auto here are some example json files and the corresponding format settings that should be used in each of the below cases the format setting was not needed as duckdb was able to infer it correctly but it is included for illustrative purposes a query of this shape would work in each case select from filename json format newline_delimited with format newline_delimited newline-delimited json can be parsed each line is a json key1 value1 key2 value1 key1 value2 key2 value2 key1 value3 key2 value3 select from read_json_auto records json format newline_delimited key1 key2 ---------- ---------- value1 value1 value2 value2 value3 value3 format array if the json file contains a json array of objects pretty-printed or not array_of_objects may be used key1 value1 key2 value1 key1 value2 key2 value2 key1 value3 key2 value3 select from read_json_auto array json format array key1 key2 ---------- ---------- value1 value1 value2 value2 value3 value3 format unstructured if the json file contains json that is not newline-delimited or an array unstructured may be used key1 value1 key2 value1 key1 value2 key2 value2 key1 value3 key2 value3 select from read_json_auto unstructured json format unstructured key1 key2 ---------- ---------- value1 value1 value2 value2 value3 value3 examples of records settings the json extension can attempt to determine whether a json file contains records when setting records auto when records true the json extension expects json objects and will unpack the fields of json objects into individual columns continuing with the same example file from before key1 value1 key2 value1 key1 value2 key2 value2 key1 value3 key2 value3 select from read_json_auto records json records true key1 key2 ---------- ---------- value1 value1 value2 value2 value3 value3 when records false the json extension will not unpack the top-level objects and create struct s instead select from read_json_auto records json records false json ------------------------------------ key1 value1 key2 value1 key1 value2 key2 value2 key1 value3 key2 value3 this is especially useful if we have non-object json for example 1 2 3 4 5 6 7 8 9 select from read_json_auto arrays json records false json ------------- 1 2 3 4 5 6 7 8 9 writing the contents of tables or the result of queries can be written directly to a json file using the copy statement see the copy documentation for more information read_json_auto function the read_json_auto is the simplest method of loading json files it automatically attempts to figure out the correct configuration of the json reader it also automatically deduces types of columns select from read_json_auto todos json limit 5 userid id title completed -------- ---- ----------------------------------------------------------------- ----------- 1 1 delectus aut autem false 1 2 quis ut nam facilis et officia qui false 1 3 fugiat veniam minus false 1 4 et porro tempora true 1 5 laboriosam mollitia et enim quasi adipisci quia provident illum false the path can either be a relative path relative to the current working directory or an absolute path we can use read_json_auto to create a persistent table as well create table todos as select from read_json_auto todos json describe todos column_name column_type null key default extra ------------- ------------- ------ ----- --------- ------- userid ubigint yes id ubigint yes title varchar yes completed boolean yes if we specify the columns we can bypass the automatic detection note that not all columns need to be specified select from read_json_auto todos json columns userid ubigint completed boolean multiple files can be read at once by providing a glob or a list of files refer to the multiple files section for more information copy statement the copy statement can be used to load data from a json file into a table for the copy statement we must first create a table with the correct schema to load the data into we then specify the json file to load from plus any configuration options separately create table todos userid ubigint id ubigint title varchar completed boolean copy todos from todos json select from todos limit 5 userid id title completed -------- ---- ----------------------------------------------------------------- ----------- 1 1 delectus aut autem false 1 2 quis ut nam facilis et officia qui false 1 3 fugiat veniam minus false 1 4 et porro tempora true 1 5 laboriosam mollitia et enim quasi adipisci quia provident illum false for more details see the page on the copy statement",
			"category": "Json",
			"url": "/docs/data/json/overview",
			"blurb": "Examples -- read a JSON file from disk, auto-infer options SELECT * FROM 'todos.json'; -- read_json with custom..."
		},
		{
			"title": "Java JDBC API",
			"text": "installation the duckdb java jdbc api can be installed from maven central please see the installation page for details basic api usage duckdb s jdbc api implements the main parts of the standard java database connectivity jdbc api version 4 1 describing jdbc is beyond the scope of this page see the official documentation for details below we focus on the duckdb-specific parts refer to the externally hosted api reference for more information about our extensions to the jdbc specification or the below arrow methods startup shutdown in jdbc database connections are created through the standard java sql drivermanager class the driver should auto-register in the drivermanager if that does not work for some reason you can enforce registration like so class forname org duckdb duckdbdriver to create a duckdb connection call drivermanager with the jdbc duckdb jdbc url prefix like so import java sql connection import java sql drivermanager connection conn drivermanager getconnection jdbc duckdb to use duckdb-specific features such as the appender cast the object to a duckdbconnection import org duckdb duckdbconnection duckdbconnection conn duckdbconnection drivermanager getconnection jdbc duckdb when using the jdbc duckdb url alone an in-memory database is created note that for an in-memory database no data is persisted to disk i e all data is lost when you exit the java program if you would like to access or create a persistent database append its file name after the path for example if your database is stored in tmp my_database use the jdbc url jdbc duckdb tmp my_database to create a connection to it it is possible to open a duckdb database file in read-only mode this is for example useful if multiple java processes want to read the same database file at the same time to open an existing database file in read-only mode set the connection property duckdb read_only like so properties ro_prop new properties ro_prop setproperty duckdb read_only true connection conn_ro drivermanager getconnection jdbc duckdb tmp my_database ro_prop additional connections can be created using the drivermanager a more efficient mechanism is to call the duckdbconnection duplicate method like so connection conn2 duckdbconnection conn duplicate multiple connections are allowed but mixing read-write and read-only connections is unsupported configuring connections configuration options can be provided to change different settings of the database system note that many of these settings can be changed later on using pragma statements as well properties connectionproperties new properties connectionproperties setproperty temp_directory path to temp dir connection conn drivermanager getconnection jdbc duckdb tmp my_database connectionproperties querying duckdb supports the standard jdbc methods to send queries and retrieve result sets first a statement object has to be created from the connection this object can then be used to send queries using execute and executequery execute is meant for queries where no results are expected like create table or update etc and executequery is meant to be used for queries that produce results e g select below two examples see also the jdbc statement and resultset documentations create a table statement stmt conn createstatement stmt execute create table items item varchar value decimal 10 2 count integer insert two items into the table stmt execute insert into items values jeans 20 0 1 hammer 42 2 2 stmt close try resultset rs stmt executequery select from items while rs next system out println rs getstring 1 system out println rs getint 3 jeans 1 hammer 2 duckdb also supports prepared statements as per the jdbc api try preparedstatement p_stmt conn preparestatement insert into items values p_stmt setstring 1 chainsaw p_stmt setdouble 2 500 0 p_stmt setint 3 42 p_stmt execute more calls to execute possible do not use prepared statements to insert large amounts of data into duckdb see the data import documentation for better options arrow methods refer to the api reference for type signatures arrow export the following demonstrates exporting an arrow stream and consuming it using the java arrow bindings import org apache arrow memory rootallocator import org apache arrow vector ipc arrowreader import org duckdb duckdbresultset try var conn drivermanager getconnection jdbc duckdb var p_stmt conn preparestatement select from generate_series 2000 var resultset duckdbresultset p_stmt executequery var allocator new rootallocator try var reader arrowreader resultset arrowexportstream allocator 256 while reader loadnextbatch system out println reader getvectorschemaroot getvector generate_series p_stmt close arrow import the following demonstrates consuming an arrow stream from the java arrow bindings import org apache arrow memory rootallocator import org apache arrow vector ipc arrowreader import org duckdb duckdbconnection arrow stuff try var allocator new rootallocator arrowstreamreader reader null should not be null of course var arrow_array_stream arrowarraystream allocatenew allocator data exportarraystream allocator reader arrow_array_stream duckdb stuff try var conn duckdbconnection drivermanager getconnection jdbc duckdb conn registerarrowstream asdf arrow_array_stream run a query try var stmt conn createstatement var rs duckdbresultset stmt executequery select count from asdf while rs next system out println rs getint 1 streaming results result streaming is opt-in in the jdbc driver - by setting the jdbc_stream_results config to true before running a query the easiest way do that is to pass it in the properties object properties props new properties props setproperty duckdbdriver jdbc_stream_results string valueof true connection conn drivermanager getconnection jdbc duckdb props appender the appender is available in the duckdb jdbc driver via the org duckdb duckdbappender class the constructor of the class requires the schema name and the table name it is applied to the appender is flushed when the close method is called example import org duckdb duckdbconnection duckdbconnection conn duckdbconnection drivermanager getconnection jdbc duckdb statement stmt conn createstatement stmt execute create table tbl x bigint y float s varchar using try-with-resources to automatically close the appender at the end of the scope try var appender conn createappender duckdbconnection default_schema tbl appender beginrow appender append 10 appender append 3 2 appender append hello appender endrow appender beginrow appender append 20 appender append -8 1 appender append world appender endrow stmt close batch writer the duckdb jdbc driver offers batch write functionality the batch writer supports prepared statements to mitigate the overhead of query parsing note that the preferred method for bulk inserts is to use the appender due to its higher performance however when using the appender is not possbile the batch writer is available as alternative batch writer with prepared statements import org duckdb duckdbconnection duckdbconnection conn duckdbconnection drivermanager getconnection jdbc duckdb preparedstatement stmt conn preparestatement insert into test x y z values stmt setobject 1 1 stmt setobject 2 2 stmt setobject 3 3 stmt addbatch stmt setobject 1 4 stmt setobject 2 5 stmt setobject 3 6 stmt addbatch stmt executebatch stmt close batch writer with vanilla statements the batch writer also supports vanilla sql statements import org duckdb duckdbconnection duckdbconnection conn duckdbconnection drivermanager getconnection jdbc duckdb statement stmt conn createstatement stmt execute create table test x int y int z int stmt addbatch insert into test x y z values 1 2 3 stmt addbatch insert into test x y z values 4 5 6 stmt executebatch stmt close",
			"category": "Api",
			"url": "/docs/api/java",
			"blurb": "Installation The DuckDB Java JDBC API can be installed from Maven Central . Please see the installation page for..."
		},
		{
			"title": "Julia Package",
			"text": "the duckdb julia package provides a high-performance front-end for duckdb much like sqlite duckdb runs in-process within the julia client and provides a dbinterface front-end the package also supports multi-threaded execution it uses julia threads tasks for this purpose if you wish to run queries in parallel you must launch julia with multi-threading support by e g setting the julia_num_threads environment variable installation install duckdb as follows using pkg pkg add duckdb alternatively pkg add duckdb basics using duckdb create a new in-memory database con dbinterface connect duckdb db memory create a table dbinterface execute con create table integers i integer insert data using a prepared statement stmt dbinterface prepare con insert into integers values dbinterface execute stmt 42 query the database results dbinterface execute con select 42 a print results scanning dataframes the duckdb julia package also provides support for querying julia dataframes note that the dataframes are directly read by duckdb - they are not inserted or copied into the database itself if you wish to load data from a dataframe into a duckdb table you can run a create table as or insert into query using duckdb using dataframes create a new in-memory dabase con dbinterface connect duckdb db create a dataframe df dataframe a 1 2 3 b 42 84 42 register it as a view in the database duckdb register_data_frame con df my_df run a sql query over the dataframe results dbinterface execute con select from my_df print results appender api the duckdb julia package also supports the appender api which is much faster than using prepared statements or individual insert into statements appends are made in row-wise format for every column an append call should be made after which the row should be finished by calling flush after all rows have been appended close should be used to finalize the appender and clean up the resulting memory using duckdb dataframes dates db duckdb db create a table dbinterface execute db create or replace table data id int primary key value float timestamp timestamp date date create data to insert len 100 df dataframes dataframe id collect 1 len value rand len timestamp dates now dates second 1 len date dates today dates day 1 len append data by row appender duckdb appender db data for i in eachrow df for j in i duckdb append appender j end duckdb end_row appender end flush the appender after all rows duckdb flush appender duckdb close appender concurrency within a julia process tasks are able to concurrency read and write to the database as long as each task maintains its own connection to the database in the example below a single task is spawned to periodically read the database and many tasks are spawned to write to the database using both insert statements as well as the appender api using dates dataframes duckdb db duckdb db dbinterface connect db dbinterface execute db create or replace table data date timestamp id int function run_reader db create a duckdb connection specifically for this task conn dbinterface connect db while true println dbinterface execute conn select id count date as count max date as max_date from data group by id order by id dataframes dataframe threads sleep 1 end dbinterface close conn end spawn one reader task threads spawn run_reader db function run_inserter db id create a duckdb connection specifically for this task conn dbinterface connect db for i in 1 1000 threads sleep 0 01 duckdb execute conn insert into data values current_timestamp id end dbinterface close conn end spawn many insert tasks for i in 1 100 threads spawn run_inserter db 1 end function run_appender db id create a duckdb connection specifically for this task appender duckdb appender db data for i in 1 1000 threads sleep 0 01 row dates now dates utc id for j in row duckdb append appender j end duckdb end_row appender duckdb flush appender end duckdb close appender end spawn many appender tasks for i in 1 100 threads spawn run_appender db 2 end original julia connector credits to kimmolinna for the original duckdb julia connector",
			"category": "Api",
			"url": "/docs/api/julia",
			"blurb": "The DuckDB Julia package provides a high-performance front-end for DuckDB. Much like SQLite, DuckDB runs in-process..."
		},
		{
			"title": "Jupyter Notebooks",
			"text": "duckdb s python client can be used directly in jupyter notebooks with no additional configuration if desired however additional libraries can be used to simplify sql query development this guide will describe how to utilize those additional libraries see other guides in the python section for how to use duckdb and python together in this example we use the jupysql package this example workflow is also available as a google colab notebook library installation four additional libraries improve the duckdb experience in jupyter notebooks jupysql convert a jupyter code cell into a sql cell pandas clean table visualizations and compatibility with other analysis matplotlib plotting with python duckdb-engine duckdb sqlalchemy driver used by sqlalchemy to connect to duckdb optional run these pip install commands from the command line if jupyter notebook is not yet installed otherwise see google collab link above for an in-notebook example pip install duckdb install jupyter notebook note you can also install jupyterlab pip install jupyterlab pip install notebook install supporting libraries pip install jupysql pip install pandas pip install matplotlib pip install duckdb-engine library import and configuration open a jupyter notebook and import the relevant libraries connecting to duckdb natively to connect to duckdb run import duckdb import pandas as pd load_ext sql conn duckdb connect sql conn --alias duckdb connecting to duckdb via sqlalchemy using duckdb_engine alternatively you can connect to duckdb via sqlalchemy using duckdb_engine see the performance and feature differences import duckdb import pandas as pd no need to import duckdb_engine jupysql will auto-detect the driver needed based on the connection string import jupysql jupyter extension to create sql cells load_ext sql set configurations on jupysql to directly output data to pandas and to simplify the output that is printed to the notebook config sqlmagic autopandas true config sqlmagic feedback false config sqlmagic displaycon false connect jupysql to duckdb using a sqlalchemy-style connection string either connect to a new in-memory duckdb the default connection or a file backed db sql duckdb default sql duckdb memory sql duckdb path to file db the sql command and duckdb sql share the same default connection if you provide duckdb default as the sqlalchemy connection string querying duckdb single line sql queries can be run using sql at the start of a line query results will be displayed as a pandas df sql select off and flying as a_duckdb_column an entire jupyter cell can be used as a sql cell by placing sql at the start of the cell query results will be displayed as a pandas df sql select schema_name function_name from duckdb_functions order by all desc limit 5 to store the query results in a python variable use as an assignment operator this can be used with both the sql and sql jupyter magics sql res select off and flying as a_duckdb_column if the config sqlmagic autopandas true option is set the variable is a pandas dataframe otherwise it is a resultset that can be converted to pandas with the dataframe function querying pandas dataframes duckdb is able to find and query any dataframe stored as a variable in the jupyter notebook input_df pd dataframe from_dict i 1 2 3 j one two three the dataframe being queried can be specified just like any other table in the from clause sql output_df select sum i as total_i from input_df visualizing duckdb data the most common way to plot datasets in python is to load them using pandas and then use matplotlib or seaborn for plotting this approach requires loading all data into memory which is highly inefficient the plotting module in jupysql runs computations in the sql engine this delegates memory management to the engine and ensures that intermediate computations do not keep eating up memory efficiently plotting massive datasets install and load duckdb httpfs extension duckdb s httpfs extension allows parquet and csv files to be queried remotely over http these examples query a parquet file that contains historical taxi data from nyc using the parquet format allows duckdb to only pull the rows and columns into memory that are needed rather than downloading the entire file duckdb can be used to process local parquet files as well which may be desirable if querying the entire parquet file or running multiple queries that require large subsets of the file sql install httpfs load httpfs boxplot histogram to create a boxplot call sqlplot boxplot passing the name of the table and the column to plot in this case the name of the table is the url of the remotely stored parquet file sqlplot boxplot --table https d37ci6vzurychx cloudfront net trip-data yellow_tripdata_2021-01 parquet --column trip_distance boxplot of the trip_distance column now create a query that filters by the 90th percentile note the use of the --save and --no-execute functions this tells jupysql to store the query but skips execution it will be referenced in the next plotting call sql --save short-trips --no-execute select from https d37ci6vzurychx cloudfront net trip-data yellow_tripdata_2021-01 parquet where trip_distance 6 3 to create a histogram call sqlplot histogram and pass the name of the table the column to plot and the number of bins this uses --with short-trips so jupysql uses the query defined previously and therefore only plots a subset of the data sqlplot histogram --table short-trips --column trip_distance --bins 10 --with short-trips histogram of the trip_distance column summary you now have the ability to alternate between sql and pandas in a simple and highly performant way you can plot massive datasets directly through the engine avoiding both the download of the entire file and loading all of it into pandas in memory dataframes can be read as tables in sql and sql results can be output into dataframes happy analyzing",
			"category": "Python",
			"url": "/docs/guides/python/jupyter",
			"blurb": "DuckDB's Python client can be used directly in Jupyter notebooks with no additional configuration if desired...."
		},
		{
			"title": "Keywords and Identifiers",
			"text": "identifiers similarly to other sql dialects and programming languages identifiers in duckdb s sql are subject to several rules unquoted identifiers need to conform to a number of rules they must not be a reserved keyword see duckdb_keywords e g select 123 as select will fail they must not start with a number or special character e g select 123 as 1col is invalid they cannot contain whitespaces including tabs and newline characters identifiers can be quoted using double-quote characters quoted identifiers can use any keyword whitespace or special character e g select and are valid identifiers quotes themselves can be escaped by repeating the quote character e g to create an identifier named identifier x use identifier x deduplicating identifiers in some cases duplicate identifiers can occur e g column names may conflict when unnesting a nested data structure in these cases duckdb automatically deduplicates column names by renaming them according to the following rules for a column named name the first instance is not renamed subsequent instances are renamed to name _ count where count starts at 1 for example select from select unnest a 42 b a 88 b 99 recursive true a a_1 b int32 int32 int32 42 88 99 database names database names are subject to the rules for identifiers additionally it is best practive to avoid duckdb s two internal database schema names system and temp by default persistent databases are named after their filename without the extension therefore the filenames system db and temp db as well as system duckdb and temp duckdb result in the database names system and temp respectively if you need to attach to a database that has one of these names use an alias e g attach temp db as temp2 use temp2 numeric literals duckdb s sql dialect allows using the underscore character _ in numeric literals as an optional separator the rules for using underscores are as follows underscores are allowed in integer decimal hexadecimal and binary notation underscores can not be the first or last character in a literal underscores have to have an integer numeric part on either side of them i e there can not be multiple underscores in a row and not immediately before after a decimal or exponent examples select 100_000_000 -- 100000000 select 0xff_ff int -- 65535 select 1_2 1_2e0_1 -- 121 2 select 0b0_1_0_1 int -- 5 rules for case-sensitivity keywords and function names sql keywords and function names are case-insensitive in duckdb for example the following two queries are equivalent select cos pi as cosineofpi select cos pi as cosineofpi cosineofpi double -1 0 case-sensitivity of identifiers following the convention of the sql standard identifiers in duckdb are case-insensitive however each character s case uppercase lowercase is maintained as originally specified by the user even if a query uses different cases when referring to the identifier for example create table tbl as select cos pi as cosineofpi select cosineofpi from tbl cosineofpi double -1 0 to change this behavior set the preserve_identifier_case configuration option to false handling conflicts in case of a conflict when the same identifier is spelt with different cases one will be selected randomly for example create table t1 idfield int x int create table t2 idfield int y int select from t1 natural join t2 idfield x y int32 int32 int32 0 rows disabling preserving cases with the preserve_identifier_case configuration option set to false all identifiers are turned into lowercase set preserve_identifier_case false create table tbl as select cos pi as cosineofpi select cosineofpi from tbl cosineofpi double -1 0",
			"category": "SQL",
			"url": "/docs/sql/keywords_and_identifiers",
			"blurb": "Identifiers Similarly to other SQL dialects and programming languages, identifiers in DuckDB's SQL are subject to..."
		},
		{
			"title": "Known Python Issues",
			"text": "unfortunately there are some issues that are either beyond our control or are very elusive hard to track down below is a list of these issues that you might have to be aware of depending on your workflow numpy import multithreading when making use of multi threading and fetching results either directly as numpy arrays or indirectly through a pandas dataframe it might be necessary to ensure that numpy core multiarray is imported if this module has not been imported from the main thread and a different thread during execution attempts to import it this causes either a deadlock or a crash to avoid this it s recommended to import numpy core multiarray before starting up threads running explain renders newlines in jupyter and ipython when duckdb is run in jupyter notebooks or in the ipython shell the output of the explain statement contains hard line breaks n in 1 import duckdb duckdb sql explain select 42 as x out 1 explain_key explain_value varchar varchar physical_plan n projection n n x to work around this print the output of the explain function in 2 print duckdb sql select 42 as x explain projection x dummy_scan please also check out the jupyter guide for tips on using jupyter with jupysql error when importing the duckdb python package on windows when importing duckdb on windows the python runtime may return the following error import duckdb importerror dll load failed while importing duckdb the specified module could not be found the solution is to install the microsoft visual c redistributable package",
			"category": "Python",
			"url": "/docs/api/python/known_issues",
			"blurb": "Unfortunately there are some issues that are either beyond our control or are very elusive / hard to track down...."
		},
		{
			"title": "LIMIT Clause",
			"text": "limit is an output modifier logically it is applied at the very end of the query the limit clause restricts the amount of rows fetched the offset clause indicates at which position to start reading the values i e the first offset values are ignored note that while limit can be used without an order by clause the results might not be deterministic without the order by clause this can still be useful however for example when you want to inspect a quick snapshot of the data examples -- select the first 5 rows from the addresses table select from addresses limit 5 -- select the 5 rows from the addresses table starting at position 5 i e ignoring the first 5 rows select from addresses limit 5 offset 5 -- select the top 5 cities with the highest population select city count as population from addresses group by city order by population desc limit 5 syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/limit",
			"blurb": "LIMIT is an output modifier. Logically it is applied at the very end of the query. The LIMIT clause restricts the..."
		},
		{
			"title": "Lambda Functions",
			"text": "lambda functions enable the use of more complex and flexible expressions in queries duckdb supports several scalar functions that accept lambda functions as parameters in the form parameter1 parameter2 - expression if the lambda function has only one parameter then the parentheses can be omitted the parameters can have any names for example the following are all valid lambda functions param - param 1 s - contains concat s db duck x y - x y scalar functions that accept lambda functions function aliases description example result -- -- --- -- - list_transform list lambda array_transform apply list_apply array_apply returns a list that is the result of applying the lambda function to each element of the input list list_transform 4 5 6 x - x 1 5 6 7 list_filter list lambda array_filter filter constructs a list from those elements of the input list for which the lambda function returns true list_filter 4 5 6 x - x 4 5 6 list_reduce list lambda array_reduce reduce reduces all elements of the input list into a single value by executing the lambda function on a running result and the next list element list_reduce 4 5 6 x y - x y 15 nesting all scalar functions can be arbitrarily nested nested lambda functions to get all squares of even list elements select list_transform list_filter 0 1 2 3 4 5 x - x 2 0 y - y y 0 4 16 nested lambda function to add each element of the first list to the sum of the second list select list_transform 1 2 3 x - list_reduce 4 5 6 a b - a b x 16 17 18 scoping lambda functions confirm to scoping rules in the following order inner lambda parameters outer lambda parameters column names macro parameters create table tbl x int insert into tbl values 10 select apply 1 2 x - apply 4 x - x tbl x 1 x from tbl 15 16 indexes as parameters all lambda functions accept an optional extra parameter that represents the index of the current element this is always the last parameter of the lambda function and is 1-based i e the first element has index 1 get all elements that are larger than their index select list_filter 1 3 1 5 x i - x i 3 5 transform signature list_transform list lambda description list_transform returns a list that is the result of applying the lambda function to each element of the input list aliases array_transform apply list_apply array_apply number of parameters excluding indexes 1 return type defined by the return type of the lambda function examples incrementing each list element by one select list_transform 1 2 null 3 x - x 1 2 3 null 4 transforming strings select list_transform duck a b s - concat s db duckdb adb bdb combining lambda functions with other functions select list_transform 5 null 6 x - coalesce x 0 1 6 1 7 filter signature list_filter list lambda description constructs a list from those elements of the input list for which the lambda function returns true duckdb must be able to cast the lambda function s return type to bool aliases array_filter filter number of parameters excluding indexes 1 return type the same type as the input list examples filter out negative values select list_filter 5 -6 null 7 x - x 0 5 7 divisible by 2 and 5 select list_filter list_filter 2 4 3 1 20 10 3 30 x - x 2 0 y - y 5 0 20 10 30 in combination with range to construct lists select list_filter 1 2 3 4 x - x 1 from range 4 1 2 3 4 2 3 4 3 4 4 reduce signature list_reduce list lambda description the scalar function returns a single value that is the result of applying the lambda function to each element of the input list starting with the first element and then repeatedly applying the lambda function to the result of the previous application and the next element of the list the list must have at least one element aliases array_reduce reduce number of parameters excluding indexes 2 return type the type of the input list s elements examples sum of all list elements select list_reduce 1 2 3 4 x y - x y 10 only add up list elements if they are greater than 2 select list_reduce list_filter 1 2 3 4 x - x 2 x y - x y 7 concat all list elements select list_reduce duckdb is awesome x y - concat x y duckdb is awesome",
			"category": "Functions",
			"url": "/docs/sql/functions/lambda",
			"blurb": "Lambda functions enable the use of more complex and flexible expressions in queries. DuckDB supports several scalar..."
		},
		{
			"title": "Legacy Authentication Scheme for S3 API",
			"text": "prior to version 0 10 0 duckdb did not have a secrets manager hence the configuration of and authentication to s3 endpoints was handled via variables this page documents the legacy authentication scheme for the s3 api the recommended way to configuration and authentication of s3 endpoints is to use secrets legacy authentication scheme to be able to read or write from s3 the correct region should be set set s3_region us-east-1 optionally the endpoint can be configured in case a non-aws object storage server is used set s3_endpoint domain tld port if the endpoint is not ssl-enabled then run set s3_use_ssl false switching between path-style and vhost-style urls is possible using set s3_url_style path however note that this may also require updating the endpoint for example for aws s3 it is required to change the endpoint to s3 region amazonaws com after configuring the correct endpoint and region public files can be read to also read private files authentication credentials can be added set s3_access_key_id aws access key id set s3_secret_access_key aws secret access key alternatively session tokens are also supported and can be used instead set s3_session_token aws session token the aws extension allows for loading aws credentials per-request configuration aside from the global s3 configuration described above specific configuration values can be used on a per-request basis this allows for use of multiple sets of credentials regions etc these are used by including them on the s3 uri as query parameters all the individual configuration values listed above can be set as query parameters for instance select from s3 bucket file parquet s3_access_key_id accesskey s3_secret_access_key secretkey multiple configurations per query are also allowed select from s3 bucket file parquet s3_region region s3_session_token session_token t1 inner join s3 bucket file csv s3_access_key_id accesskey s3_secret_access_key secretkey t2 configuration some additional configuration options exist for the s3 upload though the default values should suffice for most use cases additionally most of the configuration options can be set via environment variables duckdb setting environment variable note ----------------------- ------------------------ ----------------------------------------- s3_region aws_region takes priority over aws_default_region s3_region aws_default_region s3_access_key_id aws_access_key_id s3_secret_access_key aws_secret_access_key s3_session_token aws_session_token s3_endpoint duckdb_s3_endpoint s3_use_ssl duckdb_s3_use_ssl",
			"category": "Httpfs",
			"url": "/docs/extensions/httpfs/s3api-legacy-authentication",
			"blurb": "Prior to version 0.10.0, DuckDB did not have a Secrets manager . Hence, the configuration of and authentication to S3..."
		},
		{
			"title": "List Tables",
			"text": "the show tables command can be used to obtain a list of all tables within the selected schema create table tbl i integer show tables name ------ tbl show or show all tables can be used to obtain a list of all tables within all attached databases and schemas create table tbl i integer create schema s1 create table s1 tbl v varchar show all tables database schema table_name column_names column_types temporary ---------- -------- ------------ -------------- -------------- ----------- memory main tbl i integer false memory s1 tbl v varchar false to view the schema of an individual table use the describe command see also the sql-standard information_schema views are also defined moreover duckdb defines sqlite_master and many postgresql system catalog tables for compatibility with sqlite and postgresql respectively",
			"category": "Meta",
			"url": "/docs/guides/meta/list_tables",
			"blurb": "The SHOW TABLES command can be used to obtain a list of all tables within the selected schema. CREATE TABLE tbl (i..."
		},
		{
			"title": "List Type",
			"text": "a list column encodes lists of values fields in the column can have values with different lengths but they must all have the same underlying type list s are typically used to store arrays of numbers but can contain any uniform data type including other list s and struct s list s are similar to postgresql s array type duckdb uses the list terminology but some array functions are provided for postgresql compatibility see the data types overview for a comparison between nested data types for storing fixed-length lists duckdb uses the array type creating lists lists can be created using the list_value expr function or the equivalent bracket notation expr the expressions can be constants or arbitrary expressions to create a list from a table column use the list aggregate function -- list of integers select 1 2 3 -- list of strings with a null value select duck goose null heron -- list of lists with null values select duck goose heron null frog toad -- create a list with the list_value function select list_value 1 2 3 -- create a table with an integer list column and a varchar list column create table list_table int_list int varchar_list varchar retrieving from lists retrieving one or more values from a list can be accomplished using brackets and slicing notation or through list functions like list_extract multiple equivalent functions are provided as aliases for compatibility with systems that refer to lists as arrays for example the function array_slice -- note that we wrap the list creation in parenthesis so that it happens first -- this is only needed in our basic examples here not when working with a list column -- for example this can t be parsed select a b c 1 example result ------------------------------------------- ----------- select a b c 3 c select a b c -1 c select a b c 2 1 c select list_extract a b c 3 c select a b c 1 2 a b select a b c 2 a b select a b c -2 b c select list_slice a b c 2 3 b c ordering the ordering is defined positionally null values compare greater than all other values and are considered equal to each other null comparisons at the top level null nested values obey standard sql null comparison rules comparing a null nested value to a non- null nested value produces a null result comparing nested value members however uses the internal nested value rules for null s and a null nested value member will compare above a non- null nested value member functions see nested functions",
			"category": "Data Types",
			"url": "/docs/sql/data_types/list",
			"blurb": "A LIST column encodes lists of values. Fields in the column can have values with different lengths, but they must all..."
		},
		{
			"title": "Literal Types",
			"text": "duckdb has literal types for representing integer and string literals in queries these have their own binding and conversion rules prior to version 0 10 0 integer and string literals behaved identically to the integer and varchar types integer literals integer_literal types can be implicitly converted to any integer type in which the value fits string literals string_literal instances can be implicitly converted to any other type for example we can compare string literals with dates select d 1992-01-01 as result from values date 1992-01-01 t d result boolean false however we cannot compare varchar values with dates select d 1992-01-01 varchar from values date 1992-01-01 t d -- binder error cannot compare values of type date and type varchar -- an explicit cast is required",
			"category": "Data Types",
			"url": "/docs/sql/data_types/literal_types",
			"blurb": "DuckDB has literal types for representing integer and string literals in queries. These have their own binding and..."
		},
		{
			"title": "Logical Operators",
			"text": "the following logical operators are available and or and not sql uses a three-valuad logic system with true false and null note that logical operators involving null do not always evaluate to null for example null and false will evaluate to false and null or true will evaluate to true below are the complete truth tables a b a and b a or b --- --- --- --- true true true true true false false true true null null true false false false false false null false null null null null null a not a --- --- true false false true null null the operators and and or are commutative that is you can switch the left and right operand without affecting the result",
			"category": "Expressions",
			"url": "/docs/sql/expressions/logical_operators",
			"blurb": "The following logical operators are available: AND , OR and NOT . SQL uses a three-valuad logic system with true ,..."
		},
		{
			"title": "Map Type",
			"text": "map s are similar to struct s in that they are an ordered list of entries where a key maps to a value however map s do not need to have the same keys present for each row and thus are suitable for other use cases map s are useful when the schema is unknown beforehand or when the schema varies per row their flexibility is a key differentiator map s must have a single type for all keys and a single type for all values keys and values can be any type and the type of the keys does not need to match the type of the values ex a map of varchar to int is valid map s may not have duplicate keys map s return an empty list if a key is not found rather than throwing an error as structs do in contrast struct s must have string keys but each key may have a value of a different type see the data types overview for a comparison between nested data types to construct a map use the bracket syntax preceded by the map keyword creating maps -- a map with varchar keys and integer values this returns key1 10 key2 20 key3 30 select map key1 10 key2 20 key3 30 -- alternatively use the map_from_entries function this returns key1 10 key2 20 key3 30 select map_from_entries key1 10 key2 20 key3 30 -- a map can be also created using two lists keys and values this returns key1 10 key2 20 key3 30 select map key1 key2 key3 10 20 30 -- a map can also use integer keys and numeric values this returns 1 42 001 5 -32 100 select map 1 42 001 5 -32 1 -- keys and or values can also be nested types -- this returns a b 1 1 2 2 c d 3 3 4 4 select map a b 1 1 2 2 c d 3 3 4 4 -- create a table with a map column that has integer keys and double values create table tbl col map integer double retrieving from maps map s use bracket notation for retrieving values selecting from a map returns a list rather than an individual value with an empty list meaning that the key was not found -- use bracket notation to retrieve a list containing the value at a key s location this returns 5 -- note that the expression in bracket notation must match the type of the map s key select map key1 5 key2 43 key1 -- to retrieve the underlying value use list selection syntax to grab the first element -- this returns 5 select map key1 5 key2 43 key1 1 -- if the element is not in the map an empty list will be returned returns -- note that the expression in bracket notation must match the type of the map s key else an error is returned select map key1 5 key2 43 key3 -- the element_at function can also be used to retrieve a map value this returns 5 select element_at map key1 5 key2 43 key1 comparison operators nested types can be compared using all the comparison operators these comparisons can be used in logical expressions for both where and having clauses as well as for creating boolean values the ordering is defined positionally in the same way that words can be ordered in a dictionary null values compare greater than all other values and are considered equal to each other at the top level null nested values obey standard sql null comparison rules comparing a null nested value to a non- null nested value produces a null result comparing nested value members however uses the internal nested value rules for null s and a null nested value member will compare above a non- null nested value member functions see nested functions",
			"category": "Data Types",
			"url": "/docs/sql/data_types/map",
			"blurb": "MAP s are similar to STRUCT s in that they are an ordered list of entries where a key maps to a value. However, MAP s..."
		},
		{
			"title": "Multiple Python Threads",
			"text": "this page demonstrates how to simultaneously insert into and read from a duckdb database across multiple python threads this could be useful in scenarios where new data is flowing in and an analysis should be periodically re-run note that this is all within a single python process see the faq for details on duckdb concurrency feel free to follow along in this google colab notebook setup first import duckdb and several modules from the python standard library note if using pandas add import pandas at the top of the script as well as it must be imported prior to the multi-threading then connect to a file-backed duckdb database and create an example table to store inserted data this table will track the name of the thread that completed the insert and automatically insert the timestamp when that insert occurred using the default expression import duckdb from threading import thread current_thread import random duckdb_con duckdb connect my_peristent_db duckdb use connect without parameters for an in-memory database duckdb_con duckdb connect duckdb_con execute create or replace table my_inserts thread_name varchar insert_time timestamp default current_timestamp reader and writer functions next define functions to be executed by the writer and reader threads each thread must use the cursor method to create a thread-local connection to the same duckdb file based on the original connection this approach also works with in-memory duckdb databases def write_from_thread duckdb_con create a duckdb connection specifically for this thread local_con duckdb_con cursor insert a row with the name of the thread insert_time is auto-generated thread_name str current_thread name result local_con execute insert into my_inserts thread_name values thread_name fetchall def read_from_thread duckdb_con create a duckdb connection specifically for this thread local_con duckdb_con cursor query the current row count thread_name str current_thread name results local_con execute select as thread_name count as row_counter current_timestamp from my_inserts thread_name fetchall print results create threads we define how many writers and readers to use and define a list to track all of the threads that will be created then create first writer and then reader threads next shuffle them so that they will be kicked off in a random order to simulate simultaneous writers and readers note that the threads have not yet been executed only defined write_thread_count 50 read_thread_count 5 threads create multiple writer and reader threads in the same process pass in the same connection as an argument for i in range write_thread_count threads append thread target write_from_thread args duckdb_con name write_thread_ str i for j in range read_thread_count threads append thread target read_from_thread args duckdb_con name read_thread_ str j shuffle the threads to simulate a mix of readers and writers random seed 6 set the seed to ensure consistent results when testing random shuffle threads run threads and show results now kick off all threads to run in parallel then wait for all of them to finish before printing out the results note that the timestamps of readers and writers are interspersed as expected due to the randomization kick off all threads in parallel for thread in threads thread start ensure all threads complete before printing final results for thread in threads thread join print duckdb_con execute select from my_inserts order by insert_time df",
			"category": "Python",
			"url": "/docs/guides/python/multiple_threads",
			"blurb": "This page demonstrates how to simultaneously insert into and read from a DuckDB database across multiple Python..."
		},
		{
			"title": "My Workload Is Slow",
			"text": "if you find that your workload in duckdb is slow we recommend performing the following checks more detailed instructions are linked for each point do you have enough memory duckdb works best if you have 5-10gb memory per cpu core are you using a fast disk network-attached disks can cause the workload to slow down especially for larger than memory workloads are you using indexes or constraints primary key unique etc if possible try disabling them which boosts load and update performance are you using the correct types for example use timestamp to encode datetime values are you reading from parquet files if so do they have row group sizes between 100k and 1m and file sizes between 100mb to 10gb does the query plan look right study it with explain is the workload running in parallel use htop or the operating system s task manager to observe this is duckdb using too many threads try limiting the amount of threads are you aware of other common issues if so please click the report issue button above and describe them along with their workarounds",
			"category": "Performance",
			"url": "/docs/guides/performance/my_workload_is_slow",
			"blurb": "If you find that your workload in DuckDB is slow, we recommend performing the following checks. More detailed..."
		},
		{
			"title": "MySQL Extension",
			"text": "the mysql extension allows duckdb to directly read and write data from to a running mysql instance the data can be queried directly from the underlying mysql database data can be loaded from mysql tables into duckdb tables or vice versa installing and loading to install the mysql extension run install mysql the extension is loaded automatically upon first use if you prefer to load it manually run load mysql reading data from mysql to make a mysql database accessible to duckdb use the attach command attach host localhost user root port 0 database mysql as mysqldb type mysql use mysqldb the connection string determines the parameters for how to connect to mysql as a set of key value pairs any options not provided are replaced by their default values as per the table below setting default ---------- -------------- host localhost user current user password database null port 0 socket null the tables in the mysql database can be read as if they were normal duckdb tables but the underlying data is read directly from mysql at query time show tables name varchar signed_integers select from signed_integers t s m i b int8 int16 int32 int32 int64 -128 -32768 -8388608 -2147483648 -9223372036854775808 127 32767 8388607 2147483647 9223372036854775807 null null null null null it might be desirable to create a copy of the mysql databases in duckdb to prevent the system from re-reading the tables from mysql continuously particularly for large tables data can be copied over from mysql to duckdb using standard sql for example create table duckdb_table as from mysqlscanner mysql_table writing data to mysql in addition to reading data from mysql create tables ingest data into mysql and make other modifications to a mysql database using standard sql queries this allows you to use duckdb to for example export data that is stored in a mysql database to parquet or read data from a parquet file into mysql below is a brief example of how to create a new table in mysql and load data into it attach host localhost user root port 0 database mysqlscanner as mysql_db type mysql_scanner create table mysql_db tbl id integer name varchar insert into mysql_db tbl values 42 duckdb many operations on mysql tables are supported all these operations directly modify the mysql database and the result of subsequent operations can then be read using mysql note that if modifications are not desired attach can be run with the read_only property which prevents making modifications to the underlying database for example attach host localhost user root port 0 database mysqlscanner as mysql_db type mysql_scanner read_only supported operations below is a list of supported operations create table create table mysql_db tbl id integer name varchar insert into insert into mysql_db tbl values 42 duckdb select select from mysql_db tbl id name int64 varchar 42 duckdb copy copy mysql_db tbl to data parquet copy mysql_db tbl from data parquet update update mysql_db tbl set name woohoo where id 42 delete delete from mysql_db tbl where id 42 alter table alter table mysql_db tbl add column k integer drop table drop table mysql_db tbl create view create view mysql_db v1 as select 42 create schema and drop schema create schema mysql_db s1 create table mysql_db s1 integers i int insert into mysql_db s1 integers values 42 select from mysql_db s1 integers i int32 42 drop schema mysql_db s1 transactions create table mysql_db tmp i integer begin insert into mysql_db tmp values 42 select from mysql_db tmp i int64 42 rollback select from mysql_db tmp i int64 0 rows note that ddl statements are not transactional in mysql settings name description default ------------------------------------ ---------------------------------------------------------------- --------- mysql_experimental_filter_pushdown whether or not to use filter pushdown currently experimental false mysql_tinyint1_as_boolean whether or not to convert tinyint 1 columns to boolean true mysql_debug_show_queries debug setting print all queries sent to mysql to stdout false mysql_bit1_as_boolean whether or not to convert bit 1 columns to boolean true schema cache to avoid having to continuously fetch schema data from mysql duckdb keeps schema information - such as the names of tables their columns etc - cached if changes are made to the schema through a different connection to the mysql instance such as new columns being added to a table the cached schema information might be outdated in this case the function mysql_clear_cache can be executed to clear the internal caches call mysql_clear_cache github repository span class github github span",
			"category": "Extensions",
			"url": "/docs/extensions/mysql",
			"blurb": "The mysql extension allows DuckDB to directly read and write data from/to a running MySQL instance. The data can be..."
		},
		{
			"title": "MySQL Import",
			"text": "to run a query directly on a running mysql database the mysql extension is required installation and loading the extension can be installed use the install sql command this only needs to be run once install mysql to load the mysql extension for usage use the load sql command load mysql usage after the mysql extension is installed you can attach to a mysql database using the following command attach host localhost user root port 0 database mysqlscanner as mysql_db type mysql_scanner read_only use mysqlscanner the string used by attach is a postgresql-style connection string not a mysql connection string it is a list of connection arguments provided in key value format below is a list of valid arguments any options not provided are replaced by their default values setting default ---------- -------------- host localhost user current user password database null port 0 socket null you can directly read and write the mysql database create table tbl id integer name varchar insert into tbl values 42 duckdb for a list of supported operations see the mysql extension documentation",
			"category": "Import",
			"url": "/docs/guides/import/query_mysql",
			"blurb": "To run a query directly on a running MySQL database, the mysql extension is required. Installation and Loading The..."
		},
		{
			"title": "NULL Values",
			"text": "null values are special values that are used to represent missing data in sql columns of any type can contain null values logically a null value can be seen as the value of this field is unknown -- insert a null value into a table create table integers i integer insert into integers values null null values have special semantics in many parts of the query as well as in many functions any comparison with a null value returns null including null null you can use is not distinct from to perform an equality comparison where null values compare equal to each other use is not null to check if a value is null select null null -- returns null select null is not distinct from null -- returns true select null is null -- returns true null and functions a function that has input argument as null usually returns null select cos null -- null the coalesce function is an exception to this it takes any number of arguments and returns for each row the first argument that is not null if all arguments are null coalesce also returns null select coalesce null null 1 -- 1 select coalesce 10 20 -- 10 select coalesce null null -- null the ifnull function is a two-argument version of coalesce select ifnull null default_string -- default_string select ifnull 1 default_string -- 1 null and conjunctions null values have special semantics in and or conjunctions for the ternary logic truth tables see the boolean type documentation null and aggregate functions null values are ignored in most aggregate functions aggregate functions that do not ignore null values include first last list and array_agg to exclude null values from those aggregate functions the filter clause can be used create table integers i integer insert into integers values 1 10 null select min i from integers -- 1 select max i from integers -- 10",
			"category": "Data Types",
			"url": "/docs/sql/data_types/nulls",
			"blurb": "The NULL value represents a missing value."
		},
		{
			"title": "Nested Functions",
			"text": "this section describes functions and operators for examining and manipulating nested values there are five nested data types array list map struct and union list functions in the descriptions l is the three element list 4 5 6 function aliases description example result -- -- --- -- - list index bracket notation serves as an alias for list_extract l 3 6 list begin end bracket notation with colon is an alias for list_slice l 2 3 5 6 list begin end step list_slice in bracket notation with an added step feature l - 2 4 6 array_pop_back list returns the list without the last element array_pop_back l 4 5 array_pop_front list returns the list without the first element array_pop_front l 5 6 flatten list_of_lists concatenate a list of lists into a single list this only flattens one level of the list see examples flatten 1 2 3 4 1 2 3 4 len list array_length return the length of the list len 1 2 3 3 list_aggregate list name list_aggr aggregate array_aggregate array_aggr executes the aggregate function name on the elements of list see the list aggregates section for more details list_aggregate 1 2 null min 1 list_any_value list returns the first non-null value in the list list_any_value null -3 -3 list_append list element array_append array_push_back appends element to list list_append 2 3 4 2 3 4 list_concat list1 list2 list_cat array_concat array_cat concatenates two lists list_concat 2 3 4 5 6 2 3 4 5 6 list_contains list element list_has array_contains array_has returns true if the list contains the element list_contains 1 2 null 1 true list_cosine_similarity list1 list2 compute the cosine similarity between two lists list_cosine_similarity 1 2 3 1 2 5 0 9759000729485332 list_distance list1 list2 calculates the euclidean distance between two points with coordinates given in two inputs lists of equal length list_distance 1 2 3 1 2 5 2 0 list_distinct list array_distinct removes all duplicates and nulls from a list does not preserve the original order list_distinct 1 1 null -3 1 5 1 5 -3 list_dot_product list1 list2 list_inner_product computes the dot product of two same-sized lists of numbers list_dot_product 1 2 3 1 2 5 20 0 list_extract list index list_element array_extract extract the index th 1-based value from the list list_extract l 3 6 list_filter list lambda array_filter filter constructs a list from those elements of the input list for which the lambda function returns true see the lambda functions page for more details list_filter l x - x 4 5 6 list_grade_up list array_grade_up works like sort but the results are the indexes that correspond to the position in the original list instead of the actual values list_grade_up 30 10 40 20 2 4 1 3 list_has_all list sub-list array_has_all returns true if all elements of sub-list exist in list list_has_all l 4 6 true list_has_any list1 list2 array_has_any returns true if any elements exist is both lists list_has_any 1 2 3 2 3 4 true list_intersect list1 list2 array_intersect returns a list of all the elements that exist in both l1 and l2 without duplicates list_intersect 1 2 3 2 3 4 2 3 list_position list element list_indexof array_position array_indexof returns the index of the element if the list contains the element list_contains 1 2 null 2 2 list_prepend element list array_prepend array_push_front prepends element to list list_prepend 3 4 5 6 3 4 5 6 list_reduce list lambda array_reduce reduce returns a single value that is the result of applying the lambda function to each element of the input list see the lambda functions page for more details list_reduce l x y - x y 15 list_resize list size value array_resize resizes the list to contain size elements initializes new elements with value or null if value is not set list_resize 1 2 3 5 0 1 2 3 0 0 list_reverse_sort list array_reverse_sort sorts the elements of the list in reverse order see the sorting lists section for more details about the null sorting order list_reverse_sort 3 6 1 2 6 3 2 1 list_reverse list array_reverse reverses the list list_reverse l 6 5 4 list_select value_list index_list array_select returns a list based on the elements selected by the index_list list_select 10 20 30 40 1 4 10 40 list_slice list begin end step array_slice list_slice with added step feature list_slice l 1 3 2 4 6 list_slice list begin end array_slice extract a sublist using slice conventions negative values are accepted see slicing list_slice l 2 3 5 6 list_sort list array_sort sorts the elements of the list see the sorting lists section for more details about the sorting order and the null sorting order list_sort 3 6 1 2 1 2 3 6 list_transform list lambda array_transform apply list_apply array_apply returns a list that is the result of applying the lambda function to each element of the input list see the lambda functions page for more details list_transform l x - x 1 5 6 7 list_unique list array_unique counts the unique elements of a list list_unique 1 1 null -3 1 5 3 list_value any list_pack create a list containing the argument values list_value 4 5 6 4 5 6 list_where value_list mask_list array_where returns a list with the boolean s in mask_list applied as a mask to the value_list list_where 10 20 30 40 true false false true 10 40 list_zip list1 list2 array_zip zips k list s to a new list whose length will be that of the longest list its elements are structs of k elements list_1 list_k elements missing will be replaced with null list_zip 1 2 3 4 5 6 list_1 1 list_2 3 list_3 5 list_1 2 list_2 4 list_3 6 unnest list unnests a list by one level note that this is a special function that alters the cardinality of the result see the unnest page for more details unnest 1 2 3 1 2 3 list operators the following operators are supported for lists operator description example result - -- --- - alias for list_intersect 1 2 3 4 5 2 5 5 6 2 5 alias for list_has_all where the list on the right of the operator is the sublist 1 2 3 4 3 4 3 true alias for list_has_all where the list on the left of the operator is the sublist 1 4 1 2 3 4 true alias for list_concat 1 2 3 4 5 6 1 2 3 4 5 6 alias for list_cosine_similarity 1 2 3 1 2 5 0 9759000729485332 - alias for list_distance 1 2 3 - 1 2 5 2 0 list comprehension python-style list comprehension can be used to compute expressions over elements in a list for example select lower x for x in strings from values hello world t strings -- hello world select upper x for x in strings if len x 0 from values hello world t strings -- hello world struct functions function description example result -- --- --- -- struct entry dot notation that serves as an alias for struct_extract from named struct s i 3 s string i 3 struct entry bracket notation that serves as an alias for struct_extract from named struct s i 3 s string i 3 struct idx bracket notation that serves as an alias for struct_extract from unnamed struct s tuples using an index 1-based row 42 84 1 42 row any create an unnamed struct tuple containing the argument values row i i 4 i 4 10 2 2 5 struct_extract struct entry extract the named entry from the struct struct_extract i 3 v2 3 v3 0 i 3 struct_extract struct idx extract the entry from an unnamed struct tuple using an index 1-based struct_extract row 42 84 1 42 struct_insert struct name any add field s value s to an existing struct with the argument values the entry name s will be the bound variable name s struct_insert a 1 b 2 a 1 b 2 struct_pack name any create a struct containing the argument values the entry name will be the bound variable name struct_pack i 4 s string i 4 s string map functions function description example result -- --- --- - cardinality map return the size of the map or the number of entries in the map cardinality map 4 2 a b 2 element_at map key return a list containing the value for a given key or an empty list if the key is not contained in the map the type of the key provided in the second parameter must match the type of the map s keys else an error is returned element_at map 100 5 42 43 100 42 map_entries map return a list of struct k v for each key-value pair in the map map_entries map 100 5 42 43 key 100 value 42 key 5 value 43 map_extract map key alias of element_at return a list containing the value for a given key or an empty list if the key is not contained in the map the type of the key provided in the second parameter must match the type of the map s keys else an error is returned map_extract map 100 5 42 43 100 42 map_from_entries struct k v returns a map created from the entries of the array map_from_entries k 5 v val1 k 3 v val2 5 val1 3 val2 map_keys map return a list of all keys in the map map_keys map 100 5 42 43 100 5 map_values map return a list of all values in the map map_values map 100 5 42 43 42 43 map returns an empty map map map entry alias for element_at map 100 5 a b 100 a union functions function description example result -- --- --- - union tag dot notation serves as an alias for union_extract union_value k hello k string union_extract union tag extract the value with the named tags from the union null if the tag is not currently selected union_extract s k hello union_value tag any create a single member union containing the argument value the tag of the value will be the bound variable name union_value k hello hello union k varchar union_tag union retrieve the currently selected tag of the union as an enum union_tag union_value k foo k range functions the functions range and generate_series create a list of values in the range between start and stop the start parameter is inclusive for the range function the stop parameter is exclusive while for generate_series it is inclusive based on the number of arguments the following variants exist range start stop step range start stop range stop generate_series start stop step generate_series start stop generate_series stop the default value of start is 0 and the default value of step is 1 select range 5 -- 0 1 2 3 4 select range 2 5 -- 2 3 4 select range 2 5 3 -- 2 select generate_series 5 -- 0 1 2 3 4 5 select generate_series 2 5 -- 2 3 4 5 select generate_series 2 5 3 -- 2 5 date ranges are also supported select from range date 1992-01-01 date 1992-03-01 interval 1 month range 1992-01-01 00 00 00 1992-02-01 00 00 00 slicing the function list_slice can be used to extract a sublist from a list the following variants exist list_slice list begin end list_slice list begin end array_slice list begin end step array_slice list begin end step list begin end list begin end step list is the list to be sliced begin is the index of the first element to be included in the slice when begin 0 the index is counted from the end of the list when begin 0 and -begin length begin is clamped to the beginning of the list when begin length the result is an empty list bracket notation when begin is omitted it defaults to the beginning of the list end is the index of the last element to be included in the slice when end 0 the index is counted from the end of the list when end length end is clamped to length when end begin the result is an empty list bracket notation when end is omitted it defaults to the end of the list when end is omitted and a step is provided end must be replaced with a - step optional is the step size between elements in the slice when step 0 the slice is reversed and begin and end are swapped must be non-zero select list_slice 1 2 3 4 5 2 4 -- 2 3 4 select 1 2 3 4 5 2 4 2 -- 2 4 select 1 2 3 4 5 4 2 -2 -- 4 2 select 1 2 3 4 5 -- 1 2 3 4 5 select 1 2 3 4 5 - 2 -- 1 3 5 select 1 2 3 4 5 - -2 -- 5 3 1 list aggregates the function list_aggregate allows the execution of arbitrary existing aggregate functions on the elements of a list its first argument is the list column its second argument is the aggregate function name e g min histogram or sum list_aggregate accepts additional arguments after the aggregate function name these extra arguments are passed directly to the aggregate function which serves as the second argument of list_aggregate select list_aggregate 1 2 -4 null min -- -4 select list_aggregate 2 4 8 42 sum -- 56 select list_aggregate 1 2 null 2 10 3 last -- 2 10 3 select list_aggregate 2 4 8 42 string_agg -- 2 4 8 42 the following is a list of existing rewrites rewrites simplify the use of the list aggregate function by only taking the list column as their argument list_avg list_var_samp list_var_pop list_stddev_pop list_stddev_samp list_sem list_approx_count_distinct list_bit_xor list_bit_or list_bit_and list_bool_and list_bool_or list_count list_entropy list_last list_first list_kurtosis list_kurtosis_pop list_min list_max list_product list_skewness list_sum list_string_agg list_mode list_median list_mad and list_histogram select list_min 1 2 -4 null -- -4 select list_sum 2 4 8 42 -- 56 select list_last 1 2 null 2 10 3 -- 2 10 3 array_to_string concatenates list array elements using an optional delimiter select array_to_string 1 2 3 - as str -- 1-2-3 -- this is equivalent to the following sql select list_aggr 1 2 3 string_agg - as str -- 1-2-3 sorting lists the function list_sort sorts the elements of a list either in ascending or descending order in addition it allows to provide whether null values should be moved to the beginning or to the end of the list by default if no modifiers are provided duckdb sorts asc nulls first i e the values are sorted in ascending order and null values are placed first this is identical to the default sort order of sqlite the default sort order can be changed using pragma statements list_sort leaves it open to the user whether they want to use the default sort order or a custom order list_sort takes up to two additional optional parameters the second parameter provides the sort order and can be either asc or desc the third parameter provides the null sort order and can be either nulls first or nulls last -- default sort order and default null sort order select list_sort 1 3 null 5 null -5 ---- null null -5 1 3 5 -- only providing the sort order select list_sort 1 3 null 2 asc ---- null 1 2 3 -- providing the sort order and the null sort order select list_sort 1 3 null 2 desc nulls first ---- null 3 2 1 list_reverse_sort has an optional second parameter providing the null sort order it can be either nulls first or nulls last -- default null sort order select list_sort 1 3 null 5 null -5 ---- null null -5 1 3 5 -- providing the null sort order select list_reverse_sort 1 3 null 2 nulls last ---- 3 2 1 null lambda functions duckdb supports lambda functions in the form parameter1 parameter2 - expression for details see the lambda functions page flatten the flatten function is a scalar function that converts a list of lists into a single list by concatenating each sub-list together note that this only flattens one level at a time not all levels of sub-lists -- convert a list of lists into a single list select flatten 1 2 3 4 ---- 1 2 3 4 -- if the list has multiple levels of lists -- only the first level of sub-lists is concatenated into a single list select flatten 1 2 3 4 5 6 7 8 ---- 1 2 3 4 5 6 7 8 in general the input to the flatten function should be a list of lists not a single level list however the behavior of the flatten function has specific behavior when handling empty lists and null values -- if the input list is empty return an empty list select flatten ---- -- if the entire input to flatten is null return null select flatten null ---- null -- if a list whose only entry is null is flattened return an empty list select flatten null ---- -- if the sub-list in a list of lists only contains null -- do not modify the sub-list -- note the extra set of parentheses vs the prior example select flatten null ---- null -- even if the only contents of each sub-list is null -- still concatenate them together -- note that no de-duplication occurs when flattening -- see list_distinct function for de-duplication select flatten null null ---- null null generate_subscripts the generate_subscript arr dim function generates indexes along the dim th dimension of array arr select generate_subscripts 4 5 6 1 as i i 1 2 3 related functions there are also aggregate functions list and histogram that produces lists and lists of structs the unnest function is used to unnest a list by one level",
			"category": "Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "This section describes functions and operators for examining and manipulating nested values. There are five nested..."
		},
		{
			"title": "Node.js API",
			"text": "this package provides a node js api for duckdb the api for this client is somewhat compliant to the sqlite node js client for easier transition for typescript wrappers see the duckdb-async project initializing load the package and create a database object const duckdb require duckdb const db new duckdb database memory or a file name for a persistent db all options as described on database configuration can be optionally supplied to the database constructor as second argument the third argument can be optionally supplied to get feedback on the given options const db new duckdb database memory access_mode read_write max_memory 512mb threads 4 err if err console error err running a query the following code snippet runs a simple query using the database all method db all select 42 as fortytwo function err res if err throw err console log res 0 fortytwo other available methods are each where the callback is invoked for each row run to execute a single statement without results and exec which can execute several sql commands at once but also does not return results all those commands can work with prepared statements taking the values for the parameters as additional arguments for example like so db all select integer as fortytwo string as hello 42 hello world function err res if err throw err console log res 0 fortytwo console log res 0 hello connections a database can have multiple connection s those are created using db connect const con db connect you can create multiple connections each with their own transaction context connection objects also contain shorthands to directly call run all and each with parameters and callbacks respectively for example con all select 42 as fortytwo function err res if err throw err console log res 0 fortytwo prepared statements from connections you can create prepared statements and only that using con prepare const stmt con prepare select integer as fortytwo to execute this statement you can call for example all on the stmt object stmt all 42 function err res if err throw err console log res 0 fortytwo you can also execute the prepared statement multiple times this is for example useful to fill a table with data con run create table a i integer const stmt con prepare insert into a values for let i 0 i 10 i stmt run i stmt finalize con all select from a function err res if err throw err console log res prepare can also take a callback which gets the prepared statement as an argument const stmt con prepare select integer as fortytwo function err stmt stmt all 42 function err res if err throw err console log res 0 fortytwo inserting data via arrow apache arrow can be used to insert data into duckdb without making a copy const arrow require apache-arrow const db new duckdb database memory const jsondata userid 1 id 1 title delectus aut autem completed false userid 1 id 2 title quis ut nam facilis et officia qui completed false note doesn t work on windows yet db exec install arrow load arrow err if err throw err const arrowtable arrow tablefromjson jsondata db register_buffer jsondatatable arrow tabletoipc arrowtable true err res if err throw err select from jsondatatable would return the entries in jsondata loading unsigned extensions to load unsigned extensions instantiate the database as follows db new duckdb database memory allow_unsigned_extensions true pages in this section",
			"category": "Nodejs",
			"url": "/docs/api/nodejs/overview",
			"blurb": "This package provides a Node.js API for DuckDB. The API for this client is somewhat compliant to the SQLite Node.js..."
		},
		{
			"title": "Node.js API",
			"text": "modules typedefs a name module_duckdb a duckdb summary duckdb is an embeddable sql olap database management system duckdb connection run sql params callback code void code all sql params callback code void code arrowipcall sql params callback code void code arrowipcstream sql params callback each sql params callback code void code stream sql params register_udf name return_type fun code void code prepare sql params callback code statement code exec sql params callback code void code register_udf_bulk name return_type callback code void code unregister_udf name return_type callback code void code register_buffer name array force callback code void code unregister_buffer name callback code void code close callback code void code statement sql get run sql params callback code void code all sql params callback code void code arrowipcall sql params callback code void code each sql params callback code void code finalize sql params callback code void code stream sql params columns code array lt columninfo gt code queryresult nextchunk nextipcbuffer asynciterator database close callback code void code close_internal callback code void code wait callback code void code serialize callback code void code parallelize callback code void code connect path code connection code interrupt callback code void code prepare sql code statement code run sql params callback code void code scanarrowipc sql params callback code void code each sql params callback code void code all sql params callback code void code arrowipcall sql params callback code void code arrowipcstream sql params callback code void code exec sql params callback code void code register_udf name return_type fun code this code register_buffer name code this code unregister_buffer name code this code unregister_udf name code this code registerreplacementscan fun code this code tokenize text code scripttokens code get tokentype error code number code open_readonly code number code open_readwrite code number code open_create code number code open_fullmutex code number code open_sharedcache code number code open_privatecache code number code a name module_duckdb connection a duckdb connection kind inner class of code duckdb code connection run sql params callback code void code all sql params callback code void code arrowipcall sql params callback code void code arrowipcstream sql params callback each sql params callback code void code stream sql params register_udf name return_type fun code void code prepare sql params callback code statement code exec sql params callback code void code register_udf_bulk name return_type callback code void code unregister_udf name return_type callback code void code register_buffer name array force callback code void code unregister_buffer name callback code void code close callback code void code a name module_duckdb connection run a connection run sql params callback code void code run a sql statement and trigger a callback when done kind instance method of code connection code param type --- --- sql params code code callback a name module_duckdb connection all a connection all sql params callback code void code run a sql query and triggers the callback once for all result rows kind instance method of code connection code param type --- --- sql params code code callback a name module_duckdb connection arrowipcall a connection arrowipcall sql params callback code void code run a sql query and serialize the result into the apache arrow ipc format requires arrow extension to be loaded kind instance method of code connection code param type --- --- sql params code code callback a name module_duckdb connection arrowipcstream a connection arrowipcstream sql params callback run a sql query returns a ipcresultstreamiterator that allows streaming the result into the apache arrow ipc format requires arrow extension to be loaded kind instance method of code connection code returns promise ipcresultstreamiterator param type --- --- sql params code code callback a name module_duckdb connection each a connection each sql params callback code void code runs a sql query and triggers the callback for each result row kind instance method of code connection code param type --- --- sql params code code callback a name module_duckdb connection stream a connection stream sql params kind instance method of code connection code param type --- --- sql params code code a name module_duckdb connection register_udf a connection register _ udf name return_type fun code void code register a user defined function kind instance method of code connection code note this follows the wasm udfs somewhat but is simpler because we can pass data much more cleanly param --- name return_type fun a name module_duckdb connection prepare a connection prepare sql params callback code statement code prepare a sql query for execution kind instance method of code connection code param type --- --- sql params code code callback a name module_duckdb connection exec a connection exec sql params callback code void code execute a sql query kind instance method of code connection code param type --- --- sql params code code callback a name module_duckdb connection register_udf_bulk a connection register _ udf _ bulk name return_type callback code void code register a user defined function kind instance method of code connection code param --- name return_type callback a name module_duckdb connection unregister_udf a connection unregister _ udf name return_type callback code void code unregister a user defined function kind instance method of code connection code param --- name return_type callback a name module_duckdb connection register_buffer a connection register _ buffer name array force callback code void code register a buffer to be scanned using the apache arrow ipc scanner requires arrow extension to be loaded kind instance method of code connection code param --- name array force callback a name module_duckdb connection unregister_buffer a connection unregister _ buffer name callback code void code unregister the buffer kind instance method of code connection code param --- name callback a name module_duckdb connection close a connection close callback code void code closes connection kind instance method of code connection code param --- callback a name module_duckdb statement a duckdb statement kind inner class of code duckdb code statement sql get run sql params callback code void code all sql params callback code void code arrowipcall sql params callback code void code each sql params callback code void code finalize sql params callback code void code stream sql params columns code array lt columninfo gt code a name module_duckdb statement sql a statement sql kind instance property of code statement code returns sql contained in statement field a name module_duckdb statement get a statement get not implemented kind instance method of code statement code a name module_duckdb statement run a statement run sql params callback code void code kind instance method of code statement code param type --- --- sql params code code callback a name module_duckdb statement all a statement all sql params callback code void code kind instance method of code statement code param type --- --- sql params code code callback a name module_duckdb statement arrowipcall a statement arrowipcall sql params callback code void code kind instance method of code statement code param type --- --- sql params code code callback a name module_duckdb statement each a statement each sql params callback code void code kind instance method of code statement code param type --- --- sql params code code callback a name module_duckdb statement finalize a statement finalize sql params callback code void code kind instance method of code statement code param type --- --- sql params code code callback a name module_duckdb statement stream a statement stream sql params kind instance method of code statement code param type --- --- sql params code code a name module_duckdb statement columns a statement columns code array lt columninfo gt code kind instance method of code statement code returns code array lt columninfo gt code - - array of column names and types a name module_duckdb queryresult a duckdb queryresult kind inner class of code duckdb code queryresult nextchunk nextipcbuffer asynciterator a name module_duckdb queryresult nextchunk a queryresult nextchunk kind instance method of code queryresult code returns data chunk a name module_duckdb queryresult nextipcbuffer a queryresult nextipcbuffer function to fetch the next result blob of an arrow ipc stream in a zero-copy way requires arrow extension to be loaded kind instance method of code queryresult code returns data chunk a name module_duckdb queryresult asynciterator a queryresult asynciterator kind instance method of code queryresult code a name module_duckdb database a duckdb database main database interface kind inner property of code duckdb code param description --- --- path path to database file or memory for in-memory database access_mode access mode config the configuration object callback callback function database close callback code void code close_internal callback code void code wait callback code void code serialize callback code void code parallelize callback code void code connect path code connection code interrupt callback code void code prepare sql code statement code run sql params callback code void code scanarrowipc sql params callback code void code each sql params callback code void code all sql params callback code void code arrowipcall sql params callback code void code arrowipcstream sql params callback code void code exec sql params callback code void code register_udf name return_type fun code this code register_buffer name code this code unregister_buffer name code this code unregister_udf name code this code registerreplacementscan fun code this code tokenize text code scripttokens code get a name module_duckdb database close a database close callback code void code closes database instance kind instance method of code database code param --- callback a name module_duckdb database close_internal a database close _ internal callback code void code internal method do not use call connection close instead kind instance method of code database code param --- callback a name module_duckdb database wait a database wait callback code void code triggers callback when all scheduled database tasks have completed kind instance method of code database code param --- callback a name module_duckdb database serialize a database serialize callback code void code currently a no-op provided for sqlite compatibility kind instance method of code database code param --- callback a name module_duckdb database parallelize a database parallelize callback code void code currently a no-op provided for sqlite compatibility kind instance method of code database code param --- callback a name module_duckdb database connect a database connect path code connection code create a new database connection kind instance method of code database code param description --- --- path the database to connect to either a file path or memory a name module_duckdb database interrupt a database interrupt callback code void code supposedly interrupt queries but currently does not do anything kind instance method of code database code param --- callback a name module_duckdb database prepare a database prepare sql code statement code prepare a sql query for execution kind instance method of code database code param --- sql a name module_duckdb database run a database run sql params callback code void code convenience method for connection run using a built-in default connection kind instance method of code database code param type --- --- sql params code code callback a name module_duckdb database scanarrowipc a database scanarrowipc sql params callback code void code convenience method for connection scanarrowipc using a built-in default connection kind instance method of code database code param type --- --- sql params code code callback a name module_duckdb database each a database each sql params callback code void code kind instance method of code database code param type --- --- sql params code code callback a name module_duckdb database all a database all sql params callback code void code convenience method for connection apply using a built-in default connection kind instance method of code database code param type --- --- sql params code code callback a name module_duckdb database arrowipcall a database arrowipcall sql params callback code void code convenience method for connection arrowipcall using a built-in default connection kind instance method of code database code param type --- --- sql params code code callback a name module_duckdb database arrowipcstream a database arrowipcstream sql params callback code void code convenience method for connection arrowipcstream using a built-in default connection kind instance method of code database code param type --- --- sql params code code callback a name module_duckdb database exec a database exec sql params callback code void code kind instance method of code database code param type --- --- sql params code code callback a name module_duckdb database register_udf a database register _ udf name return_type fun code this code register a user defined function convenience method for connection register_udf kind instance method of code database code param --- name return_type fun a name module_duckdb database register_buffer a database register _ buffer name code this code register a buffer containing serialized data to be scanned from duckdb convenience method for connection unregister_buffer kind instance method of code database code param --- name a name module_duckdb database unregister_buffer a database unregister _ buffer name code this code unregister a buffer convenience method for connection unregister_buffer kind instance method of code database code param --- name a name module_duckdb database unregister_udf a database unregister _ udf name code this code unregister a udf convenience method for connection unregister_udf kind instance method of code database code param --- name a name module_duckdb database registerreplacementscan a database registerreplacementscan fun code this code register a table replace scan function kind instance method of code database code param description --- --- fun replacement scan function a name module_duckdb database tokenize a database tokenize text code scripttokens code return positions and types of tokens in given text kind instance method of code database code param --- text a name module_duckdb database get a database get not implemented kind instance method of code database code a name module_duckdb tokentype a duckdb tokentype types of tokens return by tokenize kind inner property of code duckdb code a name module_duckdb error a duckdb error code number code check that errno attribute equals this to check for a duckdb error kind inner constant of code duckdb code a name module_duckdb open_readonly a duckdb open _ readonly code number code open database in readonly mode kind inner constant of code duckdb code a name module_duckdb open_readwrite a duckdb open _ readwrite code number code currently ignored kind inner constant of code duckdb code a name module_duckdb open_create a duckdb open _ create code number code currently ignored kind inner constant of code duckdb code a name module_duckdb open_fullmutex a duckdb open _ fullmutex code number code currently ignored kind inner constant of code duckdb code a name module_duckdb open_sharedcache a duckdb open _ sharedcache code number code currently ignored kind inner constant of code duckdb code a name module_duckdb open_privatecache a duckdb open _ privatecache code number code currently ignored kind inner constant of code duckdb code a name columninfo a columninfo code object code kind global typedef properties name type description --- --- --- name code string code column name type code typeinfo code column type a name typeinfo a typeinfo code object code kind global typedef properties name type description --- --- --- id code string code type id alias code string code sql type alias sql_type code string code sql type name a name duckdberror a duckdberror code object code kind global typedef properties name type description --- --- --- errno code number code -1 for duckdb errors message code string code error message code code string code duckdb_nodejs_error for duckdb errors errortype code string code duckdb error type code eg http io catalog a name httperror a httperror code object code kind global typedef extends code duckdberror code properties name type description --- --- --- statuscode code number code http response status code reason code string code http response reason response code string code http response body headers code object code http headers",
			"category": "Nodejs",
			"url": "/docs/api/nodejs/reference",
			"blurb": "Modules Typedefs <a name=module_duckdb> </a> duckdb Summary : DuckDB is an embeddable SQL OLAP Database Management..."
		},
		{
			"title": "Numeric Functions",
			"text": "numeric operators the table below shows the available mathematical operators for numeric types operator description example result - ----- -- - addition 2 3 5 - subtraction 2 - 3 -1 multiplication 2 3 6 float division 5 2 2 5 division 5 2 2 modulo remainder 5 4 1 exponent 3 4 81 exponent alias for 3 4 81 bitwise and 91 15 11 bitwise or 32 3 35 bitwise shift left 1 4 16 bitwise shift right 8 2 2 bitwise negation 15 -16 factorial of x 4 24 division and modulo operators there are two division operators and they are equivalent when at least one of the operands is a float or a double when both operands are integers performs floating points division 5 2 2 5 while performs integer division 5 2 2 supported types the modulo bitwise and negation and factorial operators work only on integral data types whereas the others are available for all numeric data types numeric functions the table below shows the available mathematical functions function description example result --- --- --- -- absolute value parentheses optional if operating on a column -2 2 abs x absolute value abs -17 4 17 4 acos x computes the arccosine of x acos 0 5 1 0471975511965976 add x y alias for x y add 2 3 5 asin x computes the arcsine of x asin 0 5 0 5235987755982989 atan x computes the arctangent of x atan 0 5 0 4636476090008061 atan2 y x computes the arctangent y x atan2 0 5 0 5 0 7853981633974483 bit_count x returns the number of bits that are set bit_count 31 5 cbrt x returns the cube root of the number cbrt 8 2 ceil x rounds the number up ceil 17 4 18 ceiling x rounds the number up alias of ceil ceiling 17 4 18 cos x computes the cosine of x cos 90 -0 4480736161291701 cot x computes the cotangent of x cot 0 5 1 830487721712452 degrees x converts radians to degrees degrees pi 180 divide x y alias for x y divide 5 2 2 even x round to next even number by rounding away from zero even 2 9 4 exp x computes e x exp 0 693 2 factorial x see operator computes the product of the current integer and all integers below it factorial 4 24 fdiv x y performs integer division x y but returns a double value fdiv 5 2 2 0 floor x rounds the number down floor 17 4 17 fmod x y calculates the modulo value always returns a double value fmod 5 2 1 0 gamma x interpolation of x-1 factorial so decimal inputs are allowed gamma 5 5 52 34277778455352 gcd x y computes the greatest common divisor of x and y gcd 42 57 3 greatest_common_divisor x y computes the greatest common divisor of x and y greatest_common_divisor 42 57 3 greatest x1 x2 selects the largest value greatest 3 2 4 4 4 isfinite x returns true if the floating point value is finite false otherwise isfinite 5 5 true isinf x returns true if the floating point value is infinite false otherwise isinf infinity float true isnan x returns true if the floating point value is not a number false otherwise isnan nan float true lcm x y computes the least common multiple of x and y lcm 42 57 798 least_common_multiple x y computes the least common multiple of x and y least_common_multiple 42 57 798 least x1 x2 selects the smallest value least 3 2 4 4 2 lgamma x computes the log of the gamma function lgamma 2 0 ln x computes the natural logarithm of x ln 2 0 693 log x computes the 10-log of x log 100 2 log10 x alias of log computes the 10-log of x log10 1000 3 log2 x computes the 2-log of x log2 8 3 multiply x y alias for x y multiply 2 3 6 nextafter x y return the next floating point value after x in the direction of y nextafter 1 float 2 float 1 0000001 pi returns the value of pi pi 3 141592653589793 pow x y computes x to the power of y pow 2 3 8 power x y alias of pow computes x to the power of y power 2 3 8 radians x converts degrees to radians radians 90 1 5707963267948966 random returns a random number between 0 and 1 random various round_even v numeric s int alias of roundbankers v s round to s decimal places using the rounding half to even rule values s 0 are allowed round_even 24 5 0 24 0 round v numeric s int round to s decimal places values s 0 are allowed round 42 4332 2 42 43 setseed x sets the seed to be used for the random function setseed 0 42 sign x returns the sign of x as -1 0 or 1 sign -349 -1 signbit x returns whether the signbit is set or not signbit -0 0 true sin x computes the sin of x sin 90 0 8939966636005579 sqrt x returns the square root of the number sqrt 9 3 subtract x y alias for x - y subtract 2 3 -1 tan x computes the tangent of x tan 90 -1 995200412208242 trunc x truncates the number trunc 17 4 17 xor x bitwise xor xor 17 5 20",
			"category": "Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "Numeric Operators The table below shows the available mathematical operators for numeric types. | Operator |..."
		},
		{
			"title": "Numeric Types",
			"text": "integer types the types tinyint smallint integer bigint and hugeint store whole numbers that is numbers without fractional components of various ranges attempts to store values outside of the allowed range will result in an error the types utinyint usmallint uinteger ubigint and uhugeint store whole unsigned numbers attempts to store negative numbers or values outside of the allowed range will result in an error name aliases min max -- -- ---- ---- tinyint int1 -128 127 smallint int2 short -32768 32767 integer int4 int signed -2147483648 2147483647 bigint int8 long -9223372036854775808 9223372036854775807 hugeint - -170141183460469231731687303715884105728 170141183460469231731687303715884105727 utinyint - 0 255 usmallint - 0 65535 uinteger - 0 4294967295 ubigint - 0 18446744073709551615 uhugeint - 0 340282366920938463463374607431768211455 the type integer is the common choice as it offers the best balance between range storage size and performance the smallint type is generally only used if disk space is at a premium the bigint and hugeint types are designed to be used when the range of the integer type is insufficient fixed-point decimals the data type decimal width scale also available under the alias numeric width scale represents an exact fixed-point decimal value when creating a value of type decimal the width and scale can be specified to define which size of decimal values can be held in the field the width field determines how many digits can be held and the scale determines the amount of digits after the decimal point for example the type decimal 3 2 can fit the value 1 23 but cannot fit the value 12 3 or the value 1 234 the default width and scale is decimal 18 3 if none are specified internally decimals are represented as integers depending on their specified width width internal size bytes --- --- --- 1-4 int16 2 5-9 int32 4 10-18 int64 8 19-38 int128 16 performance can be impacted by using too large decimals when not required in particular decimal values with a width above 19 are very slow as arithmetic involving the int128 type is much more expensive than operations involving the int32 or int64 types it is therefore recommended to stick with a width of 18 or below unless there is a good reason for why this is insufficient floating-point types the data types real and double precision are inexact variable-precision numeric types in practice these types are usually implementations of ieee standard 754 for binary floating-point arithmetic single and double precision respectively to the extent that the underlying processor operating system and compiler support it name aliases description -- -- -------- real float4 float single precision floating-point number 4 bytes double float8 double precision floating-point number 8 bytes inexact means that some values cannot be converted exactly to the internal format and are stored as approximations so that storing and retrieving a value might show slight discrepancies managing these errors and how they propagate through calculations is the subject of an entire branch of mathematics and computer science and will not be discussed here except for the following points if you require exact storage and calculations such as for monetary amounts use the decimal data type or its numeric alias instead if you want to do complicated calculations with these types for anything important especially if you rely on certain behavior in boundary cases infinity underflow you should evaluate the implementation carefully comparing two floating-point values for equality might not always work as expected on most platforms the real type has a range of at least 1e-37 to 1e 37 with a precision of at least 6 decimal digits the double type typically has a range of around 1e-307 to 1e 308 with a precision of at least 15 digits values that are too large or too small will cause an error rounding might take place if the precision of an input number is too high numbers too close to zero that are not representable as distinct from zero will cause an underflow error in addition to ordinary numeric values the floating-point types have several special values infinity -infinity nan these represent the ieee 754 special values infinity negative infinity and not-a-number respectively on a machine whose floating-point arithmetic does not follow ieee 754 these values will probably not work as expected when writing these values as constants in an sql command you must put quotes around them for example update table set x -infinity on input these strings are recognized in a case-insensitive manner functions see numeric functions and operators",
			"category": "Data Types",
			"url": "/docs/sql/data_types/numeric",
			"blurb": "Numeric types are used to store numbers, and come in different shapes and sizes."
		},
		{
			"title": "ODBC API - Linux",
			"text": "a driver manager is required to manage communication between applications and the odbc driver we tested and support unixodbc that is a complete odbc driver manager for linux users can install it from the command line debian so flavors sudo apt get install unixodbc fedora so flavors sudo yum install unixodbc or sudo dnf install unixodbc step 1 download odbc driver duckdb releases the odbc driver as asset for linux download it from a href https github com duckdb duckdb releases download v site currentduckdbversion duckdb_odbc-linux-amd64 zip odbc linux asset a that contains the following artifacts libduckdb_odbc so the duckdb driver compiled to ubuntu 16 04 unixodbc_setup sh a setup script to aid the configuration on linux step 2 extracting odbc artifacts run unzip to extract the files to a permanent directory mkdir duckdb_odbc unzip duckdb_odbc-linux-amd64 zip -d duckdb_odbc step 3 configuring with unixodbc the unixodbc_setup sh script aids the configuration of the duckdb odbc driver it is based on the unixodbc package that provides some commands to handle the odbc setup and test like odbcinst and isql in a terminal window change to the duckdb_odbc permanent directory and run the following commands with level options -u or -s either to configure duckdb odbc user-level odbc setup -u the -u option based on the user home directory to setup the odbc init files unixodbc_setup sh -u p s the default configuration consists of a database memory system-level odbc setup -s the -s changes the system level files that will be visible for all users because of that it requires root privileges sudo unixodbc_setup sh -s p s the default configuration consists of a database memory show usage --help the option --help shows the usage of unixodbc_setup sh that provides alternative options for a customer configuration like -db and -d unixodbc_setup sh --help usage unixodbc_setup sh level options example unixodbc_setup sh -u -db database_path -d driver_path libduckdb_odbc so level -s system-level using sudo to configure duckdb odbc at the system-level changing the files etc odbc inst ini -u user-level configuring the duckdb odbc at the user-level changing the files odbc inst ini options -db database_path the duckdb database file path the default is memory if not provided -d driver_path the driver file path i e the path for libduckdb_odbc so the default is using the base script directory step 4 optional configure the odbc driver the odbc setup on linux is based on files the well-known odbc ini and odbcinst ini these files can be placed at the system etc directory or at the user home directory home user shortcut as the dm prioritizes the user configuration files and then the system files the odbc ini file the odbc ini contains the dsns for the drivers which can have specific knobs an example of odbc ini with duckdb would be duckdb driver duckdb driver database memory duckdb between the brackets is a dsn for the duckdb driver it describes the driver s name and other configurations will be placed at the odbcinst ini database it describes the database name used by duckdb and it can also be a file path to a db in the system the odbcinst ini file the odbcinst ini contains general configurations for the odbc installed drivers in the system a driver section starts with the driver name between brackets and then it follows specific configuration knobs belonging to that driver an example of odbcinst ini with the duckdb driver would be odbc trace yes tracefile tmp odbctrace duckdb driver driver home user duckdb_odbc libduckdb_odbc so odbc it is the dm configuration section trace it enables the odbc trace file using the option yes tracefile the absolute system file path for the odbc trace file duckdb driver the section of the duckdb installed driver driver the absolute system file path of the duckdb driver",
			"category": "Odbc",
			"url": "/docs/api/odbc/linux",
			"blurb": "A driver manager is required to manage communication between applications and the ODBC driver. We tested and support..."
		},
		{
			"title": "ODBC API - Overview",
			"text": "the odbc open database connectivity is a c-style api that provides access to different flavors of database management systems dbmss the odbc api consists of the driver manager dm and the odbc drivers the dm is part of the system library e g unixodbc which manages the communications between the user applications and the odbc drivers typically applications are linked against the dm which uses data source name dsn to look up the correct odbc driver the odbc driver is a dbms implementation of the odbc api which handles all the internals of that dbms the dm maps user application calls of odbc functions to the correct odbc driver that performs the specified function and returns the proper values duckdb odbc driver duckdb supports the odbc version 3 0 according to the core interface conformance we release the odbc driver as assets for linux and windows users can download them from the latest release of duckdb operating systems operating system supported versions -------------------- -------------------------------- linux ubuntu 20 04 or later microsoft windows microsoft windows 10 or later pages in this section",
			"category": "Odbc",
			"url": "/docs/api/odbc/overview",
			"blurb": "The ODBC (Open Database Connectivity) is a C-style API that provides access to different flavors of Database..."
		},
		{
			"title": "ODBC API - Windows",
			"text": "the microsoft windows requires an odbc driver manager to manage communication between applications and the odbc drivers the dm on windows is provided in a dll file odbccp32 dll and other files and tools for detailed information checkout out the common odbc component files step 1 download odbc driver duckdb releases the odbc driver as asset for windows download it from a href https github com duckdb duckdb releases download v site currentduckdbversion duckdb_odbc-windows-amd64 zip windows asset a that contains the following artifacts duckdb_odbc dll the duckdb driver compiled for windows duckdb_odbc_setup dll a setup dll used by the windows odbc data source administrator tool odbc_install exe an installation script to aid the configuration on windows step 2 extracting odbc artifacts unzip the file to a permanent directory e g duckdb_odbc an example with powershell and unzip command would be mkdir duckdb_odbc unzip duckdb_odbc-linux-amd64 zip -d duckdb_odbc step 3 odbc windows installer the odbc_install exe aids the configuration of the duckdb odbc driver on windows it depends on the odbccp32 dll that provides functions to configure the odbc registry entries inside the permanent directory e g duckdb_odbc double-click on the odbc_install exe windows administrator privileges is required in case of a non-administrator a user account control shall display step 4 configure the odbc driver the odbc_install exe adds a default dsn configuration into the odbc registries with a default database memory dsn windows setup after the installation it is possible to change the default dsn configuration or add a new one using the windows odbc data source administrator tool odbcad32 exe it also can be launched thought the windows start default duckdb dsn the newly installed dsn is visible on the system dsn in the windows odbc data source administrator tool windows odbc config tool changing duckdb dsn when selecting the default dsn i e duckdb or adding a new configuration the following setup window will display duckdb windows dsn setup this window allows you to set the dsn and the database file path associated with that dsn more detailed windows setup the odbc setup on windows is based on registry keys see registry entries for odbc components the odbc entries can be placed at the current user registry key hkcu or the system registry key hklm we have tested and used the system entries based on hklm- software- odbc the odbc_install exe changes this entry that has two subkeys odbc ini and odbcinst ini the odbc ini is where users usually insert dsn registry entries for the drivers for example the dsn registry for duckdb would look like this hklm- software- odbc- odbc ini- duckdb the odbcinst ini contains one entry for each odbc driver and other keys predefined for windows odbc configuration",
			"category": "Odbc",
			"url": "/docs/api/odbc/windows",
			"blurb": "The Microsoft Windows requires an ODBC Driver Manager to manage communication between applications and the ODBC..."
		},
		{
			"title": "ODBC API - macOS",
			"text": "a driver manager is required to manage communication between applications and the odbc driver we tested and support unixodbc that is a complete odbc driver manager for macos and linux users can install it from the command line brew brew install unixodbc step 1 download odbc driver duckdb releases the odbc driver as asset for macos download it from the a href https github com duckdb duckdb releases download v site currentduckdbversion duckdb_odbc-osx-universal zip odbc macos asset a that contains the following artifacts libduckdb_odbc dylib the duckdb driver compiled to macos with intel and apple silicon support step 2 extracting odbc artifacts run unzip to extract the files to a permanent directory mkdir duckdb_odbc unzip duckdb_odbc-osx-universal zip -d duckdb_odbc step 3 configure the odbc driver the odbc ini or odbc ini file the odbc ini contains the dsns for the drivers which can have specific knobs an example of odbc ini with duckdb would be duckdb driver duckdb driver database memory duckdb between the brackets is a dsn for the duckdb driver it describes the driver s name and other configurations will be placed at the odbcinst ini database it describes the database name used by duckdb and it can also be a file path to a db in the system the odbcinst ini file the odbcinst ini contains general configurations for the odbc installed drivers in the system a driver section starts with the driver name between brackets and then it follows specific configuration knobs belonging to that driver an example of odbcinst ini with the duckdb driver would be odbc trace yes tracefile tmp odbctrace duckdb driver driver user user duckdb_odbc libduckdb_odbc dylib odbc it is the dm configuration section trace it enables the odbc trace file using the option yes tracefile the absolute system file path for the odbc trace file duckdb driver the section of the duckdb installed driver driver the absolute system file path of the duckdb driver step 4 optional test the odbc driver after the configuration for validate the installation it is possible to use an odbc client unixodbc use a command line tool called isql use the dsn defined in odbc ini as a parameter of isql isql duckdb --------------------------------------- connected sql-statement help tablename echo string quit --------------------------------------- sql select 42 ------------ 42 ------------ 42 ------------ sqlrowcount returns -1 1 rows fetched",
			"category": "Odbc",
			"url": "/docs/api/odbc/macos",
			"blurb": "A driver manager is required to manage communication between applications and the ODBC driver. We tested and support..."
		},
		{
			"title": "ORDER BY Clause",
			"text": "order by is an output modifier logically it is applied near the very end of the query just prior to limit or offset if present the order by clause sorts the rows on the sorting criteria in either ascending or descending order in addition every order clause can specify whether null values should be moved to the beginning or to the end the order by clause may contain one or more expressions separated by commas an error will be thrown if no expressions are included since the order by clause should be removed in that situation the expressions may begin with either an arbitrary scalar expression which could be a column name a column position number ex 1 note that it is 1-indexed or the keyword all each expression can optionally be followed by an order modifier asc or desc default is asc and or a null order modifier nulls first or nulls last default is nulls last order by all the all keyword indicates that the output should be sorted by every column in order from left to right the direction of this sort may be modified using either order by all asc or order by all desc and or nulls first or nulls last note that all may not be used in combination with other expressions in the order by clause - it must be by itself see examples below null order modifier by default if no modifiers are provided duckdb sorts asc nulls last i e the values are sorted in ascending order and null values are placed last this is identical to the default sort order of postgresql the default sort order can be changed with the following configuration options using asc nulls last as default the default sorting order was a breaking change in version 0 8 0 prior to 0 8 0 duckdb sorted using asc nulls first -- change the default null sorting order to either nulls first and nulls last set default_null_order nulls first -- change the default sorting order to either desc or asc set default_order desc collations text is sorted using the binary comparison collation by default which means values are sorted on their binary utf8 values while this works well for ascii text e g for english language data the sorting order can be incorrect for other languages for this purpose duckdb provides collations for more information on collations see the collation page examples all examples use this example table create or replace table addresses as select 123 quack blvd as address ducktown as city 11111 as zip union all select 111 duck duck goose ln ducktown 11111 union all select 111 duck duck goose ln duck town 11111 union all select 111 duck duck goose ln duck town 11111-0001 -- select the addresses ordered by city name using the default null order and default order select from addresses order by city -- select the addresses ordered by city name in descending order with nulls at the end select from addresses order by city desc nulls last -- order by city and then by zip code both using the default orderings select from addresses order by city zip -- order by city using german collation rules select from addresses order by city collate de order by all examples -- order from left to right by address then by city then by zip in ascending order select from addresses order by all address city zip ------------------------ ----------- ------------ 111 duck duck goose ln duck town 11111 111 duck duck goose ln duck town 11111-0001 111 duck duck goose ln ducktown 11111 123 quack blvd ducktown 11111 -- order from left to right by address then by city then by zip in descending order select from addresses order by all desc address city zip ------------------------ ----------- ------------ 123 quack blvd ducktown 11111 111 duck duck goose ln ducktown 11111 111 duck duck goose ln duck town 11111-0001 111 duck duck goose ln duck town 11111 syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/orderby",
			"blurb": "ORDER BY is an output modifier. Logically it is applied near the very end of the query (just prior to LIMIT or OFFSET..."
		},
		{
			"title": "Official Extensions",
			"text": "list of official extensions extension name description aliases --- ----- -- arrow span class github github span a zero-copy data integration between apache arrow and duckdb autocomplete adds support for autocomplete in the shell aws span class github github span provides features that depend on the aws sdk azure span class github github span adds a filesystem abstraction for azure blob storage to duckdb excel adds support for excel-like format strings fts adds support for full-text search indexes httpfs adds support for reading and writing files over an http s or s3 connection http https s3 iceberg span class github github span adds support for apache iceberg icu adds support for time zones and collations using the icu library inet adds support for ip-related data types and functions jemalloc overwrites system allocator with jemalloc json adds support for json operations mysql span class github github span adds support for reading from and writing to a mysql database parquet adds support for reading and writing parquet files postgres span class github github span adds support for reading from and writing to a postgres database postgres_scanner spatial span class github github span geospatial extension that adds support for working with spatial data and functions sqlite span class github github span adds support for reading from and writing to sqlite database files sqlite_scanner sqlite3 substrait span class github github span adds support for the substrait integration tpcds adds tpc-ds data generation and query support tpch adds tpc-h data generation and query support default extensions different duckdb clients ship a different set of extensions we summarize the main distributions in the table below extension name cli duckdb org cli homebrew python r java node js ------ ------ ------ --- --- --- --- --- autocomplete yes yes excel yes fts yes yes httpfs yes icu yes yes yes yes yes json yes yes yes yes yes parquet yes yes yes yes yes yes tpcds yes tpch yes yes the jemalloc extension s availability is based on the operating system it is a built-in extension on linux and macos versions while on windows it is not available",
			"category": "Extensions",
			"url": "/docs/extensions/official_extensions",
			"blurb": "List of Official Extensions | Extension Name | Description | Aliases | |---|-----|--| | arrow <span class=github>..."
		},
		{
			"title": "Output Formats",
			"text": "the mode dot command may be used to change the appearance of the tables returned in the terminal output in addition to customizing the appearance these modes have additional benefits this can be useful for presenting duckdb output elsewhere by redirecting the terminal output to a file using the insert mode will build a series of sql statements that can be used to insert the data at a later point the markdown mode is particularly useful for building documentation and the latex mode is useful for writing academic papers mode description -------------- ---------------------------------------------- ascii columns rows delimited by 0x1f and 0x1e box tables using unicode box-drawing characters csv comma-separated values column output in columns see width duckbox tables with extensive features html html table code insert sql insert statements for table json results in a json array jsonlines results in a ndjson latex latex tabular environment code line one value per line list values delimited by markdown markdown table format quote escape answers as for sql table ascii-art table tabs tab-separated values tcl tcl list elements trash no output mode markdown select quacking intensifies as incoming_ducks incoming_ducks ---------------------- quacking intensifies the output appearance can also be adjusted with the separator command if using an export mode that relies on a separator csv or tabs for example the separator will be reset when the mode is changed for example mode csv will set the separator to a comma using separator will then convert the output to be pipe-separated mode csv select 1 as col_1 2 as col_2 union all select 10 as col1 20 as col_2 col_1 col_2 1 2 10 20 separator select 1 as col_1 2 as col_2 union all select 10 as col1 20 as col_2 col_1 col_2 1 2 10 20",
			"category": "Cli",
			"url": "/docs/api/cli/output_formats",
			"blurb": "The .mode dot command may be used to change the appearance of the tables returned in the terminal output. In addition..."
		},
		{
			"title": "PIVOT Statement",
			"text": "the pivot statement allows distinct values within a column to be separated into their own columns the values within those new columns are calculated using an aggregate function on the subset of rows that match each distinct value duckdb implements both the sql standard pivot syntax and a simplified pivot syntax that automatically detects the columns to create while pivoting pivot_wider may also be used in place of the pivot keyword the unpivot statement is the inverse of the pivot statement simplified pivot syntax the full syntax diagram is below but the simplified pivot syntax can be summarized using spreadsheet pivot table naming conventions as pivot dataset on columns using values group by rows order by columns-with-order-directions limit number-of-rows the on using and group by clauses are each optional but they may not all be omitted example data all examples use the dataset produced by the queries below create table cities country varchar name varchar year int population int insert into cities values nl amsterdam 2000 1005 insert into cities values nl amsterdam 2010 1065 insert into cities values nl amsterdam 2020 1158 insert into cities values us seattle 2000 564 insert into cities values us seattle 2010 608 insert into cities values us seattle 2020 738 insert into cities values us new york city 2000 8015 insert into cities values us new york city 2010 8175 insert into cities values us new york city 2020 8772 from cities country name year population --------- --------------- ------ ------------ nl amsterdam 2000 1005 nl amsterdam 2010 1065 nl amsterdam 2020 1158 us seattle 2000 564 us seattle 2010 608 us seattle 2020 738 us new york city 2000 8015 us new york city 2010 8175 us new york city 2020 8772 pivot on and using use the pivot statement below to create a separate column for each year and calculate the total population in each the on clause specifies which column s to split into separate columns it is equivalent to the columns parameter in a spreadsheet pivot table the using clause determines how to aggregate the values that are split into separate columns this is equivalent to the values parameter in a spreadsheet pivot table if the using clause is not included it defaults to count pivot cities on year using sum population country name 2000 2010 2020 --------- --------------- ------ ------ ------ nl amsterdam 1005 1065 1158 us seattle 564 608 738 us new york city 8015 8175 8772 in the above example the sum aggregate is always operating on a single value if we only want to change the orientation of how the data is displayed without aggregating use the first aggregate function in this example we are pivoting numeric values but the first function works very well for pivoting out a text column this is something that is difficult to do in a spreadsheet pivot table but easy in duckdb this query produces a result that is identical to the one above pivot cities on year using first population pivot on using and group by by default the pivot statement retains all columns not specified in the on or using clauses to include only certain columns and further aggregate specify columns in the group by clause this is equivalent to the rows parameter of a spreadsheet pivot table in the below example the name column is no longer included in the output and the data is aggregated up to the country level pivot cities on year using sum population group by country country 2000 2010 2020 --------- ------ ------ ------ nl 1005 1065 1158 us 8579 8783 9510 in filter for on clause to only create a separate column for specific values within a column in the on clause use an optional in expression let s say for example that we wanted to forget about the year 2020 for no particular reason pivot cities on year in 2000 2010 using sum population group by country country 2000 2010 --------- ------ ------ nl 1005 1065 us 8579 8783 multiple expressions per clause multiple columns can be specified in the on and group by clauses and multiple aggregate expressions can be included in the using clause multiple on columns and on expressions multiple columns can be pivoted out into their own columns duckdb will find the distinct values in each on clause column and create one new column for all combinations of those values a cartesian product in the below example all combinations of unique countries and unique cities receive their own column some combinations may not be present in the underlying data so those columns are populated with null values pivot cities on country name using sum population year nl_amsterdam nl_new york city nl_seattle us_amsterdam us_new york city us_seattle ------ -------------- ------------------ ------------ -------------- ------------------ ------------ 2000 1005 null null null 8015 564 2010 1065 null null null 8175 608 2020 1158 null null null 8772 738 to pivot only the combinations of values that are present in the underlying data use an expression in the on clause multiple expressions and or columns may be provided here country and name are concatenated together and the resulting concatenations each receive their own column any arbitrary non-aggregating expression may be used in this case concatenating with an underscore is used to imitate the naming convention the pivot clause uses when multiple on columns are provided like in the prior example pivot cities on country _ name using sum population year nl_amsterdam us_new york city us_seattle ------ -------------- ------------------ ------------ 2000 1005 8015 564 2010 1065 8175 608 2020 1158 8772 738 multiple using expressions an alias may also be included for each expression in the using clause it will be appended to the generated column names after an underscore _ this makes the column naming convention much cleaner when multiple expressions are included in the using clause in this example both the sum and max of the population column are calculated for each year and are split into separate columns pivot cities on year using sum population as total max population as max group by country country 2000_total 2000_max 2010_total 2010_max 2020_total 2020_max --------- ------------ ---------- ------------ ---------- ------------ ---------- nl 1005 1005 1065 1065 1158 1158 us 8579 8015 8783 8175 9510 8772 multiple group by columns multiple group by columns may also be provided note that column names must be used rather than column positions 1 2 etc and that expressions are not supported in the group by clause pivot cities on year using sum population group by country name country name 2000 2010 2020 --------- --------------- ------ ------ ------ nl amsterdam 1005 1065 1158 us seattle 564 608 738 us new york city 8015 8175 8772 using pivot within a select statement the pivot statement may be included within a select statement as a cte a common table expression or with clause or a subquery this allows for a pivot to be used alongside other sql logic as well as for multiple pivot s to be used in one query no select is needed within the cte the pivot keyword can be thought of as taking its place with pivot_alias as pivot cities on year using sum population group by country select from pivot_alias a pivot may be used in a subquery and must be wrapped in parentheses note that this behavior is different than the sql standard pivot as illustrated in subsequent examples select from pivot cities on year using sum population group by country pivot_alias multiple pivots each pivot can be treated as if it were a select node so they can be joined together or manipulated in other ways for example if two pivot statements share the same group by expression they can be joined together using the columns in the group by clause into a wider pivot from pivot cities on year using sum population group by country year_pivot join pivot cities on name using sum population group by country name_pivot using country country 2000 2010 2020 amsterdam new york city seattle --------- ------ ------ ------ ----------- --------------- --------- nl 1005 1065 1158 3228 null null us 8579 8783 9510 null 24962 1910 internals pivoting is implemented as a combination of sql query re-writing and a dedicated physicalpivot operator for higher performance each pivot is implemented as set of aggregations into lists and then the dedicated physicalpivot operator converts those lists into column names and values additional pre-processing steps are required if the columns to be created when pivoting are detected dynamically which occurs when the in clause is not in use duckdb like most sql engines requires that all column names and types be known at the start of a query in order to automatically detect the columns that should be created as a result of a pivot statement it must be translated into multiple queries enum types are used to find the distinct values that should become columns each enum is then injected into one of the pivot statement s in clauses after the in clauses have been populated with enum s the query is re-written again into a set of aggregations into lists for example pivot cities on year using sum population is initially translated into create temporary type __pivot_enum_0_0 as enum select distinct year varchar from cities order by year pivot cities on year in __pivot_enum_0_0 using sum population and finally translated into select country name list year list population_sum from select country name year sum population as population_sum from cities group by all group by all this produces the result country name list year list population_sum --------- --------------- -------------------- ---------------------- nl amsterdam 2000 2010 2020 1005 1065 1158 us seattle 2000 2010 2020 564 608 738 us new york city 2000 2010 2020 8015 8175 8772 the physicalpivot operator converts those lists into column names and values to return this result country name 2000 2010 2020 --------- --------------- ------ ------ ------ nl amsterdam 1005 1065 1158 us seattle 564 608 738 us new york city 8015 8175 8772 simplified pivot full syntax diagram below is the full syntax diagram of the pivot statement sql standard pivot syntax the full syntax diagram is below but the sql standard pivot syntax can be summarized as from dataset pivot values for column_1 in in_list column_2 in in_list group by rows unlike the simplified syntax the in clause must be specified for each column to be pivoted if you are interested in dynamic pivoting the simplified syntax is recommended note that no commas separate the expressions in the for clause but that value and group by expressions must be comma-separated examples this example uses a single value expression a single column expression and a single row expression from cities pivot sum population for year in 2000 2010 2020 group by country country 2000 2010 2020 --------- ------ ------ ------ nl 1005 1065 1158 us 8579 8783 9510 this example is somewhat contrived but serves as an example of using multiple value expressions and multiple columns in the for clause from cities pivot sum population as total count population as count for year in 2000 2010 country in nl us name 2000_nl_total 2000_nl_count 2000_us_total 2000_us_count 2010_nl_total 2010_nl_count 2010_us_total 2010_us_count -- - - - - - - - - amsterdam 1005 1 null 0 1065 1 null 0 seattle null 0 564 1 null 0 608 1 new york city null 0 8015 1 null 0 8175 1 sql standard pivot full syntax diagram below is the full syntax diagram of the sql standard version of the pivot statement",
			"category": "Statements",
			"url": "/docs/sql/statements/pivot",
			"blurb": "The PIVOT statement allows values within a column to be separated into their own columns."
		},
		{
			"title": "Parquet Encryption",
			"text": "starting with version 0 10 0 duckdb supports reading and writing encrypted parquet files duckdb broadly follows the parquet modular encryption specification with some limitations reading and writing encrypted files using the pragma add_parquet_key function named encryption keys of 128 192 or 256 bits can be added to a session these keys are stored in-memory pragma add_parquet_key key128 0123456789112345 pragma add_parquet_key key192 012345678911234501234567 pragma add_parquet_key key256 01234567891123450123456789112345 writing encrypted parquet files after specifying the key e g key256 files can be encrypted as follows copy tbl to tbl parquet encryption_config footer_key key256 reading encrpyted parquet files an encrypted parquet file using a specific key e g key256 can then be read as follows copy tbl from tbl parquet encryption_config footer_key key256 or select from read_parquet tbl parquet encryption_config footer_key key256 limitations duckdb s parquet encryption currently has the following limitations it is not compatible with the encryption of e g pyarrow until the missing details are implemented duckdb encrypts the footer and all columns using the footer_key the parquet specification allows encryption of individual columns with different keys e g copy tbl to tbl parquet encryption_config footer_key key256 column_keys key256 col0 col1 however this is unsupported at the moment and will cause an error to be thrown for now not implemented error parquet encryption_config column_keys not yet implemented performance implications note that encryption has some performance implications without encryption reading writing the lineitem table from tpc-h at sf1 which is 6m rows and 15 columns from to a parquet file takes 0 26 and 0 99 seconds respectively with encryption this takes 0 64 and 2 21 seconds both approximately 2 5 slower than the unencrypted version",
			"category": "Parquet",
			"url": "/docs/data/parquet/encryption",
			"blurb": "Starting with version 0.10.0, DuckDB supports reading and writing encrypted Parquet files. DuckDB broadly follows the..."
		},
		{
			"title": "Parquet Export",
			"text": "to export the data from a table to a parquet file use the copy statement copy tbl to output parquet format parquet the result of queries can also be directly exported to a parquet file copy select from tbl to output parquet format parquet the flags for setting compression row group size etc are listed in the reading and writing parquet files page",
			"category": "Import",
			"url": "/docs/guides/import/parquet_export",
			"blurb": "To export the data from a table to a Parquet file, use the COPY statement. COPY tbl TO 'output.parquet' (FORMAT..."
		},
		{
			"title": "Parquet Import",
			"text": "to read data from a parquet file use the read_parquet function in the from clause of a query select from read_parquet input parquet to create a new table using the result from a query use create table as from a select statement create table new_tbl as select from read_parquet input parquet to load data into an existing table from a query use insert into from a select statement insert into tbl select from read_parquet input parquet alternatively the copy statement can also be used to load data from a parquet file into an existing table copy tbl from input parquet format parquet for additional options see the parquet loading reference",
			"category": "Import",
			"url": "/docs/guides/import/parquet_import",
			"blurb": "To read data from a Parquet file, use the read_parquet function in the FROM clause of a query. SELECT * FROM..."
		},
		{
			"title": "Parquet Tips",
			"text": "below is a collection of tips to help when dealing with parquet files tips for reading parquet files use union_by_name when loading files with different schemas the union_by_name option can be used to unify the schema of files that have different or missing columns for files that do not have certain columns null values are filled in select from read_parquet flights parquet union_by_name true tips for writing parquet files enabling per_thread_output if the final number of parquet files is not important writing one file per thread can significantly improve performance using a glob pattern upon read or a hive partitioning structure are good ways to transparently handle multiple files copy from generate_series 10_000_000 to test parquet format parquet per_thread_output true selecting a row_group_size the row_group_size parameter specifies the minimum number of rows in a parquet row group with a minimum value equal to duckdb s vector size currently 2048 but adjustable when compiling duckdb and a default of 122 880 a parquet row group is a partition of rows consisting of a column chunk for each column in the dataset compression algorithms are only applied per row group so the larger the row group size the more opportunities to compress the data duckdb can read parquet row groups in parallel even within the same file and uses predicate pushdown to only scan the row groups whose metadata ranges match the where clause of the query however there is some overhead associated with reading the metadata in each group a good approach would be to ensure that within each file the total number of row groups is at least as large as the number of cpu threads used to query that file more row groups beyond the thread count would improve the speed of highly selective queries but slow down queries that must scan the whole file like aggregations -- write a query to a parquet file with a different row_group_size copy from generate_series 100000 to row-groups parquet format parquet row_group_size 100000 see the performance guide on file formats for more tips",
			"category": "Parquet",
			"url": "/docs/data/parquet/tips",
			"blurb": "Below is a collection of tips to help when dealing with Parquet files. Tips for Reading Parquet Files Use..."
		},
		{
			"title": "Partitioned Writes",
			"text": "examples -- write a table to a hive partitioned data set of parquet files copy orders to orders format parquet partition_by year month -- write a table to a hive partitioned data set of csv files allowing overwrites copy orders to orders format csv partition_by year month overwrite_or_ignore 1 partitioned writes when the partition_by clause is specified for the copy statement the files are written in a hive partitioned folder hierarchy the target is the name of the root directory in the example above orders the files are written in-order in the file hierarchy currently one file is written per thread to each directory orders year 2021 month 1 data_1 parquet data_2 parquet month 2 data_1 parquet year 2022 month 11 data_1 parquet data_2 parquet month 12 data_1 parquet the values of the partitions are automatically extracted from the data note that it can be very expensive to write many partitions as many files will be created the ideal partition count depends on how large your data set is writing data into many small partitions is expensive it is generally recommended to have at least 100mb of data per partition overwriting by default the partitioned write will not allow overwriting existing directories use the overwrite_or_ignore option to allow overwriting an existing directory filename pattern by default files will be named data_0 parquet or data_0 csv with the flag filename_pattern a pattern with i or uuid can be defined to create specific filenames i will be replaced by an index uuid will be replaced by a 128 bits long uuid -- write a table to a hive partitioned data set of parquet files with an index in the filename copy orders to orders format parquet partition_by year month overwrite_or_ignore filename_pattern orders_ i -- write a table to a hive partitioned data set of parquet files with unique filenames copy orders to orders format parquet partition_by year month overwrite_or_ignore filename_pattern file_ uuid",
			"category": "Partitioning",
			"url": "/docs/data/partitioning/partitioned_writes",
			"blurb": "Examples -- write a table to a hive partitioned data set of Parquet files COPY orders TO 'orders' (FORMAT PARQUET,..."
		},
		{
			"title": "Pattern Matching",
			"text": "there are four separate approaches to pattern matching provided by duckdb the traditional sql like operator the more recent similar to operator added in sql 1999 a glob operator and posix-style regular expressions like the like expression returns true if the string matches the supplied pattern as expected the not like expression returns false if like returns true and vice versa an equivalent expression is not string like pattern if pattern does not contain percent signs or underscores then the pattern only represents the string itself in that case like acts like the equals operator an underscore _ in pattern stands for matches any single character a percent sign matches any sequence of zero or more characters like pattern matching always covers the entire string therefore if it s desired to match a sequence anywhere within a string the pattern must start and end with a percent sign some examples abc like abc -- true abc like a -- true abc like _b_ -- true abc like c -- false abc like c -- false abc like c -- true abc not like c -- false the keyword ilike can be used instead of like to make the match case-insensitive according to the active locale abc ilike c -- true abc not ilike c -- false to search within a string for a character that is a wildcard or _ the pattern must use an escape clause and an escape character to indicate the wildcard should be treated as a literal character instead of a wildcard see an example below additionally the function like_escape has the same functionality as a like expression with an escape clause but using function syntax see the text functions docs for details -- search for strings with a then a literal percent sign then c a c like a c escape -- true azc like a c escape -- false -- case insensitive ilike with escape a c ilike a c escape --true there are also alternative characters that can be used as keywords in place of like expressions these enhance postgresql compatibility like-style postgresql-style --- --- like not like ilike not ilike similar to the similar to operator returns true or false depending on whether its pattern matches the given string it is similar to like except that it interprets the pattern using a regular expression like like the similar to operator succeeds only if its pattern matches the entire string this is unlike common regular expression behavior where the pattern can match any part of the string a regular expression is a character sequence that is an abbreviated definition of a set of strings a regular set a string is said to match a regular expression if it is a member of the regular set described by the regular expression as with like pattern characters match string characters exactly unless they are special characters in the regular expression language but regular expressions use different special characters than like does some examples abc similar to abc -- true abc similar to a -- false abc similar to b d -- true abc similar to b c -- false abc not similar to abc -- false there are also alternative characters that can be used as keywords in place of similar to expressions these follow posix syntax similar to-style posix-style --- --- similar to not similar to glob the glob operator returns true or false if the string matches the glob pattern the glob operator is most commonly used when searching for filenames that follow a specific pattern for example a specific file extension use the question mark wildcard to match any single character and use the asterisk to match zero or more characters in addition use bracket syntax to match any single character contained within the brackets or within the character range specified by the brackets an exclamation mark may be used inside the first bracket to search for a character that is not contained within the brackets to learn more visit the glob programming wikipedia page some examples best txt glob txt -- true best txt glob txt -- true best txt glob txt -- false best txt glob abc est txt -- true best txt glob a-z est txt -- true -- the bracket syntax is case sensitive best txt glob a-z est txt -- false best txt glob a-za-z est txt -- true -- the applies to all characters within the brackets best txt glob a-za-z est txt -- false -- to negate a glob operator negate the entire expression -- not glob is not valid syntax not best txt glob txt -- false three tildes may also be used in place of the glob keyword glob-style symbolic-style --- --- glob glob function to find filenames the glob pattern matching syntax can also be used to search for filenames using the glob table function it accepts one parameter the path to search which may include glob patterns -- search the current directory for all files select from glob file --------------- duckdb exe test csv test json test parquet test2 csv test2 parquet todos json regular expressions function description example result --- --- --- -- regexp_extract_all string regex group 0 split the string along the regex and extract all occurrences of group regexp_extract_all hello_world a-z _ 1 hello world regexp_extract string pattern name_list if string contains the regexp pattern returns the capturing groups as a struct with corresponding names from name_list regexp_extract 2023-04-15 d - d - d y m d y 2023 m 04 d 15 regexp_extract string pattern idx if string contains the regexp pattern returns the capturing group specified by optional parameter idx regexp_extract hello_world a-z _ 1 hello regexp_full_match string regex returns true if the entire string matches the regex regexp_full_match anabanana an false regexp_matches string pattern returns true if string contains the regexp pattern false otherwise regexp_matches anabanana an true regexp_replace string pattern replacement if string contains the regexp pattern replaces the matching part with replacement regexp_replace hello lo - he-lo regexp_split_to_array string regex alias of string_split_regex splits the string along the regex regexp_split_to_array hello world 42 hello world 42 the regexp_matches function is similar to the similar to operator however it does not require the entire string to match instead regexp_matches returns true if the string merely contains the pattern unless the special tokens and are used to anchor the regular expression to the start and end of the string below are some examples regexp_matches abc abc -- true regexp_matches abc abc -- true regexp_matches abc a -- true regexp_matches abc a -- false regexp_matches abc b d -- true regexp_matches abc b c -- true regexp_matches abc b c -- false regexp_matches abc i a -- true options for regular expression functions the regexp_matches and regexp_replace functions also support the following options option description --- --- c case-sensitive matching i case-insensitive matching l match literals instead of regular expression tokens m n p newline sensitive matching s non-newline sensitive matching g global replace only available for regexp_replace regexp_matches abcd abc c -- false regexp_matches abcd abc i -- true regexp_matches ab cd l -- true regexp_matches hello nworld hello world p -- false regexp_matches hello nworld hello world s -- true using regexp_matches the regexp_matches operator will be optimized to the like operator when possible to achieve best performance the s option case-sensitive matching should be passed if applicable note that by default the re2 library doesn t match to newline original optimized equivalent --- --- regexp_matches hello world hello s prefix hello world hello regexp_matches hello world world s suffix hello world world regexp_matches hello world hello world s like hello_world regexp_matches hello world he rld s like he rld using regexp_replace the regexp_replace function can be used to replace the part of a string that matches the regexp pattern with a replacement string the notation d where d is a number indicating the group can be used to refer to groups captured in the regular expression in the replacement string note that by default regexp_replace only replaces the first occurrence of the regular expression to replace all occurrences use the global replace g flag some examples for using regexp_replace regexp_replace abc b c x -- axc regexp_replace abc b c x g -- axx regexp_replace abc b c 1 1 1 1 -- abbbbc regexp_replace abc c 1e -- abe regexp_replace abc a b 2 1 -- bac using regexp_extract the regexp_extract function is used to extract a part of a string that matches the regexp pattern a specific capturing group within the pattern can be extracted using the idx parameter if idx is not specified it defaults to 0 extracting the first match with the whole pattern regexp_extract abc b -- abc regexp_extract abc b 0 -- abc regexp_extract abc b 1 -- empty regexp_extract abc a-z b 1 -- a regexp_extract abc a-z b 2 -- b if ids is a list of strings then regexp_extract will return the corresponding capture groups as fields of a struct regexp_extract 2023-04-15 d - d - d y m d -- y 2023 m 04 d 15 regexp_extract 2023-04-15 07 59 56 d - d - d d d d y m d -- y 2023 m 04 d 15 regexp_extract duckdb_0_7_1 w _ d _ d tool major minor fix -- error if the number of column names is less than the number of capture groups then only the first groups are returned if the number of column names is greater then an error is generated the re2 library duckdb uses re2 as its regular expression engine for more information see the re2 docs",
			"category": "Functions",
			"url": "/docs/sql/functions/patternmatching",
			"blurb": "There are four separate approaches to pattern matching provided by DuckDB: the traditional SQL LIKE operator, the..."
		},
		{
			"title": "Performance Guide",
			"text": "duckdb aims to automatically achieve high performance by using well-chosen default configurations and having a forgiving architecture of course there are still opportunities for tuning the system for specific workloads the performance guide s page contain guidelines and tips for achieving good performance when loading and processing data with duckdb the guides include several microbenchmarks you may find details about these on the benchmarks page pages in this section",
			"category": "Performance",
			"url": "/docs/guides/performance/overview",
			"blurb": "DuckDB aims to automatically achieve high performance by using well-chosen default configurations and having a..."
		},
		{
			"title": "PostgreSQL Extension",
			"text": "the postgres extension allows duckdb to directly read and write data from a running postgres database instance the data can be queried directly from the underlying postgres database data can be loaded from postgres tables into duckdb tables or vice versa see the official announcement for implementation details and background installing and loading to install the postgres extension run install postgres the extension is loaded automatically upon first use if you prefer to load it manually run load postgres connecting to make a postgresql database accessible to duckdb use the attach command -- connect to the public schema of the postgres instance running on localhost attach as postgres_db type postgres -- connect to the postgres instance with the given parameters attach dbname postgres user postgres host 127 0 0 1 as db type postgres the attach command takes as input either a libpq connection string or a postgresql uri below are some example connection strings and commonly used parameters a full list of available parameters can be found in the postgres documentation dbname postgresscanner host localhost port 5432 dbname mydb connect_timeout 10 name description default ---------- -------------------------------------- ---------------- host name of host to connect to localhost hostaddr host ip address localhost port port number 5432 user postgres user name os user name password postgres password dbname database name user passfile name of file passwords are stored in pgpass an example uri is postgresql username hostname dbname postgres connection information can also be specified with environment variables this can be useful in a production environment where the connection information is managed externally and passed in to the environment export pgpassword secret export pghost localhost export pguser owner export pgdatabase mydatabase duckdb attach as p type postgres usage the tables in the postgresql database can be read as if they were normal duckdb tables but the underlying data is read directly from postgres at query time show tables name varchar uuids select from uuids u uuid 6d3d2541-710b-4bde-b3af-4711738636bf null 00000000-0000-0000-0000-000000000001 ffffffff-ffff-ffff-ffff-ffffffffffff it might be desirable to create a copy of the postgres databases in duckdb to prevent the system from re-reading the tables from postgres continuously particularly for large tables data can be copied over from postgres to duckdb using standard sql for example create table duckdb_table as from postgres_db postgres_tbl writing data to postgres in addition to reading data from postgres the extension allows you to create tables ingest data into postgres and make other modifications to a postgres database using standard sql queries this allows you to use duckdb to for example export data that is stored in a postgres database to parquet or read data from a parquet file into postgres below is a brief example of how to create a new table in postgres and load data into it attach dbname postgresscanner as postgres_db type postgres create table postgres_db tbl id integer name varchar insert into postgres_db tbl values 42 duckdb many operations on postgres tables are supported all these operations directly modify the postgres database and the result of subsequent operations can then be read using postgres note that if modifications are not desired attach can be run with the read_only property which prevents making modifications to the underlying database for example attach dbname postgresscanner as postgres_db type postgres read_only below is a list of supported operations create table create table postgres_db tbl id integer name varchar insert into insert into postgres_db tbl values 42 duckdb select select from postgres_db tbl id name int64 varchar 42 duckdb copy copy postgres_db tbl to data parquet copy postgres_db tbl from data parquet update update postgres_db tbl set name woohoo where id 42 delete delete from postgres_db tbl where id 42 alter table alter table postgres_db tbl add column k integer drop table drop table postgres_db tbl create view create view postgres_db v1 as select 42 create schema drop schema create schema postgres_db s1 create table postgres_db s1 integers i int insert into postgres_db s1 integers values 42 select from postgres_db s1 integers i int32 42 sql drop schema postgres_db s1 transactions create table postgres_db tmp i integer begin insert into postgres_db tmp values 42 select from postgres_db tmp i int64 42 rollback select from postgres_db tmp i int64 0 rows running sql queries in postgres with postgres_query the postgres_query function allows you to run arbitrary sql within an attached database postgres_query takes the name of the attached postgres database to execute the query in as well as the sql query to execute the result of the query is returned single-quote strings are escaped by repeating the single quote twice postgres_query attached_database varchar query varchar example attach dbname postgresscanner as s type postgres select from postgres_query s select from cars limit 3 brand model color varchar varchar varchar ferari testarosa red aston martin db2 blue bentley mulsanne gray settings the extension exposes the following configuration parameters name description default --------------------------------- ---------------------------------------------------------------------------- --------- pg_debug_show_queries debug setting print all queries sent to postgres to stdout false pg_connection_cache whether or not to use the connection cache true pg_experimental_filter_pushdown whether or not to use filter pushdown currently experimental false pg_array_as_varchar read postgres arrays as varchar - enables reading mixed dimensional arrays false pg_connection_limit the maximum amount of concurrent postgres connections 64 pg_pages_per_task the amount of pages per task 1000 pg_use_binary_copy whether or not to use binary copy to read data true schema cache to avoid having to continuously fetch schema data from postgres duckdb keeps schema information - such as the names of tables their columns etc - cached if changes are made to the schema through a different connection to the postgres instance such as new columns being added to a table the cached schema information might be outdated in this case the function pg_clear_cache can be executed to clear the internal caches call pg_clear_cache the old postgres_attach function is deprecated it is recommended to switch over to the new attach syntax github repository span class github github span",
			"category": "Extensions",
			"url": "/docs/extensions/postgres",
			"blurb": "The postgres extension allows DuckDB to directly read and write data from a running Postgres database instance. The..."
		},
		{
			"title": "PostgreSQL Import",
			"text": "to run a query directly on a running postgresql database the postgres extension is required installation and loading the extension can be installed use the install sql command this only needs to be run once install postgres to load the postgres extension for usage use the load sql command load postgres usage after the postgres extension is installed tables can be queried from postgresql using the postgres_scan function -- scan the table mytable from the schema public in the database mydb select from postgres_scan host localhost port 5432 dbname mydb public mytable the first parameter to the postgres_scan function is the postgresql connection string a list of connection arguments provided in key value format below is a list of valid arguments name description default ---------- -------------------------------------- ---------------- host name of host to connect to localhost hostaddr host ip address localhost port port number 5432 user postgres user name os user name password postgres password dbname database name user passfile name of file passwords are stored in pgpass alternatively the entire database can be attached using the attach command this allows you to query all tables stored within the postgresql database as if it was a regular database -- attach the postgres database using the given connection string attach host localhost port 5432 dbname mydb as test type postgres -- the table tbl_name can now be queried as if it is a regular table select from test tbl_name -- switch the active database to test use test -- list all tables in the file show tables for more information see the postgresql extension documentation",
			"category": "Import",
			"url": "/docs/guides/import/query_postgres",
			"blurb": "To run a query directly on a running PostgreSQL database, the postgres extension is required. Installation and..."
		},
		{
			"title": "Pragmas",
			"text": "the pragma statement is an sql extension adopted by duckdb from sqlite pragma statements can be issued in a similar manner to regular sql statements pragma commands may alter the internal state of the database engine and can influence the subsequent execution or behavior of the engine pragma statements that assign a value to an option can also be issued using the set statement and the value of an option can be retrieved using select current_setting option_name list of supported pragma statements below is a list of supported pragma statements schema information list all databases pragma database_list list all tables pragma show_tables list all tables with extra information similarly to describe pragma show_tables_expanded to list all functions pragma functions table information get info for a specific table pragma table_info table_name call pragma_table_info table_name table_info returns information about the columns of the table with name table_name the exact format of the table returned is given below cid integer -- cid of the column name varchar -- name of the column type varchar -- type of the column notnull boolean -- if the column is marked as not null dflt_value varchar -- default value of the column or null if not specified pk boolean -- part of the primary key or not to also show table structure but in a slightly different format included for compatibility pragma show table_name memory limit set the memory limit for the buffer manager set memory_limit 1gb set max_memory 1gb the specified memory limit is only applied to the buffer manager for most queries the buffer manager handles the majority of the data processed however certain in-memory data structures such as vectors and query results are allocated outside of the buffer manager additionally aggregate functions with complex state e g list mode quantile string_agg and approx functions use memory outside of the buffer manager therefore the actual memory consumption can be higher than the specified memory limit threads set the amount of threads for parallel query execution set threads 4 database size get the file and memory size of each database set database_size call pragma_database_size database_size returns information about the file and memory size of each database the column types of the returned results are given below database_name varchar -- database name database_size varchar -- total block count times the block size block_size bigint -- database block size total_blocks bigint -- total blocks in the database used_blocks bigint -- used blocks in the database free_blocks bigint -- free blocks in the database wal_size varchar -- write ahead log size memory_usage varchar -- memory used by the database buffer manager memory_limit varchar -- maximum memory allowed for the database collations list all available collations pragma collations set the default collation to one of the available ones set default_collation nocase implicit casting to varchar prior to version 0 10 0 duckdb would automatically allow any type to be implicitly cast to varchar during function binding as a result it was possible to e g compute the substring of an integer without using an implicit cast for version v0 10 0 and later an explicit cast is needed instead to revert to the old behaviour that performs implicit casting set the old_implicit_casting variable to true set old_implicit_casting true default ordering for nulls set the default ordering for nulls to be either nulls first or nulls last set default_null_order nulls first set default_null_order nulls last set the default result set ordering direction to ascending or descending set default_order ascending set default_order descending version show duckdb version pragma version call pragma_version platform platform returns an identifier for the platform the current duckdb executable has been compiled for e g osx_arm64 the format of this identifier matches the platform name as described on the extension loading explainer pragma platform call pragma_platform progress bar show progress bar when running queries pragma enable_progress_bar don t show a progress bar for running queries pragma disable_progress_bar profiling enable profiling to enable profiling pragma enable_profiling pragma enable_profile profiling format the format of the resulting profiling information can be specified as either json query_tree or query_tree_optimizer the default format is query_tree which prints the physical operator tree together with the timings and cardinalities of each operator in the tree to the screen to return the logical query plan as json set enable_profiling json to return the logical query plan set enable_profiling query_tree to return the physical query plan set enable_profiling query_tree_optimizer disable profiling to disable profiling pragma disable_profiling pragma disable_profile profiling output by default profiling information is printed to the console however if you prefer to write the profiling information to a file the pragma profiling_output can be used to write to a specified file note that the file contents will be overwritten for every new query that is issued hence the file will only contain the profiling information of the last query that is run set profiling_output path to file json set profile_output path to file json profiling mode by default a limited amount of profiling information is provided standard for more details use the detailed profiling mode by setting profiling_mode to detailed the output of this mode shows how long it takes to apply certain optimizers on the query tree and how long physical planning takes set profiling_mode detailed optimizer to disable the query optimizer pragma disable_optimizer to enable the query optimizer pragma enable_optimizer logging set a path for query logging set log_query_path tmp duckdb_log disable query logging set log_query_path explain plan output the output of explain output can be configured to show only the physical plan this is the default configuration set explain_output physical_only to only show the optimized query plan set explain_output optimized_only to show all query plans set explain_output all full-text search indexes the create_fts_index and drop_fts_index options are only available when the fts extension is loaded their usage is documented on the full-text search extension page verification of external operators enable verification of external operators pragma verify_external disable verification of external operators pragma disable_verify_external verification of round-trip capabilities enable verification of round-trip capabilities for supported logical plans pragma verify_serializer disable verification of round-trip capabilities pragma disable_verify_serializer object cache enable caching of objects for e g parquet metadata pragma enable_object_cache disable caching of objects pragma disable_object_cache checkpoint force checkpoint when checkpoint is called when no changes are made force a checkpoint regardless pragma force_checkpoint checkpoint on shutdown run a checkpoint on successful shutdown and delete the wal to leave only a single database file behind pragma enable_checkpoint_on_shutdown don t run a checkpoint on shutdown pragma disable_checkpoint_on_shutdown progress bar enable printing of the progress bar if it s possible pragma enable_print_progress_bar disable printing of the progress bar pragma disable_print_progress_bar temp directory for spilling data to disk by default duckdb uses a temporary directory named database_file_name tmp to spill to disk located in the same directory as the database file to change this use set temp_directory path to temp_dir tmp storage information to get storage information pragma storage_info table_name call pragma_storage_info table_name this call returns the following information for the given table name type description ---------------- ----------- ------------------------------------------------------- row_group_id bigint column_name varchar column_id bigint column_path varchar segment_id bigint segment_type varchar start bigint the start row id of this chunk count bigint the amount of entries in this storage chunk compression varchar compression type used for this column - see blog post stats varchar has_updates boolean persistent boolean false if temporary table block_id bigint empty unless persistent block_offset bigint empty unless persistent see storage for more information show databases the following statement is equivalent to the show databases statement pragma show_databases user agent the following statement returns the user agent information e g duckdb v0 10 0 osx_arm64 pragma user_agent metadata information the following statement returns information on the metadata store block_id total_blocks free_blocks and free_list pragma metadata_info selectively disabling optimizers the disabled_optimizers option allows selectively disabling optimization steps for example to disable filter_pushdown and statistics_propagation run set disabled_optimizers filter_pushdown statistics_propagation the available optimizations can be queried using the duckdb_optimizers table function the disabled_optimizers option should only be used for debugging performance issues and should be avoided in production returning errors as json the errors_as_json option can be set to obtain error information in raw json format for certain errors extra information or decomposed information is provided for easier machine processing for example set errors_as_json true then running a query that results in an error produces a json output select from nonexistent_tbl exception_type catalog exception_message table with name nonexistent_tbl does not exist ndid you mean temp information_schema tables name nonexistent_tbl candidates temp information_schema tables position 14 type table error_subtype missing_entry query verification for development the following pragma s are mostly used for development and internal testing enable query verification pragma enable_verification disable query verification pragma disable_verification enable force parallel query processing pragma verify_parallelism disable force parallel query processing pragma disable_verify_parallelism",
			"category": "SQL",
			"url": "/docs/sql/pragmas",
			"blurb": "The PRAGMA statement is an SQL extension adopted by DuckDB from SQLite. PRAGMA statements can be issued in a similar..."
		},
		{
			"title": "Prepared Statements",
			"text": "duckdb supports prepared statements where parameters are substituted when the query is executed this can improve readability and is useful for preventing sql injections syntax there are three syntaxes for denoting parameters in prepared statements auto-incremented positional 1 and named param note that not all clients support all of these syntaxes e g the jdbc client only supports auto-incremented parameters in prepared statements example data set in the following we introduce the three different syntaxes and illustrate them with examples using the following table create table person name varchar age bigint insert into person values alice 37 ana 35 bob 41 bea 25 in our example query we ll look for people whose name starts with a b and are at least 40 years old this will return a single row bob 41 auto-incremented parameters duckdb support using prepared statements with auto-incremented indexing i e the position of the parameters in the query corresponds to their position in the execution statement for example prepare query_person as select from person where starts_with name and age using the cli client the statement is executed as follows execute query_person b 40 positional parameters 1 prepared statements can use positional parameters where parameters are denoted with an integer 1 2 for example prepare query_person as select from person where starts_with name 2 and age 1 using the cli client the statement is executed as follows note that the first parameter corresponds to 1 the second to 2 and so on execute query_person 40 b named parameters parameter duckdb also supports names parameters where parameters are denoted with parameter_name for example prepare query_person as select from person where starts_with name name_start_letter and age minimum_age using the cli client the statement is executed as follows execute query_person name_start_letter b minimum_age 40",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/prepared_statements",
			"blurb": "DuckDB supports prepared statements where parameters are substituted when the query is executed. This can improve..."
		},
		{
			"title": "Profile Queries Using EXPLAIN ANALYZE",
			"text": "in order to profile a query prepend explain analyze to a query explain analyze select from tbl the query plan will be pretty-printed to the screen using timings for every operator note that the cumulative wall-clock time that is spent on every operator is shown when multiple threads are processing the query in parallel the total processing time of the query may be lower than the sum of all the times spent on the individual operators below is an example of running explain analyze on q1 of the tpc-h benchmark total time 0 0496s explain_analyze 0 0 00s order_by lineitem l_returnflag asc lineitem l_linestatus asc 4 0 00s hash_group_by 0 1 sum 2 sum 3 sum 4 sum 5 avg 6 avg 7 avg 8 count_star 4 0 28s projection l_returnflag l_linestatus l_quantity l_extendedprice 4 4 1 00 l_tax l_quantity l_extendedprice l_discount 5916591 0 02s projection l_returnflag l_linestatus l_quantity l_extendedprice l_extendedprice 1 00 - l_discount l_tax l_discount 5916591 0 02s seq_scan lineitem l_shipdate l_returnflag l_linestatus l_quantity l_extendedprice l_discount l_tax filters l_shipdate 1998 -09-02 and l_shipdate null 5916591 0 08s",
			"category": "Meta",
			"url": "/docs/guides/meta/explain_analyze",
			"blurb": "In order to profile a query, prepend EXPLAIN ANALYZE to a query. EXPLAIN ANALYZE SELECT * FROM tbl; The query plan..."
		},
		{
			"title": "Profiling Queries",
			"text": "duckdb supports profiling queries via the explain and explain analyze statements explain to see the query plan of a query without executing it run explain query the output of explain contains the estimated cardinalities for each operator explain analyze to profile a query run explain analyze query the explain analyze statement runs the query and shows the actual cardinalities for each operator as well as the cumulative wall-clock time spent in each operator",
			"category": "Statements",
			"url": "/docs/sql/statements/profiling",
			"blurb": "DuckDB supports profiling queries via the EXPLAIN and EXPLAIN ANALYZE statements. EXPLAIN To see the query plan of a..."
		},
		{
			"title": "Python API",
			"text": "installation the duckdb python api can be installed using pip pip install duckdb please see the installation page for details it is also possible to install duckdb using conda conda install python-duckdb -c conda-forge python version duckdb requires python 3 7 or newer basic api usage the most straight-forward manner of running sql queries using duckdb is using the duckdb sql command import duckdb duckdb sql select 42 show this will run queries using an in-memory database that is stored globally inside the python module the result of the query is returned as a relation a relation is a symbolic representation of the query the query is not executed until the result is fetched or requested to be printed to the screen relations can be referenced in subsequent queries by storing them inside variables and using them as tables this way queries can be constructed incrementally import duckdb r1 duckdb sql select 42 as i duckdb sql select i 2 as k from r1 show data input duckdb can ingest data from a wide variety of formats both on-disk and in-memory see the data ingestion page for more information import duckdb duckdb read_csv example csv read a csv file into a relation duckdb read_parquet example parquet read a parquet file into a relation duckdb read_json example json read a json file into a relation duckdb sql select from example csv directly query a csv file duckdb sql select from example parquet directly query a parquet file duckdb sql select from example json directly query a json file dataframes duckdb can also directly query pandas dataframes polars dataframes and arrow tables import duckdb directly query a pandas dataframe import pandas as pd pandas_df pd dataframe a 42 duckdb sql select from pandas_df directly query a polars dataframe import polars as pl polars_df pl dataframe a 42 duckdb sql select from polars_df directly query a pyarrow table import pyarrow as pa arrow_table pa table from_pydict a 42 duckdb sql select from arrow_table result conversion duckdb supports converting query results efficiently to a variety of formats see the result conversion page for more information import duckdb duckdb sql select 42 fetchall python objects duckdb sql select 42 df pandas dataframe duckdb sql select 42 pl polars dataframe duckdb sql select 42 arrow arrow table duckdb sql select 42 fetchnumpy numpy arrays writing data to disk duckdb supports writing relation objects directly to disk in a variety of formats the copy statement can be used to write data to disk using sql as an alternative import duckdb duckdb sql select 42 write_parquet out parquet write to a parquet file duckdb sql select 42 write_csv out csv write to a csv file duckdb sql copy select 42 to out parquet copy to a parquet file using an in-memory database when using duckdb through duckdb sql it operates on an in-memory database i e no tables are persisted on disk invoking the duckdb connect method without arguments returns a connection which also uses an in-memory database import duckdb con duckdb connect con sql select 42 as x show persistent storage the duckdb connect dbname creates a connection to a persistent database any data written to that connection will be persisted and can be reloaded by re-connecting to the same file both from python and from other duckdb clients import duckdb create a connection to a file called file db con duckdb connect file db create a table and load data into it con sql create table test i integer con sql insert into test values 42 query the table con table test show explicitly close the connection con close note connections also closed implicitly when they go out of scope you can also use a context manager to ensure that the connection is closed import duckdb with duckdb connect file db as con con sql create table test i integer con sql insert into test values 42 con table test show the context manager closes the connection automatically connection object and module the connection object and the duckdb module can be used interchangeably they support the same methods the only difference is that when using the duckdb module a global in-memory database is used note that if you are developing a package designed for others to use and use duckdb in the package it is recommend that you create connection objects instead of using the methods on the duckdb module that is because the duckdb module uses a shared global database which can cause hard to debug issues if used from within multiple different packages using connections in parallel python programs the duckdbpyconnection object is not thread-safe if you would like to write to the same database from multiple threads create a cursor for each thread with the duckdbpyconnection cursor method loading and installing extensions duckdb s python api provides functions for installing and loading extensions which perform the equivalent operations to running the install and load sql commands respectively an example that installs and loads the spatial extension looks like follows import duckdb con duckdb connect con install_extension spatial con load_extension spatial to load unsigned extensions add the config allow_unsigned_extensions true argument to the duckdb connect method pages in this section",
			"category": "Python",
			"url": "/docs/api/python/overview",
			"blurb": "Installation The DuckDB Python API can be installed using pip : pip install duckdb . Please see the installation page..."
		},
		{
			"title": "Python Client API",
			"text": "",
			"category": "Reference",
			"url": "/docs/api/python/reference/index",
			"blurb": ""
		},
		{
			"title": "Python DB API",
			"text": "the standard duckdb python api provides a sql interface compliant with the db-api 2 0 specification described by pep 249 similar to the sqlite python api connection to use the module you must first create a duckdbpyconnection object that represents the database the connection object takes as a parameter the database file to read and write from if the database file does not exist it will be created the file extension may be db duckdb or anything else the special value memory the default can be used to create an in-memory database note that for an in-memory database no data is persisted to disk i e all data is lost when you exit the python process if you would like to connect to an existing database in read-only mode you can set the read_only flag to true read-only mode is required if multiple python processes want to access the same database file at the same time by default we create an in-memory-database that lives inside the duckdb module every method of duckdbpyconnection is also available on the duckdb module this connection is what s used by these methods you can also get a reference to this connection by providing the special value default to connect import duckdb duckdb execute create table tbl as select 42 a con duckdb connect default con sql select from tbl a int32 42 import duckdb to start an in-memory database con duckdb connect database memory to use a database file not shared between processes con duckdb connect database my-db duckdb read_only false to use a database file shared between processes con duckdb connect database my-db duckdb read_only true to explicitly get the default connection con duckdb connect database default if you want to create a second connection to an existing database you can use the cursor method this might be useful for example to allow parallel threads running queries independently a single connection is thread-safe but is locked for the duration of the queries effectively serializing database access in this case connections are closed implicitly when they go out of scope or if they are explicitly closed using close once the last connection to a database instance is closed the database instance is closed as well querying sql queries can be sent to duckdb using the execute method of connections once a query has been executed results can be retrieved using the fetchone and fetchall methods on the connection fetchall will retrieve all results and complete the transaction fetchone will retrieve a single row of results each time that it is invoked until no more results are available the transaction will only close once fetchone is called and there are no more results remaining the return value will be none as an example in the case of a query only returning a single row fetchone should be called once to retrieve the results and a second time to close the transaction below are some short examples create a table con execute create table items item varchar value decimal 10 2 count integer insert two items into the table con execute insert into items values jeans 20 0 1 hammer 42 2 2 retrieve the items again con execute select from items print con fetchall jeans decimal 20 00 1 hammer decimal 42 20 2 retrieve the items one at a time con execute select from items print con fetchone jeans decimal 20 00 1 print con fetchone hammer decimal 42 20 2 print con fetchone this closes the transaction any subsequent calls to fetchone will return none none the description property of the connection object contains the column names as per the standard prepared statements duckdb also supports prepared statements in the api with the execute and executemany methods the values may be passed as an additional parameter after a query that contains or 1 dollar symbol and a number placeholders using the notation adds the values in the same sequence as passed within the python parameter using the notation allows for values to be reused within the sql statement based on the number and index of the value found within the python parameter here are some examples insert a row using prepared statements con execute insert into items values laptop 2000 1 insert several rows using prepared statements con executemany insert into items values chainsaw 500 10 iphone 300 2 query the database using a prepared statement con execute select item from items where value 400 print con fetchall laptop chainsaw query using notation for prepared statement and reused values con execute select 1 1 2 duck goose print con fetchall duck duck goose do not use executemany to insert large amounts of data into duckdb see the data ingestion page for better options named parameters besides the standard unnamed parameters like 1 2 etc it s also possible to supply named parameters like my_parameter when using named parameters you have to provide a dictionary mapping of str to value in the parameters argument an example use import duckdb res duckdb execute select my_param other_param also_param my_param 5 other_param duckdb also_param 42 fetchall print res 5 duckdb 42",
			"category": "Python",
			"url": "/docs/api/python/dbapi",
			"blurb": "The standard DuckDB Python API provides a SQL interface compliant with the DB-API 2.0 specification described by PEP..."
		},
		{
			"title": "Python Function API",
			"text": "you can create a duckdb user-defined function udf out of a python function so it can be used in sql queries similarly to regular functions they need to have a name a return type and parameter types here is an example using a python function that calls a third-party library import duckdb from duckdb typing import from faker import faker def random_name fake faker return fake name duckdb create_function random_name random_name varchar res duckdb sql select random_name fetchall print res gerald ashley creating functions to register a python udf simply use the create_function method from a duckdb connection here is the syntax import duckdb con duckdb connect con create_function name function argument_type_list return_type type null_handling the create_function method requires the following parameters name a string representing the unique name of the udf within the connection catalog function the python function you wish to register as a udf return_type scalar functions return one element per row this parameter specifies the return type of the function parameters scalar functions can operate on one or more columns this parameter takes a list of column types used as input type optional duckdb supports both built-in python types and pyarrow tables by default built-in types are assumed but you can specify type arrow to use pyarrow tables null_handling optional by default null values are automatically handled as null-in null-out users can specify a desired behavior for null values by setting null_handling special exception_handling optional by default when an exception is thrown from the python function it will be re-thrown in python users can disable this behavior and instead return null by set this parameter to return_null side_effects optional by default functions are expected to produce the same result for the same input if the result of a function is impacted by any type of randomness side_effects must be set to true to unregister a udf you can call the remove_function method with the udf name con remove_function name type annotation when the function has type annotation it s often possible to leave out all of the optional parameters using duckdbpytype we can implicitly convert many known types to duckdbs type system for example import duckdb def my_function x int - str return x duckdb create_function my_func my_function duckdb sql select my_func 42 my_func 42 varchar 42 if only the parameter list types can be inferred you ll need to pass in none as argument_type_list null handling by default when functions receive a null value this instantly returns null as part of the default null -handling when this is not desired you need to explicitly set this parameter to special import duckdb from duckdb typing import def dont_intercept_null x return 5 duckdb create_function dont_intercept dont_intercept_null bigint bigint res duckdb sql select dont_intercept null fetchall print res none duckdb remove_function dont_intercept duckdb create_function dont_intercept dont_intercept_null bigint bigint null_handling special res duckdb sql select dont_intercept null fetchall print res 5 exception handling by default when an exception is thrown from the python function we ll forward re-throw the exception if you want to disable this behavior and instead return null you ll need to set this parameter to return_null import duckdb from duckdb typing import def will_throw raise valueerror error duckdb create_function throws will_throw bigint try res duckdb sql select throws fetchall except duckdb invalidinputexception as e print e duckdb create_function doesnt_throw will_throw bigint exception_handling return_null res duckdb sql select doesnt_throw fetchall print res none side effects by default duckdb will assume the created function is a pure function meaning it will produce the same output when given the same input if your function does not follow that rule for example when your function makes use of randomness then you will need to mark this function as having side_effects for example this function will produce a new count for every invocation def count - int old count counter count counter 1 return old count counter 0 if we create this function without marking it as having side effects the result will be the following con duckdb connect con create_function my_counter count side_effects false res con sql select my_counter from range 10 fetchall print res 0 0 0 0 0 0 0 0 0 0 which is obviously not the desired result when we add side_effects true the result is as we would expect con remove_function my_counter count counter 0 con create_function my_counter count side_effects true res con sql select my_counter from range 10 fetchall print res 0 1 2 3 4 5 6 7 8 9 python function types currently two function types are supported native default and arrow arrow if the function is expected to receive arrow arrays set the type parameter to arrow this will let the system know to provide arrow arrays of up to standard_vector_size tuples to the function and also expect an array of the same amount of tuples to be returned from the function native when the function type is set to native the function will be provided with a single tuple at a time and expect only a single value to be returned this can be useful to interact with python libraries that don t operate on arrow such as faker import duckdb from duckdb typing import from faker import faker def random_date fake faker return fake date_between duckdb create_function random_date random_date date type native res duckdb sql select random_date fetchall print res datetime date 2019 5 15",
			"category": "Python",
			"url": "/docs/api/python/function",
			"blurb": "You can create a DuckDB user-defined function (UDF) out of a Python function so it can be used in SQL queries...."
		},
		{
			"title": "QUALIFY Clause",
			"text": "the qualify clause is used to filter the results of window functions this filtering of results is similar to how a having clause filters the results of aggregate functions applied based on the group by clause the qualify clause avoids the need for a subquery or with clause to perform this filtering much like having avoids a subquery an example using a with clause instead of qualify is included below the qualify examples note that this is filtering based on window functions not necessarily based on the window clause the window clause is optional and can be used to simplify the creation of multiple window function expressions the position of where to specify a qualify clause is following the window clause in a select statement window does not need to be specified and before the order by examples each of the following examples produce the same output located below -- filter based on a window function defined in the qualify clause select schema_name function_name -- in this example the function_rank column in the select clause is for reference row_number over partition by schema_name order by function_name as function_rank from duckdb_functions qualify row_number over partition by schema_name order by function_name 3 -- filter based on a window function defined in the select clause select schema_name function_name row_number over partition by schema_name order by function_name as function_rank from duckdb_functions qualify function_rank 3 -- filter based on a window function defined in the qualify clause but using the window clause select schema_name function_name -- in this example the function_rank column in the select clause is for reference row_number over my_window as function_rank from duckdb_functions window my_window as partition by schema_name order by function_name qualify row_number over my_window 3 -- filter based on a window function defined in the select clause but using the window clause select schema_name function_name row_number over my_window as function_rank from duckdb_functions window my_window as partition by schema_name order by function_name qualify function_rank 3 -- equivalent query based on a with clause without qualify clause with ranked_functions as select schema_name function_name row_number over partition by schema_name order by function_name as function_rank from duckdb_functions select from ranked_functions where function_rank 3 schema_name function_name function_rank --- --- --- main __postfix 1 main 2 pg_catalog col_description 1 pg_catalog format_pg_type 2 syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/qualify",
			"blurb": "The QUALIFY clause is used to filter the results of WINDOW functions."
		},
		{
			"title": "Query",
			"text": "duckdb-wasm provides functions for querying data queries are run sequentially first a connection need to be created by calling connect then queries can be run by calling query or send query execution create a new connection const conn await db connect either materialize the query result await conn query v arrow int select from generate_series 1 100 t v or fetch the result chunks lazily for await const batch of await conn send v arrow int select from generate_series 1 100 t v close the connection to release memory await conn close prepared statements create a new connection const conn await db connect prepare query const stmt await conn prepare select v from generate_series 0 10000 as t v and run the query with materialized results await stmt query 234 or result chunks for await const batch of await stmt send 234 close the statement to release memory await stmt close closing the connection will release statements as well await conn close arrow table to json create a new connection const conn await db connect query const arrowresult await conn query v arrow int select from generate_series 1 100 t v convert arrow table to json const result arrowresult toarray map row row tojson close the connection to release memory await conn close export parquet create a new connection const conn await db connect export parquet conn send copy select from tbl to result-snappy parquet format parquet const parquet_buffer await this _db copyfiletobuffer result-snappy parquet generate a download link const link url createobjecturl new blob parquet_buffer close the connection to release memory await conn close",
			"category": "Wasm",
			"url": "/docs/api/wasm/query",
			"blurb": "DuckDB-Wasm provides functions for querying data. Queries are run sequentially. First, a connection need to be..."
		},
		{
			"title": "Querying Parquet Files",
			"text": "to run a query directly on a parquet file use the read_parquet function in the from clause of a query select from read_parquet input parquet the parquet file will be processed in parallel filters will be automatically pushed down into the parquet scan and only the relevant columns will be read automatically for more information see the blog post querying parquet with precision using duckdb",
			"category": "Import",
			"url": "/docs/guides/import/query_parquet",
			"blurb": "To run a query directly on a Parquet file, use the read_parquet function in the FROM clause of a query. SELECT * FROM..."
		},
		{
			"title": "Querying Parquet Metadata",
			"text": "parquet metadata the parquet_metadata function can be used to query the metadata contained within a parquet file which reveals various internal details of the parquet file such as the statistics of the different columns this can be useful for figuring out what kind of skipping is possible in parquet files or even to obtain a quick overview of what the different columns contain select from parquet_metadata test parquet below is a table of the columns returned by parquet_metadata field type ------------------------- ----------------- file_name varchar row_group_id bigint row_group_num_rows bigint row_group_num_columns bigint row_group_bytes bigint column_id bigint file_offset bigint num_values bigint path_in_schema varchar type varchar stats_min varchar stats_max varchar stats_null_count bigint stats_distinct_count bigint stats_min_value varchar stats_max_value varchar compression varchar encodings varchar index_page_offset bigint dictionary_page_offset bigint data_page_offset bigint total_compressed_size bigint total_uncompressed_size bigint key_value_metadata map blob blob parquet schema the parquet_schema function can be used to query the internal schema contained within a parquet file note that this is the schema as it is contained within the metadata of the parquet file if you want to figure out the column names and types contained within a parquet file it is easier to use describe -- fetch the column names and column types describe select from test parquet -- fetch the internal schema of a parquet file select from parquet_schema test parquet below is a table of the columns returned by parquet_schema field type ----------------- --------- file_name varchar name varchar type varchar type_length varchar repetition_type varchar num_children bigint converted_type varchar scale bigint precision bigint field_id bigint logical_type varchar parquet file metadata the parquet_file_metadata function can be used to query file-level metadata such as the format version and the encryption algorithm used select from parquet_file_metadata test parquet below is a table of the columns returned by parquet_file_metadata field type ----------------------------- --------- file_name varchar created_by varchar num_rows bigint num_row_groups bigint format_version bigint encryption_algorithm varchar footer_signing_key_metadata varchar parquet key-value metadata the parquet_kv_metadata function can be used to query custom metadata defined as key-value pairs select from parquet_kv_metadata test parquet below is a table of the columns returned by parquet_kv_metadata field type ----------- --------- file_name varchar key blob value blob",
			"category": "Parquet",
			"url": "/docs/data/parquet/metadata",
			"blurb": "Parquet Metadata The parquet_metadata function can be used to query the metadata contained within a Parquet file,..."
		},
		{
			"title": "R API",
			"text": "installation the duckdb r api can be installed using install packages duckdb please see the installation page for details reference manual the reference manual for the duckdb r api is available at r duckdb org basic api usage the standard duckdb r api implements the dbi interface for r if you are not familiar with dbi yet see here for an introduction startup shutdown to use duckdb you must first create a connection object that represents the database the connection object takes as parameter the database file to read and write from if the database file does not exist it will be created the file extension may be db duckdb or anything else the special value memory the default can be used to create an in-memory database note that for an in-memory database no data is persisted to disk i e all data is lost when you exit the r process if you would like to connect to an existing database in read-only mode set the read_only flag to true read-only mode is required if multiple r processes want to access the same database file at the same time library duckdb to start an in-memory database con - dbconnect duckdb or con - dbconnect duckdb dbdir memory to use a database file not shared between processes con - dbconnect duckdb dbdir my-db duckdb read_only false to use a database file shared between processes con - dbconnect duckdb dbdir my-db duckdb read_only true connections are closed implicitly when they go out of scope or if they are explicitly closed using dbdisconnect to shut down the database instance associated with the connection use dbdisconnect con shutdown true querying duckdb supports the standard dbi methods to send queries and retrieve result sets dbexecute is meant for queries where no results are expected like create table or update etc and dbgetquery is meant to be used for queries that produce results e g select below an example create a table dbexecute con create table items item varchar value decimal 10 2 count integer insert two items into the table dbexecute con insert into items values jeans 20 0 1 hammer 42 2 2 retrieve the items again res - dbgetquery con select from items print res item value count 1 jeans 20 0 1 2 hammer 42 2 2 duckdb also supports prepared statements in the r api with the dbexecute and dbgetquery methods here is an example prepared statement parameters are given as a list dbexecute con insert into items values list laptop 2000 1 if you want to reuse a prepared statement multiple times use dbsendstatement and dbbind stmt - dbsendstatement con insert into items values dbbind stmt list iphone 300 2 dbbind stmt list android 3 5 1 dbclearresult stmt query the database using a prepared statement res - dbgetquery con select item from items where value list 400 print res item 1 laptop do not use prepared statements to insert large amounts of data into duckdb see below for better options efficient transfer to write a r data frame into duckdb use the standard dbi function dbwritetable this creates a table in duckdb and populates it with the data frame contents for example dbwritetable con iris_table iris res - dbgetquery con select from iris_table limit 1 print res sepal length sepal width petal length petal width species 1 5 1 3 5 1 4 0 2 setosa it is also possible to register a r data frame as a virtual table comparable to a sql view this does not actually transfer data into duckdb yet below is an example duckdb_register con iris_view iris res - dbgetquery con select from iris_view limit 1 print res sepal length sepal width petal length petal width species 1 5 1 3 5 1 4 0 2 setosa duckdb keeps a reference to the r data frame after registration this prevents the data frame from being garbage-collected the reference is cleared when the connection is closed but can also be cleared manually using the duckdb_unregister method also refer to the data import documentation for more options of efficiently importing data dbplyr duckdb also plays well with the dbplyr dplyr packages for programmatic query construction from r here is an example library duckdb library dplyr con - dbconnect duckdb duckdb_register con flights nycflights13 flights tbl con flights group_by dest summarise delay mean dep_time na rm true collect when using dbplyr csv and parquet files can be read using the dplyr tbl function establish a csv for the sake of this example write csv mtcars mtcars csv summarize the dataset in duckdb to avoid reading the entire csv into r s memory tbl con mtcars csv group_by cyl summarise across disp wt fns mean collect establish a set of parquet files dbexecute con copy flights to dataset format parquet partition_by year month summarize the dataset in duckdb to avoid reading 12 parquet files into r s memory tbl con read_parquet dataset parquet hive_partitioning 1 filter month 3 summarise delay mean dep_time na rm true collect github repository span class github github span",
			"category": "Api",
			"url": "/docs/api/r",
			"blurb": "Installation The DuckDB R API can be installed using install.packages(duckdb) . Please see the installation page for..."
		},
		{
			"title": "Reading Faulty CSV Files",
			"text": "reading erroneous csv files is possible by utilizing the ignore_errors option with that option set rows containing data that would otherwise cause the csv parser to generate an error will be ignored using the ignore_errors option for example consider the following csv file faulty csv pedro 31 oogie boogie three if you read the csv file specifying that the first column is a varchar and the second column is an integer loading the file would fail as the string three cannot be converted to an integer for example the following query will throw a casting error from read_csv faulty csv columns name varchar age integer however with ignore_errors set the second row of the file is skipped outputting only the complete first row for example from read_csv faulty csv columns name varchar age integer ignore_errors true outputs name age ------- ----- pedro 31 one should note that the csv parser is affected by the projection pushdown optimization hence if we were to select only the name column both rows would be considered valid as the casting error on the age would never occur for example select name from read_csv faulty csv columns name varchar age integer outputs name -------------- pedro oogie boogie retrieving faulty csv lines being able to read faulty csv files is important but for many data cleaning operations it is also necessary to know exactly which lines are corrupted and what errors the parser discovered on them for scenarios like these it is possible to use duckdb s csv rejects table feature it is important to note that the rejects table can only be used when ignore_errors is set and currently only stores casting errors and does not save errors when the number of columns differ the csv rejects table returns the following information column name description type -- ----- - file file path varchar line line number from the csv file where the error occured integer column column number from the csv file where the error occured integer column_name column name from the csv file where the error occured varchar parsed_value the value where the casting error happened in a string format varchar recovery_columns an optional primary key of the csv file struct name value error exact error encountered by the parser varchar parameters the parameters listed below are used in the read_csv function to configure the csv rejects table name description type default -- ----- - - rejects_table name of a temporary table where the information of the faulty lines of a csv file are stored varchar empty rejects_limit upper limit on the number of faulty records from a csv file that will be recorded in the rejects table 0 is used when no limit should be applied bigint 0 rejects_recovery_columns column values that serve as a primary key to the csv file the are stored in the csv rejects table to help identify the faulty tuples varchar empty to store the information of the faulty csv lines in a rejects table the user must simply provide the rejects table name in the rejects_table option for example from read_csv faulty csv columns name varchar age integer rejects_table rejects_table ignore_errors true you can then query the rejects_table table to retrieve information about the rejected tuples for example from rejects_table outputs file line column column_name parsed_value error ------------ ------ -------- ------------- -------------- ------------------------------------------------ faulty csv 2 1 age three could not convert string three to integer additionally the name column could also be provided as a primary key via the rejects_recovery_columns option to provide more information over the faulty lines for example from read_csv faulty csv columns name varchar age integer rejects_table rejects_table rejects_recovery_columns name ignore_errors true reading from the rejects_table will return file line column column_name parsed_value recovery_columns error ------------ ------ -------- ------------- -------------- -------------------------- ------------------------------------------------ faulty csv 2 1 age three name oogie boogie could not convert string three to integer",
			"category": "Csv",
			"url": "/docs/data/csv/reading_faulty_csv_files",
			"blurb": "Reading erroneous CSV files is possible by utilizing the ignore_errors option. With that option set, rows containing..."
		},
		{
			"title": "Reading Multiple Files",
			"text": "duckdb can read multiple files of different types csv parquet json files at the same time using either the glob syntax or by providing a list of files to read see the combining schemas page for tips on reading files with different schemas csv -- read all files with a name ending in csv in the folder dir select from dir csv -- read all files with a name ending in csv two directories deep select from csv -- read all files with a name ending in csv at any depth in the folder dir select from dir csv -- read the csv files flights1 csv and flights2 csv select from read_csv flights1 csv flights2 csv -- read the csv files flights1 csv and flights2 csv unifying schemas by name and outputting a filename column select from read_csv flights1 csv flights2 csv union_by_name true filename true parquet -- read all files that match the glob pattern select from test parquet -- read 3 parquet files and treat them as a single table select from read_parquet file1 parquet file2 parquet file3 parquet -- read all parquet files from 2 specific folders select from read_parquet folder1 parquet folder2 parquet -- read all parquet files that match the glob pattern at any depth select from read_parquet dir parquet multi-file reads and globs duckdb can also read a series of parquet files and treat them as if they were a single table note that this only works if the parquet files have the same schema you can specify which parquet files you want to read using a list parameter glob pattern matching syntax or a combination of both list parameter the read_parquet function can accept a list of filenames as the input parameter -- read 3 parquet files and treat them as a single table select from read_parquet file1 parquet file2 parquet file3 parquet glob syntax any file name input to the read_parquet function can either be an exact filename or use a glob syntax to read multiple files that match a pattern wildcard description ------------ ----------------------------------------------------------- matches any number of any characters including none matches any number of subdirectories including none matches any single character abc matches one character given in the bracket a-z matches one character from the range given in the bracket note that the wildcard in globs is not supported for reads over s3 due to http encoding issues here is an example that reads all the files that end with parquet located in the test folder -- read all files that match the glob pattern select from read_parquet test parquet list of globs the glob syntax and the list input parameter can be combined to scan files that meet one of multiple patterns -- read all parquet files from 2 specific folders select from read_parquet folder1 parquet folder2 parquet duckdb can read multiple csv files at the same time using either the glob syntax or by providing a list of files to read filename the filename argument can be used to add an extra filename column to the result that indicates which row came from which file for example select from read_csv flights1 csv flights2 csv union_by_name true filename true flightdate origincityname destcityname uniquecarrier filename ------------ ---------------- ----------------- --------------- -------------- 1988-01-01 new york ny los angeles ca null flights1 csv 1988-01-02 new york ny los angeles ca null flights1 csv 1988-01-03 new york ny los angeles ca aa flights2 csv glob function to find filenames the glob pattern matching syntax can also be used to search for filenames using the glob table function it accepts one parameter the path to search which may include glob patterns -- search the current directory for all files select from glob file --------------- duckdb exe test csv test json test parquet test2 csv test2 parquet todos json pages in this section",
			"category": "Multiple Files",
			"url": "/docs/data/multiple_files/overview",
			"blurb": "DuckDB can read multiple files of different types (CSV, Parquet, JSON files) at the same time using either the glob..."
		},
		{
			"title": "Reading and Writing Parquet Files",
			"text": "examples -- read a single parquet file select from test parquet -- figure out which columns types are in a parquet file describe select from test parquet -- create a table from a parquet file create table test as select from test parquet -- if the file does not end in parquet use the read_parquet function select from read_parquet test parq -- use list parameter to read 3 parquet files and treat them as a single table select from read_parquet file1 parquet file2 parquet file3 parquet -- read all files that match the glob pattern select from test parquet -- read all files that match the glob pattern and include a filename column -- that specifies which file each row came from select from read_parquet test parquet filename true -- use a list of globs to read all parquet files from 2 specific folders select from read_parquet folder1 parquet folder2 parquet -- read over https select from read_parquet https some url some_file parquet -- query the metadata of a parquet file select from parquet_metadata test parquet -- query the schema of a parquet file select from parquet_schema test parquet -- write the results of a query to a parquet file using the default compression snappy copy select from tbl to result-snappy parquet format parquet -- write the results from a query to a parquet file with specific compression and row group size copy from generate_series 100_000 to test parquet format parquet compression zstd row_group_size 100_000 -- export the table contents of the entire database as parquet export database target_directory format parquet parquet files parquet files are compressed columnar files that are efficient to load and process duckdb provides support for both reading and writing parquet files in an efficient manner as well as support for pushing filters and projections into the parquet file scans parquet data sets differ based on the number of files the size of individual files the compression algorithm used row group size etc these have a significant effect on performance please consult the performance guide for details read_parquet function function description example -- -- ----- read_parquet path s read parquet file s select from read_parquet test parquet parquet_scan path s alias for read_parquet select from parquet_scan test parquet if your file ends in parquet the function syntax is optional the system will automatically infer that you are reading a parquet file select from test parquet multiple files can be read at once by providing a glob or a list of files refer to the multiple files section for more information parameters there are a number of options exposed that can be passed to the read_parquet function or the copy statement name description type default -- ----- - - binary_as_string parquet files generated by legacy writers do not correctly set the utf8 flag for strings causing string columns to be loaded as blob instead set this to true to load binary columns as strings bool false encryption_config configuration for parquet encryption struct - filename whether or not an extra filename column should be included in the result bool false file_row_number whether or not to include the file_row_number column bool false hive_partitioning whether or not to interpret the path as a hive partitioned path bool false union_by_name whether the columns of multiple schemas should be unified by name rather than by position bool false partial reading duckdb supports projection pushdown into the parquet file itself that is to say when querying a parquet file only the columns required for the query are read this allows you to read only the part of the parquet file that you are interested in this will be done automatically by duckdb duckdb also supports filter pushdown into the parquet reader when you apply a filter to a column that is scanned from a parquet file the filter will be pushed down into the scan and can even be used to skip parts of the file using the built-in zonemaps note that this will depend on whether or not your parquet file contains zonemaps filter and projection pushdown provide significant performance benefits see our blog post on this for more information inserts and views you can also insert the data into a table or create a table from the parquet file directly this will load the data from the parquet file and insert it into the database -- insert the data from the parquet file in the table insert into people select from read_parquet test parquet -- create a table directly from a parquet file create table people as select from read_parquet test parquet if you wish to keep the data stored inside the parquet file but want to query the parquet file directly you can create a view over the read_parquet function you can then query the parquet file as if it were a built-in table -- create a view over the parquet file create view people as select from read_parquet test parquet -- query the parquet file select from people writing to parquet files duckdb also has support for writing to parquet files using the copy statement syntax see the copy statement page for details including all possible parameters for the copy statement -- write a query to a snappy compressed parquet file copy select from tbl to result-snappy parquet format parquet -- write tbl to a zstd compressed parquet file copy tbl to result-zstd parquet format parquet codec zstd -- write a csv file to an uncompressed parquet file copy test csv to result-uncompressed parquet format parquet codec uncompressed -- write a query to a parquet file with zstd compression same as codec and row_group_size copy from generate_series 100000 to row-groups-zstd parquet format parquet compression zstd row_group_size 100000 duckdb s export command can be used to export an entire database to a series of parquet files see the export statement documentation for more details -- export the table contents of the entire database as parquet export database target_directory format parquet encryption duckdb supports reading and writing encrypted parquet files installing and loading the parquet extension the support for parquet files is enabled via extension the parquet extension is bundled with almost all clients however if your client does not bundle the parquet extension the extension must be installed and loaded separately -- run once install parquet -- run before usage load parquet pages in this section",
			"category": "Parquet",
			"url": "/docs/data/parquet/overview",
			"blurb": "Examples -- read a single Parquet file SELECT * FROM 'test.parquet'; -- figure out which columns/types are in a..."
		},
		{
			"title": "Relational API",
			"text": "the relational api is an alternative api that can be used to incrementally construct queries the api is centered around duckdbpyrelation nodes the relations can be seen as symbolic representations of sql queries they do not hold any data - and nothing is executed - until a method that triggers execution is called constructing relations relations can be created from sql queries using the duckdb sql method alternatively they can be created from the various data ingestion methods read_parquet read_csv read_json for example here we create a relation from a sql query import duckdb rel duckdb sql select from range 10000000000 tbl id rel show id int64 0 1 2 3 4 5 6 7 8 9 9990 9991 9992 9993 9994 9995 9996 9997 9998 9999 rows 9999 rows 20 shown note how we are constructing a relation that computes an immense amount of data 10b rows or 74gb of data the relation is constructed instantly - and we can even print the relation instantly when printing a relation using show or displaying it in the terminal the first 10k rows are fetched if there are more than 10k rows the output window will show 9999 rows as the amount of rows in the relation is unknown data ingestion outside of sql queries the following methods are provided to construct relation objects from external data from_arrow from_df read_csv read_json read_parquet sql queries relation objects can be queried through sql through so-called replacement scans if you have a relation object stored in a variable you can refer to that variable as if it was a sql table in the from clause this allows you to incrementally build queries using relation objects import duckdb rel duckdb sql select from range 1000000 tbl id duckdb sql select sum id from rel show sum id int128 499999500000 operations there are a number of operations that can be performed on relations these are all short-hand for running the sql queries - and will return relations again themselves aggregate expr groups apply an optionally grouped aggregate over the relation the system will automatically group by any columns that are not aggregates import duckdb rel duckdb sql select from range 1000000 tbl id rel aggregate id 2 as g sum id min id max id g sum id min id max id int64 int128 int64 int64 0 249999500000 0 999998 1 250000000000 1 999999 except_ rel select all rows in the first relation that do not occur in the second relation the relations must have the same number of columns import duckdb r1 duckdb sql select from range 10 tbl id r2 duckdb sql select from range 5 tbl id r1 except_ r2 show id int64 5 6 7 8 9 filter condition apply the given condition to the relation filtering any rows that do not satisfy the condition import duckdb rel duckdb sql select from range 1000000 tbl id rel filter id 5 limit 3 show id int64 6 7 8 intersect rel select the intersection of two relations - returning all rows that occur in both relations the relations must have the same number of columns import duckdb r1 duckdb sql select from range 10 tbl id r2 duckdb sql select from range 5 tbl id r1 intersect r2 show id int64 0 1 2 3 4 join rel condition type inner combine two relations joining them based on the provided condition import duckdb r1 duckdb sql select from range 5 tbl id set_alias r1 r2 duckdb sql select from range 10 15 tbl id set_alias r2 r1 join r2 r1 id 10 r2 id show id id int64 int64 0 10 1 11 2 12 3 13 4 14 limit n offset 0 select the first n rows optionally offset by offset import duckdb rel duckdb sql select from range 1000000 tbl id rel limit 3 show id int64 0 1 2 order expr sort the relation by the given set of expressions import duckdb rel duckdb sql select from range 1000000 tbl id rel order id desc limit 3 show id int64 999999 999998 999997 project expr apply the given expression to each row in the relation import duckdb rel duckdb sql select from range 1000000 tbl id rel project id 10 as id_plus_ten limit 3 show id_plus_ten int64 10 11 12 union rel combine two relations returning all rows in r1 followed by all rows in r2 the relations must have the same number of columns import duckdb r1 duckdb sql select from range 5 tbl id r2 duckdb sql select from range 10 15 tbl id r1 union r2 show id int64 0 1 2 3 4 10 11 12 13 14 result output the result of relations can be converted to various types of python structures see the result conversion page for more information the result of relations can also be directly written to files using the below methods write_csv write_parquet",
			"category": "Python",
			"url": "/docs/api/python/relational_api",
			"blurb": "The Relational API is an alternative API that can be used to incrementally construct queries. The API is centered..."
		},
		{
			"title": "Relational API on Pandas",
			"text": "duckdb offers a relational api that can be used to chain together query operations these are lazily evaluated so that duckdb can optimize their execution these operators can act on pandas dataframes duckdb tables or views which can point to any underlying storage format that duckdb can read such as csv or parquet files etc here we show a simple example of reading from a pandas dataframe and returning a dataframe import duckdb import pandas connect to an in-memory database con duckdb connect input_df pandas dataframe from_dict i 1 2 3 4 j one two three four create a duckdb relation from a dataframe rel con from_df input_df chain together relational operators this is a lazy operation so the operations are not yet executed equivalent to select i j i 2 as two_i from input_df order by i desc limit 2 transformed_rel rel filter i 2 project i j i 2 as two_i order i desc limit 2 trigger execution by requesting df of the relation df could have been added to the end of the chain above - it was separated for clarity output_df transformed_rel df relational operators can also be used to group rows aggregate find distinct combinations of values join union and more they are also able to directly insert results into a duckdb table or write to a csv please see these additional examples and the available relational methods on the duckdbpyrelation class",
			"category": "Python",
			"url": "/docs/guides/python/relational_api_pandas",
			"blurb": "DuckDB offers a relational API that can be used to chain together query operations. These are lazily evaluated so..."
		},
		{
			"title": "Result Conversion",
			"text": "duckdb s python client provides multiple additional methods that can be used to efficiently retrieve data numpy fetchnumpy fetches the data as a dictionary of numpy arrays pandas df fetches the data as a pandas dataframe fetchdf is an alias of df fetch_df is an alias of df fetch_df_chunk vector_multiple fetches a portion of the results into a dataframe the number of rows returned in each chunk is the vector size 2048 by default vector_multiple 1 by default apache arrow arrow fetches the data as an arrow table fetch_arrow_table is an alias of arrow fetch_record_batch chunk_size returns an arrow record batch reader with chunk_size rows per batch polars pl fetches the data as a polars dataframe below are some examples using this functionality see the python guides for more examples fetch as pandas dataframe df con execute select from items fetchdf print df item value count 0 jeans 20 0 1 1 hammer 42 2 2 2 laptop 2000 0 1 3 chainsaw 500 0 10 4 iphone 300 0 2 fetch as dictionary of numpy arrays arr con execute select from items fetchnumpy print arr item masked_array data jeans hammer laptop chainsaw iphone mask false false false false false fill_value dtype object value masked_array data 20 0 42 2 2000 0 500 0 300 0 mask false false false false false fill_value 1e 20 count masked_array data 1 2 1 10 2 mask false false false false false fill_value 999999 dtype int32 fetch as an arrow table converting to pandas afterwards just for pretty printing tbl con execute select from items fetch_arrow_table print tbl to_pandas item value count 0 jeans 20 00 1 1 hammer 42 20 2 2 laptop 2000 00 1 3 chainsaw 500 00 10 4 iphone 300 00 2",
			"category": "Python",
			"url": "/docs/api/python/result_conversion",
			"blurb": "DuckDB's Python client provides multiple additional methods that can be used to efficiently retrieve data. NumPy..."
		},
		{
			"title": "Rust API",
			"text": "installation the duckdb rust api can be installed from crates io please see the docs rs for details basic api usage duckdb-rs is an ergonomic wrapper based on the duckdb c api please refer to the readme for details startup shutdown to use duckdb you must first initialize a connection handle using connection open connection open takes as parameter the database file to read and write from if the database file does not exist it will be created the file extension may be db duckdb or anything else you can also use connection open_in_memory to create an in-memory database note that for an in-memory database no data is persisted to disk i e all data is lost when you exit the process use duckdb params connection result let conn connection open_in_memory you can conn close the connection manually or just leave it out of scope we had implement the drop trait which will automatically close the underlining db connection for you querying sql queries can be sent to duckdb using the execute method of connections or we can also prepare the statement and then query on that derive debug struct person id i32 name string data option vec u8 conn execute insert into person name data values params me name me data let mut stmt conn prepare select id name data from person let person_iter stmt query_map row ok person id row get 0 name row get 1 data row get 2 for person in person_iter println found person person unwrap appender the rust client supports the duckdb appender api for bulk inserts for example fn insert_rows conn connection - result let mut app conn appender foo app append_rows 1 2 3 4 5 6 7 8 9 10 ok",
			"category": "Api",
			"url": "/docs/api/rust",
			"blurb": "Installation The DuckDB Rust API can be installed from crates.io . Please see the docs.rs for details. Basic API..."
		},
		{
			"title": "S3 API Support",
			"text": "the httpfs extension supports reading writing globbing files on object storage servers using the s3 api s3 offers a standard api to read and write to remote files while regular http servers predating s3 do not offer a common write api duckdb conforms to the s3 api that is now common among industry storage providers platforms the httpfs filesystem is tested with aws s3 minio google cloud and lakefs other services that implement the s3 api such as cloudflare r2 should also work but not all features may be supported the following table shows which parts of the s3 api are required for each httpfs feature feature required s3 api features --- --- public file reads http range requests private file reads secret key or session token authentication file glob listobjectv2 file writes multipart upload configuration and authentication the preferred way to configure and authenticate to s3 endpoints is to use secrets multiple secret providers are available prior to version 0 10 0 duckdb did not have a secrets manager hence the configuration of and authentication to s3 endpoints was handled via variables see the legacy authentication scheme for the s3 api config provider the default provider config i e user-configured allows access to the s3 bucket by manually providing a key for example create secret secret1 type s3 key_id akiaiosfodnn7example secret wjalrxutnfemi k7mdeng bpxrficyexamplekey region us-east-1 now to query using the above secret simply query any s3 prefixed file select from s3 my-bucket file parquet credential_chain provider the credential_chain provider allows automatically fetching credentials using mechanisms provided by the aws sdk for example to use the aws sdk default provider create secret secret2 type s3 provider credential_chain again to query a file using the above secret simply query any s3 prefixed file duckdb also allows specifying a specific chain using the chain keyword this takes a separated list of providers that will be tried in order for example create secret secret3 type s3 provider credential_chain chain env config the possible values for chain are the following config sts sso env instance process task_role the credential_chain provider also allows overriding the automatically fetched config for example to automatically load credentials and then override the region run create secret secret4 type azure provider credential_chain chain config region eu-west-1 overview of s3 secret parameters below is a complete list of the supported parameters that can be used for both the config and credential_chain providers name description secret type default ------------------------------ -------------------------------------------------------------------------------------- ------------------ ---------- -------------------------------------------- key_id the id of the key to use s3 gcs r2 string - secret the secret of the key to use s3 gcs r2 string - region the region for which to authenticate should match the region of the bucket to query s3 gcs r2 string us-east-1 session_token optionally a session token can be passed to use temporary credentials s3 gcs r2 string - endpoint specify a custom s3 endpoint s3 gcs r2 string s3 amazonaws com for s3 url_style either vhost or path s3 gcs r2 string vhost for s3 path for r2 and gcs use_ssl whether to use https or http s3 gcs r2 boolean true url_compatibility_mode can help when urls contain problematic characters s3 gcs r2 boolean true account_id the r2 account id to use for generating the endpoint url r2 string - platform-specific secret types r2 secrets while cloudflare r2 uses the regular s3 api duckdb has a special secret type r2 to make configuring it a bit simpler create secret secret5 type r2 key_id akiaiosfodnn7example secret wjalrxutnfemi k7mdeng bpxrficyexamplekey account_id my_account_id note the addition of the account_id which is used to generate to correct endpoint url for you also note that for r2 secrets can also use both the config and credential_chain providers finally r2 secrets are only available when using urls starting with r2 for example select from read_parquet r2 some file that uses r2 secret file parquet gcs secrets while google cloud storage is accessed by duckdb using the s3 api duckdb has a special secret type gcs to make configuring it a bit simpler create secret secret6 type gcs key_id my_key secret my_secret note that the above secret will automatically have the correct google cloud storage endpoint configured also note that for gcs secrets can also use both the config and credential_chain providers finally gcs secrets are only available when using urls starting with gcs or gs for example select from read_parquet gcs some file that uses gcs secret file parquet reading reading files from s3 is now as simple as select from s3 bucket file extension multiple files are also possible for example select from read_parquet s3 bucket file1 parquet s3 bucket file2 parquet glob file globbing is implemented using the listobjectv2 api call and allows to use filesystem-like glob patterns to match multiple files for example select from read_parquet s3 bucket parquet this query matches all files in the root of the bucket with the parquet extension several features for matching are supported such as to match any number of any character for any single character or 0-9 for a single character in a range of characters select count from read_parquet s3 bucket folder 100 t 0-9 parquet a useful feature when using globs is the filename option which adds a column named filename that encodes the file that a particular row originated from select from read_parquet s3 bucket parquet filename true could for example result in column_a column_b filename --- --- --- 1 examplevalue1 s3 bucket file1 parquet 2 examplevalue1 s3 bucket file2 parquet hive partitioning duckdb also offers support for the hive partitioning scheme which is available when using http s and s3 endpoints writing writing to s3 uses the multipart upload api this allows duckdb to robustly upload files at high speed writing to s3 works for both csv and parquet copy table_name to s3 bucket file extension partitioned copy to s3 also works copy table to s3 my-bucket partitioned format parquet partition_by part_col_a part_col_b an automatic check is performed for existing files directories which is currently quite conservative and on s3 will add a bit of latency to disable this check and force writing an overwrite_or_ignore flag is added copy table to s3 my-bucket partitioned format parquet partition_by part_col_a part_col_b overwrite_or_ignore true the naming scheme of the written files looks like this s3 my-bucket partitioned part_col_a val part_col_b val data_ thread_number parquet configuration some additional configuration options exist for the s3 upload though the default values should suffice for most use cases setting description --- --- s3_uploader_max_parts_per_file used for part size calculation see aws docs s3_uploader_max_filesize used for part size calculation see aws docs s3_uploader_thread_limit maximum number of uploader threads",
			"category": "Httpfs",
			"url": "/docs/extensions/httpfs/s3api",
			"blurb": "The httpfs extension supports reading/writing/globbing files on object storage servers using the S3 API. S3 offers a..."
		},
		{
			"title": "S3 Express One",
			"text": "in late 2023 aws announced the s3 express one zone a high-speed variant of traditional s3 buckets duckdb can read s3 express one buckets using the httpfs extension credentials and configuration the configuration of s3 express one buckets is similar to regular s3 buckets with one exception we have to specify the endpoint according to the following pattern s3express- availability zone region amazonaws com where the availability zone e g use-az5 can be obtained from the s3 express one bucket s configuration page and the region is the aws region e g us-east-1 for example to allow duckdb to use an s3 express one bucket configure the secrets manager as follows create secret type s3 region us-east-1 key_id akiaiosfodnn7example secret wjalrxutnfemi k7mdeng bpxrficyexamplekey endpoint s3express-use1-az5 us-east-1 amazonaws com querying you can query the s3 express one bucket as any other s3 bucket select from s3 express-bucket-name--use1-az5--x-s3 my-file parquet performance we ran two experiments on a c7gd 12xlarge instance using the ldbc sf300 comments creationdate parquet file file also used in the microbenchmarks of the performance guide experiment file size runtime --- - --- loading only from parquet 4 1 gb 3 5s creating local table from parquet 4 1 gb 5 1s the loading only variant is running the load as part of an explain analyze statement to measure the runtime without account creating a local table while the creating local table variant uses create table as to create a persistent table on the local disk",
			"category": "Import",
			"url": "/docs/guides/import/s3_express_one",
			"blurb": "In late 2023, AWS announced the S3 Express One Zone , a high-speed variant of traditional S3 buckets. DuckDB can read..."
		},
		{
			"title": "S3 Iceberg Import",
			"text": "prerequisites to load an iceberg file from s3 both the httpfs and iceberg extensions are required they can be installed use the install sql command the extensions only need to be installed once install httpfs install iceberg to load the extensions for usage use the load command load httpfs load iceberg credentials after loading the extensions set up the credentials and s3 region to read data you may either use an access key and secret or a token create secret type s3 key_id akiaiosfodnn7example secret wjalrxutnfemi k7mdeng bpxrficyexamplekey region us-east-1 alternatively use the aws extension to retrieve the credentials automatically create secret type s3 provider credential_chain loading iceberg tables from s3 after the extensions are set up and the s3 credentials are correctly configured iceberg table can be read from s3 using the following command select from iceberg_scan s3 bucket iceberg-table-folder metadata id metadata json note that you need to link directly to the manifest file otherwise you ll get an error like this error io error cannot open file s3 bucket iceberg-table-folder metadata version-hint text no such file or directory",
			"category": "Import",
			"url": "/docs/guides/import/s3_iceberg_import",
			"blurb": "Prerequisites To load an Iceberg file from S3, both the httpfs and iceberg extensions are required. They can be..."
		},
		{
			"title": "S3 Parquet Export",
			"text": "to write a parquet file to s3 the httpfs extension is required this can be installed use the install sql command this only needs to be run once install httpfs to load the httpfs extension for usage use the load sql command load httpfs after loading the httpfs extension set up the credentials to write data note that the region parameter should match the region of the bucket you want to access create secret type s3 key_id akiaiosfodnn7example secret wjalrxutnfemi k7mdeng bpxrficyexamplekey region us-east-1 alternatively use the aws extension to retrieve the credentials automatically create secret type s3 provider credential_chain after the httpfs extension is set up and the s3 credentials are correctly configured parquet files can be written to s3 using the following command copy table_name to s3 bucket file parquet similarly google cloud storage gcs is supported through the interoperability api you need to create hmac keys and provide the credentials as follows create secret type gcs key_id akiaiosfodnn7example secret wjalrxutnfemi k7mdeng bpxrficyexamplekey after setting up the gcs credentials you can export using copy table_name to gs gcs_bucket file parquet",
			"category": "Import",
			"url": "/docs/guides/import/s3_export",
			"blurb": "To write a Parquet file to S3, the httpfs extension is required. This can be installed use the INSTALL SQL command...."
		},
		{
			"title": "S3 Parquet Import",
			"text": "prerequisites to load a parquet file from s3 the httpfs extension is required this can be installed use the install sql command this only needs to be run once install httpfs to load the httpfs extension for usage use the load sql command load httpfs credentials and configuration after loading the httpfs extension set up the credentials and s3 region to read data create secret type s3 key_id akiaiosfodnn7example secret wjalrxutnfemi k7mdeng bpxrficyexamplekey region us-east-1 alternatively use the aws extension to retrieve the credentials automatically create secret type s3 provider credential_chain querying after the httpfs extension is set up and the s3 configuration is set correctly parquet files can be read from s3 using the following command select from read_parquet s3 bucket file google cloud storage gcs and cloudflare r2 duckdb can also handle google cloud storage gcs and cloudflare r2 via the s3 api see the relevant guides for details",
			"category": "Import",
			"url": "/docs/guides/import/s3_import",
			"blurb": "Prerequisites To load a Parquet file from S3, the httpfs extension is required. This can be installed use the INSTALL..."
		},
		{
			"title": "SAMPLE Clause",
			"text": "the sample clause allows you to run the query on a sample from the base table this can significantly speed up processing of queries at the expense of accuracy in the result samples can also be used to quickly see a snapshot of the data when exploring a data set the sample clause is applied right after anything in the from clause i e after any joins but before the where clause or any aggregates see the sample page for more information examples -- select a sample of 1 of the addresses table using default system sampling select from addresses using sample 1 -- select a sample of 1 of the addresses table using bernoulli sampling select from addresses using sample 1 bernoulli -- select a sample of 10 rows from the subquery select from select from addresses using sample 10 rows syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/sample",
			"blurb": "The SAMPLE clause allows you to run the query on a sample from the base table. This can significantly speed up..."
		},
		{
			"title": "SELECT Clause",
			"text": "the select clause specifies the list of columns that will be returned by the query while it appears first in the clause logically the expressions here are executed only at the end the select clause can contain arbitrary expressions that transform the output as well as aggregates and window functions examples -- select all columns from the table called table_name select from table_name -- perform arithmetic on columns in a table and provide an alias select col1 col2 as res sqrt col1 as root from table_name -- select all unique cities from the addresses table select distinct city from addresses -- return the total number of rows in the addresses table select count from addresses -- select all columns except the city column from the addresses table select exclude city from addresses -- select all columns from the addresses table but replace city with lower city select replace lower city as city from addresses -- select all columns matching the given regex from the table select columns number d from addresses -- compute a function on all given columns of a table select min columns from addresses -- to select columns with spaces or special characters use double quotes select some column name from tbl syntax select list the select clause contains a list of expressions that specify the result of a query the select list can refer to any columns in the from clause and combine them using expressions as the output of a sql query is a table - every expression in the select clause also has a name the expressions can be explicitly named using the as clause e g expr as name if a name is not provided by the user the expressions are named automatically by the system column names are case-insensitive see the rules for case sensitivity for more details star expressions -- select all columns from the table called table_name select from table_name -- select all columns matching the given regex from the table select columns number d from addresses the star expression is a special expression that expands to multiple expressions based on the contents of the from clause in the simplest case expands to all expressions in the from clause columns can also be selected using regular expressions or lambda functions see the star expression page for more details distinct clause -- select all unique cities from the addresses table select distinct city from addresses the distinct clause can be used to return only the unique rows in the result - so that any duplicate rows are filtered out queries starting with select distinct run deduplication which is an expensive operation therefore only use distinct if necessary distinct on clause -- select only the highest population city for each country select distinct on country city population from cities order by population desc the distinct on clause returns only one row per unique value in the set of expressions as defined in the on clause if an order by clause is present the row that is returned is the first row that is encountered as per the order by criteria if an order by clause is not present the first row that is encountered is not defined and can be any row in the table when querying large data sets using distinct on all columns can be expensive therefore consider using distinct on on a column or a set of columns which guaranetees a sufficient degree of uniqueness for your results for example using distinct on on the key column s of a table guarantees full uniqueness aggregates -- return the total number of rows in the addresses table select count from addresses -- return the total number of rows in the addresses table grouped by city select city count from addresses group by city aggregate functions are special functions that combine multiple rows into a single value when aggregate functions are present in the select clause the query is turned into an aggregate query in an aggregate query all expressions must either be part of an aggregate function or part of a group as specified by the group by clause window functions -- generate a row_number column containing incremental identifiers for each row select row_number over from sales -- compute the difference between the current amount and the previous amount by order of time select amount - lag amount over order by time from sales window functions are special functions that allow the computation of values relative to other rows in a result window functions are marked by the over clause which contains the window specification the window specification defines the frame or context in which the window function is computed see the window functions page for more information unnest function -- unnest an array by one level select unnest 1 2 3 -- unnest a struct by one level select unnest a 42 b 84 the unnest function is a special function that can be used together with arrays lists or structs the unnest function strips one level of nesting from the type for example int is transformed into int struct a int b int is transformed into a int b int the unnest function can be used to transform nested types into regular scalar types which makes them easier to operate on",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/select",
			"blurb": "The SELECT clause specifies the list of columns that will be returned by the query."
		},
		{
			"title": "SELECT Statement",
			"text": "the select statement retrieves rows from the database examples -- select all columns from the table tbl select from tbl -- select the rows from tbl select j from tbl where i 3 -- perform an aggregate grouped by the column i select i sum j from tbl group by i -- select only the top 3 rows from the tbl select from tbl order by i desc limit 3 -- join two tables together using the using clause select from t1 join t2 using a b -- use column indexes to select the first and third column from the table tbl select 1 3 from tbl -- select all unique cities from the addresses table select distinct city from addresses syntax the select statement retrieves rows from the database the canonical order of a select statement is as follows with less common clauses being indented select select_list from tables using sample sample_expr where condition group by groups having group_filter window window_expr qualify qualify_filter order by order_expr limit n optionally the select statement can be prefixed with a with clause as the select statement is so complex we have split up the syntax diagrams into several parts the full syntax diagram can be found at the bottom of the page select clause the select clause specifies the list of columns that will be returned by the query while it appears first in the clause logically the expressions here are executed only at the end the select clause can contain arbitrary expressions that transform the output as well as aggregates and window functions the distinct keyword ensures that only unique tuples are returned column names are case-insensitive see the rules for case sensitivity for more details from clause the from clause specifies the source of the data on which the remainder of the query should operate logically the from clause is where the query starts execution the from clause can contain a single table a combination of multiple tables that are joined together or another select query inside a subquery node sample clause the sample clause allows you to run the query on a sample from the base table this can significantly speed up processing of queries at the expense of accuracy in the result samples can also be used to quickly see a snapshot of the data when exploring a data set the sample clause is applied right after anything in the from clause i e after any joins but before the where clause or any aggregates see the sample page for more information where clause the where clause specifies any filters to apply to the data this allows you to select only a subset of the data in which you are interested logically the where clause is applied immediately after the from clause group by and having clauses the group by clause specifies which grouping columns should be used to perform any aggregations in the select clause if the group by clause is specified the query is always an aggregate query even if no aggregations are present in the select clause window clause the window clause allows you to specify named windows that can be used within window functions these are useful when you have multiple window functions as they allow you to avoid repeating the same window clause qualify clause the qualify clause is used to filter the result of window functions order by and limit clauses order by and limit are output modifiers logically they are applied at the very end of the query the limit clause restricts the amount of rows fetched and the order by clause sorts the rows on the sorting criteria in either ascending or descending order values list a values list is a set of values that is supplied instead of a select statement row ids for each table the rowid pseudocolumn returns the row identifiers based on the physical storage create table t id int content string insert into t values 42 hello 43 world select rowid id content from t rowid id content 0 42 hello 1 43 world in the current storage these identifiers are contiguous unsigned integers 0 1 if no rows were deleted deletions introduce gaps in the rowids which may be reclaimed later therefore it is strongly recommended not to use rowids as identifiers the rowid values are stable within a transaction if there is a user-defined column named rowid it shadows the rowid pseudocolumn common table expressions full syntax diagram below is the full syntax diagram of the select statement",
			"category": "Statements",
			"url": "/docs/sql/statements/select",
			"blurb": "The SELECT statement retrieves rows from the database."
		},
		{
			"title": "SET/RESET Statements",
			"text": "the set statement modifies the provided duckdb configuration option at the specified scope examples -- update the memory_limit configuration value set memory_limit 10gb -- configure the system to use 1 thread set threads 1 -- or use the to keyword set threads to 1 -- change configuration option to default value reset threads -- retrieve configuration value select current_setting threads -- set the default catalog search path globally set global search_path db1 db2 -- set the default collation for the session set session default_collation nocase syntax set updates a duckdb configuration option to the provided value reset the reset statement changes the given duckdb configuration option to the default value scopes configuration options can have different scopes global configuration value is used or reset across the entire duckdb instance session configuration value is used or reset only for the current session attached to a duckdb instance local not yet implemented when not specified the default scope for the configuration option is used for most options this is global configuration see the configuration page for the full list of configuration options",
			"category": "Statements",
			"url": "/docs/sql/statements/set",
			"blurb": "The SET statement modifies the provided DuckDB configuration option at the specified scope. Examples -- Update the..."
		},
		{
			"title": "SQL Introduction",
			"text": "here we provide an overview of how to perform simple operations in sql this tutorial is only intended to give you an introduction and is in no way a complete tutorial on sql this tutorial is adapted from the postgresql tutorial in the examples that follow we assume that you have installed the duckdb command line interface cli shell see the installation page for information on how to install the cli concepts duckdb is a relational database management system rdbms that means it is a system for managing data stored in relations a relation is essentially a mathematical term for a table each table is a named collection of rows each row of a given table has the same set of named columns and each column is of a specific data type tables themselves are stored inside schemas and a collection of schemas constitutes the entire database that you can access creating a new table you can create a new table by specifying the table name along with all column names and their types create table weather city varchar temp_lo integer -- minimum temperature on a day temp_hi integer -- maximum temperature on a day prcp real date date you can enter this into the shell with the line breaks the command is not terminated until the semicolon white space i e spaces tabs and newlines can be used freely in sql commands that means you can type the command aligned differently than above or even all on one line two dash characters -- introduce comments whatever follows them is ignored up to the end of the line sql is case insensitive about key words and identifiers in the sql command we first specify the type of command that we want to perform create table after that follows the parameters for the command first the table name weather is given then the column names and column types follow city varchar specifies that the table has a column called city that is of type varchar varchar specifies a data type that can store text of arbitrary length the temperature fields are stored in an integer type a type that stores integer numbers i e whole numbers without a decimal point real columns store single precision floating-point numbers i e numbers with a decimal point date stores a date i e year month day combination date only stores the specific day not a time associated with that day duckdb supports the standard sql types integer smallint real double decimal char n varchar n date time and timestamp the second example will store cities and their associated geographical location create table cities name varchar lat decimal lon decimal finally it should be mentioned that if you don t need a table any longer or want to recreate it differently you can remove it using the following command drop table tablename populating a table with rows the insert statement is used to populate a table with rows insert into weather values san francisco 46 50 0 25 1994-11-27 constants that are not numeric values e g text and dates must be surrounded by single quotes as in the example input dates for the date type must be formatted as yyyy-mm-dd we can insert into the cities table in the same manner insert into cities values san francisco -194 0 53 0 the syntax used so far requires you to remember the order of the columns an alternative syntax allows you to list the columns explicitly insert into weather city temp_lo temp_hi prcp date values san francisco 43 57 0 0 1994-11-29 you can list the columns in a different order if you wish or even omit some columns e g if the prcp is unknown insert into weather date city temp_hi temp_lo values 1994-11-29 hayward 54 37 many developers consider explicitly listing the columns better style than relying on the order implicitly please enter all the commands shown above so you have some data to work with in the following sections you could also have used copy to load large amounts of data from csv files this is usually faster because the copy command is optimized for this application while allowing less flexibility than insert an example with weather csv would be copy weather from weather csv where the file name for the source file must be available on the machine running the process there are many other ways of loading data into duckdb see the corresponding documentation section for more information querying a table to retrieve data from a table the table is queried a sql select statement is used to do this the statement is divided into a select list the part that lists the columns to be returned a table list the part that lists the tables from which to retrieve the data and an optional qualification the part that specifies any restrictions for example to retrieve all the rows of table weather type select from weather here is a shorthand for all columns so the same result would be had with select city temp_lo temp_hi prcp date from weather the output should be city temp_lo temp_hi prcp date varchar int32 int32 float date san francisco 46 50 0 25 1994-11-27 san francisco 43 57 0 0 1994-11-29 hayward 37 54 1994-11-29 you can write expressions not just simple column references in the select list for example you can do select city temp_hi temp_lo 2 as temp_avg date from weather this should give city temp_avg date varchar double date san francisco 48 0 1994-11-27 san francisco 50 0 1994-11-29 hayward 45 5 1994-11-29 notice how the as clause is used to relabel the output column the as clause is optional a query can be qualified by adding a where clause that specifies which rows are wanted the where clause contains a boolean truth value expression and only rows for which the boolean expression is true are returned the usual boolean operators and or and not are allowed in the qualification for example the following retrieves the weather of san francisco on rainy days select from weather where city san francisco and prcp 0 0 result city temp_lo temp_hi prcp date varchar int32 int32 float date san francisco 46 50 0 25 1994-11-27 you can request that the results of a query be returned in sorted order select from weather order by city city temp_lo temp_hi prcp date varchar int32 int32 float date hayward 37 54 1994-11-29 san francisco 46 50 0 25 1994-11-27 san francisco 43 57 0 0 1994-11-29 in this example the sort order isn t fully specified and so you might get the san francisco rows in either order but you d always get the results shown above if you do select from weather order by city temp_lo you can request that duplicate rows be removed from the result of a query select distinct city from weather city varchar hayward san francisco here again the result row ordering might vary you can ensure consistent results by using distinct and order by together select distinct city from weather order by city joins between tables thus far our queries have only accessed one table at a time queries can access multiple tables at once or access the same table in such a way that multiple rows of the table are being processed at the same time a query that accesses multiple rows of the same or different tables at one time is called a join query as an example say you wish to list all the weather records together with the location of the associated city to do that we need to compare the city column of each row of the weather table with the name column of all rows in the cities table and select the pairs of rows where these values match this would be accomplished by the following query select from weather cities where city name city temp_lo temp_hi prcp date name lat lon varchar int32 int32 float date varchar decimal 18 3 decimal 18 3 san francisco 46 50 0 25 1994-11-27 san francisco -194 000 53 000 san francisco 43 57 0 0 1994-11-29 san francisco -194 000 53 000 observe two things about the result set there is no result row for the city of hayward this is because there is no matching entry in the cities table for hayward so the join ignores the unmatched rows in the weather table we will see shortly how this can be fixed there are two columns containing the city name this is correct because the lists of columns from the weather and cities tables are concatenated in practice this is undesirable though so you will probably want to list the output columns explicitly rather than using select city temp_lo temp_hi prcp date lon lat from weather cities where city name city temp_lo temp_hi prcp date lon lat varchar int32 int32 float date decimal 18 3 decimal 18 3 san francisco 46 50 0 25 1994-11-27 53 000 -194 000 san francisco 43 57 0 0 1994-11-29 53 000 -194 000 since the columns all had different names the parser automatically found which table they belong to if there were duplicate column names in the two tables you d need to qualify the column names to show which one you meant as in select weather city weather temp_lo weather temp_hi weather prcp weather date cities lon cities lat from weather cities where cities name weather city it is widely considered good style to qualify all column names in a join query so that the query won t fail if a duplicate column name is later added to one of the tables join queries of the kind seen thus far can also be written in this alternative form select from weather inner join cities on weather city cities name this syntax is not as commonly used as the one above but we show it here to help you understand the following topics now we will figure out how we can get the hayward records back in what we want the query to do is to scan the weather table and for each row to find the matching cities row s if no matching row is found we want some empty values to be substituted for the cities table s columns this kind of query is called an outer join the joins we have seen so far are inner joins the command looks like this select from weather left outer join cities on weather city cities name city temp_lo temp_hi prcp date name lat lon varchar int32 int32 float date varchar decimal 18 3 decimal 18 3 san francisco 46 50 0 25 1994-11-27 san francisco -194 000 53 000 san francisco 43 57 0 0 1994-11-29 san francisco -194 000 53 000 hayward 37 54 1994-11-29 this query is called a left outer join because the table mentioned on the left of the join operator will have each of its rows in the output at least once whereas the table on the right will only have those rows output that match some row of the left table when outputting a left-table row for which there is no right-table match empty null values are substituted for the right-table columns aggregate functions like most other relational database products duckdb supports aggregate functions an aggregate function computes a single result from multiple input rows for example there are aggregates to compute the count sum avg average max maximum and min minimum over a set of rows as an example we can find the highest low-temperature reading anywhere with select max temp_lo from weather max temp_lo int32 46 if we wanted to know what city or cities that reading occurred in we might try select city from weather where temp_lo max temp_lo -- wrong but this will not work since the aggregate max cannot be used in the where clause this restriction exists because the where clause determines which rows will be included in the aggregate calculation so obviously it has to be evaluated before aggregate functions are computed however as is often the case the query can be restated to accomplish the desired result here by using a subquery select city from weather where temp_lo select max temp_lo from weather city varchar san francisco this is ok because the subquery is an independent computation that computes its own aggregate separately from what is happening in the outer query aggregates are also very useful in combination with group by clauses for example we can get the maximum low temperature observed in each city with select city max temp_lo from weather group by city city max temp_lo varchar int32 san francisco 46 hayward 37 which gives us one output row per city each aggregate result is computed over the table rows matching that city we can filter these grouped rows using having select city max temp_lo from weather group by city having max temp_lo 40 city max temp_lo varchar int32 hayward 37 which gives us the same results for only the cities that have all temp_lo values below 40 finally if we only care about cities whose names begin with s we can use the like operator select city max temp_lo from weather where city like s -- 1 group by city having max temp_lo 40 more information about the like operator can be found in the pattern matching page it is important to understand the interaction between aggregates and sql s where and having clauses the fundamental difference between where and having is this where selects input rows before groups and aggregates are computed thus it controls which rows go into the aggregate computation whereas having selects group rows after groups and aggregates are computed thus the where clause must not contain aggregate functions it makes no sense to try to use an aggregate to determine which rows will be inputs to the aggregates on the other hand the having clause always contains aggregate functions in the previous example we can apply the city name restriction in where since it needs no aggregate this is more efficient than adding the restriction to having because we avoid doing the grouping and aggregate calculations for all rows that fail the where check updates you can update existing rows using the update command suppose you discover the temperature readings are all off by 2 degrees after november 28 you can correct the data as follows update weather set temp_hi temp_hi - 2 temp_lo temp_lo - 2 where date 1994-11-28 look at the new state of the data select from weather city temp_lo temp_hi prcp date varchar int32 int32 float date san francisco 46 50 0 25 1994-11-27 san francisco 41 55 0 0 1994-11-29 hayward 35 52 1994-11-29 deletions rows can be removed from a table using the delete command suppose you are no longer interested in the weather of hayward then you can do the following to delete those rows from the table delete from weather where city hayward all weather records belonging to hayward are removed select from weather city temp_lo temp_hi prcp date varchar int32 int32 float date san francisco 46 50 0 25 1994-11-27 san francisco 41 55 0 0 1994-11-29 one should be wary of statements of the form delete from tablename without a qualification delete will remove all rows from the given table leaving it empty the system will not request confirmation before doing this",
			"category": "SQL",
			"url": "/docs/sql/introduction",
			"blurb": "Here we provide an overview of how to perform simple operations in SQL. This tutorial is only intended to give you an..."
		},
		{
			"title": "SQL on Apache Arrow",
			"text": "duckdb can query multiple different types of apache arrow objects apache arrow tables arrow tables stored in local variables can be queried as if they are regular tables within duckdb import duckdb import pyarrow as pa connect to an in-memory database con duckdb connect my_arrow_table pa table from_pydict i 1 2 3 4 j one two three four query the apache arrow table my_arrow_table and return as an arrow table results con execute select from my_arrow_table where i 2 arrow apache arrow datasets arrow datasets stored as variables can also be queried as if they were regular tables datasets are useful to point towards directories of parquet files to analyze large datasets duckdb will push column selections and row filters down into the dataset scan operation so that only the necessary data is pulled into memory import duckdb import pyarrow as pa import tempfile import pathlib import pyarrow parquet as pq import pyarrow dataset as ds connect to an in-memory database con duckdb connect my_arrow_table pa table from_pydict i 1 2 3 4 j one two three four create example parquet files and save in a folder base_path pathlib path tempfile gettempdir base_path parquet_folder mkdir exist_ok true pq write_to_dataset my_arrow_table str base_path parquet_folder link to parquet files using an arrow dataset my_arrow_dataset ds dataset str base_path parquet_folder query the apache arrow dataset my_arrow_dataset and return as an arrow table results con execute select from my_arrow_dataset where i 2 arrow apache arrow scanners arrow scanners stored as variables can also be queried as if they were regular tables scanners read over a dataset and select specific columns or apply row-wise filtering this is similar to how duckdb pushes column selections and filters down into an arrow dataset but using arrow compute operations instead arrow can use asynchronous io to quickly access files import duckdb import pyarrow as pa import tempfile import pathlib import pyarrow parquet as pq import pyarrow dataset as ds import pyarrow compute as pc connect to an in-memory database con duckdb connect my_arrow_table pa table from_pydict i 1 2 3 4 j one two three four create example parquet files and save in a folder base_path pathlib path tempfile gettempdir base_path parquet_folder mkdir exist_ok true pq write_to_dataset my_arrow_table str base_path parquet_folder link to parquet files using an arrow dataset my_arrow_dataset ds dataset str base_path parquet_folder define the filter to be applied while scanning equivalent to where i 2 scanner_filter pc field i pc scalar 2 arrow_scanner ds scanner from_dataset my_arrow_dataset filter scanner_filter query the apache arrow scanner arrow_scanner and return as an arrow table results con execute select from arrow_scanner arrow apache arrow recordbatchreaders arrow recordbatchreaders are a reader for arrow s streaming binary format and can also be queried directly as if they were tables this streaming format is useful when sending arrow data for tasks like interprocess communication or communicating between language runtimes import duckdb import pyarrow as pa connect to an in-memory database con duckdb connect my_recordbatch pa recordbatch from_pydict i 1 2 3 4 j one two three four my_recordbatchreader pa ipc recordbatchreader from_batches my_recordbatch schema my_recordbatch query the apache arrow recordbatchreader my_recordbatchreader and return as an arrow table results con execute select from my_recordbatchreader where i 2 arrow",
			"category": "Python",
			"url": "/docs/guides/python/sql_on_arrow",
			"blurb": "DuckDB can query multiple different types of Apache Arrow objects. Apache Arrow Tables Arrow Tables stored in local..."
		},
		{
			"title": "SQL on Pandas",
			"text": "pandas dataframes stored in local variables can be queried as if they are regular tables within duckdb import duckdb import pandas create a pandas dataframe my_df pandas dataframe from_dict a 42 query the pandas dataframe my_df note duckdb sql connects to the default in-memory database connection results duckdb sql select from my_df df the seamless integration of pandas dataframes to duckdb sql queries is allowed by replacement scans which replace instances of accessing the my_df table which does not exist in duckdb with a table function that reads the my_df dataframe",
			"category": "Python",
			"url": "/docs/guides/python/sql_on_pandas",
			"blurb": "Pandas DataFrames stored in local variables can be queried as if they are regular tables within DuckDB. import duckdb..."
		},
		{
			"title": "SQLite Extension",
			"text": "the sqlite extension allows duckdb to directly read and write data from a sqlite database file the data can be queried directly from the underlying sqlite tables data can be loaded from sqlite tables into duckdb tables or vice versa installing and loading to install the sqlite extension run install sqlite the extension is loaded automatically upon first use if you prefer to load it manually run load sqlite usage to make a sqlite file accessible to duckdb use the attach statement which supports read write for example with the sakila db file attach sakila db type sqlite use sakila the tables in the file can be read as if they were normal duckdb tables but the underlying data is read directly from the sqlite tables in the file at query time show tables name actor address category city country customer customer_list film film_actor film_category film_list film_text inventory language payment rental sales_by_film_category sales_by_store staff staff_list store you can query the tables using sql e g using the example queries from sakila-examples sql select cat name as category_name sum ifnull pay amount 0 as revenue from category cat left join film_category flm_cat on cat category_id flm_cat category_id left join film fil on flm_cat film_id fil film_id left join inventory inv on fil film_id inv film_id left join rental ren on inv inventory_id ren inventory_id left join payment pay on ren rental_id pay rental_id group by cat name order by revenue desc limit 5 data types sqlite is a weakly typed database system as such when storing data in a sqlite table types are not enforced the following is valid sql in sqlite create table numbers i integer insert into numbers values hello duckdb is a strongly typed database system as such it requires all columns to have defined types and the system rigorously checks data for correctness when querying sqlite duckdb must deduce a specific column type mapping duckdb follows sqlite s type affinity rules with a few extensions if the declared type contains the string int then it is translated into the type bigint if the declared type of the column contains any of the strings char clob or text then it is translated into varchar if the declared type for a column contains the string blob or if no type is specified then it is translated into blob if the declared type for a column contains any of the strings real floa doub dec or num then it is translated into double if the declared type is date then it is translated into date if the declared type contains the string time then it is translated into timestamp if none of the above apply then it is translated into varchar as duckdb enforces the corresponding columns to contain only correctly typed values we cannot load the string hello into a column of type bigint as such an error is thrown when reading from the numbers table above error mismatch type error invalid type in column i column was declared as integer found hello of type text instead this error can be avoided by setting the sqlite_all_varchar option set global sqlite_all_varchar true when set this option overrides the type conversion rules described above and instead always converts the sqlite columns into a varchar column note that this setting must be set before sqlite_attach is called opening sqlite databases directly sqlite databases can also be opened directly and can be used transparently instead of a duckdb database file in any client when connecting a path to a sqlite database file can be provided and the sqlite database will be opened instead for example with the shell duckdb data db sakila db show tables name varchar actor address category staff_list store 21 rows 5 shown writing data to sqlite in addition to reading data from sqlite the extension also allows you to create new sqlite database files create tables ingest data into sqlite and make other modifications to sqlite database files using standard sql queries this allows you to use duckdb to for example export data that is stored in a sqlite database to parquet or read data from a parquet file into sqlite below is a brief example of how to create a new sqlite database and load data into it attach new_sqlite_database db as sqlite_db type sqlite create table sqlite_db tbl id integer name varchar insert into sqlite_db tbl values 42 duckdb the resulting sqlite database can then be read into from sqlite sqlite3 new_sqlite_database db sqlite version 3 39 5 2022-10-14 20 58 05 sqlite select from tbl id name -- ------ 42 duckdb many operations on sqlite tables are supported all these operations directly modify the sqlite database and the result of subsequent operations can then be read using sqlite below is a list of supported operations create table create table sqlite_db tbl id integer name varchar insert into insert into sqlite_db tbl values 42 duckdb select select from sqlite_db tbl id name int64 varchar 42 duckdb copy copy sqlite_db tbl to data parquet copy sqlite_db tbl from data parquet update update sqlite_db tbl set name woohoo where id 42 delete delete from sqlite_db tbl where id 42 alter table alter table sqlite_db tbl add column k integer drop table drop table sqlite_db tbl create view create view sqlite_db v1 as select 42 transactions create table sqlite_db tmp i integer begin insert into sqlite_db tmp values 42 select from sqlite_db tmp i int64 42 rollback select from sqlite_db tmp i int64 0 rows the old sqlite_attach function is deprecated it is recommended to switch over to the new attach syntax github repository span class github github span",
			"category": "Extensions",
			"url": "/docs/extensions/sqlite",
			"blurb": "The SQLite extension allows DuckDB to directly read and write data from a SQLite database file. The data can be..."
		},
		{
			"title": "SQLite Import",
			"text": "to run a query directly on a sqlite file the sqlite extension is required installation and loading the extension can be installed use the install sql command this only needs to be run once install sqlite to load the sqlite extension for usage use the load sql command load sqlite usage after the sqlite extension is installed tables can be queried from sqlite using the sqlite_scan function -- scan the table tbl_name from the sqlite file test db select from sqlite_scan test db tbl_name alternatively the entire file can be attached using the attach command this allows you to query all tables stored within a sqlite database file as if they were a regular database -- attach the sqlite file test db attach test db as test type sqlite -- the table tbl_name can now be queried as if it is a regular table select from test tbl_name -- switch the active database to test use test -- list all tables in the file show tables for more information see the sqlite extension documentation",
			"category": "Import",
			"url": "/docs/guides/import/query_sqlite",
			"blurb": "To run a query directly on a SQLite file, the sqlite extension is required. Installation and Loading The extension..."
		},
		{
			"title": "Samples",
			"text": "samples are used to randomly select a subset of a dataset examples -- select a sample of 5 rows from tbl using reservoir sampling select from tbl using sample 5 -- select a sample of 10 of the table using system sampling cluster sampling select from tbl using sample 10 -- select a sample of 10 of the table using bernoulli sampling select from tbl using sample 10 percent bernoulli -- select a sample of 50 rows of the table using reservoir sampling with a fixed seed 100 select from tbl using sample reservoir 50 rows repeatable 100 -- select a sample of 20 of the table using system sampling with a fixed seed 377 select from tbl using sample 10 system 377 -- select a sample of 10 of tbl before the join with tbl2 select from tbl tablesample reservoir 20 tbl2 where tbl i tbl2 i -- select a sample of 10 of tbl after the join with tbl2 select from tbl tbl2 where tbl i tbl2 i using sample reservoir 20 syntax samples allow you to randomly extract a subset of a dataset samples are useful for exploring a dataset faster as often you might not be interested in the exact answers to queries but only in rough indications of what the data looks like and what is in the data samples allow you to get approximate answers to queries faster as they reduce the amount of data that needs to pass through the query engine duckdb supports three different types of sampling methods reservoir bernoulli and system by default duckdb uses reservoir sampling when an exact number of rows is sampled and system sampling when a percentage is specified the sampling methods are described in detail below samples require a sample size which is an indication of how many elements will be sampled from the total population samples can either be given as a percentage 10 or as a fixed number of rows 10 rows all three sampling methods support sampling over a percentage but only reservoir sampling supports sampling a fixed number of rows samples are probablistic that is to say samples can be different between runs unless the seed is specifically specified specifying the seed only guarantees that the sample is the same if multi-threading is not enabled i e set threads 1 in the case of multiple threads running over a sample samples are not necessarily consistent even with a fixed seed reservoir reservoir sampling is a stream sampling technique that selects a random sample by keeping a reservoir of size equal to the sample size and randomly replacing elements as more elements come in reservoir sampling allows us to specify exactly how many elements we want in the resulting sample by selecting the size of the reservoir as a result reservoir sampling always outputs the same amount of elements unlike system and bernoulli sampling reservoir sampling is only recommended for small sample sizes and is not recommended for use with percentages that is because reservoir sampling needs to materialize the entire sample and randomly replace tuples within the materialized sample the larger the sample size the higher the performance hit incurred by this process reservoir sampling also incurs an additional performance penalty when multi-processing is used since the reservoir is to be shared amongst the different threads to ensure unbiased sampling this is not a big problem when the reservoir is very small but becomes costly when the sample is large avoid using reservoir sample with large sample sizes if possible reservoir sampling requires the entire sample to be materialized in memory bernoulli bernoulli sampling can only be used when a sampling percentage is specified it is rather straightforward every tuple in the underlying table is included with a chance equal to the specified percentage as a result bernoulli sampling can return a different number of tuples even if the same percentage is specified the amount of rows will generally be more or less equal to the specified percentage of the table but there will be some variance because bernoulli sampling is completely independent there is no shared state there is no penalty for using bernoulli sampling together with multiple threads system system sampling is a variant of bernoulli sampling with one crucial difference every vector is included with a chance equal to the sampling percentage this is a form of cluster sampling system sampling is more efficient than bernoulli sampling as no per-tuple selections have to be performed there is almost no extra overhead for using system sampling whereas bernoulli sampling can add additional cost as it has to perform random number generation for every single tuple system sampling is not suitable for smaller data sets as the granularity of the sampling is on the order of 1000 tuples that means that if system sampling is used for small data sets e g 100 rows either all the data will be filtered out or all the data will be included table samples the tablesample and using sample clauses are identical in terms of syntax and effect with one important difference tablesamples sample directly from the table for which they are specified whereas the sample clause samples after the entire from clause has been resolved this is relevant when there are joins present in the query plan the tablesample clause is essentially equivalent to creating a subquery with the using sample clause i e the following two queries are identical -- sample 20 of tbl before the join select from tbl tablesample reservoir 20 tbl2 where tbl i tbl2 i -- sample 20 of tbl before the join select from select from tbl using sample reservoir 20 tbl tbl2 where tbl i tbl2 i -- sample 20 after the join i e sample 20 of the join result select from tbl tbl2 where tbl i tbl2 i using sample reservoir 20",
			"category": "SQL",
			"url": "/docs/sql/samples",
			"blurb": "Samples are used to randomly select a subset of a dataset. Examples -- select a sample of 5 rows from tbl using..."
		},
		{
			"title": "Schema",
			"text": "types it is important to use the correct type for encoding columns e g bigint date datetime while it is always possible to use string types varchar etc to encode more specific values this is not recommended strings use more space and are slower to process in operations such as filtering join and aggregation when loading csv files you may leverage the csv reader s auto-detection mechanism to get the correct types for csv inputs if you run in a memory-constrained environment using smaller data types e g tinyint can reduce the amount of memory and disk space required to complete a query duckdb s bitpacking compression means small values stored in larger data types will not take up larger sizes on disk but they will take up more memory during processing bestpractice use the most restrictive types possible when creating columns avoid using strings for encoding more specific data items microbenchmark using timestamps we illustrate the difference in aggregation speed using the creationdate column of the ldbc comment table on scale factor 300 this table has approx 554 million unordered timestamp values we run a simple aggregation query that returns the average day-of-the month from the timestamps in two configurations first we use a datetime to encode the values and run the query using the extract datetime function select avg extract day from creationdate from comment second we use the varchar type and use string operations select avg cast creationdate 9 10 as int from comment the results of the microbenchmark are as follows column type storage size query time ----------- ------------ ---------- datetime 3 3 gb 0 9 s varchar 5 2 gb 3 9 s the results show that using the datetime value yields smaller storage sizes and faster processing microbenchmark joining on strings we illustrate the difference caused by joining on different types by computing a self-join on the ldbc comment table at scale factor 100 the table has 64-bit integer identifiers used as the id attribute of each row we perform the following join operation select count as count from comment c1 join comment c2 on c1 parentcommentid c2 id in the first experiment we use the correct most restrictive types i e both the id and the parentcommentid columns are defined as bigint in the second experiment we define all columns with the varchar type while the results of the queries are the same for all both experiments their runtime vary significantly the results below show that joining on bigint columns is approx 1 8 faster than performing the same join on varchar -typed columns encoding the same value join column payload type join column schema type example value query time ------------------------ ----------------------- ---------------------------------------- ---------- bigint bigint 70368755640078 1 2 s bigint varchar 70368755640078 2 1 s constraints duckdb allows defining constraints such as unique primary key and foreign key these constraints can be beneficial for ensuring data integrity but they have a negative effect on load performance as they necessitate building indexes and performing checks moreover they very rarely improve the performance of queries as duckdb does not rely on these indexes for join and aggregation operators see indexing for more details bestpractice do not define constraints unless your goal is to ensure data integrity microbenchmark the effect of primary keys we illustrate the effect of using primary keys with the ldbc comment table at scale factor 300 this table has approx 554 million entries we first create the schema without a primary key then load the data in the second experiment we create the schema with a primary key then load the data in both cases we take the data from csv gz files and measure the time required to perform the loading operation execution time ------------------------ -------------- load without primary key 92 168s load with primary key 286 765s",
			"category": "Performance",
			"url": "/docs/guides/performance/schema",
			"blurb": "Types It is important to use the correct type for encoding columns (e.g., BIGINT , DATE , DATETIME ). While it is..."
		},
		{
			"title": "Set Operations",
			"text": "set operations allow queries to be combined according to set operation semantics set operations refer to the union all intersect all and except all clauses the vanilla variants use set semantics i e they eliminate duplicates while the variants with all use bag semantics traditional set operations unify queries by column position and require the to-be-combined queries to have the same number of input columns if the columns are not of the same type casts may be added the result will use the column names from the first query duckdb also supports union all by name which joins columns by name instead of by position union by name does not require the inputs to have the same number of columns null values will be added in case of missing columns union the union clause can be used to combine rows from multiple queries the queries are required to have the same number of columns and the same column types vanilla union set semantics the vanilla union clause follows set semantics therefore it performs duplicate elimination i e only unique rows will be included in the result -- the values 0 2 select from range 2 t1 x union select from range 3 t2 x x int64 0 2 1 union all bag semantics union all returns all rows of both queries following bag semantics i e without duplicate elimination -- the values 0 2 and 0 3 select from range 2 t1 x union all select from range 3 t2 x x int64 0 1 0 1 2 union all by name the union all by name clause can be used to combine rows from different tables by name instead of by position union by name does not require both queries to have the same number of columns any columns that are only found in one of the queries are filled with null values for the other query take the following tables for example create table capitals city varchar country varchar insert into capitals values amsterdam nl berlin germany create table weather city varchar degrees integer date date insert into weather values amsterdam 10 2022-10-14 seattle 8 2022-10-12 select from capitals union by name select from weather city country degrees date varchar varchar int32 date amsterdam null 10 2022-10-14 seattle null 8 2022-10-12 amsterdam nl null null berlin germany null null union by name follows set semantics therefore it performs duplicate elimination whereas union all by name follows bag semantics intersect the intersect clause can be used to select all rows that occur in the result of both queries vanilla intersect set semantics vanilla intersect performs duplicate elimination so only unique rows are returned -- the values 0 5 all values that are both in t1 and t2 select from range 2 t1 x intersect select from range 6 t2 x x int64 0 1 intersect all bag semantics intersect all follows bag semantics so duplicates are returned -- intersect all uses bag semantics select unnest 5 5 6 6 6 6 7 8 as x intersect all select unnest 5 6 6 7 7 9 x int32 7 6 6 5 except the except clause can be used to select all rows that only occur in the left query vanilla except set semantics vanilla except follows set semantics therefore it performs duplicate elimination so only unique rows are returned -- the values 2 5 select from range 5 t1 x except select from range 2 t2 x x int64 4 3 2 except all bag semantics -- except all uses bag semantics select unnest 5 5 6 6 6 6 7 8 as x except all select unnest 5 6 6 7 7 9 x int32 6 6 8 5 syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/setops",
			"blurb": "Set operations allow queries to be combined according to set operation semantics . Set operations refer to the UNION..."
		},
		{
			"title": "Spark API",
			"text": "the duckdb spark api implements the pyspark api allowing you to use the familiar spark api to interact with duckdb all statements are translated to duckdb s internal plans using our relational api and executed using duckdb s query engine the duckdb spark api is currently experimental and features are still missing we are very interested in feedback please report any functionality that you are missing either through discord or on github example from duckdb experimental spark sql import sparksession as session from duckdb experimental spark sql functions import lit col import pandas as pd spark session builder getorcreate pandas_df pd dataframe age 34 45 23 56 name joan peter john bob df spark createdataframe pandas_df df df withcolumn location lit seattle res df select col age col location collect print res row age 34 location seattle row age 45 location seattle row age 23 location seattle row age 56 location seattle contribution guidelines contributions to the experimental spark api are welcome when making a contribution please follow these guidelines instead of using temporary files use our pytest testing framework when adding new functions ensure that method signatures comply with those in the pyspark api",
			"category": "Python",
			"url": "/docs/api/python/spark_api",
			"blurb": "The DuckDB Spark API implements the PySpark API , allowing you to use the familiar Spark API to interact with DuckDB...."
		},
		{
			"title": "Spatial Extension",
			"text": "the spatial extension provides support for geospatial data processing in duckdb for an overview of the extension see our blog post installing and loading to install and load the spatial extension run install spatial load spatial geometry type the core of the spatial extension is the geometry type if you re unfamiliar with geospatial data and gis tooling this type probably works very different from what you d expect in short while the geometry type is a binary representation of geometry data made up out of sets of vertices pairs of x and y double precision floats it actually stores one of several geometry subtypes these are point linestring polygon as well as their collection equivalents multipoint multilinestring and multipolygon lastly there is geometrycollection which can contain any of the other subtypes as well as other geometrycollection s recursively this may seem strange at first since duckdb already have types like list struct and union which could be used in a similar way but the design and behaviour of the geometry type is actually based on the simple features geometry model which is a standard used by many other databases and gis software that said the spatial extension also includes a couple of experimental non-standard explicit geometry types such as point_2d linestring_2d polygon_2d and box_2d that are based on duckdbs native nested types such as structs and lists in theory it should be possible to optimize a lot of operations for these types much better than for the geometry type which is just a binary blob but only a couple functions are implemented so far all of these are implicitly castable to geometry but with a conversion cost so the geometry type is still the recommended type to use for now if you are planning to work with a lot of different spatial functions geometry is not currently capable of storing additional geometry types z m coordinates or srid information these features may be added in the future spatial scalar functions the spatial extension implements a large number of scalar functions and overloads most of these are implemented using the geos library but we d like to implement more of them natively in this extension to better utilize duckdb s vectorized execution and memory management the following symbols are used to indicate which implementation is used - geos - functions that are implemented using the geos library - duckdb - functions that are implemented natively in this extension that are capable of operating directly on the duckdb types - cast geometry - functions that are supported by implicitly casting to geometry and then using the geometry implementation the currently implemented spatial functions can roughly be categorized into the following groups geometry conversion convert between geometries and other formats scalar functions geometry point_2d linestring_2d polygon_2d box_2d ----- --- -- -- -- --- varchar st_astext geometry as polygon wkb_blob st_aswkb geometry varchar st_ashexwkb geometry varchar st_asgeojson geometry as polygon geometry st_geomfromtext varchar as polygon geometry st_geomfromwkb blob as polygon geometry st_geomfromhexwkb varchar geometry st_geomfromgeojson varchar geometry construction construct new geometries from other geometries or other data scalar functions geometry point_2d linestring_2d polygon_2d box_2d ----- --- -- -- -- --- geometry st_point double double geometry st_convexhull geometry as polygon geometry st_boundary geometry as polygon geometry st_buffer geometry as polygon geometry st_centroid geometry geometry st_collect geometry geometry st_normalize geometry as polygon geometry st_simplifypreservetopology geometry double as polygon geometry st_simplify geometry double as polygon geometry st_union geometry geometry as polygon geometry st_intersection geometry geometry as polygon geometry st_makeline geometry geometry st_envelope geometry as polygon geometry st_flipcoordinates geometry geometry st_transform geometry varchar varchar box_2d st_extent geometry geometry st_pointn geometry integer geometry st_startpoint geometry geometry st_endpoint geometry geometry st_exteriorring geometry geometry st_reverse geometry geometry st_removerepeatedpoints geometry as polygon geometry st_removerepeatedpoints geometry double as polygon geometry st_reduceprecision geometry double as polygon geometry st_pointonsurface geometry as polygon geometry st_collectionextract geometry geometry st_collectionextract geometry integer spatial properties calculate and access spatial properties of geometries scalar functions geometry point_2d linestring_2d polygon_2d box_2d ----- --- -- -- -- --- double st_area geometry boolean st_isclosed geometry as polygon boolean st_isempty geometry as polygon boolean st_isring geometry as polygon boolean st_issimple geometry as polygon boolean st_isvalid geometry as polygon double st_x geometry double st_y geometry double st_xmax geometry double st_ymax geometry double st_xmin geometry double st_ymin geometry geometrytype st_geometrytype geometry as polygon double st_length geometry as polygon integer st_ngeometries geometry integer st_npoints geometry integer st_ninteriorrings geometry spatial relationships compute relationships and spatial predicates between geometries scalar functions geometry point_2d linestring_2d polygon_2d box_2d ----- --- -- -- -- --- boolean st_within geometry geometry or as polygon boolean st_touches geometry geometry as polygon boolean st_overlaps geometry geometry as polygon boolean st_contains geometry geometry or as polygon boolean st_coveredby geometry geometry as polygon boolean st_covers geometry geometry as polygon boolean st_crosses geometry geometry as polygon boolean st_difference geometry geometry as polygon boolean st_disjoint geometry geometry as polygon boolean st_intersects geometry geometry boolean st_equals geometry geometry as polygon double st_distance geometry geometry or or as polygon boolean st_dwithin geometry geometry double as polygon boolean st_intersects_extent geometry geometry spatial aggregate functions aggregate functions implemented with ------------------------------------------- ------------------ geometry st_envelope_agg geometry geometry st_union_agg geometry geometry st_intersection_agg geometry spatial table functions st_read - read spatial data from files the spatial extension provides a st_read table function based on the gdal translator library to read spatial data from a variety of geospatial vector file formats as if they were duckdb tables for example to create a new table from a geojson file you can use the following query create table table as select from st_read some file path filename json st_read can take a number of optional arguments the full signature is st_read varchar sequential_layer_scan boolean spatial_filter wkb_blob open_options varchar layer varchar allowed_drivers varchar sibling_files varchar spatial_filter_box box_2d keep_wkb boolean sequential_layer_scan default false if set to true the table function will scan through all layers sequentially and return the first layer that matches the given layer name this is required for some drivers to work properly e g the osm driver spatial_filter default null if set to a wkb blob the table function will only return rows that intersect with the given wkb geometry some drivers may support efficient spatial filtering natively in which case it will be pushed down otherwise the filtering is done by gdal which may be much slower open_options default a list of key-value pairs that are passed to the gdal driver to control the opening of the file e g the geojson driver supports a flatten_nested_attributes yes option to flatten nested attributes layer default null the name of the layer to read from the file if null the first layer is returned can also be a layer index starting at 0 allowed_drivers default a list of gdal driver names that are allowed to be used to open the file if empty all drivers are allowed sibling_files default a list of sibling files that are required to open the file e g the esri shapefile driver requires a shx file to be present although most of the time these can be discovered automatically spatial_filter_box default null if set to a box_2d the table function will only return rows that intersect with the given bounding box similar to spatial_filter keep_wkb default false if set the table function will return geometries in a wkb_geometry column with the type wkb_blob which can be cast to blob instead of geometry this is useful if you want to use duckdb with more exotic geometry subtypes that duckdb spatial doesnt support representing in the geometry type yet note that gdal is single-threaded so this table function will not be able to make full use of parallelism we re planning to implement support for the most common vector formats natively in this extension with additional table functions in the future we currently support over 50 different formats you can generate the following table of supported gdal drivers yourself by executing select from st_drivers short_name long_name can_create can_copy can_open help_url -- --- - - - --- esri shapefile esri shapefile true false true https gdal org drivers vector shapefile html mapinfo file mapinfo file true false true https gdal org drivers vector mitab html uk ntf uk ntf false false true https gdal org drivers vector ntf html lvbag kadaster lv bag extract 2 0 false false true https gdal org drivers vector lvbag html s57 iho s-57 enc true false true https gdal org drivers vector s57 html dgn microstation dgn true false true https gdal org drivers vector dgn html ogr_vrt vrt - virtual datasource false false true https gdal org drivers vector vrt html memory memory true false true csv comma separated value csv true false true https gdal org drivers vector csv html gml geography markup language gml true false true https gdal org drivers vector gml html gpx gpx true false true https gdal org drivers vector gpx html kml keyhole markup language kml true false true https gdal org drivers vector kml html geojson geojson true false true https gdal org drivers vector geojson html geojsonseq geojson sequence true false true https gdal org drivers vector geojsonseq html esrijson esrijson false false true https gdal org drivers vector esrijson html topojson topojson false false true https gdal org drivers vector topojson html ogr_gmt gmt ascii vectors gmt true false true https gdal org drivers vector gmt html gpkg geopackage true true true https gdal org drivers vector gpkg html sqlite sqlite spatialite true false true https gdal org drivers vector sqlite html wasp wasp map format true false true https gdal org drivers vector wasp html openfilegdb esri filegdb true false true https gdal org drivers vector openfilegdb html dxf autocad dxf true false true https gdal org drivers vector dxf html cad autocad driver false false true https gdal org drivers vector cad html flatgeobuf flatgeobuf true false true https gdal org drivers vector flatgeobuf html geoconcept geoconcept true false true georss georss true false true https gdal org drivers vector georss html vfk czech cadastral exchange data format false false true https gdal org drivers vector vfk html pgdump postgresql sql dump true false false https gdal org drivers vector pgdump html osm openstreetmap xml and pbf false false true https gdal org drivers vector osm html gpsbabel gpsbabel true false true https gdal org drivers vector gpsbabel html wfs ogc wfs web feature service false false true https gdal org drivers vector wfs html oapif ogc api - features false false true https gdal org drivers vector oapif html edigeo french edigeo exchange format false false true https gdal org drivers vector edigeo html svg scalable vector graphics false false true https gdal org drivers vector svg html ods open document libreoffice openoffice spreadsheet true false true https gdal org drivers vector ods html xlsx ms office open xml spreadsheet true false true https gdal org drivers vector xlsx html elasticsearch elastic search true false true https gdal org drivers vector elasticsearch html carto carto true false true https gdal org drivers vector carto html amigocloud amigocloud true false true https gdal org drivers vector amigocloud html sxf storage and exchange format false false true https gdal org drivers vector sxf html selafin selafin true false true https gdal org drivers vector selafin html jml openjump jml true false true https gdal org drivers vector jml html plscenes planet labs scenes api false false true https gdal org drivers vector plscenes html csw ogc csw catalog service for the web false false true https gdal org drivers vector csw html vdv vdv-451 vdv-452 intrest data format true false true https gdal org drivers vector vdv html mvt mapbox vector tiles true false true https gdal org drivers vector mvt html ngw nextgis web true true true https gdal org drivers vector ngw html mapml mapml true false true https gdal org drivers vector mapml html tiger u s census tiger line false false true https gdal org drivers vector tiger html avcbin arc info binary coverage false false true https gdal org drivers vector avcbin html avce00 arc info e00 ascii coverage false false true https gdal org drivers vector avce00 html note that far from all of these drivers have been tested properly and some may require additional options to be passed to work as expected if you run into any issues please first consult the gdal docs st_readosm - read compressed osm data the spatial extension also provides an experimental st_readosm table function to read compressed osm data directly from a osm pbf file this will use multithreading and zero-copy protobuf parsing which makes it a lot faster than using the st_read osm driver but it only outputs the raw osm data nodes ways relations without constructing any geometries for node entities you can trivially construct point geometries but it is also possible to construct linestring and polygon by manually joining refs and nodes together in sql example usage select from st_readosm tmp data germany osm pbf where tags highway limit 5 kind id tags refs lat lon ref_roles ref_types enum node way int64 map varchar varch int64 double double varchar enum node way node 122351 bicycle yes butt 53 5492951 9 977553 node 122397 crossing no high 53 520990100000006 10 0156924 node 122493 tmc cid_58 tabcd_ 53 129614600000004 8 1970173 node 123566 highway traffic_s 54 617268200000005 8 9718171 node 125801 tmc cid_58 tabcd_ 53 070685000000005 8 7819939 spatial replacement scans the spatial extension also provides replacement scans for common geospatial file formats allowing you to query files of these formats as if they were tables select from path to some shapefile dataset shp in practice this is just syntax-sugar for calling st_read so there is no difference in performance if you want to pass additional options you should use the st_read table function directly the following formats are currently recognized by their file extension esri shapefile shp geopackage gpkg flatgeobuf fgb similarly there is a osm pbf replacement scan for st_readosm spatial copy functions much like the st_read table function the spatial extension provides a gdal based copy function to export duckdb tables to different geospatial vector formats for example to export a table to a geojson file with generated bounding boxes you can use the following query copy table to some file path filename geojson with format gdal driver geojson layer_creation_options write_bbox yes available options format is the only required option and must be set to gdal to use the gdal based copy function driver is the gdal driver to use for the export see the table above for a list of available drivers layer_creation_options list of options to pass to the gdal driver see the gdal docs for the driver you are using for a list of available options srs set a spatial reference system as metadata to use for the export this can be a wkt string an epsg code or a proj-string basically anything you would normally be able to pass to gdal ogr this will not perform any reprojection of the input geometry though it just sets the metadata if the target driver supports it limitations raster types are not supported and there is currently no plan to add them to the extension github repository span class github github span",
			"category": "Extensions",
			"url": "/docs/extensions/spatial",
			"blurb": "The spatial extension provides support for geospatial data processing in DuckDB. For an overview of the extension,..."
		},
		{
			"title": "Specifier",
			"text": "specifier - description",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "Specifier - Description"
		},
		{
			"title": "Star Expression",
			"text": "examples -- select all columns present in the from clause select from table_name -- select all columns from the table called table_name select table_name from table_name join other_table_name using id -- select all columns except the city column from the addresses table select exclude city from addresses -- select all columns from the addresses table but replace city with lower city select replace lower city as city from addresses -- select all columns matching the given expression select columns c - c like num from addresses -- select all columns matching the given regex from the table select columns number d from addresses syntax star expression the expression can be used in a select statement to select all columns that are projected in the from clause select from tbl the expression can be modified using the exclude and replace exclude clause exclude allows us to exclude specific columns from the expression select exclude col from tbl replace clause replace allows us to replace specific columns with different expressions select replace col 1000 as col from tbl columns expression the columns expression can be used to execute the same expression on multiple columns like the expression it can only be used in the select clause create table numbers id int number int insert into numbers values 1 10 2 20 3 null select min columns count columns from numbers min numbers id min numbers number count numbers id count numbers number ----------------- --------------------- ------------------- ----------------------- 1 10 3 2 the expression in the columns statement can also contain exclude or replace similar to regular star expressions select min columns replace number id as number count columns exclude number from numbers min numbers id min number number id count numbers id ----------------- ------------------------------ ------------------- 1 11 3 columns expressions can also be combined as long as the columns contains the same star expression select columns columns from numbers numbers id numbers id numbers number numbers number --------------------------- ----------------------------------- 2 20 4 40 6 null columns regular expression columns supports passing a regex in as a string constant select columns id numbers from numbers id number ---- -------- 1 10 2 20 3 null columns lambda function columns also supports passing in a lambda function the lambda function will be evaluated for all columns present in the from clause and only columns that match the lambda function will be returned this allows the execution of arbitrary expressions in order to select columns select columns c - c like num from numbers number -------- 10 20 null struct the expression can also be used to retrieve all keys from a struct as separate columns this is particularly useful when a prior operation creates a struct of unknown shape or if a query must handle any potential struct keys see the struct data type and nested functions pages for more details on working with structs -- all keys within a struct can be returned as separate columns using select st from select x 1 y 2 z 3 as st x y z --- --- --- 1 2 3",
			"category": "Expressions",
			"url": "/docs/sql/expressions/star",
			"blurb": "Examples -- select all columns present in the FROM clause SELECT * FROM table_name; -- select all columns from the..."
		},
		{
			"title": "Statements Overview",
			"text": "pages in this section",
			"category": "Statements",
			"url": "/docs/sql/statements/overview",
			"blurb": ""
		},
		{
			"title": "Struct Data Type",
			"text": "conceptually a struct column contains an ordered list of columns called entries the entries are referenced by name using strings this document refers to those entry names as keys each row in the struct column must have the same keys the names of the struct entries are part of the schema each row in a struct column must have the same layout the names of the struct entries are case-insensitive struct s are typically used to nest multiple columns into a single column and the nested column can be of any type including other struct s and list s struct s are similar to postgresql s row type the key difference is that duckdb struct s require the same keys in each row of a struct column this allows duckdb to provide significantly improved performance by fully utilizing its vectorized execution engine and also enforces type consistency for improved correctness duckdb includes a row function as a special way to produce a struct but does not have a row data type see an example below and the nested functions docs for details see the data types overview for a comparison between nested data types creating structs structs can be created using the struct_pack name expr function or the equivalent array notation name expr notation the expressions can be constants or arbitrary expressions -- struct of integers select x 1 y 2 z 3 -- struct of strings with a null value select yes duck maybe goose huh null no heron -- struct with a different type for each key select key1 string key2 1 key3 12 345 -- struct using the struct_pack function -- note the lack of single quotes around the keys and the use of the operator select struct_pack key1 value1 key2 42 -- struct of structs with null values select birds yes duck maybe goose huh null no heron aliens null amphibians yes frog maybe salamander huh dragon no toad -- create a struct from columns and or expressions using the row function -- this returns 1 2 a select row x x 1 y from select 1 as x a as y -- if using multiple expressions when creating a struct the row function is optional -- this also returns 1 2 a select x x 1 y from select 1 as x a as y adding field s value s to structs -- add to a struct of integers select struct_insert a 1 b 2 c 3 d 4 retrieving from structs retrieving a value from a struct can be accomplished using dot notation bracket notation or through struct functions like struct_extract -- use dot notation to retrieve the value at a key s location this returns 1 -- the subquery generates a struct column a which we then query with a x select a x from select x 1 y 2 z 3 as a -- if key contains a space simply wrap it in double quotes this returns 1 -- note use double quotes not single quotes -- this is because this action is most similar to selecting a column from within the struct select a x space from select x space 1 y 2 z 3 as a -- bracket notation may also be used this returns 1 -- note use single quotes since the goal is to specify a certain string key -- only constant expressions may be used inside the brackets no columns select a x space from select x space 1 y 2 z 3 as a -- the struct_extract function is also equivalent this returns 1 select struct_extract x space 1 y 2 z 3 x space struct rather than retrieving a single key from a struct star notation can be used to retrieve all keys from a struct as separate columns this is particularly useful when a prior operation creates a struct of unknown shape or if a query must handle any potential struct keys -- all keys within a struct can be returned as separate columns using select a from select x 1 y 2 z 3 as a x y z --- --- --- 1 2 3 dot notation order of operations referring to structs with dot notation can be ambiguous with referring to schemas and tables in general duckdb looks for columns first then for struct keys within columns duckdb resolves references in these orders using the first match to occur no dots select part1 from tbl part1 is a column one dot select part1 part2 from tbl part1 is a table part2 is a column part1 is a column part2 is a property of that column two or more dots select part1 part2 part3 from tbl part1 is a schema part2 is a table part3 is a column part1 is a table part2 is a column part3 is a property of that column part1 is a column part2 is a property of that column part3 is a property of that column any extra parts e g part4 part5 etc are always treated as properties creating structs with the row function the row function can be used to automatically convert multiple columns to a single struct column when using row the keys will be empty strings allowing for easy insertion into a table with a struct column columns however cannot be initialized with the row function and must be explicitly named for example -- inserting values into a struct column using the row function create table t1 s struct v varchar i integer insert into t1 values row a 42 -- the table will contain a single entry -- v a i 42 -- the following produces the same result as above create table t1 as select row a 42 struct v varchar i integer -- initializing a struct column with the row function will fail create table t2 as select row a -- the following error is thrown -- error invalid input error a table cannot be created from an unnamed struct when casting structs the names of fields have to match therefore the following query will fail select a struct y integer as b from select x 42 as a error mismatch type error type struct x integer does not match with struct y integer cannot cast structs with different names a workaround for this would be to use struct_pack instead select struct_pack y a x as b from select x 42 as a this behavior was introduced in duckdb v0 9 0 previously this query ran successfully and returned struct y 42 as column b comparison operators nested types can be compared using all the comparison operators these comparisons can be used in logical expressions for both where and having clauses as well as for creating boolean values the ordering is defined positionally in the same way that words can be ordered in a dictionary null values compare greater than all other values and are considered equal to each other at the top level null nested values obey standard sql null comparison rules comparing a null nested value to a non- null nested value produces a null result comparing nested value members however uses the internal nested value rules for null s and a null nested value member will compare above a non- null nested value member functions see nested functions",
			"category": "Data Types",
			"url": "/docs/sql/data_types/struct",
			"blurb": "Conceptually, a STRUCT column contains an ordered list of columns called entries. The entries are referenced by name..."
		},
		{
			"title": "Subqueries",
			"text": "scalar subquery scalar subqueries are subqueries that return a single value they can be used anywhere where a regular expression can be used if a scalar subquery returns more than a single value the first value returned will be used consider the following table grades grade course --- --- 7 math 9 math 8 cs create table grades grade integer course varchar insert into grades values 7 math 9 math 8 cs we can run the following query to obtain the minimum grade select min grade from grades -- 7 by using a scalar subquery in the where clause we can figure out for which course this grade was obtained select course from grades where grade select min grade from grades -- math exists the exists operator tests for the existence of any row inside the subquery it returns either true when the subquery returns one or more records and false otherwise the exists operator is generally the most useful as a correlated subquery to express semijoin operations however it can be used as an uncorrelated subquery as well for example we can use it to figure out if there are any grades present for a given course select exists select from grades where course math -- true select exists select from grades where course history -- false not exists the not exists operator tests for the absence of any row inside the subquery it returns either true when the subquery returns an empty result and false otherwise the not exists operator is generally the most useful as a correlated subquery to express antijoin operations for example to find person nodes without an interest create table person id bigint name varchar create table interest personid bigint topic varchar insert into person values 1 jane 2 joe insert into interest values 2 music select from person where not exists select from interest where interest personid person id id name int64 varchar 1 jane duckdb automatically detects when a not exists query expresses an antijoin operation there is no need to manually rewrite such queries to use left outer join where is null in operator the in operator checks containment of the left expression inside the result defined by the subquery or the set of expressions on the right hand side rhs the in operator returns true if the expression is present in the rhs false if the expression is not in the rhs and the rhs has no null values or null if the expression is not in the rhs and the rhs has null values we can use the in operator in a similar manner as we used the exists operator select math in select course from grades -- true correlated subqueries all the subqueries presented here so far have been uncorrelated subqueries where the subqueries themselves are entirely self-contained and can be run without the parent query there exists a second type of subqueries called correlated subqueries for correlated subqueries the subquery uses values from the parent subquery conceptually the subqueries are run once for every single row in the parent query perhaps a simple way of envisioning this is that the correlated subquery is a function that is applied to every row in the source data set for example suppose that we want to find the minimum grade for every course we could do that as follows select from grades grades_parent where grade select min grade from grades where grades course grades_parent course -- 7 math 8 cs the subquery uses a column from the parent query grades_parent course conceptually we can see the subquery as a function where the correlated column is a parameter to that function select min grade from grades where course now when we execute this function for each of the rows we can see that for math this will return 7 and for cs it will return 8 we then compare it against the grade for that actual row as a result the row math 9 will be filtered out as 9 7 returning each row of the subquery as a struct using the name of a subquery in the select clause without referring to a specific column turns each row of the subquery into a struct whose fields correspond to the columns of the subquery for example select t from select unnest generate_series 41 43 as x hello as y t t struct x bigint y varchar x 41 y hello x 42 y hello x 43 y hello",
			"category": "Expressions",
			"url": "/docs/sql/expressions/subqueries",
			"blurb": "Scalar Subquery Scalar subqueries are subqueries that return a single value. They can be used anywhere where a..."
		},
		{
			"title": "Substrait Extension",
			"text": "the main goal of the substrait extension is to support both production and consumption of substrait query plans in duckdb this extension is mainly exposed via 3 different apis - the sql api the python api and the r api here we depict how to consume and produce substrait query plans in each api the substrait integration is currently experimental support is currently only available on request if you have not asked for permission to ask for support contact us prior to opening an issue if you open an issue without doing so we will close it without further review installing and loading the substrait extension is an autoloadable extensions meaning that it will be loaded at runtime whenever one of the substrait functions is called to explicitly install and load the released version of the substrait extension you can also use the following sql commands install substrait load substrait sql in the sql api users can generate substrait plans into a blob or a json and consume substrait plans blob generation to generate a substrait blob the get_substrait sql function must be called with a valid sql select query create table crossfit exercise text difficulty_level int insert into crossfit values push ups 3 pull ups 5 push jerk 7 bar muscle up 10 mode line call get_substrait select count exercise as exercise from crossfit where difficulty_level 5 plan blob x12 x09 x1a x07 x10 x01 x1a x03lte x12 x11 x1a x0f x10 x02 x1a x0bis_not_null x12 x09 x1a x07 x10 x03 x1a x03and x12 x0b x1a x09 x10 x04 x1a x05count x1a xc8 x01 x12 xc5 x01 x0a xb8 x01 xb5 x01 x12 xa8 x01 x22 xa5 x01 x12 x94 x01 x0a x91 x01 x12 x0a x08exercise x0a x10difficulty_level x12 x11 x0a x07 xb2 x01 x04 x08 x0d x18 x01 x0a x04 x02 x10 x01 x18 x02 x1aj x1ah x08 x03 x1a x04 x0a x02 x10 x01 x22 x22 x1a x1a x1e x08 x01 x1a x04 x02 x10 x01 x22 x0c x1a x0a x12 x08 x0a x04 x12 x02 x08 x01 x22 x00 x22 x06 x1a x04 x0a x02 x05 x22 x1a x1a x18 x1a x16 x08 x02 x1a x04 x02 x10 x01 x22 x0c x1a x0a x12 x08 x0a x04 x12 x02 x08 x01 x22 x00 x22 x06 x0a x02 x0a x00 x10 x01 x0a x0a x08crossfit x1a x00 x22 x0a x0a x08 x08 x04 x04 x02 x10 x01 x1a x08 x12 x06 x0a x02 x12 x00 x22 x00 x12 x08exercise2 x0a x10 x18 x06duckdb json generation to generate a json representing the substrait plan the get_substrait_json sql function must be called with a valid sql select query call get_substrait_json select count exercise as exercise from crossfit where difficulty_level 5 json extensions extensionfunction functionanchor 1 name lte extensionfunction functionanchor 2 name is_not_null extensionfunction functionanchor 3 name and extensionfunction functionanchor 4 name count relations root input project input aggregate input read baseschema names exercise difficulty_level struct types varchar length 13 nullability nullability_nullable i32 nullability nullability_nullable nullability nullability_required filter scalarfunction functionreference 3 outputtype bool nullability nullability_nullable arguments value scalarfunction functionreference 1 outputtype i32 nullability nullability_nullable arguments value selection directreference structfield field 1 rootreference value literal i32 5 value scalarfunction functionreference 2 outputtype i32 nullability nullability_nullable arguments value selection directreference structfield field 1 rootreference projection select structitems maintainsingularstruct true namedtable names crossfit groupings measures measure functionreference 4 outputtype i64 nullability nullability_nullable expressions selection directreference structfield rootreference names exercise version minornumber 24 producer duckdb blob consumption to consume a substrait blob the from_substrait blob function must be called with a valid substrait blob plan call from_substrait x12 x09 x1a x07 x10 x01 x1a x03lte x12 x11 x1a x0f x10 x02 x1a x0bis_not_null x12 x09 x1a x07 x10 x03 x1a x03and x12 x0b x1a x09 x10 x04 x1a x05count x1a xc8 x01 x12 xc5 x01 x0a xb8 x01 xb5 x01 x12 xa8 x01 x22 xa5 x01 x12 x94 x01 x0a x91 x01 x12 x0a x08exercise x0a x10difficulty_level x12 x11 x0a x07 xb2 x01 x04 x08 x0d x18 x01 x0a x04 x02 x10 x01 x18 x02 x1aj x1ah x08 x03 x1a x04 x0a x02 x10 x01 x22 x22 x1a x1a x1e x08 x01 x1a x04 x02 x10 x01 x22 x0c x1a x0a x12 x08 x0a x04 x12 x02 x08 x01 x22 x00 x22 x06 x1a x04 x0a x02 x05 x22 x1a x1a x18 x1a x16 x08 x02 x1a x04 x02 x10 x01 x22 x0c x1a x0a x12 x08 x0a x04 x12 x02 x08 x01 x22 x00 x22 x06 x0a x02 x0a x00 x10 x01 x0a x0a x08crossfit x1a x00 x22 x0a x0a x08 x08 x04 x04 x02 x10 x01 x1a x08 x12 x06 x0a x02 x12 x00 x22 x00 x12 x08exercise2 x0a x10 x18 x06duckdb blob exercise 2 python substrait extension is autoloadable but if you prefer to do so explicitly you can use the relevant python syntax within a connection import duckdb con duckdb connect con install_extension substrait con load_extension substrait blob generation to generate a substrait blob the get_substrait sql function must be called from a connection with a valid sql select query con execute query create table crossfit exercise text difficulty_level int con execute query insert into crossfit values push ups 3 pull ups 5 push jerk 7 bar muscle up 10 proto_bytes con get_substrait query select count exercise as exercise from crossfit where difficulty_level 5 fetchone 0 json generation to generate a json representing the substrait plan the get_substrait_json sql function from a connection must be called with a valid sql select query json con get_substrait_json select count exercise as exercise from crossfit where difficulty_level 5 fetchone 0 blob consumption to consume a substrait blob the from_substrait blob function must be called from the connection with a valid substrait blob plan query_result con from_substrait proto proto_bytes r by default the extension will be autoloaded on first use to explicitly install and load this extension in r use the following commands library duckdb con - dbconnect duckdb duckdb dbexecute con install substrait dbexecute con load substrait blob generation to generate a substrait blob the duckdb_get_substrait con sql function must be called with a connection and a valid sql select query dbexecute con create table crossfit exercise text difficulty_level int dbexecute con insert into crossfit values push ups 3 pull ups 5 push jerk 7 bar muscle up 10 proto_bytes - duckdb duckdb_get_substrait con select from crossfit limit 5 json generation to generate a json representing the substrait plan duckdb_get_substrait_json con sql function with a connection and a valid sql select query json - duckdb duckdb_get_substrait_json con select count exercise as exercise from crossfit where difficulty_level 5 blob consumption to consume a substrait blob the duckdb_prepare_substrait con blob function must be called with a connection and a valid substrait blob plan result - duckdb duckdb_prepare_substrait con proto_bytes df - dbfetch result github repository span class github github span",
			"category": "Extensions",
			"url": "/docs/extensions/substrait",
			"blurb": "The main goal of the substrait extension is to support both production and consumption of Substrait query plans in..."
		},
		{
			"title": "Summarize",
			"text": "the summarize command can be used to easily compute a number of aggregates over a table or a query the summarize command launches a query that computes a number of aggregates over all columns including min max avg std and approx_unique usage in order to summarize the contents of a table use summarize followed by the table name summarize tbl in order to summarize a query prepend summarize to a query summarize select from tbl example below is an example of summarize on the lineitem table of tpc-h sf1 table generated using the tpch extension install tpch load tpch call dbgen sf 1 summarize lineitem column_name column_type min max approx_unique avg std q25 q50 q75 count null_percentage ----------------- --------------- ------------- --------------------- --------------- --------------------- ---------------------- --------- --------- --------- --------- ----------------- l_orderkey integer 1 6000000 1508227 3000279 604204982 1732187 8734803519 1509447 2989869 4485232 6001215 0 0 l_partkey integer 1 200000 202598 100017 98932999402 57735 69082650496 49913 99992 150039 6001215 0 0 l_suppkey integer 1 10000 10061 5000 602606138924 2886 9619987306114 2501 4999 7500 6001215 0 0 l_linenumber integer 1 7 7 3 0005757167506912 1 7324314036519328 2 3 4 6001215 0 0 l_quantity decimal 15 2 1 00 50 00 50 25 507967136654827 14 426262537016918 13 26 38 6001215 0 0 l_extendedprice decimal 15 2 901 00 104949 50 923139 38255 138484656854 23300 43871096221 18756 36724 55159 6001215 0 0 l_discount decimal 15 2 0 00 0 10 11 0 04999943011540163 0 03161985510812596 0 0 0 6001215 0 0 l_tax decimal 15 2 0 00 0 08 9 0 04001350893110812 0 025816551798842728 0 0 0 6001215 0 0 l_returnflag varchar a r 3 null null null null null 6001215 0 0 l_linestatus varchar f o 2 null null null null null 6001215 0 0 l_shipdate date 1992-01-02 1998-12-01 2516 null null null null null 6001215 0 0 l_commitdate date 1992-01-31 1998-10-31 2460 null null null null null 6001215 0 0 l_receiptdate date 1992-01-04 1998-12-31 2549 null null null null null 6001215 0 0 l_shipinstruct varchar collect cod take back return 4 null null null null null 6001215 0 0 l_shipmode varchar air truck 7 null null null null null 6001215 0 0 l_comment varchar tiresias zzle furiously iro 3558599 null null null null null 6001215 0 0 using summarize in a subquery summarize can be used a subquery this allows creating a table from the summary for example create table tbl_summary as select from summarize tbl summarizing remote tables it is possible to summarize remote tables via the httpfs extension using the summarize table statement for example summarize table https blobs duckdb org data star_trek-season_1 csv",
			"category": "Meta",
			"url": "/docs/guides/meta/summarize",
			"blurb": "The SUMMARIZE command can be used to easily compute a number of aggregates over a table or a query. The SUMMARIZE..."
		},
		{
			"title": "Swift API",
			"text": "duckdb offers a swift api see the announcement post for details instantiating duckdb duckdb supports both in-memory and persistent databases to work with an in-memory datatabase run let database try database store inmemory to work with a persistent database run let database try database store file at test db queries can be issued through a database connection let connection try database connect duckdb supports multiple connections per database application example the rest of the page is based on the example of our announcement post which uses raw data from nasa s exoplanet archive loaded directly into duckdb creating an application-specific type we first create an application-specific type that we ll use to house our database and connection and through which we ll eventually define our app-specific queries import duckdb final class exoplanetstore let database database let connection connection init database database connection connection self database database self connection connection loading a csv file we load the data from nasa s exoplanet archive wget https exoplanetarchive ipac caltech edu tap sync query select pl_name disc_year from pscomppars format csv -o downloaded_exoplanets csv once we have our csv downloaded locally we can use the following sql command to load it as a new table to duckdb create table exoplanets as select from read_csv downloaded_exoplanets csv let s package this up as a new asynchronous factory method on our exoplanetstore type import duckdb import foundation final class exoplanetstore factory method to create and prepare a new exoplanetstore static func create async throws - exoplanetstore create our database and connection as described above let database try database store inmemory let connection try database connect download the csv from the exoplanet archive let csvfileurl _ try await urlsession shared download from url string https exoplanetarchive ipac caltech edu tap sync query select pl_name disc_year from pscomppars format csv issue our first query to duckdb try connection execute create table exoplanets as select from read_csv csvfileurl path create our pre-populated exoplanetstore instance return exoplanetstore database database connection connection let s make the initializer we defined previously private this prevents anyone accidentally instantiating the store without having pre-loaded our exoplanet csv into the database private init database database connection connection querying the database the following example queires duckdb from within swift via an async function this means the callee won t be blocked while the query is executing we ll then cast the result columns to swift native types using duckdb s resultset cast to family of methods before finally wrapping them up in a dataframe from the tabulardata framework import tabulardata extension exoplanetstore retrieves the number of exoplanets discovered by year func groupedbydiscoveryyear async throws - dataframe issue the query we described above let result try connection query select disc_year count disc_year as count from exoplanets group by disc_year order by disc_year cast our duckdb columns to their native swift equivalent types let discoveryyearcolumn result 0 cast to int self let countcolumn result 1 cast to int self use our duckdb columns to instantiate tabulardata columns and populate a tabulardata dataframe return dataframe columns tabulardata column discoveryyearcolumn erasetoanycolumn tabulardata column countcolumn erasetoanycolumn complete project for the complete example project clone the duckdb swift repo and open up the runnable app project located in examples swiftui exoplanetexplorer xcodeproj github repository span class github github span",
			"category": "Api",
			"url": "/docs/api/swift",
			"blurb": "DuckDB offers a Swift API. See the announcement post for details. Instantiating DuckDB DuckDB supports both in-memory..."
		},
		{
			"title": "Syntax Highlighting",
			"text": "syntax highlighting in the cli is currently only available for macos and linux sql queries that are written in the shell are automatically highlighted using syntax highlighting image showing syntax highlighting in the shell there are several components of a query that are highlighted in different colors the colors can be configured using dot commands syntax highlighting can also be disabled entirely using the highlight off command below is a list of components that can be configured type command default color ------------------------- ----------- --------------- keywords keyword green constants literals constant yellow comments comment brightblack errors error red continuation cont brightblack continuation selected cont_sel green the components can be configured using either a supported color name e g keyword red or by directly providing a terminal code to use for rendering e g keywordcode 033 31m below is a list of supported color names and their corresponding terminal codes color terminal code --------------- --------------- red 033 31m green 033 32m yellow 033 33m blue 033 34m magenta 033 35m cyan 033 36m white 033 37m brightblack 033 90m brightred 033 91m brightgreen 033 92m brightyellow 033 93m brightblue 033 94m brightmagenta 033 95m brightcyan 033 96m brightwhite 033 97m for example here is an alternative set of syntax highlighting colors keyword brightred constant brightwhite comment cyan error yellow cont blue cont_sel brightblue if you wish to start up the cli with a different set of colors every time you can place these commands in the duckdbrc file that is loaded on start-up of the cli error highlighting the shell has support for highlighting certain errors in particular mismatched brackets and unclosed quotes are highlighted in red or another color if specified this highlighting is automatically disabled for large queries in addition it can be disabled manually using the render_errors off command",
			"category": "Cli",
			"url": "/docs/api/cli/syntax_highlighting",
			"blurb": "Syntax highlighting in the CLI is currently only available for macOS and Linux. SQL queries that are written in the..."
		},
		{
			"title": "TPC-DS Extension",
			"text": "the tpcds extension implements the data generator and queries for the tpc-ds benchmark installing and loading the tpcds extension will be transparently autoloaded on first use from the official extension repository if you would like to install and load it manually run install tpcds load tpcds usage to generate data for scale factor 1 use call dsdgen sf 1 to run a query e g query 8 use pragma tpcds 8 s_store_name sum ss_net_profit varchar decimal 38 2 able -10354620 18 ation -10576395 52 bar -10625236 01 ese -10076698 16 ought -10994052 78 limitations the tpchds query_id function runs a fixed tpc-ds query with pre-defined bind parameters a k a substitution parameters it is not possible to change the query parameters using the tpcds extension github the tpcds extension is part of the main duckdb repository",
			"category": "Extensions",
			"url": "/docs/extensions/tpcds",
			"blurb": "The tpcds extension implements the data generator and queries for the TPC-DS benchmark . Installing and Loading The..."
		},
		{
			"title": "TPC-H Extension",
			"text": "the tpch extension implements the data generator and queries for the tpc-h benchmark installing and loading the tpch extension is shipped by default in some duckdb builds otherwise it will be transparently autoloaded on first use if you would like to install and load it manually run install tpch load tpch usage generating data to generate data for scale factor 1 use call dbgen sf 1 calling dbgen does not clean up existing tpc-h tables to clean up existing tables use drop table before running dbgen drop table if exists customer drop table if exists lineitem drop table if exists nation drop table if exists orders drop table if exists part drop table if exists partsupp drop table if exists region drop table if exists supplier running a query to run a query e g query 4 use pragma tpch 4 o_orderpriority order_count varchar int64 1-urgent 21188 2-high 20952 3-medium 20820 4-not specified 21112 5-low 20974 listing queries to list all 22 queries run from tpch_queries this function returns a table with columns query_nr and query listing expected answers to produced the expected results for all queries on scale factors 0 01 0 1 and 1 run from tpch_answers this function returns a table with columns query_nr scale_factor and answer data generator parameters the data generator function dbgen has the following parameters name type description -- -- ------------ catalog varchar target catalog children uinteger number of partitions overwrite boolean not used sf double scale factor step uinteger defines the partition to be generated indexed from 0 to children - 1 must be defined when the children arguments is defined suffix varchar append the suffix to table names generating larger than memory data sets to generate data sets for large scale factors which yield larger than memory data sets run the dbgen function in steps for example you may generate sf300 in 10 steps call dbgen sf 300 children 10 step 0 call dbgen sf 300 children 10 step 1 call dbgen sf 300 children 10 step 9 limitations the data generator function dbgen is single-threaded and does not support concurrency running multiple steps to parallelize over different partitions is also not supported at the moment the tpch query_id function runs a fixed tpc-h query with pre-defined bind parameters a k a substitution parameters it is not possible to change the query parameters using the tpch extension github the tpch extension is part of the main duckdb repository",
			"category": "Extensions",
			"url": "/docs/extensions/tpch",
			"blurb": "The tpch extension implements the data generator and queries for the TPC-H benchmark . Installing and Loading The..."
		},
		{
			"title": "Tableau - A Data Visualization Tool",
			"text": "tableau is a popular commercial data visualization tool in addition to a large number of built in connectors it also provides generic database connectivity via odbc and jdbc connectors tableau has two main versions desktop and online server for desktop connecting to a duckdb database is similar to working in an embedded environment like python for online since duckdb is in-process the data needs to be either on the server itself or in a remote data bucket that is accessible from the server database creation when using a duckdb database file the data sets do not actually need to be imported into duckdb tables it suffices to create views of the data for example this will create a view of the h2oai parquet test file in the current duckdb code base create view h2oai as from read_parquet users username duckdb data parquet-testing h2oai h2oai_group_small parquet note that you should use full path names to local files so that they can be found from inside tableau also note that you will need to use a version of the driver that is compatible i e from the same release as the database format used by the duckdb tool e g python module command line that was used to create the file installing the jdbc driver tableau provides documentation on how to install a jdbc driver for tableau to use tableau both desktop and server versions need to be restarted any time you add or modify drivers driver links the link here is for a recent version of the jdbc driver that is compatible with tableau if you wish to connect to a database file you will need to make sure the file was created with a file-compatible version of duckdb also check that there is only one version of the driver installed as there are multiple filenames in use download the snapshot jar macos copy it to library tableau drivers windows copy it to c program files tableau drivers linux copy it to opt tableau tableau_driver jdbc using the postgresql dialect if you just want to do something simple you can try connecting directly to the jdbc driver and using tableau-provided postgresql dialect create a duckdb file containing your views and or data launch tableau under connect to a server more click on other databases jdbc this will bring up the connection dialogue box for the url enter jdbc duckdb user username path to database db for the dialect choose postgresql the rest of the fields can be ignored tableau postgresql however functionality will be missing such as median and percentile aggregate functions to make the data source connection more compatible with the postgresql dialect please use the duckdb taco connector as described below installing the tableau duckdb connector while it is possible to use the tableau-provided postgresql dialect to communicate with the duckdb jdbc driver we strongly recommend using the duckdb taco connector this connector has been fully tested against the tableau dialect generator and is more compatible than the provided postgresql dialect the documentation on how to install and use the connector is in its repository but essentially you will need the duckdb_jdbc taco file the current version of the taco is not signed so you will need to launch tableau with signature validation disabled despite what the tableau documentation says the real security risk is in the jdbc driver code not the small amount of javascript in the taco server online on linux copy the taco file to opt tableau connectors on windows copy the taco file to c program files tableau connectors then issue these commands to disable signature validation tsm configuration set -k native_api disable_verify_connector_plugin_signature -v true tsm pending-changes apply the last command will restart the server with the new settings macos copy the taco file to the users user documents my tableau repository connectors folder then launch tableau desktop from the terminal with the command line argument to disable signature validation applications tableau desktop year quarter app contents macos tableau -ddisableverifyconnectorpluginsignature true you can also package this up with applescript by using the following script do shell script applications tableau desktop 2023 2 app contents macos tableau -ddisableverifyconnectorpluginsignature true quit create this file with the script editor located in applications utilities and save it as a packaged application tableau-applescript you can then double-click it to launch tableau you will need to change the application name in the script when you get upgrades windows desktop copy the taco file to the c users windows user documents my tableau repository connectors directory then launch tableau desktop from a shell with the -ddisableverifyconnectorpluginsignature true argument to disable signature validation output once loaded you can run queries against your data here is the result of the first h2o ai benchmark query from the parquet test file tableau-parquet",
			"category": "Data Viewers",
			"url": "/docs/guides/data_viewers/tableau",
			"blurb": "Tableau is a popular commercial data visualization tool. In addition to a large number of built in connectors, it..."
		},
		{
			"title": "Text Functions",
			"text": "this section describes functions and operators for examining and manipulating string values the symbol denotes a space character function description example result alias -- -- --- -- -- string search_string alias for starts_with abc a true starts_with string string string concatenation duck db duckdb string index alias for array_extract duckdb 4 k array_extract string begin end alias for array_slice missing begin or end arguments are interpreted as the beginning or end of the list respectively duckdb 4 duck array_slice string like target returns true if the string matches the like specifier see pattern matching hello like lo true string similar to regex returns true if the string matches the regex identical to regexp_full_match see pattern matching hello similar to l false array_extract list index extract a single character using a 1-based index array_extract duckdb 2 u list_element list_extract array_slice list begin end extract a string using slice conventions negative values are accepted array_slice duckdb 5 null db ascii string returns an integer that represents the unicode code point of the first character of the string ascii \u03c9 937 bar x min max width draw a band whose width is proportional to x - min and equal to width characters when x max width defaults to 80 bar 5 0 20 10 bit_length string number of bits in a string bit_length abc 24 chr x returns a character which is corresponding the ascii code value or unicode code point chr 65 a concat_ws separator string concatenate strings together separated by the specified separator concat_ws banana apple melon banana apple melon concat string concatenate many strings together concat hello world hello world contains string search_string return true if search_string is found within string contains abc a true ends_with string search_string return true if string ends with search_string ends_with abc c true suffix format_bytes bytes converts bytes to a human-readable representation using units based on powers of 2 kib mib gib etc format_bytes 16384 16 0 kib format format parameters formats a string using the fmt syntax format benchmark took seconds csv 42 benchmark csv took 42 seconds from_base64 string convert a base64 encoded string to a character string from_base64 qq a hash value returns a ubigint with the hash of the value hash 2595805878642663834 ilike_escape string like_specifier escape_character returns true if the string matches the like_specifier see pattern matching using case-insensitive matching escape_character is used to search for wildcard characters in the string ilike_escape a c a c true instr string search_string return location of first occurrence of search_string in string counting from 1 returns 0 if no match found instr test test es 2 left_grapheme string count extract the left-most grapheme clusters left_grapheme 1 left string count extract the left-most count characters left hello 2 he length_grapheme string number of grapheme clusters in string length_grapheme 2 length string number of characters in string length hello 6 like_escape string like_specifier escape_character returns true if the string matches the like_specifier see pattern matching using case-sensitive matching escape_character is used to search for wildcard characters in the string like_escape a c a c true lower string convert string to lower case lower hello hello lcase lpad string count character pads the string with the character from the left until it has count characters lpad hello 10 hello ltrim string characters removes any occurrences of any of the characters from the left sduide of the string ltrim test test ltrim string removes any spaces from the left side of the string ltrim test test md5 value returns the md5 hash of the value md5 123 202cb962ac59075b964b07152d234b70 nfc_normalize string convert string to unicode nfc normalized string useful for comparisons and ordering if text data is mixed between nfc normalized and not nfc_normalize arde ch ard\u00e8ch not_ilike_escape string like_specifier escape_character returns false if the string matches the like_specifier see pattern matching using case-sensitive matching escape_character is used to search for wildcard characters in the string not_ilike_escape a c a c false not_like_escape string like_specifier escape_character returns false if the string matches the like_specifier see pattern matching using case-insensitive matching escape_character is used to search for wildcard characters in the string not_like_escape a c a c false ord string return ascii character code of the leftmost character in a string ord \u00fc 252 parse_dirname path separator returns the top-level directory name from the given path separator options system both_slash default forward_slash backslash parse_dirname path to file csv system path parse_dirpath path separator returns the head of the path the pathname until the last slash similarly to python s os path dirname function separator options system both_slash default forward_slash backslash parse_dirpath path to file csv forward_slash path to parse_filename path trim_extension separator returns the last component of the path similarly to python s os path basename function if trim_extension is true the file extension will be removed defaults to false separator options system both_slash default forward_slash backslash parse_filename path to file csv true system file parse_path path separator returns a list of the components directories and filename in the path similarly to python s pathlib parts function separator options system both_slash default forward_slash backslash parse_path path to file csv system path to file csv position search_string in string return location of first occurrence of search_string in string counting from 1 returns 0 if no match found position b in abc 2 printf format parameters formats a string using printf syntax printf benchmark s took d seconds csv 42 benchmark csv took 42 seconds regexp_escape string escapes special patterns to turn string into a regular expression similarly to python s re escape function regexp_escape https duckdb org https duckdb org regexp_extract_all string regex group 0 split the string along the regex and extract all occurrences of group regexp_extract_all hello_world a-z _ 1 hello world regexp_extract string pattern name_list if string contains the regexp pattern returns the capturing groups as a struct with corresponding names from name_list see pattern matching regexp_extract 2023-04-15 d - d - d y m d y 2023 m 04 d 15 regexp_extract string pattern idx if string contains the regexp pattern returns the capturing group specified by optional parameter idx see pattern matching regexp_extract hello_world a-z _ 1 hello regexp_full_match string regex returns true if the entire string matches the regex see pattern matching regexp_full_match anabanana an false regexp_matches string pattern returns true if string contains the regexp pattern false otherwise see pattern matching regexp_matches anabanana an true regexp_replace string pattern replacement if string contains the regexp pattern replaces the matching part with replacement see pattern matching regexp_replace hello lo - he-lo regexp_split_to_array string regex splits the string along the regex regexp_split_to_array hello world 42 hello world 42 string_split_regex str_split_regex repeat string count repeats the string count number of times repeat a 5 aaaaa replace string source target replaces any occurrences of the source with target in string replace hello l - he--o reverse string reverses the string reverse hello olleh right_grapheme string count extract the right-most count grapheme clusters right_grapheme 1 right string count extract the right-most count characters right hello 3 lo rpad string count character pads the string with the character from the right until it has count characters rpad hello 10 hello rtrim string characters removes any occurrences of any of the characters from the right side of the string rtrim test test rtrim string removes any spaces from the right side of the string rtrim test test sha256 value returns a varchar with the sha-256 hash of the value sha-256 d7a5c5e0d1d94c32218539e7e47d4ba9c3c7b77d61332fb60d633dde89e473fb split_part string separator index split the string along the separator and return the data at the 1-based index of the list if the index is outside the bounds of the list return an empty string to match postgresql s behavior split_part a b c 2 b starts_with string search_string return true if string begins with search_string starts_with abc a true str_split_regex string regex splits the string along the regex str_split_regex hello world 42 hello world 42 string_split_regex regexp_split_to_array string_split_regex string regex splits the string along the regex string_split_regex hello world 42 hello world 42 str_split_regex regexp_split_to_array string_split string separator splits the string along the separator string_split hello world hello world str_split string_to_array strip_accents string strips accents from string strip_accents m\u00fchleisen muhleisen strlen string number of bytes in string strlen 4 strpos string search_string alias for instr return location of first occurrence of search_string in string counting from 1 returns 0 if no match found strpos test test es 2 instr substring string start length extract substring of length characters starting from character start note that a start value of 1 refers to the first character of the string substring hello 2 2 el substr substring_grapheme string start length extract substring of length grapheme clusters starting from character start note that a start value of 1 refers to the first character of the string substring_grapheme 3 2 to_base64 blob convert a blob to a base64 encoded string to_base64 a blob qq base64 trim string characters removes any occurrences of any of the characters from either side of the string trim test test trim string removes any spaces from either side of the string trim test test unicode string returns the unicode code of the first character of the string unicode \u00fc 252 upper string convert string to upper case upper hello hello ucase text similarity functions these functions are used to measure the similarity of two strings using various similarity measures function description example result -- -- --- - damerau_levenshtein s1 s2 extension of levenshtein distance to also include transposition of adjacent characters as an allowed edit operation in other words the minimum number of edit operations insertions deletions substitutions or transpositions required to change one string to another different case is considered different damerau_levenshtein duckdb udckbd 2 editdist3 s1 s2 alias of levenshtein for sqlite compatibility the minimum number of single-character edits insertions deletions or substitutions required to change one string to the other different case is considered different editdist3 duck db 3 jaccard s1 s2 the jaccard similarity between two strings different case is considered different returns a number between 0 and 1 jaccard duck luck 0 6 jaro_similarity s1 s2 the jaro similarity between two strings different case is considered different returns a number between 0 and 1 jaro_similarity duck duckdb 0 88 jaro_winkler_similarity s1 s2 the jaro-winkler similarity between two strings different case is considered different returns a number between 0 and 1 jaro_winkler_similarity duck duckdb 0 93 levenshtein s1 s2 the minimum number of single-character edits insertions deletions or substitutions required to change one string to the other different case is considered different levenshtein duck db 3 mismatches s1 s2 alias for hamming s1 s2 the number of positions with different characters for two strings of equal length different case is considered different mismatches duck luck 1 formatters fmt syntax the format format parameters function formats strings loosely following the syntax of the fmt open-source formatting library -- format without additional parameters select format hello world -- hello world -- format a string using select format the answer is 42 -- the answer is 42 s the answer is 42 -- format a string using positional arguments select format i d rather be 1 than 0 right happy -- i d rather be happy than right format specifiers specifier description example - ------ --- d integer 123456 e scientific notation 3 141593e 00 f float 4 560000 o octal 361100 s string asd x hexadecimal 1e240 tx integer x is the thousand separator 123 456 formatting types -- integers select format 3 5 3 5 -- 3 5 8 -- booleans select format true false -- true false -- format datetime values select format date 1992-01-01 -- 1992-01-01 select format time 12 01 00 -- 12 01 00 select format timestamp 1992-01-01 12 01 00 -- 1992-01-01 12 01 00 -- format blob select format blob x00hello -- x00hello -- pad integers with 0s select format 04d 33 -- 0033 -- create timestamps from integers select format 02d 02d 02d 12 3 16 am -- 12 03 16 am -- convert to hexadecimal select format x 123456789 -- 75bcd15 -- convert to binary select format b 123456789 -- 111010110111100110100010101 print numbers with thousand separators select format 123456789 -- 123 456 789 select format t 123456789 -- 123 456 789 select format 123456789 -- 123 456 789 select format _ 123456789 -- 123_456_789 select format t 123456789 -- 123 456 789 select format tx 123456789 -- 123x456x789 printf syntax the printf format parameters function formats strings using the printf syntax -- format without additional parameters select printf hello world -- hello world -- format a string using select printf the answer is d 42 -- the answer is 42 s the answer is 42 -- format a string using positional arguments position formatter -- e g the second parameter as a string is encoded as 2 s select printf i d rather be 2 s than 1 s right happy -- i d rather be happy than right format specifiers specifier description example - ------ --- c character code to character a d integer 123456 xd integer with thousand seperarator x from _ 123_456 e scientific notation 3 141593e 00 f float 4 560000 hd integer 123456 hhd integer 123456 lld integer 123456 o octal 361100 s string asd x hexadecimal 1e240 formatting types -- integers select printf d d d 3 5 3 5 -- 3 5 8 -- booleans select printf s s true false -- true false -- format datetime values select printf s date 1992-01-01 -- 1992-01-01 select printf s time 12 01 00 -- 12 01 00 select printf s timestamp 1992-01-01 12 01 00 -- 1992-01-01 12 01 00 -- format blob select printf s blob x00hello -- x00hello -- pad integers with 0s select printf 04d 33 -- 0033 -- create timestamps from integers select printf 02d 02d 02d s 12 3 16 am -- 12 03 16 am -- convert to hexadecimal select printf x 123456789 -- 75bcd15 -- convert to binary select printf b 123456789 -- 111010110111100110100010101 thousand separators select printf d 123456789 -- 123 456 789 select printf d 123456789 -- 123 456 789 select printf d 123456789 -- 123 456 789 select printf _d 123456789 -- 123_456_789",
			"category": "Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "This section describes functions and operators for examining and manipulating string values. The \u2423 symbol denotes a..."
		},
		{
			"title": "Text Types",
			"text": "in duckdb strings can be stored in the varchar field the field allows storage of unicode characters internally the data is encoded as utf-8 name aliases description --- --- --- varchar char bpchar string text variable-length character string varchar n string n text n variable-length character string the maximum length n has no effect and is only provided for compatibility specifying a length limit specifying the length for the varchar string and text types is not required and has no effect on the system specifying the length will not improve performance or reduce storage space of the strings in the database these variants variant is supported for compatibility reasons with other systems that do require a length to be specified for strings if you wish to restrict the number of characters in a varchar column for data integrity reasons the check constraint should be used for example create table strings val varchar check length val 10 -- val has a maximum length of 10 the varchar field allows storage of unicode characters internally the data is encoded as utf-8 formatting strings strings in duckdb are surrounded by single quote apostrophe characters select hello world as msg msg varchar hello world to include a single quote character in a string use select hello world as msg msg varchar hello world to include special characters such as newline use the chr character function select hello chr 10 world as msg msg varchar hello nworld double quote characters double quote characters are used to denote table and column names surrounding their names allows the use of keywords e g create table table order bigint while duckdb occasionally accepts both single quote and double quotes for strings e g both from filename csv and from filename csv work their use is not recommended functions see character functions and pattern matching",
			"category": "Data Types",
			"url": "/docs/sql/data_types/text",
			"blurb": "In DuckDB, strings can be stored in the VARCHAR field."
		},
		{
			"title": "Time Functions",
			"text": "this section describes functions and operators for examining and manipulating time values time operators the table below shows the available mathematical operators for time types operator description example result - --- ---- -- addition of an interval time 01 02 03 interval 5 hour 06 02 03 - subtraction of an interval time 06 02 03 - interval 5 hour 01 02 03 time functions the table below shows the available scalar functions for time types function description example result -- -- --- -- current_time get_current_time current time start of current transaction date_diff part starttime endtime the number of partition boundaries between the times date_diff hour time 01 02 03 time 06 01 03 5 date_part part time get subfield equivalent to extract date_part minute time 14 21 13 21 date_sub part starttime endtime the number of complete partitions between the times date_sub hour time 01 02 03 time 06 01 03 4 datediff part starttime endtime alias of date_diff the number of partition boundaries between the times datediff hour time 01 02 03 time 06 01 03 5 datepart part time alias of date_part get subfield equivalent to extract datepart minute time 14 21 13 21 datesub part starttime endtime alias of date_sub the number of complete partitions between the times datesub hour time 01 02 03 time 06 01 03 4 extract part from time get subfield from a time extract hour from time 14 21 13 14 make_time bigint bigint double the time for the given parts make_time 13 34 27 123456 13 34 27 123456 the only date parts that are defined for times are epoch hours minutes seconds milliseconds and microseconds",
			"category": "Functions",
			"url": "/docs/sql/functions/time",
			"blurb": "This section describes functions and operators for examining and manipulating TIME values. Time Operators The table..."
		},
		{
			"title": "Time Types",
			"text": "the time and timetz types specify the hour minute second microsecond of a day name aliases description ------- ----------------------- ------------------------------ time time without time zone time of day ignores time zone timetz time with time zone time of day uses time zone instances can be created using the type names as a keyword where the data must be formatted according to the iso 8601 format hh mm ss zzzzzz -tt tt select time 1992-09-20 11 30 00 123456 -- 11 30 00 123456 select timetz 1992-09-20 11 30 00 123456 -- 11 30 00 123456 00 select timetz 1992-09-20 11 30 00 123456-02 00 -- 13 30 00 123456 00 select timetz 1992-09-20 11 30 00 123456 05 30 -- 06 00 00 123456 00 the time type should only be used in rare cases where the date part of the timestamp can be disregarded most applications should use the timestamp types to represent their timestamps",
			"category": "Data Types",
			"url": "/docs/sql/data_types/time",
			"blurb": "A time instance represents the time of a day (hour, minute, second, microsecond)."
		},
		{
			"title": "Time Zone Reference List",
			"text": "an up-to-date version of this list can be pulled from the pg_timezone_names table function select name abbrev from pg_timezone_names order by name name abbrev ---------------------------------- ---------------------------------- act act aet aet agt agt art art ast ast africa abidjan iceland africa accra iceland africa addis_ababa eat africa algiers africa algiers africa asmara eat africa asmera eat africa bamako iceland africa bangui africa bangui africa banjul iceland africa bissau africa bissau africa blantyre cat africa brazzaville africa brazzaville africa bujumbura cat africa cairo art africa casablanca africa casablanca africa ceuta africa ceuta africa conakry iceland africa dakar iceland africa dar_es_salaam eat africa djibouti eat africa douala africa douala africa el_aaiun africa el_aaiun africa freetown iceland africa gaborone cat africa harare cat africa johannesburg africa johannesburg africa juba africa juba africa kampala eat africa khartoum africa khartoum africa kigali cat africa kinshasa africa kinshasa africa lagos africa lagos africa libreville africa libreville africa lome iceland africa luanda africa luanda africa lubumbashi cat africa lusaka cat africa malabo africa malabo africa maputo cat africa maseru africa maseru africa mbabane africa mbabane africa mogadishu eat africa monrovia africa monrovia africa nairobi eat africa ndjamena africa ndjamena africa niamey africa niamey africa nouakchott iceland africa ouagadougou iceland africa porto-novo africa porto-novo africa sao_tome africa sao_tome africa timbuktu iceland africa tripoli libya africa tunis africa tunis africa windhoek africa windhoek america adak america adak america anchorage ast america anguilla prt america antigua prt america araguaina america araguaina america argentina buenos_aires agt america argentina catamarca america argentina catamarca america argentina comodrivadavia america argentina comodrivadavia america argentina cordoba america argentina cordoba america argentina jujuy america argentina jujuy america argentina la_rioja america argentina la_rioja america argentina mendoza america argentina mendoza america argentina rio_gallegos america argentina rio_gallegos america argentina salta america argentina salta america argentina san_juan america argentina san_juan america argentina san_luis america argentina san_luis america argentina tucuman america argentina tucuman america argentina ushuaia america argentina ushuaia america aruba prt america asuncion america asuncion america atikokan america atikokan america atka america atka america bahia america bahia america bahia_banderas america bahia_banderas america barbados america barbados america belem america belem america belize america belize america blanc-sablon prt america boa_vista america boa_vista america bogota america bogota america boise america boise america buenos_aires agt america cambridge_bay america cambridge_bay america campo_grande america campo_grande america cancun america cancun america caracas america caracas america catamarca america catamarca america cayenne america cayenne america cayman america cayman america chicago cst america chihuahua america chihuahua america ciudad_juarez america ciudad_juarez america coral_harbour america coral_harbour america cordoba america cordoba america costa_rica america costa_rica america creston pnt america cuiaba america cuiaba america curacao prt america danmarkshavn america danmarkshavn america dawson america dawson america dawson_creek america dawson_creek america denver navajo america detroit america detroit america dominica prt america edmonton america edmonton america eirunepe america eirunepe america el_salvador america el_salvador america ensenada america ensenada america fort_nelson america fort_nelson america fort_wayne iet america fortaleza america fortaleza america glace_bay america glace_bay america godthab america godthab america goose_bay america goose_bay america grand_turk america grand_turk america grenada prt america guadeloupe prt america guatemala america guatemala america guayaquil america guayaquil america guyana america guyana america halifax america halifax america havana cuba america hermosillo america hermosillo america indiana indianapolis iet america indiana knox america indiana knox america indiana marengo america indiana marengo america indiana petersburg america indiana petersburg america indiana tell_city america indiana tell_city america indiana vevay america indiana vevay america indiana vincennes america indiana vincennes america indiana winamac america indiana winamac america indianapolis iet america inuvik america inuvik america iqaluit america iqaluit america jamaica jamaica america jujuy america jujuy america juneau america juneau america kentucky louisville america kentucky louisville america kentucky monticello america kentucky monticello america knox_in america knox_in america kralendijk prt america la_paz america la_paz america lima america lima america los_angeles pst america louisville america louisville america lower_princes prt america maceio america maceio america managua america managua america manaus america manaus america marigot prt america martinique america martinique america matamoros america matamoros america mazatlan america mazatlan america mendoza america mendoza america menominee america menominee america merida america merida america metlakatla america metlakatla america mexico_city america mexico_city america miquelon america miquelon america moncton america moncton america monterrey america monterrey america montevideo america montevideo america montreal america montreal america montserrat prt america nassau america nassau america new_york america new_york america nipigon america nipigon america nome america nome america noronha america noronha america north_dakota beulah america north_dakota beulah america north_dakota center america north_dakota center america north_dakota new_salem america north_dakota new_salem america nuuk america nuuk america ojinaga america ojinaga america panama america panama america pangnirtung america pangnirtung america paramaribo america paramaribo america phoenix pnt america port-au-prince america port-au-prince america port_of_spain prt america porto_acre america porto_acre america porto_velho america porto_velho america puerto_rico prt america punta_arenas america punta_arenas america rainy_river america rainy_river america rankin_inlet america rankin_inlet america recife america recife america regina america regina america resolute america resolute america rio_branco america rio_branco america rosario america rosario america santa_isabel america santa_isabel america santarem america santarem america santiago america santiago america santo_domingo america santo_domingo america sao_paulo bet america scoresbysund america scoresbysund america shiprock navajo america sitka america sitka america st_barthelemy prt america st_johns cnt america st_kitts prt america st_lucia prt america st_thomas prt america st_vincent prt america swift_current america swift_current america tegucigalpa america tegucigalpa america thule america thule america thunder_bay america thunder_bay america tijuana america tijuana america toronto america toronto america tortola prt america vancouver america vancouver america virgin prt america whitehorse america whitehorse america winnipeg america winnipeg america yakutat america yakutat america yellowknife america yellowknife antarctica casey antarctica casey antarctica davis antarctica davis antarctica dumontdurville antarctica dumontdurville antarctica macquarie antarctica macquarie antarctica mawson antarctica mawson antarctica mcmurdo nz antarctica palmer antarctica palmer antarctica rothera antarctica rothera antarctica south_pole nz antarctica syowa antarctica syowa antarctica troll antarctica troll antarctica vostok antarctica vostok arctic longyearbyen arctic longyearbyen asia aden asia aden asia almaty asia almaty asia amman asia amman asia anadyr asia anadyr asia aqtau asia aqtau asia aqtobe asia aqtobe asia ashgabat asia ashgabat asia ashkhabad asia ashkhabad asia atyrau asia atyrau asia baghdad asia baghdad asia bahrain asia bahrain asia baku asia baku asia bangkok asia bangkok asia barnaul asia barnaul asia beirut asia beirut asia bishkek asia bishkek asia brunei asia brunei asia calcutta ist asia chita asia chita asia choibalsan asia choibalsan asia chongqing ctt asia chungking ctt asia colombo asia colombo asia dacca bst asia damascus asia damascus asia dhaka bst asia dili asia dili asia dubai asia dubai asia dushanbe asia dushanbe asia famagusta asia famagusta asia gaza asia gaza asia harbin ctt asia hebron asia hebron asia ho_chi_minh vst asia hong_kong hongkong asia hovd asia hovd asia irkutsk asia irkutsk asia istanbul turkey asia jakarta asia jakarta asia jayapura asia jayapura asia jerusalem israel asia kabul asia kabul asia kamchatka asia kamchatka asia karachi plt asia kashgar asia kashgar asia kathmandu asia kathmandu asia katmandu asia katmandu asia khandyga asia khandyga asia kolkata ist asia krasnoyarsk asia krasnoyarsk asia kuala_lumpur singapore asia kuching asia kuching asia kuwait asia kuwait asia macao asia macao asia macau asia macau asia magadan asia magadan asia makassar asia makassar asia manila asia manila asia muscat asia muscat asia nicosia asia nicosia asia novokuznetsk asia novokuznetsk asia novosibirsk asia novosibirsk asia omsk asia omsk asia oral asia oral asia phnom_penh asia phnom_penh asia pontianak asia pontianak asia pyongyang asia pyongyang asia qatar asia qatar asia qostanay asia qostanay asia qyzylorda asia qyzylorda asia rangoon asia rangoon asia riyadh asia riyadh asia saigon vst asia sakhalin asia sakhalin asia samarkand asia samarkand asia seoul rok asia shanghai ctt asia singapore singapore asia srednekolymsk asia srednekolymsk asia taipei roc asia tashkent asia tashkent asia tbilisi asia tbilisi asia tehran iran asia tel_aviv israel asia thimbu asia thimbu asia thimphu asia thimphu asia tokyo jst asia tomsk asia tomsk asia ujung_pandang asia ujung_pandang asia ulaanbaatar asia ulaanbaatar asia ulan_bator asia ulan_bator asia urumqi asia urumqi asia ust-nera asia ust-nera asia vientiane asia vientiane asia vladivostok asia vladivostok asia yakutsk asia yakutsk asia yangon asia yangon asia yekaterinburg asia yekaterinburg asia yerevan net atlantic azores atlantic azores atlantic bermuda atlantic bermuda atlantic canary atlantic canary atlantic cape_verde atlantic cape_verde atlantic faeroe atlantic faeroe atlantic faroe atlantic faroe atlantic jan_mayen atlantic jan_mayen atlantic madeira atlantic madeira atlantic reykjavik iceland atlantic south_georgia atlantic south_georgia atlantic st_helena iceland atlantic stanley atlantic stanley australia act aet australia adelaide australia adelaide australia brisbane australia brisbane australia broken_hill australia broken_hill australia canberra aet australia currie australia currie australia darwin act australia eucla australia eucla australia hobart australia hobart australia lhi australia lhi australia lindeman australia lindeman australia lord_howe australia lord_howe australia melbourne australia melbourne australia nsw aet australia north act australia perth australia perth australia queensland australia queensland australia south australia south australia sydney aet australia tasmania australia tasmania australia victoria australia victoria australia west australia west australia yancowinna australia yancowinna bet bet bst bst brazil acre brazil acre brazil denoronha brazil denoronha brazil east bet brazil west brazil west cat cat cet cet cnt cnt cst cst cst6cdt cst6cdt ctt ctt canada atlantic canada atlantic canada central canada central canada east-saskatchewan canada east-saskatchewan canada eastern canada eastern canada mountain canada mountain canada newfoundland cnt canada pacific canada pacific canada saskatchewan canada saskatchewan canada yukon canada yukon chile continental chile continental chile easterisland chile easterisland cuba cuba eat eat ect ect eet eet est est est5edt est5edt egypt art eire eire etc gmt gmt etc gmt 0 gmt etc gmt 1 etc gmt 1 etc gmt 10 etc gmt 10 etc gmt 11 etc gmt 11 etc gmt 12 etc gmt 12 etc gmt 2 etc gmt 2 etc gmt 3 etc gmt 3 etc gmt 4 etc gmt 4 etc gmt 5 etc gmt 5 etc gmt 6 etc gmt 6 etc gmt 7 etc gmt 7 etc gmt 8 etc gmt 8 etc gmt 9 etc gmt 9 etc gmt-0 gmt etc gmt-1 etc gmt-1 etc gmt-10 etc gmt-10 etc gmt-11 etc gmt-11 etc gmt-12 etc gmt-12 etc gmt-13 etc gmt-13 etc gmt-14 etc gmt-14 etc gmt-2 etc gmt-2 etc gmt-3 etc gmt-3 etc gmt-4 etc gmt-4 etc gmt-5 etc gmt-5 etc gmt-6 etc gmt-6 etc gmt-7 etc gmt-7 etc gmt-8 etc gmt-8 etc gmt-9 etc gmt-9 etc gmt0 gmt etc greenwich gmt etc uct uct etc utc uct etc universal uct etc zulu uct europe amsterdam europe amsterdam europe andorra europe andorra europe astrakhan europe astrakhan europe athens europe athens europe belfast gb europe belgrade europe belgrade europe berlin europe berlin europe bratislava europe bratislava europe brussels europe brussels europe bucharest europe bucharest europe budapest europe budapest europe busingen europe busingen europe chisinau europe chisinau europe copenhagen europe copenhagen europe dublin eire europe gibraltar europe gibraltar europe guernsey gb europe helsinki europe helsinki europe isle_of_man gb europe istanbul turkey europe jersey gb europe kaliningrad europe kaliningrad europe kiev europe kiev europe kirov europe kirov europe kyiv europe kyiv europe lisbon portugal europe ljubljana europe ljubljana europe london gb europe luxembourg europe luxembourg europe madrid europe madrid europe malta europe malta europe mariehamn europe mariehamn europe minsk europe minsk europe monaco ect europe moscow w-su europe nicosia europe nicosia europe oslo europe oslo europe paris ect europe podgorica europe podgorica europe prague europe prague europe riga europe riga europe rome europe rome europe samara europe samara europe san_marino europe san_marino europe sarajevo europe sarajevo europe saratov europe saratov europe simferopol europe simferopol europe skopje europe skopje europe sofia europe sofia europe stockholm europe stockholm europe tallinn europe tallinn europe tirane europe tirane europe tiraspol europe tiraspol europe ulyanovsk europe ulyanovsk europe uzhgorod europe uzhgorod europe vaduz europe vaduz europe vatican europe vatican europe vienna europe vienna europe vilnius europe vilnius europe volgograd europe volgograd europe warsaw poland europe zagreb europe zagreb europe zaporozhye europe zaporozhye europe zurich europe zurich factory factory gb gb gb-eire gb gmt gmt gmt 0 gmt gmt-0 gmt gmt0 gmt greenwich gmt hst hst hongkong hongkong iet iet ist ist iceland iceland indian antananarivo eat indian chagos indian chagos indian christmas indian christmas indian cocos indian cocos indian comoro eat indian kerguelen indian kerguelen indian mahe indian mahe indian maldives indian maldives indian mauritius indian mauritius indian mayotte eat indian reunion indian reunion iran iran israel israel jst jst jamaica jamaica japan jst kwajalein kwajalein libya libya met met mit mit mst mst mst7mdt mst7mdt mexico bajanorte mexico bajanorte mexico bajasur mexico bajasur mexico general mexico general net net nst nz nz nz nz-chat nz-chat navajo navajo plt plt pnt pnt prc ctt prt prt pst pst pst8pdt pst8pdt pacific apia mit pacific auckland nz pacific bougainville pacific bougainville pacific chatham nz-chat pacific chuuk pacific chuuk pacific easter pacific easter pacific efate pacific efate pacific enderbury pacific enderbury pacific fakaofo pacific fakaofo pacific fiji pacific fiji pacific funafuti pacific funafuti pacific galapagos pacific galapagos pacific gambier pacific gambier pacific guadalcanal sst pacific guam pacific guam pacific honolulu pacific honolulu pacific johnston pacific johnston pacific kanton pacific kanton pacific kiritimati pacific kiritimati pacific kosrae pacific kosrae pacific kwajalein kwajalein pacific majuro pacific majuro pacific marquesas pacific marquesas pacific midway pacific midway pacific nauru pacific nauru pacific niue pacific niue pacific norfolk pacific norfolk pacific noumea pacific noumea pacific pago_pago pacific pago_pago pacific palau pacific palau pacific pitcairn pacific pitcairn pacific pohnpei sst pacific ponape sst pacific port_moresby pacific port_moresby pacific rarotonga pacific rarotonga pacific saipan pacific saipan pacific samoa pacific samoa pacific tahiti pacific tahiti pacific tarawa pacific tarawa pacific tongatapu pacific tongatapu pacific truk pacific truk pacific wake pacific wake pacific wallis pacific wallis pacific yap pacific yap poland poland portugal portugal roc roc rok rok sst sst singapore singapore systemv ast4 systemv ast4 systemv ast4adt systemv ast4adt systemv cst6 systemv cst6 systemv cst6cdt systemv cst6cdt systemv est5 systemv est5 systemv est5edt systemv est5edt systemv hst10 systemv hst10 systemv mst7 systemv mst7 systemv mst7mdt systemv mst7mdt systemv pst8 systemv pst8 systemv pst8pdt systemv pst8pdt systemv yst9 systemv yst9 systemv yst9ydt systemv yst9ydt turkey turkey uct uct us alaska ast us aleutian us aleutian us arizona pnt us central cst us east-indiana iet us eastern us eastern us hawaii us hawaii us indiana-starke us indiana-starke us michigan us michigan us mountain navajo us pacific pst us pacific-new pst us samoa us samoa utc uct universal uct vst vst w-su w-su wet wet zulu uct",
			"category": "Data Types",
			"url": "/docs/sql/data_types/timezones",
			"blurb": "A reference list for Time Zones"
		},
		{
			"title": "Timestamp Functions",
			"text": "this section describes functions and operators for examining and manipulating timestamp values timestamp operators the table below shows the available mathematical operators for timestamp types operator description example result - -- ---- -- addition of an interval timestamp 1992-03-22 01 02 03 interval 5 day 1992-03-27 01 02 03 - subtraction of timestamp s timestamp 1992-03-27 - timestamp 1992-03-22 5 days - subtraction of an interval timestamp 1992-03-27 01 02 03 - interval 5 day 1992-03-22 01 02 03 adding to or subtracting from infinite values produces the same infinite value timestamp functions the table below shows the available scalar functions for timestamp values function description example result -- -- --- -- age timestamp timestamp subtract arguments resulting in the time difference between the two timestamps age timestamp 2001-04-10 timestamp 1992-09-20 8 years 6 months 20 days age timestamp subtract from current_date age timestamp 1992-09-20 29 years 1 month 27 days 12 39 00 844 century timestamp extracts the century of a timestamp century timestamp 1992-03-22 20 date_diff part startdate enddate the number of partition boundaries between the timestamps date_diff hour timestamp 1992-09-30 23 59 59 timestamp 1992-10-01 01 58 00 2 date_part part timestamp get the listed subfields as a struct the list must be constant date_part year month day timestamp 1992-09-20 20 38 40 year 1992 month 9 day 20 date_part part timestamp get subfield equivalent to extract date_part minute timestamp 1992-09-20 20 38 40 38 date_sub part startdate enddate the number of complete partitions between the timestamps date_sub hour timestamp 1992-09-30 23 59 59 timestamp 1992-10-01 01 58 00 1 date_trunc part timestamp truncate to specified precision date_trunc hour timestamp 1992-09-20 20 38 40 1992-09-20 20 00 00 datediff part startdate enddate alias of date_diff the number of partition boundaries between the timestamps datediff hour timestamp 1992-09-30 23 59 59 timestamp 1992-10-01 01 58 00 2 datepart part timestamp alias of date_part get the listed subfields as a struct the list must be constant datepart year month day timestamp 1992-09-20 20 38 40 year 1992 month 9 day 20 datepart part timestamp alias of date_part get subfield equivalent to extract datepart minute timestamp 1992-09-20 20 38 40 38 datesub part startdate enddate alias of date_sub the number of complete partitions between the timestamps datesub hour timestamp 1992-09-30 23 59 59 timestamp 1992-10-01 01 58 00 1 datetrunc part timestamp alias of date_trunc truncate to specified precision datetrunc hour timestamp 1992-09-20 20 38 40 1992-09-20 20 00 00 dayname timestamp the english name of the weekday dayname timestamp 1992-03-22 sunday epoch_ms ms converts ms since epoch to a timestamp epoch_ms 701222400000 1992-03-22 00 00 00 epoch_ms timestamp converts a timestamp to milliseconds since the epoch epoch_ms 2022-11-07 08 43 04 123456 timestamp 1667810584123 epoch_ms timestamp return the total number of milliseconds since the epoch epoch_ms timestamp 2021-08-03 11 59 44 123456 1627991984123 epoch_ns timestamp return the total number of nanoseconds since the epoch epoch_ns timestamp 2021-08-03 11 59 44 123456 1627991984123456000 epoch_us timestamp return the total number of microseconds since the epoch epoch_ms timestamp 2021-08-03 11 59 44 123456 1627991984123456 epoch timestamp converts a timestamp to seconds since the epoch epoch 2022-11-07 08 43 04 timestamp 1667810584 extract field from timestamp get subfield from a timestamp extract hour from timestamp 1992-09-20 20 38 48 20 greatest timestamp timestamp the later of two timestamps greatest timestamp 1992-09-20 20 38 48 timestamp 1992-03-22 01 02 03 1234 1992-09-20 20 38 48 isfinite timestamp returns true if the timestamp is finite false otherwise isfinite timestamp 1992-03-07 true isinf timestamp returns true if the timestamp is infinite false otherwise isinf timestamp -infinity true last_day timestamp the last day of the month last_day timestamp 1992-03-22 01 02 03 1234 1992-03-31 least timestamp timestamp the earlier of two timestamps least timestamp 1992-09-20 20 38 48 timestamp 1992-03-22 01 02 03 1234 1992-03-22 01 02 03 1234 make_timestamp bigint bigint bigint bigint bigint double the timestamp for the given parts make_timestamp 1992 9 20 13 34 27 123456 1992-09-20 13 34 27 123456 make_timestamp microseconds the timestamp for the given number of \u00b5s since the epoch make_timestamp 1667810584123456 2022-11-07 08 43 04 123456 monthname timestamp the english name of the month monthname timestamp 1992-09-20 september strftime timestamp format converts timestamp to string according to the format string strftime timestamp 1992-01-01 20 38 40 a -d b y - i m s p wed 1 january 1992 - 08 38 40 pm strptime text format-list converts string to timestamp applying the format strings in the list until one succeeds throws on failure strptime 4 15 2023 10 56 00 d m y h m s m d y h m s 2023-04-15 10 56 00 strptime text format converts string to timestamp according to the format string throws on failure strptime wed 1 january 1992 - 08 38 40 pm a -d b y - i m s p 1992-01-01 20 38 40 time_bucket bucket_width timestamp offset truncate timestamp by the specified interval bucket_width buckets are offset by offset interval time_bucket interval 10 minutes timestamp 1992-04-20 15 26 00-07 interval 5 minutes 1992-04-20 15 25 00 time_bucket bucket_width timestamp origin truncate timestamp by the specified interval bucket_width buckets are aligned relative to origin timestamp origin defaults to 2000-01-03 00 00 00 for buckets that don t include a month or year interval and to 2000-01-01 00 00 00 for month and year buckets time_bucket interval 2 weeks timestamp 1992-04-20 15 26 00 timestamp 1992-04-01 00 00 00 1992-04-15 00 00 00 to_timestamp double converts seconds since the epoch to a timestamp with time zone to_timestamp 1284352323 5 2010-09-13 04 32 03 5 00 try_strptime text format-list converts string to timestamp applying the format strings in the list until one succeeds returns null on failure try_strptime 4 15 2023 10 56 00 d m y h m s m d y h m s 2023-04-15 10 56 00 try_strptime text format converts string to timestamp according to the format string returns null on failure try_strptime wed 1 january 1992 - 08 38 40 pm a -d b y - i m s p 1992-01-01 20 38 40 there are also dedicated extraction functions to get the subfields functions applied to infinite dates will either return the same infinite dates e g greatest or null e g date_part depending on what makes sense in general if the function needs to examine the parts of the infinite date the result will be null timestamp table functions the table below shows the available table functions for timestamp types function description example -- -- --- generate_series timestamp timestamp interval generate a table of timestamps in the closed range stepping by the interval generate_series timestamp 2001-04-10 timestamp 2001-04-11 interval 30 minute range timestamp timestamp interval generate a table of timestamps in the half open range stepping by the interval range timestamp 2001-04-10 timestamp 2001-04-11 interval 30 minute infinite values are not allowed as table function bounds",
			"category": "Functions",
			"url": "/docs/sql/functions/timestamp",
			"blurb": "This section describes functions and operators for examining and manipulating TIMESTAMP values. Timestamp Operators..."
		},
		{
			"title": "Timestamp Types",
			"text": "timestamps represent points in absolute time usually called instants duckdb represents instants as the number of microseconds \u00b5s since 1970-01-01 00 00 00 00 timestamp types name aliases description --- --- --- timestamp_ns timestamp datetime timestamp with nanosecond precision ignores time zone timestamp_ms timestamp with millisecond precision ignores time zone timestamp_s timestamp with second precision ignores time zone timestamptz timestamp with time zone timestamp uses time zone a timestamp specifies a combination of date year month day and a time hour minute second microsecond timestamps can be created using the timestamp keyword where the data must be formatted according to the iso 8601 format yyyy-mm-dd hh mm ss zzzzzz -tt tt select timestamp_ns 1992-09-20 11 30 00 123456 -- 1992-09-20 11 30 00 123456 select timestamp 1992-09-20 11 30 00 123456 -- 1992-09-20 11 30 00 123456 select datetime 1992-09-20 11 30 00 123456 -- 1992-09-20 11 30 00 123456 select timestamp_ms 1992-09-20 11 30 00 123456 -- 1992-09-20 11 30 00 123 select timestamp_s 1992-09-20 11 30 00 123456 -- 1992-09-20 11 30 00 select timestamptz 1992-09-20 11 30 00 123456 -- 1992-09-20 11 30 00 123456 00 select timestamp with time zone 1992-09-20 11 30 00 123456 -- 1992-09-20 11 30 00 123456 00 special values there are also three special date values that can be used on input input string valid types description ------------- -------------------------------------- ----------------------------------------------- epoch timestamp timestamptz 1970-01-01 00 00 00 00 unix system time zero infinity timestamp timestamptz later than all other time stamps -infinity timestamp timestamptz earlier than all other time stamps the values infinity and -infinity are specially represented inside the system and will be displayed unchanged but epoch is simply a notational shorthand that will be converted to the time stamp value when read select -infinity timestamp epoch timestamp infinity timestamp negative epoch positive ---------- -------------------- --------- -infinity 1970-01-01 00 00 00 infinity functions see timestamp functions time zones the timestamptz type can be binned into calendar and clock bins using a suitable extension the built-in icu extension implements all the binning and arithmetic functions using the international components for unicode time zone and calendar functions to set the time zone to use first load the icu extension the icu extension comes pre-bundled with several duckdb clients including python r jdbc and odbc so this step can be skipped in those cases in other cases you might first need to install and load the icu extension install icu load icu next use the set timezone command set timezone america los_angeles time binning operations for timestamptz will then be implemented using the given time zone a list of available time zones can be pulled from the pg_timezone_names table function select name abbrev utc_offset from pg_timezone_names order by name you can also find a reference table of available time zones calendars the icu extension also supports non-gregorian calendars using the set calendar command note that the install and load steps are only required if the duckdb client does not bundle the icu extension install icu load icu set calendar japanese time binning operations for timestamptz will then be implemented using the given calendar in this example the era part will now report the japanese imperial era number a list of available calendars can be pulled from the icu_calendar_names table function select name from icu_calendar_names order by 1 settings the current value of the timezone and calendar settings are determined by icu when it starts up they can be looked from in the duckdb_settings table function select from duckdb_settings where name timezone -- america los_angeles select from duckdb_settings where name calendar -- gregorian",
			"category": "Data Types",
			"url": "/docs/sql/data_types/timestamp",
			"blurb": "A timestamp specifies a combination of a date (year, month, day) and a time (hour, minute, second, microsecond)."
		},
		{
			"title": "Timestamp with Time Zone Functions",
			"text": "this section describes functions and operators for examining and manipulating timestamp with time zone or timestamptz values despite the name these values do not store a time zone - just an instant like timestamp instead they request that the instant be binned and formatted using the current time zone time zone support is not built in but can be provided by an extension such as the icu extension that ships with duckdb in the examples below the current time zone is presumed to be america los_angeles using the gregorian calendar built-in timestamp with time zone functions the table below shows the available scalar functions for timestamptz values since these functions do not involve binning or display they are always available function description example result -- -- --- -- current_timestamp current date and time start of current transaction current_timestamp 2022-10-08 12 44 46 122-07 get_current_timestamp current date and time start of current transaction get_current_timestamp 2022-10-08 12 44 46 122-07 greatest timestamptz timestamptz the later of two timestamps greatest timestamptz 1992-09-20 20 38 48 timestamptz 1992-03-22 01 02 03 1234 1992-09-20 20 38 48-07 isfinite timestamptz returns true if the timestamp with time zone is finite false otherwise isfinite timestamptz 1992-03-07 true isinf timestamptz returns true if the timestamp with time zone is infinite false otherwise isinf timestamptz -infinity true least timestamptz timestamptz the earlier of two timestamps least timestamptz 1992-09-20 20 38 48 timestamptz 1992-03-22 01 02 03 1234 1992-03-22 01 02 03 1234-08 now current date and time start of current transaction now 2022-10-08 12 44 46 122-07 transaction_timestamp current date and time start of current transaction transaction_timestamp 2022-10-08 12 44 46 122-07 timestamp with time zone strings with no time zone extension loaded timestamptz values will be cast to and from strings using offset notation this will let you specify an instant correctly without access to time zone information for portability timestamptz values will always be displayed using gmt offsets select 2022-10-08 13 13 34-07 timestamptz -- 2022-10-08 20 13 34 00 if a time zone extension such as icu is loaded then a time zone can be parsed from a string and cast to a representation in the local time zone select 2022-10-08 13 13 34 europe amsterdam timestamptz varchar -- 2022-10-08 04 13 34-07 -- the offset will differ based on your local time zone icu timestamp with time zone operators the table below shows the available mathematical operators for timestamp with time zone values provided by the icu extension operator description example result - -- ---- -- addition of an interval timestamptz 1992-03-22 01 02 03 interval 5 day 1992-03-27 01 02 03 - subtraction of timestamptz s timestamptz 1992-03-27 - timestamptz 1992-03-22 5 days - subtraction of an interval timestamptz 1992-03-27 01 02 03 - interval 5 day 1992-03-22 01 02 03 adding to or subtracting from infinite values produces the same infinite value icu timestamp with time zone functions the table below shows the icu provided scalar functions for timestamp with time zone values function description example result --- --- --- --- age timestamptz timestamptz subtract arguments resulting in the time difference between the two timestamps age timestamptz 2001-04-10 timestamptz 1992-09-20 8 years 6 months 20 days age timestamptz subtract from current_date age timestamp 1992-09-20 29 years 1 month 27 days 12 39 00 844 date_diff part startdate enddate the number of partition boundaries between the timestamps date_diff hour timestamptz 1992-09-30 23 59 59 timestamptz 1992-10-01 01 58 00 2 date_part part timestamptz get the listed subfields as a struct the list must be constant date_part year month day timestamptz 1992-09-20 20 38 40-07 year 1992 month 9 day 20 date_part part timestamptz get subfield equivalent to extract date_part minute timestamptz 1992-09-20 20 38 40 38 date_sub part startdate enddate the number of complete partitions between the timestamps date_sub hour timestamptz 1992-09-30 23 59 59 timestamptz 1992-10-01 01 58 00 1 date_trunc part timestamptz truncate to specified precision date_trunc hour timestamptz 1992-09-20 20 38 40 1992-09-20 20 00 00 datediff part startdate enddate alias of date_diff the number of partition boundaries between the timestamps datediff hour timestamptz 1992-09-30 23 59 59 timestamptz 1992-10-01 01 58 00 2 datepart part timestamptz alias of date_part get the listed subfields as a struct the list must be constant datepart year month day timestamptz 1992-09-20 20 38 40-07 year 1992 month 9 day 20 datepart part timestamptz alias of date_part get subfield equivalent to extract datepart minute timestamptz 1992-09-20 20 38 40 38 datesub part startdate enddate alias of date_sub the number of complete partitions between the timestamps datesub hour timestamptz 1992-09-30 23 59 59 timestamptz 1992-10-01 01 58 00 1 datetrunc part timestamptz alias of date_trunc truncate to specified precision datetrunc hour timestamptz 1992-09-20 20 38 40 1992-09-20 20 00 00 epoch_ms timestamptz converts a timestamptz to milliseconds since the epoch epoch_ms 2022-11-07 08 43 04 123456 00 timestamptz 1667810584123 epoch_ns timestamptz converts a timestamptz to nanoseconds since the epoch epoch_ns 2022-11-07 08 43 04 123456 00 timestamptz 1667810584123456000 epoch_us timestamptz converts a timestamptz to microseconds since the epoch epoch_us 2022-11-07 08 43 04 123456 00 timestamptz 1667810584123456 extract field from timestamptz get subfield from a timestamp with time zone extract hour from timestamptz 1992-09-20 20 38 48 20 last_day timestamptz the last day of the month last_day timestamptz 1992-03-22 01 02 03 1234 1992-03-31 make_timestamptz bigint bigint bigint bigint bigint double string the timestamp with time zone for the given parts and time zone make_timestamptz 1992 9 20 15 34 27 123456 cet 1992-09-20 06 34 27 123456-07 make_timestamptz bigint bigint bigint bigint bigint double the timestamp with time zone for the given parts in the current time zone make_timestamptz 1992 9 20 13 34 27 123456 1992-09-20 13 34 27 123456-07 make_timestamptz microseconds the timestamp with time zone for the given \u00b5s since the epoch make_timestamptz 1667810584123456 2022-11-07 16 43 04 123456-08 strftime timestamptz format converts timestamp with time zone to string according to the format string strftime timestamptz 1992-01-01 20 38 40 a -d b y - i m s p wed 1 january 1992 - 08 38 40 pm strptime text format converts string to timestamp with time zone according to the format string if z is specified strptime wed 1 january 1992 - 08 38 40 pst a -d b y - h m s z 1992-01-01 08 38 40-08 time_bucket bucket_width timestamptz offset truncate timestamptz by the specified interval bucket_width buckets are offset by offset interval time_bucket interval 10 minutes timestamptz 1992-04-20 15 26 00-07 interval 5 minutes 1992-04-20 15 25 00-07 time_bucket bucket_width timestamptz origin truncate timestamptz by the specified interval bucket_width buckets are aligned relative to origin timestamptz origin defaults to 2000-01-03 00 00 00 00 for buckets that don t include a month or year interval and to 2000-01-01 00 00 00 00 for month and year buckets time_bucket interval 2 weeks timestamptz 1992-04-20 15 26 00-07 timestamptz 1992-04-01 00 00 00-07 1992-04-15 00 00 00-07 time_bucket bucket_width timestamptz timezone truncate timestamptz by the specified interval bucket_width bucket starts and ends are calculated using timezone timezone is a varchar and defaults to utc time_bucket interval 2 days timestamptz 1992-04-20 15 26 00-07 europe berlin 1992-04-19 15 00 00-07 there are also dedicated extraction functions to get the subfields icu timestamp table functions the table below shows the available table functions for timestamp with time zone types function description example -- --- --- generate_series timestamptz timestamptz interval generate a table of timestamps in the closed range including both the starting timestamp and the ending timestamp stepping by the interval generate_series timestamptz 2001-04-10 timestamptz 2001-04-11 interval 30 minute range timestamptz timestamptz interval generate a table of timestamps in the half open range including the starting timestamp but stopping before the ending timestamp stepping by the interval range timestamptz 2001-04-10 timestamptz 2001-04-11 interval 30 minute infinite values are not allowed as table function bounds icu timestamp without time zone functions the table below shows the icu provided scalar functions that operate on plain timestamp values these functions assume that the timestamp is a local timestamp a local timestamp is effectively a way of encoding the part values from a time zone into a single value they should be used with caution because the produced values can contain gaps and ambiguities thanks to daylight savings time often the same functionality can be implemented more reliably using the struct variant of the date_part function function description example result -- -- --- -- current_localtime returns a time whose gmt bin values correspond to local time in the current time zone current_localtime 08 47 56 497 current_localtimestamp returns a timestamp whose gmt bin values correspond to local date and time in the current time zone current_localtimestamp 2022-12-17 08 47 56 497 localtime synonym for the current_localtime function call localtime 08 47 56 497 localtimestamp synonym for the current_localtimestamp function call localtimestamp 2022-12-17 08 47 56 497 timezone text timestamp use the date parts of the timestamp in gmt to construct a timestamp in the given time zone effectively the argument is a local time timezone america denver timestamp 2001-02-16 20 38 40 2001-02-16 19 38 40-08 timezone text timestamptz use the date parts of the timestamp in the given time zone to construct a timestamp effectively the result is a local time timezone america denver timestamptz 2001-02-16 20 38 40-05 2001-02-16 18 38 40 at time zone the at time zone syntax is syntactic sugar for the two argument timezone function listed above timestamp 2001-02-16 20 38 40 at time zone america denver -- 2001-02-16 19 38 40-08 timestamp with time zone 2001-02-16 20 38 40-05 at time zone america denver -- 2001-02-16 18 38 40 infinities functions applied to infinite dates will either return the same infinite dates e g greatest or null e g date_part depending on what makes sense in general if the function needs to examine the parts of the infinite temporal value the result will be null calendars the icu extension also supports non-gregorian calendars if such a calendar is current then the display and binning operations will use that calendar",
			"category": "Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "This section describes functions and operators for examining and manipulating TIMESTAMP WITH TIME ZONE (or..."
		},
		{
			"title": "Transaction Management",
			"text": "duckdb supports acid database transactions transactions provide isolation i e changes made by a transaction are not visible from concurrent transactions until it is committed a transaction can also be aborted which discards any changes it made so far statements duckdb provides the following statements for transaction management starting a transaction to start a transaction run begin transaction committing a transaction you can commit a transaction to make it visible to other transactions and to write it to persistent storage if using duckdb in persistent mode to commit a transaction run commit if you are not in an active transaction the commit statement will fail rolling back a transaction you can abort a transaction this operation also known as rolling back will discard any changes the transaction made to the database to abort a transaction run rollback you can also use the abort command which has an identical behavior abort if you are not in an active transaction the rollback and abort statements will fail example we illustrate the use of transactions through a simple example create table person name varchar age bigint begin transaction insert into person values ada 52 commit begin transaction delete from person where name ada insert into person values bruce 39 rollback select from person the first transaction inserting ada was committed but the second deleting ada and inserting bob was aborted therefore the resulting table will only contain ada 52",
			"category": "Statements",
			"url": "/docs/sql/statements/transactions",
			"blurb": "DuckDB supports ACID database transactions . Transactions provide isolation, i.e., changes made by a transaction are..."
		},
		{
			"title": "Tuning Workloads",
			"text": "parallelism multi-core processing the effect of row groups on parallelism duckdb parallelizes the workload based on row groups i e groups of rows that are stored together at the storage level a row group in duckdb s database format consists of max 122 880 rows parallelism starts at the level of row groups therefore for a query to run on k threads it needs to scan at least k 122 880 rows too many threads note that in certain cases duckdb may launch too many threads e g due to hyperthreading which can lead to slowdowns in these cases it s worth manually limiting the number of threads using set threads x larger-than-memory workloads out-of-core processing a key strength of duckdb is support for larger-than-memory workloads i e it is able to process data sets that are larger than the available system memory also known as out-of-core processing it can also run queries where the intermediate results cannot fit into memory this section explains the prerequisites scope and known limitations of larger-than-memory processing in duckdb spilling to disk larger-than-memory workloads are supported by spilling to disk if duckdb is connected to a persistent database file duckdb will create a temporary directory named database_file_name tmp when the available memory is no longer sufficient to continue processing if duckdb is running in in-memory mode it cannot use disk to offload data if it does not fit into main memory to enable offloading in the absence of a persistent database file use the set temp_directory statement set temp_directory path to temp_dir tmp operators some operators cannot output a single row until the last row of their input has been seen these are called blocking operators as they require their entire input to be buffered and are the most memory-instensive operators in relational database systems the main blocking operators are the following sorting order by grouping group by windowing over partition by order by joining join duckdb supports larger-than-memory processing for all of these operators limitations duckdb strives to always complete workloads even if they are larger-than-memory that said there are some limitations at the moment if multiple blocking operators appear in the same query duckdb may still throw an out-of-memory exception due to the complex interplay of these operators some aggregate functions such as list and string_agg do not support offloading to disk aggregate functions that use sorting are holistic i e they need all inputs before the aggregation can start as duckdb cannot yet offload some complex intermediate aggregate states to disk these functions can cause an out-of-memory exception when run on large data sets the pivot operation internally uses the list function therefore it is subject to the same limitation profiling if your queries are not performing as well as expected it s worth studying their query plans use explain to print the physical query plan without running the query use explain analyze to run and profile the query this will show the cpu time that each step in the query takes note that due to multi-threading adding up the individual times will be larger than the total query processing time query plans can point to the root of performance issues a few general directions avoid nested loop joins in favor or hash joins a scan that does not include a filter pushdown for a filter condition that is later applied performs unnecessary io try rewriting the query to apply a pushdown bad join orders where the cardinality of an operator explodes to billions of tuples should be avoided at all costs prepared statements prepared statements can improve performance when running the same query many times but with different parameters when a statement is prepared it completes several of the initial portions of the query execution process parsing planning etc and caches their output when it is executed those steps can be skipped improving performance this is beneficial mostly for repeatedly running small queries with a runtime of 100ms with different sets of parameters note that it is not a primary design goal for duckdb to quickly execute many small queries concurrently rather it is optimized for running larger less frequent queries querying remote files duckdb uses synchronous io when reading remote files this means that each duckdb thread can make at most one http request at a time if a query must make many small requests over the network increasing duckdb s threads setting to larger than the total number of cpu cores approx 2-5 times cpu cores can improve parallelism and performance best practices for using connections duckdb will perform best when reusing the same database connection many times disconnecting and reconnecting on every query will incur some overhead which can reduce performance when running many small queries duckdb also caches some data and metadata in memory and that cache is lost when the last open connection is closed frequently a single connection will work best but a connection pool may also be used using multiple connections can parallelize some operations although it is typically not necessary duckdb does attempt to parallelize as much as possible within each individual query but it is not possible to parallelize in all cases making multiple connections can process more operations concurrently this can be more helpful if duckdb is not cpu limited but instead bottlenecked by another resource like network transfer speed the preserve_insertion_order option when importing or exporting data sets that are much larger than the available memory out of memory errors may occur in these cases it s worth setting the preserve_insertion_order configuration option to false set preserve_insertion_order false this allows the systems to re-order any results that do not contain order by clauses potentially reducing memory usage",
			"category": "Performance",
			"url": "/docs/guides/performance/how_to_tune_workloads",
			"blurb": "Parallelism (Multi-Core Processing) The Effect of Row Groups on Parallelism DuckDB parallelizes the workload based on..."
		},
		{
			"title": "Typecasting",
			"text": "typecasting is an operation that converts a value in one particular data type to the closest corresponding value in another data type like other sql engines duckdb supports both implicit and explicit typecasting the following matrix describes which conversions are supported when implicit casting is allowed it implies that explicit casting is also possible typecasting matrix even though a casting operation is supported based on the source and target data type it does not necessarily mean the cast operation will succeeed at runtime casting operations that result in loss of precision are typically allowed for example it is possible to cast a numeric type with fractional digits like decimal float or double to an integral type like integer select cast pi as integer casting operations that would result in a value overflow are typically not allowed for example the value 999 is too large to be represented by the tinyint data type therefore an attempt to cast that value to that type results in a runtime error select cast 999 as tinyint so even though the cast operation from integer to tinyint is supported it is not possible for this particular value the varchar type also available under the aliases text and string acts like an univeral target any arbitrary value of any arbitrary type can always be cast to the varchar type casting from varchar to another data type is generally supported but may fail at runtime if duckdb cannot figure out how to parse and convert the provided value to the target data type",
			"category": "Data Types",
			"url": "/docs/sql/data_types/typecasting",
			"blurb": "Typecasting is an operation that converts a value in one particular data type to the closest corresponding value in..."
		},
		{
			"title": "Types API",
			"text": "the duckdbpytype class represents a type instance of our data types converting from other types to make the api as easy to use as possible we have added implicit conversions from existing type objects to a duckdbpytype instance this means that wherever a duckdbpytype object is expected it is also possible to provide any of the options listed below python built-ins the table below shows the mapping of python built-in types to duckdb type built-in types duckdb type ---------------------------------------------- ------------------- bool boolean bytearray blob bytes blob float double int bigint str varchar numpy dtypes the table below shows the mapping of numpy dtype to duckdb type type duckdb type ---------------------------------------------- ------------------- bool boolean float32 float float64 double int16 smallint int32 integer int64 bigint int8 tinyint uint16 usmallint uint32 uinteger uint64 ubigint uint8 utinyint nested types list child_type list type objects map to a list type of the child type which can also be arbitrarily nested import duckdb from typing import union duckdb typing duckdbpytype list dict union str int str map union u1 varchar u2 bigint varchar dict key_type value_type dict type objects map to a map type of the key type and the value type import duckdb duckdb typing duckdbpytype dict str int map varchar bigint a field_one b field_two n field_n dict objects map to a struct composed of the keys and values of the dict import duckdb duckdb typing duckdbpytype a str b int struct a varchar b bigint union type_one type_n typing union objects map to a union type of the provided types import duckdb from typing import union duckdb typing duckdbpytype union int str bool bytearray union u1 bigint u2 varchar u3 boolean u4 blob creation functions for the built-in types you can use the constants defined in duckdb typing duckdb type ------------------- bigint bit blob boolean date double float hugeint integer interval smallint sqlnull time_tz time timestamp_ms timestamp_ns timestamp_s timestamp_tz timestamp tinyint ubigint uhugeint uinteger usmallint utinyint uuid varchar for the complex types there are methods available on the duckdbpyconnection object or the duckdb module anywhere a duckdbpytype is accepted we will also accept one of the type objects that can implicitly convert to a duckdbpytype list_type array_type parameters child_type duckdbpytype struct_type row_type parameters fields union list duckdbpytype dict str duckdbpytype map_type parameters key_type duckdbpytype value_type duckdbpytype decimal_type parameters width int scale int union_type parameters members union list duckdbpytype dict str duckdbpytype string_type parameters collation optional str",
			"category": "Python",
			"url": "/docs/api/python/types",
			"blurb": "The DuckDBPyType class represents a type instance of our data types . Converting from Other Types To make the API as..."
		},
		{
			"title": "UNPIVOT Statement",
			"text": "the unpivot statement allows multiple columns to be stacked into fewer columns in the basic case multiple columns are stacked into two columns a name column which contains the name of the source column and a value column which contains the value from the source column duckdb implements both the sql standard unpivot syntax and a simplified unpivot syntax both can utilize a columns expression to automatically detect the columns to unpivot pivot_longer may also be used in place of the unpivot keyword the pivot statement is the inverse of the unpivot statement simplified unpivot syntax the full syntax diagram is below but the simplified unpivot syntax can be summarized using spreadsheet pivot table naming conventions as unpivot dataset on column s into name name-column-name value value-column-name s order by column s -with-order-direction s limit number-of-rows example data all examples use the dataset produced by the queries below create or replace table monthly_sales empid int dept text jan int feb int mar int apr int may int jun int insert into monthly_sales values 1 electronics 1 2 3 4 5 6 2 clothes 10 20 30 40 50 60 3 cars 100 200 300 400 500 600 from monthly_sales empid dept jan feb mar apr may jun ------- ------------- ----- ----- ----- ----- ----- ----- 1 electronics 1 2 3 4 5 6 2 clothes 10 20 30 40 50 60 3 cars 100 200 300 400 500 600 unpivot manually the most typical unpivot transformation is to take already pivoted data and re-stack it into a column each for the name and value in this case all months will be stacked into a month column and a sales column unpivot monthly_sales on jan feb mar apr may jun into name month value sales empid dept month sales ------- ------------- ------- ------- 1 electronics jan 1 1 electronics feb 2 1 electronics mar 3 1 electronics apr 4 1 electronics may 5 1 electronics jun 6 2 clothes jan 10 2 clothes feb 20 2 clothes mar 30 2 clothes apr 40 2 clothes may 50 2 clothes jun 60 3 cars jan 100 3 cars feb 200 3 cars mar 300 3 cars apr 400 3 cars may 500 3 cars jun 600 unpivot dynamically using columns expression in many cases the number of columns to unpivot is not easy to predetermine ahead of time in the case of this dataset the query above would have to change each time a new month is added the columns expression can be used to select all columns that are not empid or dept this enables dynamic unpivoting that will work regardless of how many months are added the query below returns identical results to the one above unpivot monthly_sales on columns exclude empid dept into name month value sales empid dept month sales ------- ------------- ------- ------- 1 electronics jan 1 1 electronics feb 2 1 electronics mar 3 1 electronics apr 4 1 electronics may 5 1 electronics jun 6 2 clothes jan 10 2 clothes feb 20 2 clothes mar 30 2 clothes apr 40 2 clothes may 50 2 clothes jun 60 3 cars jan 100 3 cars feb 200 3 cars mar 300 3 cars apr 400 3 cars may 500 3 cars jun 600 unpivot into multiple value columns the unpivot statement has additional flexibility more than 2 destination columns are supported this can be useful when the goal is to reduce the extent to which a dataset is pivoted but not completely stack all pivoted columns to demonstrate this the query below will generate a dataset with a separate column for the number of each month within the quarter month 1 2 or 3 and a separate row for each quarter since there are fewer quarters than months this does make the dataset longer but not as long as the above to accomplish this multiple sets of columns are included in the on clause the q1 and q2 aliases are optional the number of columns in each set of columns in the on clause must match the number of columns in the value clause unpivot monthly_sales on jan feb mar as q1 apr may jun as q2 into name quarter value month_1_sales month_2_sales month_3_sales empid dept quarter month_1_sales month_2_sales month_3_sales ------- ------------- --------- --------------- --------------- --------------- 1 electronics q1 1 2 3 1 electronics q2 4 5 6 2 clothes q1 10 20 30 2 clothes q2 40 50 60 3 cars q1 100 200 300 3 cars q2 400 500 600 using unpivot within a select statement the unpivot statement may be included within a select statement as a cte a common table expression or with clause or a subquery this allows for an unpivot to be used alongside other sql logic as well as for multiple unpivot s to be used in one query no select is needed within the cte the unpivot keyword can be thought of as taking its place with unpivot_alias as unpivot monthly_sales on columns exclude empid dept into name month value sales select from unpivot_alias an unpivot may be used in a subquery and must be wrapped in parentheses note that this behavior is different than the sql standard unpivot as illustrated in subsequent examples select from unpivot monthly_sales on columns exclude empid dept into name month value sales unpivot_alias internals unpivoting is implemented entirely as rewrites into sql queries each unpivot is implemented as set of unnest functions operating on a list of the column names and a list of the column values if dynamically unpivoting the columns expression is evaluated first to calculate the column list for example unpivot monthly_sales on jan feb mar apr may jun into name month value sales is translated into select empid dept unnest jan feb mar apr may jun as month unnest jan feb mar apr may jun as sales from monthly_sales note the single quotes to build a list of text strings to populate month and the double quotes to pull the column values for use in sales this produces the same result as the initial example empid dept month sales ------- ------------- ------- ------- 1 electronics jan 1 1 electronics feb 2 1 electronics mar 3 1 electronics apr 4 1 electronics may 5 1 electronics jun 6 2 clothes jan 10 2 clothes feb 20 2 clothes mar 30 2 clothes apr 40 2 clothes may 50 2 clothes jun 60 3 cars jan 100 3 cars feb 200 3 cars mar 300 3 cars apr 400 3 cars may 500 3 cars jun 600 simplified unpivot full syntax diagram below is the full syntax diagram of the unpivot statement sql standard unpivot syntax the full syntax diagram is below but the sql standard unpivot syntax can be summarized as from dataset unpivot include nulls value-column-name s for name-column-name in column s note that only one column can be included in the name-column-name expression sql standard unpivot manually to complete the basic unpivot operation using the sql standard syntax only a few additions are needed from monthly_sales unpivot sales for month in jan feb mar apr may jun empid dept month sales ------- ------------- ------- ------- 1 electronics jan 1 1 electronics feb 2 1 electronics mar 3 1 electronics apr 4 1 electronics may 5 1 electronics jun 6 2 clothes jan 10 2 clothes feb 20 2 clothes mar 30 2 clothes apr 40 2 clothes may 50 2 clothes jun 60 3 cars jan 100 3 cars feb 200 3 cars mar 300 3 cars apr 400 3 cars may 500 3 cars jun 600 sql standard unpivot dynamically using the columns expression the columns expression can be used to determine the in list of columns dynamically this will continue to work even if additional month columns are added to the dataset it produces the same result as the query above from monthly_sales unpivot sales for month in columns exclude empid dept sql standard unpivot into multiple value columns the unpivot statement has additional flexibility more than 2 destination columns are supported this can be useful when the goal is to reduce the extent to which a dataset is pivoted but not completely stack all pivoted columns to demonstrate this the query below will generate a dataset with a separate column for the number of each month within the quarter month 1 2 or 3 and a separate row for each quarter since there are fewer quarters than months this does make the dataset longer but not as long as the above to accomplish this multiple columns are included in the value-column-name portion of the unpivot statement multiple sets of columns are included in the in clause the q1 and q2 aliases are optional the number of columns in each set of columns in the in clause must match the number of columns in the value-column-name portion from monthly_sales unpivot month_1_sales month_2_sales month_3_sales for quarter in jan feb mar as q1 apr may jun as q2 empid dept quarter month_1_sales month_2_sales month_3_sales ------- ------------- --------- --------------- --------------- --------------- 1 electronics q1 1 2 3 1 electronics q2 4 5 6 2 clothes q1 10 20 30 2 clothes q2 40 50 60 3 cars q1 100 200 300 3 cars q2 400 500 600 sql standard unpivot full syntax diagram below is the full syntax diagram of the sql standard version of the unpivot statement",
			"category": "Statements",
			"url": "/docs/sql/statements/unpivot",
			"blurb": "The UNPIVOT statement allows columns to be stacked into rows that indicate the prior column name and value."
		},
		{
			"title": "UPDATE Statement",
			"text": "the update statement modifies the values of rows in a table examples -- for every row where i is null set the value to 0 instead update tbl set i 0 where i is null -- set all values of i to 1 and all values of j to 2 update tbl set i 1 j 2 syntax update changes the values of the specified columns in all rows that satisfy the condition only the columns to be modified need be mentioned in the set clause columns not explicitly modified retain their previous values update from other table a table can be updated based upon values from another table this can be done by specifying a table in a from clause or using a sub-select statement both approaches have the benefit of completing the update operation in bulk for increased performance create or replace table original as select 1 as key original value as value union all select 2 as key original value 2 as value create or replace table new as select 1 as key new value as value union all select 2 as key new value 2 as value select from original key value ----- ------------------ 1 original value 2 original value 2 update original set value new value from new where original key new key -- or update original set value select new value from new where original key new key select from original key value ----- ------------- 1 new value 2 new value 2 update from same table the only difference between this case and the above is that a different table alias must be specified on both the target table and the source table in this example as true_original and as new are both required update original as true_original set value select new value a change as value from original as new where true_original key new key update using joins to select the rows to update update statements can use the from clause and express joins via the where clause for example create table city name varchar revenue bigint country_code varchar create table country code varchar name varchar insert into city values paris 700 fr lyon 200 fr brussels 400 be insert into country values fr france be belgium to increase the revenue of all cities in france join the city and the country tables and filter on the latter update city set revenue revenue 100 from country where city country_code country code and country name france select from city name revenue country_code varchar int64 varchar paris 800 fr lyon 300 fr brussels 400 be upsert insert or update see the insert documentation for details",
			"category": "Statements",
			"url": "/docs/sql/statements/update",
			"blurb": "The UPDATE statement modifies the values of rows in a table. Examples -- for every row where i is NULL, set the value..."
		},
		{
			"title": "USE Statement",
			"text": "the use statement selects a database and optional schema to use as the default examples --- sets the memory database as the default use memory --- sets the duck main database and schema as the default use duck main syntax the use statement sets a default database or database schema combination to use for future operations for instance tables created without providing a fully qualified table name will be created in the default database",
			"category": "Statements",
			"url": "/docs/sql/statements/use",
			"blurb": "The USE statement selects a database and optional schema to use as the default. Examples --- Sets the 'memory'..."
		},
		{
			"title": "Union Type",
			"text": "a union type not to be confused with the sql union operator is a nested type capable of holding one of multiple alternative values much like the union in c the main difference being that these union types are tagged unions and thus always carry a discriminator tag which signals which alternative it is currently holding even if the inner value itself is null union types are thus more similar to c 17 s std variant rust s enum or the sum type present in most functional languages union types must always have at least one member and while they can contain multiple members of the same type the tag names must be unique union types can have at most 256 members under the hood union types are implemented on top of struct types and simply keep the tag as the first entry union values can be created with the union_value tag expr function or by casting from a member type example -- create a table with a union column create table tbl1 u union num int str varchar -- any type can be implicitly cast to a union containing the type -- any union can also be implicitly cast to another union if -- the source union members are a subset of the targets -- note only if the cast is unambiguous -- more details in the union casts section below insert into tbl1 values 1 two union_value str three -- union use the member types varchar cast functions when casting to varchar select u from tbl1 -- returns -- 1 -- two -- three -- select all the str members select union_extract u str from tbl1 -- alternatively you can use dot syntax like with structs select u str from tbl1 -- returns -- null -- two -- three -- select the currently active tag from the union as an enum select union_tag u from tbl1 -- returns -- num -- str -- str union casts compared to other nested types union s allow a set of implicit casts to facilitate unintrusive and natural usage when working with their members as subtypes however these casts have been designed with two principles in mind to avoid ambiguity and to avoid casts that could lead to loss of information this prevents union s from being completely transparent while still allowing union types to have a supertype relationship with their members thus union types can t be implicitly cast to any of their member types in general since the information in the other members not matching the target type would be lost if you want to coerce a union into one of its members you should use the union_extract function explicitly instead the only exception to this is when casting a union to varchar in which case the members will all use their corresponding varchar casts since everything can be cast to varchar this is safe in a sense casting to unions a type can always be implicitly cast to a union if it can be implicitly cast to one of the union member types if there are multiple candidates the built in implicit casting priority rules determine the target type for example a float - union i int v varchar cast will always cast the float to the int member before varchar if the cast still is ambiguous i e there are multiple candidates with the same implicit casting priority an error is raised this usually happens when the union contains multiple members of the same type e g a float - union i int num int is always ambiguous so how do we disambiguate if we want to create a union with multiple members of the same type by using the union_value function which takes a keyword argument specifying the tag for example union_value num 2 int will create a union with a single member of type int with the tag num this can then be used to disambiguate in an explicit or implicit read on below union to union cast like cast union_value b 2 as union a int b int casting between unions union types can be cast between each other if the source type is a subset of the target type in other words all the tags in the source union must be present in the target union and all the types of the matching tags must be implicitly castable between source and target in essence this means that union types are covariant with respect to their members ok source target comments ---- ---------------------- ----------------------- ------------------------------------ union a a b b union a a b b c c union a a b b union a a b c if b can be implicitly cast to c union a a b b c c union a a b b union a a b b union a a b c if b can t be implicitly cast to c union a b d union a b c comparison and sorting since union types are implemented on top of struct types internally they can be used with all the comparison operators as well as in both where and having clauses with the same semantics as struct s the tag is always stored as the first struct entry which ensures that the union types are compared and ordered by tag first functions see nested functions",
			"category": "Data Types",
			"url": "/docs/sql/data_types/union",
			"blurb": "A UNION type (not to be confused with the SQL UNION operator ) is a nested type capable of holding one of multiple..."
		},
		{
			"title": "Unnesting",
			"text": "examples -- unnest a list generating 3 rows 1 2 3 select unnest 1 2 3 -- unnesting a struct generating two columns a b select unnest a 42 b 84 -- recursive unnest of a list of structs select unnest a 42 b 84 a 100 b null recursive true the unnest function is used to unnest lists or structs by one level the function can be used as a regular scalar function but only in the select clause invoking unnest with the recursive parameter will unnest lists and structs of multiple levels unnesting lists -- unnest a list generating 3 rows 1 2 3 select unnest 1 2 3 -- unnest a scalar list generating 3 rows 1 10 2 11 3 null select unnest 1 2 3 unnest 10 11 -- unnest a scalar list generating 3 rows 1 10 2 10 3 10 select unnest 1 2 3 10 -- unnest a list column generated from a subquery select unnest l 10 from values 1 2 3 4 5 tbl l -- empty result select unnest -- empty result select unnest null using unnest on a list will emit one tuple per entry in the list when unnest is combined with regular scalar expressions those expressions are repeated for every entry in the list when multiple lists are unnested in the same select clause the lists are unnested side-by-side if one list is longer than the other the shorter list will be padded with null values an empty list and a null list will both unnest to zero elements unnesting structs -- unnesting a struct generating two columns a b select unnest a 42 b 84 -- unnesting a struct generating two columns a b select unnest a 42 b x 84 unnest on a struct will emit one column per entry in the struct recursive unnest -- unnesting a list of lists recursively generating 5 rows 1 2 3 4 5 select unnest 1 2 3 4 5 recursive true -- unnesting a list of structs recursively generating two rows of two columns a b select unnest a 42 b 84 a 100 b null recursive true -- unnesting a struct generating two columns a b select unnest a 1 2 3 b 88 recursive true calling unnest with the recursive setting will fully unnest lists followed by fully unnesting structs this can be useful to fully flatten columns that contain lists within lists or lists of structs note that lists within structs are not unnested",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/unnest",
			"blurb": "Examples -- unnest a list, generating 3 rows (1, 2, 3) SELECT unnest([1, 2, 3]); -- unnesting a struct, generating..."
		},
		{
			"title": "Using fsspec Filesystems",
			"text": "duckdb support for fsspec filesystems allows querying data in filesystems that duckdb s httpfs extension does not support fsspec has a large number of inbuilt filesystems and there are also many external implementations this capability is only available in duckdb s python client because fsspec is a python library while the httpfs extension is available in many duckdb clients example the following is an example of using fsspec to query a file in google cloud storage instead of using their s3 inter-compatibility api firstly you must install duckdb and fsspec and a filesystem interface of your choice pip install duckdb fsspec gcsfs then you can register whichever filesystem you d like to query import duckdb from fsspec import filesystem this line will throw an exception if the appropriate filesystem interface is not installed duckdb register_filesystem filesystem gcs duckdb sql select from read_csv gcs bucket file csv these filesystems are not implemented in c hence their performance may not be comparable to the ones provided by the httpfs extension it is also worth noting that as they are third party libraries they may contain bugs that are beyond our control",
			"category": "Python",
			"url": "/docs/guides/python/filesystems",
			"blurb": "DuckDB support for fsspec filesystems allows querying data in filesystems that DuckDB's httpfs extension does not..."
		},
		{
			"title": "Utility Functions",
			"text": "utility functions the functions below are difficult to categorize into specific function types and are broadly useful function description example result -- -- --- -- alias column return the name of the column alias column1 column1 checkpoint database synchronize wal with file for optional database without interrupting transactions checkpoint my_db success boolean coalesce expr return the first expression that evaluates to a non- null value accepts 1 or more parameters each expression can be a column literal value function result or many others coalesce null null default_string default_string constant_or_null arg1 arg2 if arg2 is null return null otherwise return arg1 constant_or_null 42 null null count_if x returns 1 if x is true or a non-zero number count_if 42 1 current_catalog return the name of the currently active catalog default is memory current_catalog memory current_schema return the name of the currently active schema default is main current_schema main current_schemas boolean return list of schemas pass a parameter of true to include implicit schemas current_schemas true temp main pg_catalog current_setting setting_name return the current value of the configuration setting current_setting access_mode automatic currval sequence_name return the current value of the sequence note that nextval must be called at least once prior to calling currval currval my_sequence_name 1 error message throws the given error message error access_mode force_checkpoint database synchronize wal with file for optional database interrupting transactions force_checkpoint my_db success boolean gen_random_uuid alias of uuid return a random uuid similar to this eeccb8c5-9943-b2bb-bb5e-222f4e14b687 gen_random_uuid various hash value returns a ubigint with the hash of the value hash 2595805878642663834 icu_sort_key string collator surrogate key used to sort special characters according to the specific locale collator parameter is optional valid only when icu extension is installed icu_sort_key \u00f6 de 460145960106 ifnull expr other a two-argument version of coalesce ifnull null default_string default_string md5 string return an md5 one-way hash of the string md5 123 202cb962ac59075b964b07152d234b70 nextval sequence_name return the following value of the sequence nextval my_sequence_name 2 nullif a b return null if a b else return a equivalent to case when a b then null else a end nullif 1 1 2 null pg_typeof expression returns the lower case name of the data type of the result of the expression for postgresql compatibility pg_typeof abc varchar read_blob source returns the content from source a filename a list of filenames or a glob pattern as a blob see the read_blob guide for more details hello x0a read_text source returns the content from source a filename a list of filenames or a glob pattern as a varchar the file content is first validated to be valid utf-8 if read_text attempts to read a file with invalid utf-8 an error is thrown suggesting to use read_blob instead see the read_text guide for more details hello n repeat_row varargs num_rows returns a table with num_rows rows each containing the fields defined in varargs repeat_row 1 2 foo num_rows 3 3 rows of 1 2 foo sha256 value returns a varchar with the sha-256 hash of the value sha-256 d7a5c5e0d1d94c32218539e7e47d4ba9c3c7b77d61332fb60d633dde89e473fb stats expression returns a string with statistics about the expression expression can be a column constant or sql expression stats 5 min 5 max 5 has null false txid_current returns the current transaction s id a bigint it will assign a new one if the current transaction does not have one already txid_current various typeof expression returns the name of the data type of the result of the expression typeof abc varchar uuid return a random uuid similar to this eeccb8c5-9943-b2bb-bb5e-222f4e14b687 uuid various version return the currently active version of duckdb in this format version various utility table functions a table function is used in place of a table in a from clause function description example -- --- - glob search_path return filenames found at the location indicated by the search_path in a single column named file the search_path may contain glob pattern matching syntax glob",
			"category": "Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "Utility Functions The functions below are difficult to categorize into specific function types and are broadly..."
		},
		{
			"title": "VACUUM Statement",
			"text": "the vacuum statement alone does nothing and is at present provided for postgresql-compatibility the vacuum analyze statement recomputes table statistics if they have become stale due to table updates or deletions examples -- no-op vacuum -- rebuild database statistics vacuum analyze -- rebuild statistics for the table column vacuum analyze memory main my_table my_column -- not supported vacuum full -- error reclaiming space to reclaim space after deleting rows use the checkpoint statement syntax",
			"category": "Statements",
			"url": "/docs/sql/statements/vacuum",
			"blurb": "The VACUUM statement alone does nothing and is at present provided for PostgreSQL-compatibility. The VACUUM ANALYZE..."
		},
		{
			"title": "VALUES Clause",
			"text": "the values clause is used to specify a fixed number of rows the values clause can be used as a stand-alone statement as part of the from clause or as input to an insert into statement examples -- generate two rows and directly return them values amsterdam 1 london 2 -- generate two rows as part of a from clause and rename the columns select from values amsterdam 1 london 2 cities name id -- generate two rows and insert them into a table insert into cities values amsterdam 1 london 2 -- create a table directly from a values clause create table cities as select from values amsterdam 1 london 2 cities name id syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/values",
			"blurb": "The VALUES clause is used to specify a fixed number of rows. The VALUES clause can be used as a stand-alone..."
		},
		{
			"title": "Versioning of Extensions",
			"text": "currently an extension and the duckdb instance it is loaded into must have the exact same version",
			"category": "Extensions",
			"url": "/docs/extensions/versioning_of_extensions",
			"blurb": "Currently, an extension and the DuckDB instance it is loaded into must have the exact same version."
		},
		{
			"title": "WHERE Clause",
			"text": "the where clause specifies any filters to apply to the data this allows you to select only a subset of the data in which you are interested logically the where clause is applied immediately after the from clause examples -- select all rows that have id equal to 3 select from table_name where id 3 -- select all rows that match the given case-insensitive like expression select from table_name where name ilike mark -- select all rows that match the given composite expression select from table_name where id 3 or id 7 syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/where",
			"blurb": "The WHERE clause specifies any filters to apply to the data. This allows you to select only a subset of the data in..."
		},
		{
			"title": "WINDOW Clause",
			"text": "the window clause allows you to specify named windows that can be used within window functions these are useful when you have multiple window functions as they allow you to avoid repeating the same window clause syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/window",
			"blurb": "The WINDOW clause allows you to specify named windows that can be used within window functions . These are useful..."
		},
		{
			"title": "WITH Clause",
			"text": "the with clause allows you to specify common table expressions ctes regular non-recursive common-table-expressions are essentially views that are limited in scope to a particular query ctes can reference each-other and can be nested basic cte examples -- create a cte called cte and use it in the main query with cte as select 42 as x select from cte x 42 -- create two ctes where the second cte references the first cte with cte as select 42 as i cte2 as select i 100 as x from cte select from cte2 x 4200 materialized ctes by default ctes are inlined into the main query inlining can result in duplicate work because the definition is copied for each reference take this query for example with t x as q_t select from t as t1 t as t2 t as t3 inlining duplicates the definition of t for each reference which results in the following query select from q_t as t1 x q_t as t2 x q_t as t3 x if q_t is expensive materializing it with the materialized keyword can improve performance in this case q_t is evaluated only once with t x as materialized q_t select from t as t1 t as t2 t as t3 recursive ctes with recursive allows the definition of ctes which can refer to themselves note that the query must be formulated in a way that ensures termination otherwise it may run into an infinite loop tree traversal with recursive can be used to traverse trees for example take a hierarchy of tags create table tag id int name varchar subclassof int insert into tag values 1 u2 5 2 blur 5 3 oasis 5 4 2pac 6 5 rock 7 6 rap 7 7 music 9 8 movies 9 9 art null the following query returns the path from the node oasis to the root of the tree art with recursive tag_hierarchy id source path as select id name name as path from tag where subclassof is null union all select tag id tag name list_prepend tag name tag_hierarchy path from tag tag_hierarchy where tag subclassof tag_hierarchy id select path from tag_hierarchy where source oasis path oasis rock music art graph traversal the with recursive clause can be used to express graph traversal on arbitrary graphs however if the graph has cycles the query must perform cycle detection to prevent infinite loops one way to achieve this is to store the path of a traversal in a list and before extending the path with a new edge check whether its endpoint has been visited before see the example later take the following directed graph from the ldbc graphalytics benchmark create table edge node1id int node2id int insert into edge values 1 3 1 5 2 4 2 5 2 10 3 1 3 5 3 8 3 10 5 3 5 4 5 8 6 3 6 4 7 4 8 1 9 4 note that the graph contains directed cycles e g between nodes 1 2 and 5 enumerate all paths from a node the following query returns all paths starting in node 1 with recursive paths startnode endnode path as select -- define the path as the first edge of the traversal node1id as startnode node2id as endnode node1id node2id as path from edge where startnode 1 union all select -- concatenate new edge to the path paths startnode as startnode node2id as endnode array_append path node2id as path from paths join edge on paths endnode node1id -- prevent adding a repeated node to the path -- this ensures that no cycles occur where node2id all paths path select startnode endnode path from paths order by length path path startnode endnode path 1 3 1 3 1 5 1 5 1 5 1 3 5 1 8 1 3 8 1 10 1 3 10 1 3 1 5 3 1 4 1 5 4 1 8 1 5 8 1 4 1 3 5 4 1 8 1 3 5 8 1 8 1 5 3 8 1 10 1 5 3 10 note that the result of this query is not restricted to shortest paths e g for node 5 the results include paths 1 5 and 1 3 5 enumerate unweighted shortest paths from a node in most cases enumerating all paths is not practical or feasible instead only the unweighted shortest paths are of interest to find these the second half of the with recursive query should be adjusted such that it only includes a node if it has not yet been visited this is implemented by using a subquery that checks if any of the previous paths includes the node with recursive paths startnode endnode path as select -- define the path as the first edge of the traversal node1id as startnode node2id as endnode node1id node2id as path from edge where startnode 1 union all select -- concatenate new edge to the path paths startnode as startnode node2id as endnode array_append path node2id as path from paths join edge on paths endnode node1id -- prevent adding a node that was visited previously by any path -- this ensures that 1 no cycles occur and 2 only nodes that -- were not visited by previous shorter paths are added to a path where not exists select 1 from paths previous_paths where list_contains previous_paths path node2id select startnode endnode path from paths order by length path path startnode endnode path 1 3 1 3 1 5 1 5 1 8 1 3 8 1 10 1 3 10 1 4 1 5 4 1 8 1 5 8 enumerate unweighted shortest paths between two nodes with recursive can also be used to find all unweighted shortest paths between two nodes to ensure that the recursive query is stopped as soon as we reach the end node we use a window function which checks whether the end node is among the newly added nodes the following query returns all unweighted shortest paths between nodes 1 start node and 8 end node with recursive paths startnode endnode path endreached as select -- define the path as the first edge of the traversal node1id as startnode node2id as endnode node1id node2id as path node2id 8 as endreached from edge where startnode 1 union all select -- concatenate new edge to the path paths startnode as startnode node2id as endnode array_append path node2id as path max case when node2id 8 then 1 else 0 end over rows between unbounded preceding and unbounded following as endreached from paths join edge on paths endnode node1id where not exists select 1 from paths previous_paths where list_contains previous_paths path node2id and paths endreached 0 select startnode endnode path from paths where endnode 8 order by length path path startnode endnode path 1 8 1 3 8 1 8 1 5 8 common table expressions",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/with",
			"blurb": "The WITH clause allows you to specify common table expressions (CTEs). Regular (non-recursive) common-table-..."
		},
		{
			"title": "Window Functions",
			"text": "examples -- generate a row_number column containing incremental identifiers for each row select row_number over from sales -- generate a row_number column by order of time select row_number over order by time from sales -- generate a row_number column by order of time partitioned by region select row_number over partition by region order by time from sales -- compute the difference between the current amount and the previous amount by order of time select amount - lag amount over order by time from sales -- compute the percentage of the total amount of sales per region for each row select amount sum amount over partition by region from sales syntax window functions can only be used in the select clause to share over specifications between functions use the statement s window clause and use the over window-name syntax general-purpose window functions the table below shows the available general window functions function return type description example --- - --- -- cume_dist double the cumulative distribution number of partition rows preceding or peer with current row total partition rows cume_dist dense_rank bigint the rank of the current row without gaps this function counts peer groups dense_rank first_value expr any same type as expr returns expr evaluated at the row that is the first row of the window frame first_value column first expr any same type as expr alias for first_value first column lag expr any offset integer default any same type as expr returns expr evaluated at the row that is offset rows before the current row within the partition if there is no such row instead return default which must be of the same type as expr both offset and default are evaluated with respect to the current row if omitted offset defaults to 1 and default to null lag column 3 0 last_value expr any same type as expr returns expr evaluated at the row that is the last row of the window frame last_value column last expr any same type as expr alias for last_value last column lead expr any offset integer default any same type as expr returns expr evaluated at the row that is offset rows after the current row within the partition if there is no such row instead return default which must be of the same type as expr both offset and default are evaluated with respect to the current row if omitted offset defaults to 1 and default to null lead column 3 0 nth_value expr any nth integer same type as expr returns expr evaluated at the nth row of the window frame counting from 1 null if no such row nth_value column 2 ntile num_buckets integer bigint an integer ranging from 1 to the argument value dividing the partition as equally as possible ntile 4 percent_rank double the relative rank of the current row rank - 1 total partition rows - 1 percent_rank rank_dense bigint alias for dense_rank rank_dense rank bigint the rank of the current row with gaps same as row_number of its first peer rank row_number bigint the number of the current row within the partition counting from 1 row_number aggregate window functions all aggregate functions can be used in a windowing context ignoring nulls the following functions support the ignore nulls specification function description example --- - --- -- lag expr any offset integer default any skips null values when counting lag column 3 ignore nulls lead expr any offset integer default any skips null values when counting lead column 3 ignore nulls first_value expr any skips leading null s first_value column ignore nulls last_value expr any skips trailing null s last_value column ignore nulls nth_value expr any nth integer skips null values when counting nth_value column 2 ignore nulls note that there is no comma separating the arguments from the ignore nulls specification the inverse of ignore nulls is respect nulls which is the default for all functions evaluation windowing works by breaking a relation up into independent partitions ordering those partitions and then computing a new column for each row as a function of the nearby values some window functions depend only on the partition boundary and the ordering but a few including all the aggregates also use a frame frames are specified as a number of rows on either side preceding or following of the current row the distance can either be specified as a number of rows or a range of values using the partition s ordering value and a distance the full syntax is shown in the diagram at the top of the page and this diagram visually illustrates computation environment partition and ordering partitioning breaks the relation up into independent unrelated pieces partitioning is optional and if none is specified then the entire relation is treated as a single partition window functions cannot access values outside of the partition containing the row they are being evaluated at ordering is also optional but without it the results are not well-defined each partition is ordered using the same ordering clause here is a table of power generation data available as a csv file power-plant-generation-history csv to load the data run create table generation history as from power-plant-generation-history csv after partitioning by plant and ordering by date it will have this layout plant date mwh --- --- --- boston 2019-01-02 564337 boston 2019-01-03 507405 boston 2019-01-04 528523 boston 2019-01-05 469538 boston 2019-01-06 474163 boston 2019-01-07 507213 boston 2019-01-08 613040 boston 2019-01-09 582588 boston 2019-01-10 499506 boston 2019-01-11 482014 boston 2019-01-12 486134 boston 2019-01-13 531518 worcester 2019-01-02 118860 worcester 2019-01-03 101977 worcester 2019-01-04 106054 worcester 2019-01-05 92182 worcester 2019-01-06 94492 worcester 2019-01-07 99932 worcester 2019-01-08 118854 worcester 2019-01-09 113506 worcester 2019-01-10 96644 worcester 2019-01-11 93806 worcester 2019-01-12 98963 worcester 2019-01-13 107170 in what follows we shall use this table or small sections of it to illustrate various pieces of window function evaluation the simplest window function is row_number this function just computes the 1-based row number within the partition using the query select plant date row_number over partition by plant order by date as row from generation history order by 1 2 the result will be the following plant date row --- --- --- boston 2019-01-02 1 boston 2019-01-03 2 boston 2019-01-04 3 worcester 2019-01-02 1 worcester 2019-01-03 2 worcester 2019-01-04 3 note that even though the function is computed with an order by clause the result does not have to be sorted so the select also needs to be explicitly sorted if that is desired framing framing specifies a set of rows relative to each row where the function is evaluated the distance from the current row is given as an expression either preceding or following the current row this distance can either be specified as an integral number of rows or as a range delta expression from the value of the ordering expression for a range specification there must be only one ordering expression and it has to support addition and subtraction i e numbers or interval s the default values for frames are from unbounded preceding to current row it is invalid for a frame to start after it ends using the exclude clause rows around the current row can be excluded from the frame row framing here is a simple row frame query using an aggregate function select points sum points over rows between 1 preceding and 1 following we from results this query computes the sum of each point and the points on either side of it notice that at the edge of the partition there are only two values added together this is because frames are cropped to the edge of the partition range framing returning to the power data suppose the data is noisy we might want to compute a 7 day moving average for each plant to smooth out the noise to do this we can use this window query select plant date avg mwh over partition by plant order by date asc range between interval 3 days preceding and interval 3 days following as mwh 7-day moving average from generation history order by 1 2 this query partitions the data by plant to keep the different power plants data separate orders each plant s partition by date to put the energy measurements next to each other and uses a range frame of three days on either side of each day for the avg to handle any missing days this is the result plant date mwh 7-day moving average --- --- --- boston 2019-01-02 517450 75 boston 2019-01-03 508793 20 boston 2019-01-04 508529 83 boston 2019-01-13 499793 00 worcester 2019-01-02 104768 25 worcester 2019-01-03 102713 00 worcester 2019-01-04 102249 50 exclude clause the exclude clause allows rows around the current row to be excluded from the frame it has the following options exclude no others exclude nothing default exclude current row exclude the current row from the window frame exclude group exclude the current row and all its peers according to the columns specified by order by from the window frame exclude ties exclude only the current row s peers from the window frame window clauses multiple different over clauses can be specified in the same select and each will be computed separately often however we want to use the same layout for multiple window functions the window clause can be used to define a named window that can be shared between multiple window functions select plant date min mwh over seven as mwh 7-day moving minimum avg mwh over seven as mwh 7-day moving average max mwh over seven as mwh 7-day moving maximum from generation history window seven as partition by plant order by date asc range between interval 3 days preceding and interval 3 days following order by 1 2 the three window functions will also share the data layout which will improve performance multiple windows can be defined in the same window clause by comma-separating them select plant date min mwh over seven as mwh 7-day moving minimum avg mwh over seven as mwh 7-day moving average max mwh over seven as mwh 7-day moving maximum min mwh over three as mwh 3-day moving minimum avg mwh over three as mwh 3-day moving average max mwh over three as mwh 3-day moving maximum from generation history window seven as partition by plant order by date asc range between interval 3 days preceding and interval 3 days following three as partition by plant order by date asc range between interval 1 days preceding and interval 1 days following order by 1 2 the queries above do not use a number of clauses commonly found in select statements like where group by etc for more complex queries you can find where window clauses fall in the canonical order of the select statement filtering the results of window functions using qualify window functions are executed after the where and having clauses have been already evaluated so it s not possible to use these clauses to filter the results of window functions the qualify clause avoids the need for a subquery or with clause to perform this filtering box and whisker queries all aggregates can be used as windowing functions including the complex statistical functions these function implementations have been optimised for windowing and we can use the window syntax to write queries that generate the data for moving box-and-whisker plots select plant date min mwh over seven as mwh 7-day moving minimum quantile_cont mwh 0 25 0 5 0 75 over seven as mwh 7-day moving iqr max mwh over seven as mwh 7-day moving maximum from generation history window seven as partition by plant order by date asc range between interval 3 days preceding and interval 3 days following order by 1 2",
			"category": "SQL",
			"url": "/docs/sql/window_functions",
			"blurb": "Examples -- generate a row_number column containing incremental identifiers for each row SELECT row_number() OVER ()..."
		},
		{
			"title": "Working with Extensions",
			"text": "downloading extensions directly from s3 downloading an extension directly could be helpful when building a lambda service or container that uses duckdb duckdb extensions are stored in public s3 buckets but the directory structure of those buckets is not searchable as a result a direct url to the file must be used to directly download an extension file use the following format http extensions duckdb org v duckdb_version platform_name extension_name duckdb_extension gz for example http extensions duckdb org v site currentduckdbversion windows_amd64 json duckdb_extension gz platforms extension binaries must be built for each platform we distribute pre-built binaries for several platforms see below for platforms where packages for certain extensions are not available users can build them from source and install the resulting binaries manually all official extensions are distributed for the following platforms linux_amd64 linux_amd64_gcc4 linux_arm64 osx_amd64 osx_arm64 windows_amd64 only core extensions are distributed for the following platforms windows_amd64_rtools wasm_eh and wasm_mvp see duckdb-wasm s extensions we currently do not distribute binaries for extensions on the linux_arm64_gcc4 platform using a custom extension repository to load extensions from a custom extension repository set the following configuration option local files set custom_extension_repository path to folder this assumes the pointed folder has a structure similar to folder 0fd6fb9198 osx_arm64 autocomplete duckdb_extension httpfs duckdb_extension icu duckdb_extension gz inet duckdb_extension json duckdb_extension parquet duckdb_extension tpcds duckdb_extension tpcds duckdb_extension gz tpch duckdb_extension gz with at the first level the duckdb version at the second the duckdb platform and then extensions either as name duckdb_extension or gzip-compressed files name duckdb_extension gz install icu for example will look for either icu duckdb_extension gz first or icu duckdb_extension second in the repository structure and install it to the extension_directory that defaults to duckdb extensions if file is compressed decompression will be handled at this step remote file over http set custom_extension_repository http nightly-extensions duckdb org they work the same as local ones and expect a similar folder structure remote files over https or s3 protocol set custom_extension_repository s3 bucket your-repository-name remote extension repositories act similarly to local ones as in the file structure should be the same and either gzipped or non-gzipped file are supported only special case here is that httpfs extension should be available locally you can get it for example doing reset custom_extension_repository install httpfs that will install the official httpfs extension locally this is since httpfs extension will be needed to actually access remote encrypted files install x from y you can also use the install command s from clause to specify the path of the custom extension repository for example force install azure from http nightly-extensions duckdb org this will force install the azure extension from the specified url loading and installing an extension from explicit paths installing extensions from an explicit path install can be used with the path to either a duckdb_extension file or a duckdb_extension gz file for example if the file was available into the same directory as where duckdb is being executed you can install it as follows -- uncompressed file install path to httpfs duckdb_extension -- gzip-compressed file install path to httpfs duckdb_extension gz these will have the same results it is also possible to specify remote paths force installing extensions when duckdb installs an extension it is copied to a local directory to be cached avoiding any network traffic any subsequent calls to install extension_name will use the local version instead of downloading the extension again to force re-downloading the extension run by default in duckdb extensions but configurable via set extension_directory path to existing directory force install extension_name loading extension from a path load can be used with the path to a duckdb_extension for example if the file was available at the relative path path to httpfs duckdb_extension you can load it as follows -- uncompressed file load path to httpfs duckdb_extension this will skip any currently installed file in the specifed path using remote paths for compressed files is currently not possible building extensions build the extension following the extension s readme statically linking extensions to statically link extensions follow the developer documentation s using extension config files section",
			"category": "Extensions",
			"url": "/docs/extensions/working_with_extensions",
			"blurb": "Downloading Extensions Directly from S3 Downloading an extension directly could be helpful when building a lambda..."
		},
		{
			"title": "`",
			"text": "` - `",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "` - `"
		},
		{
			"title": "abs",
			"text": "abs(x) - absolute value",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "abs(x) - Absolute value"
		},
		{
			"title": "acos",
			"text": "acos(x) - computes the arccosine of x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "acos(x) - Computes the arccosine of x"
		},
		{
			"title": "age",
			"text": "age(timestamptz) - subtract from current_date",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "age(timestamptz) - Subtract from current_date"
		},
		{
			"title": "array_extract",
			"text": "array_extract(list, index) - extract a single character using a (1-based) index.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "array_extract(list, index) - Extract a single character using a (1-based) index."
		},
		{
			"title": "array_pop_back",
			"text": "array_pop_back(list) -",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "array_pop_back(list) -"
		},
		{
			"title": "array_pop_front",
			"text": "array_pop_front(list) -",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "array_pop_front(list) -"
		},
		{
			"title": "array_slice",
			"text": "array_slice(list, begin, end) - extract a string using slice conventions. negative values are accepted.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "array_slice(list, begin, end) - Extract a string using slice conventions. Negative values are accepted."
		},
		{
			"title": "ascii",
			"text": "ascii(string) - returns an integer that represents the unicode code point of the first character of the string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "ascii(string) - Returns an integer that represents the Unicode code point of the first character of the string"
		},
		{
			"title": "asin",
			"text": "asin(x) - computes the arcsine of x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "asin(x) - Computes the arcsine of x"
		},
		{
			"title": "atan",
			"text": "atan(x) - computes the arctangent of x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "atan(x) - Computes the arctangent of x"
		},
		{
			"title": "atan2",
			"text": "atan2(y, x) - computes the arctangent (y, x)",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "atan2(y, x) - Computes the arctangent (y, x)"
		},
		{
			"title": "bar",
			"text": "bar(x, min, max [, width ]) - draw a band whose width is proportional to ( x - min ) and equal to width characters when x = max. width defaults to 80.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "bar(x, min, max [, width ]) - Draw a band whose width is proportional to ( x - min ) and equal to width characters..."
		},
		{
			"title": "bit_count",
			"text": "bit_count(x) - returns the number of bits that are set",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "bit_count(x) - Returns the number of bits that are set"
		},
		{
			"title": "bit_length",
			"text": "bit_length(string) - number of bits in a string.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "bit_length(string) - Number of bits in a string."
		},
		{
			"title": "bit_position",
			"text": "bit_position(substring, bitstring) - returns first starting index of the specified substring within bits, or zero if it's not present. the first (leftmost) bit is indexed 1",
			"category": "Bitstring Functions",
			"url": "/docs/sql/functions/bitstring",
			"blurb": "bit_position(substring, bitstring) - Returns first starting index of the specified substring within bits, or zero if..."
		},
		{
			"title": "bitstring",
			"text": "bitstring(bitstring, length) - returns a bitstring of determined length.",
			"category": "Bitstring Functions",
			"url": "/docs/sql/functions/bitstring",
			"blurb": "bitstring(bitstring, length) - Returns a bitstring of determined length."
		},
		{
			"title": "cardinality",
			"text": "cardinality(map) - return the size of the map (or the number of entries in the map).",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "cardinality(map) - Return the size of the map (or the number of entries in the map)."
		},
		{
			"title": "cbrt",
			"text": "cbrt(x) - returns the cube root of the number",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "cbrt(x) - Returns the cube root of the number"
		},
		{
			"title": "ceil",
			"text": "ceil(x) - rounds the number up",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "ceil(x) - Rounds the number up"
		},
		{
			"title": "century",
			"text": "century(timestamp) - extracts the century of a timestamp",
			"category": "Timestamp Functions",
			"url": "/docs/sql/functions/timestamp",
			"blurb": "century(timestamp) - Extracts the century of a timestamp"
		},
		{
			"title": "checkpoint",
			"text": "checkpoint(database) - synchronize wal with file for (optional) database without interrupting transactions.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "checkpoint(database) - Synchronize WAL with file for (optional) database without interrupting transactions."
		},
		{
			"title": "chr",
			"text": "chr(x) - returns a character which is corresponding the ascii code value or unicode code point",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "chr(x) - Returns a character which is corresponding the ASCII code value or Unicode code point"
		},
		{
			"title": "coalesce",
			"text": "coalesce(expr, ...) - return the first expression that evaluates to a non- null value. accepts 1 or more parameters. each expression can be a column, literal value, function result, or many others.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "coalesce(expr, ...) - Return the first expression that evaluates to a non- NULL value. Accepts 1 or more parameters...."
		},
		{
			"title": "concat",
			"text": "concat(string, ...) - concatenate many strings together",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "concat(string, ...) - Concatenate many strings together"
		},
		{
			"title": "concat_ws",
			"text": "concat_ws(separator, string, ...) - concatenate strings together separated by the specified separator",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "concat_ws(separator, string, ...) - Concatenate strings together separated by the specified separator"
		},
		{
			"title": "constant_or_null",
			"text": "constant_or_null(arg1, arg2) - if arg2 is null , return null. otherwise, return arg1.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "constant_or_null(arg1, arg2) - If arg2 is NULL , return NULL. Otherwise, return arg1."
		},
		{
			"title": "contains",
			"text": "contains(string, search_string) - return true if search_string is found within string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "contains(string, search_string) - Return true if search_string is found within string"
		},
		{
			"title": "cos",
			"text": "cos(x) - computes the cosine of x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "cos(x) - Computes the cosine of x"
		},
		{
			"title": "cot",
			"text": "cot(x) - computes the cotangent of x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "cot(x) - Computes the cotangent of x"
		},
		{
			"title": "count_if",
			"text": "count_if(x) - returns 1 if x is true or a non-zero number",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "count_if(x) - Returns 1 if x is true or a non-zero number"
		},
		{
			"title": "current_catalog",
			"text": "current_catalog() - return the name of the currently active catalog. default is memory.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "current_catalog() - Return the name of the currently active catalog. Default is memory."
		},
		{
			"title": "current_date",
			"text": "current_date - current date (at start of current transaction)",
			"category": "Date Functions",
			"url": "/docs/sql/functions/date",
			"blurb": "current_date - Current date (at start of current transaction)"
		},
		{
			"title": "current_localtime",
			"text": "current_localtime() - returns a time whose gmt bin values correspond to local time in the current time zone.",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "current_localtime() - Returns a TIME whose GMT bin values correspond to local time in the current time zone."
		},
		{
			"title": "current_localtimestamp",
			"text": "current_localtimestamp() - returns a timestamp whose gmt bin values correspond to local date and time in the current time zone.",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "current_localtimestamp() - Returns a TIMESTAMP whose GMT bin values correspond to local date and time in the current..."
		},
		{
			"title": "current_schema",
			"text": "current_schema() - return the name of the currently active schema. default is main.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "current_schema() - Return the name of the currently active schema. Default is main."
		},
		{
			"title": "current_schemas",
			"text": "current_schemas(boolean) - return list of schemas. pass a parameter of true to include implicit schemas.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "current_schemas(boolean) - Return list of schemas. Pass a parameter of true to include implicit schemas."
		},
		{
			"title": "current_setting",
			"text": "current_setting(setting_name) - return the current value of the configuration setting",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "current_setting(setting_name) - Return the current value of the configuration setting"
		},
		{
			"title": "current_time / get_current_time",
			"text": "current_time / get_current_time() - current time (start of current transaction)",
			"category": "Time Functions",
			"url": "/docs/sql/functions/time",
			"blurb": "current_time / get_current_time() - Current time (start of current transaction)"
		},
		{
			"title": "current_timestamp",
			"text": "current_timestamp - current date and time (start of current transaction)",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "current_timestamp - Current date and time (start of current transaction)"
		},
		{
			"title": "currval",
			"text": "currval(sequence_name) - return the current value of the sequence. note that nextval must be called at least once prior to calling currval.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "currval(sequence_name) - Return the current value of the sequence. Note that nextval must be called at least once..."
		},
		{
			"title": "damerau_levenshtein",
			"text": "damerau_levenshtein(s1, s2) - extension of levenshtein distance to also include transposition of adjacent characters as an allowed edit operation. in other words, the minimum number of edit operations (insertions, deletions, substitutions or transpositions) required to change one string to another. different case is considered different.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "damerau_levenshtein(s1, s2) - Extension of Levenshtein distance to also include transposition of adjacent characters..."
		},
		{
			"title": "date_add",
			"text": "date_add(date, interval) - add the interval to the date",
			"category": "Date Functions",
			"url": "/docs/sql/functions/date",
			"blurb": "date_add(date, interval) - Add the interval to the date"
		},
		{
			"title": "date_diff",
			"text": "date_diff(part, startdate, enddate) - the number of partition boundaries between the timestamps",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "date_diff(part, startdate, enddate) - The number of partition boundaries between the timestamps"
		},
		{
			"title": "date_part",
			"text": "date_part(part, timestamptz) - get subfield (equivalent to extract )",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "date_part(part, timestamptz) - Get subfield (equivalent to extract )"
		},
		{
			"title": "date_sub",
			"text": "date_sub(part, startdate, enddate) - the number of complete partitions between the timestamps",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "date_sub(part, startdate, enddate) - The number of complete partitions between the timestamps"
		},
		{
			"title": "date_trunc",
			"text": "date_trunc(part, timestamptz) - truncate to specified precision",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "date_trunc(part, timestamptz) - Truncate to specified precision"
		},
		{
			"title": "day",
			"text": "day(date) - day",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "day(date) - Day"
		},
		{
			"title": "dayname",
			"text": "dayname(timestamp) - the (english) name of the weekday",
			"category": "Timestamp Functions",
			"url": "/docs/sql/functions/timestamp",
			"blurb": "dayname(timestamp) - The (English) name of the weekday"
		},
		{
			"title": "dayofmonth",
			"text": "dayofmonth(date) - day (synonym)",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "dayofmonth(date) - Day (synonym)"
		},
		{
			"title": "dayofweek",
			"text": "dayofweek(date) - numeric weekday (sunday = 0, saturday = 6)",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "dayofweek(date) - Numeric weekday (Sunday = 0, Saturday = 6)"
		},
		{
			"title": "dayofyear",
			"text": "dayofyear(date) - day of the year (starts from 1, i.e., january 1 = 1)",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "dayofyear(date) - Day of the year (starts from 1, i.e., January 1 = 1)"
		},
		{
			"title": "decade",
			"text": "decade(date) - decade (year / 10)",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "decade(date) - Decade (year / 10)"
		},
		{
			"title": "decode",
			"text": "decode(blob) - convert blob to varchar. fails if blob is not valid utf-8.",
			"category": "Blob Functions",
			"url": "/docs/sql/functions/blob",
			"blurb": "decode(blob) - Convert blob to varchar. Fails if blob is not valid utf-8."
		},
		{
			"title": "degrees",
			"text": "degrees(x) - converts radians to degrees",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "degrees(x) - Converts radians to degrees"
		},
		{
			"title": "element_at",
			"text": "element_at(map, key) - return a list containing the value for a given key or an empty list if the key is not contained in the map. the type of the key provided in the second parameter must match the type of the map's keys else an error is returned.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "element_at(map, key) - Return a list containing the value for a given key or an empty list if the key is not..."
		},
		{
			"title": "encode",
			"text": "encode(string) - convert varchar to blob. converts utf-8 characters into literal encoding.",
			"category": "Blob Functions",
			"url": "/docs/sql/functions/blob",
			"blurb": "encode(string) - Convert varchar to blob. Converts utf-8 characters into literal encoding."
		},
		{
			"title": "ends_with",
			"text": "ends_with(string, search_string) - return true if string ends with search_string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "ends_with(string, search_string) - Return true if string ends with search_string"
		},
		{
			"title": "enum_code",
			"text": "enum_code(enum_value) - returns the numeric value backing the given enum value",
			"category": "Enum Functions",
			"url": "/docs/sql/functions/enum",
			"blurb": "enum_code(enum_value) - Returns the numeric value backing the given enum value"
		},
		{
			"title": "enum_first",
			"text": "enum_first(enum) - returns the first value of the input enum type.",
			"category": "Enum Functions",
			"url": "/docs/sql/functions/enum",
			"blurb": "enum_first(enum) - Returns the first value of the input enum type."
		},
		{
			"title": "enum_last",
			"text": "enum_last(enum) - returns the last value of the input enum type.",
			"category": "Enum Functions",
			"url": "/docs/sql/functions/enum",
			"blurb": "enum_last(enum) - Returns the last value of the input enum type."
		},
		{
			"title": "enum_range",
			"text": "enum_range(enum) - returns all values of the input enum type as an array.",
			"category": "Enum Functions",
			"url": "/docs/sql/functions/enum",
			"blurb": "enum_range(enum) - Returns all values of the input enum type as an array."
		},
		{
			"title": "enum_range_boundary",
			"text": "enum_range_boundary(enum, enum) - returns the range between the two given enum values as an array. the values must be of the same enum type. when the first parameter is null , the result starts with the first value of the enum type. when the second parameter is null , the result ends with the last value of the enum type.",
			"category": "Enum Functions",
			"url": "/docs/sql/functions/enum",
			"blurb": "enum_range_boundary(enum, enum) - Returns the range between the two given enum values as an array. The values must be..."
		},
		{
			"title": "epoch",
			"text": "epoch(timestamp) - converts a timestamp to seconds since the epoch",
			"category": "Timestamp Functions",
			"url": "/docs/sql/functions/timestamp",
			"blurb": "epoch(timestamp) - Converts a timestamp to seconds since the epoch"
		},
		{
			"title": "epoch_ms",
			"text": "epoch_ms(timestamptz) - converts a timestamptz to milliseconds since the epoch",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "epoch_ms(timestamptz) - Converts a timestamptz to milliseconds since the epoch"
		},
		{
			"title": "epoch_ns",
			"text": "epoch_ns(timestamptz) - converts a timestamptz to nanoseconds since the epoch",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "epoch_ns(timestamptz) - Converts a timestamptz to nanoseconds since the epoch"
		},
		{
			"title": "epoch_us",
			"text": "epoch_us(timestamptz) - converts a timestamptz to microseconds since the epoch",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "epoch_us(timestamptz) - Converts a timestamptz to microseconds since the epoch"
		},
		{
			"title": "era",
			"text": "era(date) - calendar era",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "era(date) - Calendar era"
		},
		{
			"title": "error",
			"text": "error(message) - throws the given error message",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "error(message) - Throws the given error message"
		},
		{
			"title": "even",
			"text": "even(x) - round to next even number by rounding away from zero",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "even(x) - Round to next even number by rounding away from zero"
		},
		{
			"title": "exp",
			"text": "exp(x) - computes e ** x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "exp(x) - Computes e ** x"
		},
		{
			"title": "extract",
			"text": "extract(field from timestamptz) - get subfield from a timestamp with time zone",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "extract(field from timestamptz) - Get subfield from a timestamp with time zone"
		},
		{
			"title": "factorial",
			"text": "factorial(x) - see ! operator. computes the product of the current integer and all integers below it",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "factorial(x) - See ! operator. Computes the product of the current integer and all integers below it"
		},
		{
			"title": "fdiv",
			"text": "fdiv(x, y) - performs integer division ( x // y ) but returns a double value",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "fdiv(x, y) - Performs integer division ( x // y ) but returns a DOUBLE value"
		},
		{
			"title": "flatten",
			"text": "flatten(list_of_lists) -",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "flatten(list_of_lists) -"
		},
		{
			"title": "floor",
			"text": "floor(x) - rounds the number down",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "floor(x) - Rounds the number down"
		},
		{
			"title": "fmod",
			"text": "fmod(x, y) - calculates the modulo value. always returns a double value",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "fmod(x, y) - Calculates the modulo value. Always returns a DOUBLE value"
		},
		{
			"title": "force_checkpoint",
			"text": "force_checkpoint(database) - synchronize wal with file for (optional) database interrupting transactions.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "force_checkpoint(database) - Synchronize WAL with file for (optional) database interrupting transactions."
		},
		{
			"title": "format",
			"text": "format(format, parameters ...) - formats a string using the fmt syntax",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "format(format, parameters ...) - Formats a string using the fmt syntax"
		},
		{
			"title": "format_bytes",
			"text": "format_bytes(bytes) - converts bytes to a human-readable representation using units based on powers of 2 (kib, mib, gib, etc.).",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "format_bytes(bytes) - Converts bytes to a human-readable representation using units based on powers of 2 (KiB, MiB,..."
		},
		{
			"title": "from_base64",
			"text": "from_base64(string) - convert a base64 encoded string to a character string.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "from_base64(string) - Convert a base64 encoded string to a character string."
		},
		{
			"title": "gamma",
			"text": "gamma(x) - interpolation of (x-1) factorial (so decimal inputs are allowed)",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "gamma(x) - Interpolation of (x-1) factorial (so decimal inputs are allowed)"
		},
		{
			"title": "gcd",
			"text": "gcd(x, y) - computes the greatest common divisor of x and y",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "gcd(x, y) - Computes the greatest common divisor of x and y"
		},
		{
			"title": "get_bit",
			"text": "get_bit(bitstring, index) - extracts the nth bit from bitstring; the first (leftmost) bit is indexed 0.",
			"category": "Bitstring Functions",
			"url": "/docs/sql/functions/bitstring",
			"blurb": "get_bit(bitstring, index) - Extracts the nth bit from bitstring; the first (leftmost) bit is indexed 0."
		},
		{
			"title": "get_current_timestamp",
			"text": "get_current_timestamp() - current date and time (start of current transaction)",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "get_current_timestamp() - Current date and time (start of current transaction)"
		},
		{
			"title": "greatest",
			"text": "greatest(timestamptz, timestamptz) - the later of two timestamps",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "greatest(timestamptz, timestamptz) - The later of two timestamps"
		},
		{
			"title": "greatest_common_divisor",
			"text": "greatest_common_divisor(x, y) - computes the greatest common divisor of x and y",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "greatest_common_divisor(x, y) - Computes the greatest common divisor of x and y"
		},
		{
			"title": "hash",
			"text": "hash(value) - returns a ubigint with the hash of the value",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "hash(value) - Returns a UBIGINT with the hash of the value"
		},
		{
			"title": "hour",
			"text": "hour(date) - hours",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "hour(date) - Hours"
		},
		{
			"title": "httpfs Extension for HTTP and S3 Support",
			"text": "the httpfs extension is an autoloadable extension implementing a file system that allows reading remote writing remote files for plain http s only file reading is supported for object storage using the s3 api the httpfs extension supports reading writing globbing files installation and loading the httpfs extension will be by default autoloaded on first use of any functionality exposed by this extension to manually install and load the httpfs extension run install httpfs load httpfs http s the httpfs extension supports connecting to http s endpoints s3 api the httpfs extension supports connecting to s3 api endpoints github the httpfs extension is part of the main duckdb repository pages in this section",
			"category": "Httpfs",
			"url": "/docs/extensions/httpfs/overview",
			"blurb": "The httpfs extension is an autoloadable extension implementing a file system that allows reading remote/writing..."
		},
		{
			"title": "icu_sort_key",
			"text": "icu_sort_key(string, collator) - surrogate key used to sort special characters according to the specific locale. collator parameter is optional. valid only when icu extension is installed.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "icu_sort_key(string, collator) - Surrogate key used to sort special characters according to the specific locale...."
		},
		{
			"title": "ifnull",
			"text": "ifnull(expr, other) - a two-argument version of coalesce",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "ifnull(expr, other) - A two-argument version of coalesce"
		},
		{
			"title": "ilike_escape",
			"text": "ilike_escape(string, like_specifier, escape_character) - returns true if the string matches the like_specifier (see pattern matching ) using case-insensitive matching. escape_character is used to search for wildcard characters in the string.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "ilike_escape(string, like_specifier, escape_character) - Returns true if the string matches the like_specifier (see..."
		},
		{
			"title": "inet Extension",
			"text": "the inet extension defines the inet data type for storing ipv4 network addresses it supports the cidr notation for subnet masks e g 198 51 100 0 22 installing and loading to install and load the inet extension run install inet load inet examples select 127 0 0 1 inet as addr addr inet 127 0 0 1 create table tbl id integer ip inet insert into tbl values 1 192 168 0 0 16 2 127 0 0 1 2 8 8 8 8 id ip int32 inet 1 192 168 0 0 16 2 127 0 0 1 2 8 8 8 8 github the inet extension is part of the main duckdb repository",
			"category": "Extensions",
			"url": "/docs/extensions/inet",
			"blurb": "The inet extension defines the INET data type for storing IPv4 network addresses . It supports the CIDR notation for..."
		},
		{
			"title": "instr",
			"text": "instr(string, search_string) - return location of first occurrence of search_string in string , counting from 1. returns 0 if no match found.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "instr(string, search_string) - Return location of first occurrence of search_string in string , counting from 1...."
		},
		{
			"title": "isfinite",
			"text": "isfinite(timestamptz) - returns true if the timestamp with time zone is finite, false otherwise",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "isfinite(timestamptz) - Returns true if the timestamp with time zone is finite, false otherwise"
		},
		{
			"title": "isinf",
			"text": "isinf(timestamptz) - returns true if the timestamp with time zone is infinite, false otherwise",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "isinf(timestamptz) - Returns true if the timestamp with time zone is infinite, false otherwise"
		},
		{
			"title": "isnan",
			"text": "isnan(x) - returns true if the floating point value is not a number, false otherwise",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "isnan(x) - Returns true if the floating point value is not a number, false otherwise"
		},
		{
			"title": "isodow",
			"text": "isodow(date) - numeric iso weekday (monday = 1, sunday = 7)",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "isodow(date) - Numeric ISO weekday (Monday = 1, Sunday = 7)"
		},
		{
			"title": "isoyear",
			"text": "isoyear(date) - iso year number (starts on monday of week containing jan 4th)",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "isoyear(date) - ISO Year number (Starts on Monday of week containing Jan 4th)"
		},
		{
			"title": "jaccard",
			"text": "jaccard(s1, s2) - the jaccard similarity between two strings. different case is considered different. returns a number between 0 and 1.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "jaccard(s1, s2) - The Jaccard similarity between two strings. Different case is considered different. Returns a..."
		},
		{
			"title": "jaro_similarity",
			"text": "jaro_similarity(s1, s2) - the jaro similarity between two strings. different case is considered different. returns a number between 0 and 1.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "jaro_similarity(s1, s2) - The Jaro similarity between two strings. Different case is considered different. Returns a..."
		},
		{
			"title": "jaro_winkler_similarity",
			"text": "jaro_winkler_similarity(s1, s2) - the jaro-winkler similarity between two strings. different case is considered different. returns a number between 0 and 1.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "jaro_winkler_similarity(s1, s2) - The Jaro-Winkler similarity between two strings. Different case is considered..."
		},
		{
			"title": "jemalloc Extension",
			"text": "the jemalloc extension replaces the system s memory allocator with jemalloc unlike other duckdb extensions the jemalloc extension is statically linked and cannot be installed or loaded during runtime availability the linux and macos versions of duckdb ship with the jemalloc extension by default on windows this extension is not available github the jemalloc extension is part of the main duckdb repository",
			"category": "Extensions",
			"url": "/docs/extensions/jemalloc",
			"blurb": "The jemalloc extension replaces the system's memory allocator with jemalloc . Unlike other DuckDB extensions, the..."
		},
		{
			"title": "last_day",
			"text": "last_day(timestamptz) - the last day of the month.",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "last_day(timestamptz) - The last day of the month."
		},
		{
			"title": "lcm",
			"text": "lcm(x, y) - computes the least common multiple of x and y",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "lcm(x, y) - Computes the least common multiple of x and y"
		},
		{
			"title": "least",
			"text": "least(timestamptz, timestamptz) - the earlier of two timestamps",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "least(timestamptz, timestamptz) - The earlier of two timestamps"
		},
		{
			"title": "least_common_multiple",
			"text": "least_common_multiple(x, y) - computes the least common multiple of x and y",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "least_common_multiple(x, y) - Computes the least common multiple of x and y"
		},
		{
			"title": "left",
			"text": "left(string, count) - extract the left-most count characters",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "left(string, count) - Extract the left-most count characters"
		},
		{
			"title": "left_grapheme",
			"text": "left_grapheme(string, count) - extract the left-most grapheme clusters",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "left_grapheme(string, count) - Extract the left-most grapheme clusters"
		},
		{
			"title": "len",
			"text": "len(list) - array_length",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "len(list) - array_length"
		},
		{
			"title": "length",
			"text": "length(string) - number of characters in string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "length(string) - Number of characters in string"
		},
		{
			"title": "length_grapheme",
			"text": "length_grapheme(string) - number of grapheme clusters in string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "length_grapheme(string) - Number of grapheme clusters in string"
		},
		{
			"title": "levenshtein",
			"text": "levenshtein(s1, s2) - the minimum number of single-character edits (insertions, deletions or substitutions) required to change one string to the other. different case is considered different.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "levenshtein(s1, s2) - The minimum number of single-character edits (insertions, deletions or substitutions) required..."
		},
		{
			"title": "lgamma",
			"text": "lgamma(x) - computes the log of the gamma function",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "lgamma(x) - Computes the log of the gamma function"
		},
		{
			"title": "like_escape",
			"text": "like_escape(string, like_specifier, escape_character) - returns true if the string matches the like_specifier (see pattern matching ) using case-sensitive matching. escape_character is used to search for wildcard characters in the string.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "like_escape(string, like_specifier, escape_character) - Returns true if the string matches the like_specifier (see..."
		},
		{
			"title": "list [ begin : end : step ]",
			"text": "list [ begin : end : step ] -",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list [ begin : end : step ] -"
		},
		{
			"title": "list [ begin : end ]",
			"text": "list [ begin : end ] -",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list [ begin : end ] -"
		},
		{
			"title": "list [ index ]",
			"text": "list [ index ] -",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list [ index ] -"
		},
		{
			"title": "list_aggregate",
			"text": "list_aggregate(list, name) - list_aggr , aggregate , array_aggregate , array_aggr",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_aggregate(list, name) - list_aggr , aggregate , array_aggregate , array_aggr"
		},
		{
			"title": "list_any_value",
			"text": "list_any_value(list) -",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_any_value(list) -"
		},
		{
			"title": "list_append",
			"text": "list_append(list, element) - array_append , array_push_back",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_append(list, element) - array_append , array_push_back"
		},
		{
			"title": "list_concat",
			"text": "list_concat(list1, list2) - list_cat , array_concat , array_cat",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_concat(list1, list2) - list_cat , array_concat , array_cat"
		},
		{
			"title": "list_contains",
			"text": "list_contains(list, element) - list_has , array_contains , array_has",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_contains(list, element) - list_has , array_contains , array_has"
		},
		{
			"title": "list_cosine_similarity",
			"text": "list_cosine_similarity(list1, list2) -",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_cosine_similarity(list1, list2) -"
		},
		{
			"title": "list_distance",
			"text": "list_distance(list1, list2) -",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_distance(list1, list2) -"
		},
		{
			"title": "list_distinct",
			"text": "list_distinct(list) - array_distinct",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_distinct(list) - array_distinct"
		},
		{
			"title": "list_dot_product",
			"text": "list_dot_product(list1, list2) - list_inner_product",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_dot_product(list1, list2) - list_inner_product"
		},
		{
			"title": "list_extract",
			"text": "list_extract(list, index) - list_element , array_extract",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_extract(list, index) - list_element , array_extract"
		},
		{
			"title": "list_filter",
			"text": "list_filter(list, lambda) - array_filter , filter",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_filter(list, lambda) - array_filter , filter"
		},
		{
			"title": "list_grade_up",
			"text": "list_grade_up(list) - array_grade_up",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_grade_up(list) - array_grade_up"
		},
		{
			"title": "list_has_all",
			"text": "list_has_all(list, sub-list) - array_has_all",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_has_all(list, sub-list) - array_has_all"
		},
		{
			"title": "list_has_any",
			"text": "list_has_any(list1, list2) - array_has_any",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_has_any(list1, list2) - array_has_any"
		},
		{
			"title": "list_intersect",
			"text": "list_intersect(list1, list2) - array_intersect",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_intersect(list1, list2) - array_intersect"
		},
		{
			"title": "list_position",
			"text": "list_position(list, element) - list_indexof , array_position , array_indexof",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_position(list, element) - list_indexof , array_position , array_indexof"
		},
		{
			"title": "list_prepend",
			"text": "list_prepend(element, list) - array_prepend , array_push_front",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_prepend(element, list) - array_prepend , array_push_front"
		},
		{
			"title": "list_reduce",
			"text": "list_reduce(list, lambda) - array_reduce , reduce",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_reduce(list, lambda) - array_reduce , reduce"
		},
		{
			"title": "list_resize",
			"text": "list_resize(list, size [, value ]) - array_resize",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_resize(list, size [, value ]) - array_resize"
		},
		{
			"title": "list_reverse",
			"text": "list_reverse(list) - array_reverse",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_reverse(list) - array_reverse"
		},
		{
			"title": "list_reverse_sort",
			"text": "list_reverse_sort(list) - array_reverse_sort",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_reverse_sort(list) - array_reverse_sort"
		},
		{
			"title": "list_select",
			"text": "list_select(value_list, index_list) - array_select",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_select(value_list, index_list) - array_select"
		},
		{
			"title": "list_slice",
			"text": "list_slice(list, begin, end) - array_slice",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_slice(list, begin, end) - array_slice"
		},
		{
			"title": "list_sort",
			"text": "list_sort(list) - array_sort",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_sort(list) - array_sort"
		},
		{
			"title": "list_transform",
			"text": "list_transform(list, lambda) - array_transform , apply , list_apply , array_apply",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_transform(list, lambda) - array_transform , apply , list_apply , array_apply"
		},
		{
			"title": "list_unique",
			"text": "list_unique(list) - array_unique",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_unique(list) - array_unique"
		},
		{
			"title": "list_value",
			"text": "list_value(any, ...) - list_pack",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_value(any, ...) - list_pack"
		},
		{
			"title": "list_where",
			"text": "list_where(value_list, mask_list) - array_where",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_where(value_list, mask_list) - array_where"
		},
		{
			"title": "list_zip",
			"text": "list_zip(list1, list2, ...) - array_zip",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_zip(list1, list2, ...) - array_zip"
		},
		{
			"title": "ln",
			"text": "ln(x) - computes the natural logarithm of x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "ln(x) - Computes the natural logarithm of x"
		},
		{
			"title": "localtime",
			"text": "localtime - synonym for the current_localtime() function call.",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "localtime - Synonym for the current_localtime() function call."
		},
		{
			"title": "localtimestamp",
			"text": "localtimestamp - synonym for the current_localtimestamp() function call.",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "localtimestamp - Synonym for the current_localtimestamp() function call."
		},
		{
			"title": "log",
			"text": "log(x) - computes the 10-log of x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "log(x) - Computes the 10-log of x"
		},
		{
			"title": "log2",
			"text": "log2(x) - computes the 2-log of x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "log2(x) - Computes the 2-log of x"
		},
		{
			"title": "lower",
			"text": "lower(string) - convert string to lower case",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "lower(string) - Convert string to lower case"
		},
		{
			"title": "lpad",
			"text": "lpad(string, count, character) - pads the string with the character from the left until it has count characters",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "lpad(string, count, character) - Pads the string with the character from the left until it has count characters"
		},
		{
			"title": "ltrim",
			"text": "ltrim(string) - removes any spaces from the left side of the string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "ltrim(string) - Removes any spaces from the left side of the string"
		},
		{
			"title": "make_date",
			"text": "make_date(bigint, bigint, bigint) - the date for the given parts",
			"category": "Date Functions",
			"url": "/docs/sql/functions/date",
			"blurb": "make_date(bigint, bigint, bigint) - The date for the given parts"
		},
		{
			"title": "make_time",
			"text": "make_time(bigint, bigint, double) - the time for the given parts",
			"category": "Time Functions",
			"url": "/docs/sql/functions/time",
			"blurb": "make_time(bigint, bigint, double) - The time for the given parts"
		},
		{
			"title": "make_timestamp",
			"text": "make_timestamp(microseconds) - the timestamp for the given number of \u00b5s since the epoch",
			"category": "Timestamp Functions",
			"url": "/docs/sql/functions/timestamp",
			"blurb": "make_timestamp(microseconds) - The timestamp for the given number of \u00b5s since the epoch"
		},
		{
			"title": "make_timestamptz",
			"text": "make_timestamptz(microseconds) - the timestamp with time zone for the given \u00b5s since the epoch",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "make_timestamptz(microseconds) - The timestamp with time zone for the given \u00b5s since the epoch"
		},
		{
			"title": "map",
			"text": "map() - returns an empty map.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "map() - Returns an empty map."
		},
		{
			"title": "map_entries",
			"text": "map_entries(map) - return a list of struct(k, v) for each key-value pair in the map.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "map_entries(map) - Return a list of struct(k, v) for each key-value pair in the map."
		},
		{
			"title": "map_from_entries[])",
			"text": "map_from_entries(struct(k, v)[]) - returns a map created from the entries of the array",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "map_from_entries(STRUCT(k, v)[]) - Returns a map created from the entries of the array"
		},
		{
			"title": "map_keys",
			"text": "map_keys(map) - return a list of all keys in the map.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "map_keys(map) - Return a list of all keys in the map."
		},
		{
			"title": "map_values",
			"text": "map_values(map) - return a list of all values in the map.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "map_values(map) - Return a list of all values in the map."
		},
		{
			"title": "md5",
			"text": "md5(string) - return an md5 one-way hash of the string.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "md5(string) - Return an md5 one-way hash of the string."
		},
		{
			"title": "microsecond",
			"text": "microsecond(date) - sub-minute microseconds",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "microsecond(date) - Sub-minute microseconds"
		},
		{
			"title": "microseconds",
			"text": "microseconds - sub-minute microseconds",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "microseconds - Sub-minute microseconds"
		},
		{
			"title": "millennium",
			"text": "millennium(date) - millennium",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "millennium(date) - Millennium"
		},
		{
			"title": "millisecond",
			"text": "millisecond(date) - sub-minute milliseconds",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "millisecond(date) - Sub-minute milliseconds"
		},
		{
			"title": "milliseconds",
			"text": "milliseconds - sub-minute milliseconds",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "milliseconds - Sub-minute milliseconds"
		},
		{
			"title": "minute",
			"text": "minute(date) - minutes",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "minute(date) - Minutes"
		},
		{
			"title": "month",
			"text": "month(date) - month",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "month(date) - Month"
		},
		{
			"title": "monthname",
			"text": "monthname(timestamp) - the (english) name of the month.",
			"category": "Timestamp Functions",
			"url": "/docs/sql/functions/timestamp",
			"blurb": "monthname(timestamp) - The (English) name of the month."
		},
		{
			"title": "nextafter",
			"text": "nextafter(x, y) - return the next floating point value after x in the direction of y",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "nextafter(x, y) - Return the next floating point value after x in the direction of y"
		},
		{
			"title": "nextval",
			"text": "nextval(sequence_name) - return the following value of the sequence.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "nextval(sequence_name) - Return the following value of the sequence."
		},
		{
			"title": "nfc_normalize",
			"text": "nfc_normalize(string) - convert string to unicode nfc normalized string. useful for comparisons and ordering if text data is mixed between nfc normalized and not.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "nfc_normalize(string) - Convert string to Unicode NFC normalized string. Useful for comparisons and ordering if text..."
		},
		{
			"title": "not_ilike_escape",
			"text": "not_ilike_escape(string, like_specifier, escape_character) - returns false if the string matches the like_specifier (see pattern matching ) using case-sensitive matching. escape_character is used to search for wildcard characters in the string.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "not_ilike_escape(string, like_specifier, escape_character) - Returns false if the string matches the like_specifier..."
		},
		{
			"title": "not_like_escape",
			"text": "not_like_escape(string, like_specifier, escape_character) - returns false if the string matches the like_specifier (see pattern matching ) using case-insensitive matching. escape_character is used to search for wildcard characters in the string.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "not_like_escape(string, like_specifier, escape_character) - Returns false if the string matches the like_specifier..."
		},
		{
			"title": "now",
			"text": "now() - current date and time (start of current transaction)",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "now() - Current date and time (start of current transaction)"
		},
		{
			"title": "nullif",
			"text": "nullif(a, b) - return null if a = b, else return a. equivalent to case when a = b then null else a end.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "nullif(a, b) - Return null if a = b, else return a. Equivalent to CASE WHEN a = b THEN NULL ELSE a END."
		},
		{
			"title": "octet_length",
			"text": "octet_length(blob) - number of bytes in blob",
			"category": "Blob Functions",
			"url": "/docs/sql/functions/blob",
			"blurb": "octet_length(blob) - Number of bytes in blob"
		},
		{
			"title": "ord",
			"text": "ord(string) - return ascii character code of the leftmost character in a string.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "ord(string) - Return ASCII character code of the leftmost character in a string."
		},
		{
			"title": "parse_dirname",
			"text": "parse_dirname(path, separator) - returns the top-level directory name from the given path. separator options: system , both_slash (default), forward_slash , backslash.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "parse_dirname(path, separator) - Returns the top-level directory name from the given path. separator options: system..."
		},
		{
			"title": "parse_dirpath",
			"text": "parse_dirpath(path, separator) - returns the head of the path (the pathname until the last slash) similarly to python's os.path.dirname function. separator options: system , both_slash (default), forward_slash , backslash.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "parse_dirpath(path, separator) - Returns the head of the path (the pathname until the last slash) similarly to..."
		},
		{
			"title": "parse_filename",
			"text": "parse_filename(path, trim_extension, separator) - returns the last component of the path similarly to python's os.path.basename function. if trim_extension is true, the file extension will be removed (defaults to false ). separator options: system , both_slash (default), forward_slash , backslash.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "parse_filename(path, trim_extension, separator) - Returns the last component of the path similarly to Python's..."
		},
		{
			"title": "parse_path",
			"text": "parse_path(path, separator) - returns a list of the components (directories and filename) in the path similarly to python's pathlib.parts function. separator options: system , both_slash (default), forward_slash , backslash.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "parse_path(path, separator) - Returns a list of the components (directories and filename) in the path similarly to..."
		},
		{
			"title": "pg_typeof",
			"text": "pg_typeof(expression) - returns the lower case name of the data type of the result of the expression. for postgresql compatibility.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "pg_typeof(expression) - Returns the lower case name of the data type of the result of the expression. For PostgreSQL..."
		},
		{
			"title": "pi",
			"text": "pi() - returns the value of pi",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "pi() - Returns the value of pi"
		},
		{
			"title": "position",
			"text": "position(search_string in string) - return location of first occurrence of search_string in string , counting from 1. returns 0 if no match found.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "position(search_string in string) - Return location of first occurrence of search_string in string , counting from 1...."
		},
		{
			"title": "pow",
			"text": "pow(x, y) - computes x to the power of y",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "pow(x, y) - Computes x to the power of y"
		},
		{
			"title": "printf",
			"text": "printf(format, parameters ...) - formats a string using printf syntax",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "printf(format, parameters ...) - Formats a string using printf syntax"
		},
		{
			"title": "quarter",
			"text": "quarter(date) - quarter",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "quarter(date) - Quarter"
		},
		{
			"title": "radians",
			"text": "radians(x) - converts degrees to radians",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "radians(x) - Converts degrees to radians"
		},
		{
			"title": "random",
			"text": "random() - returns a random number between 0 and 1",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "random() - Returns a random number between 0 and 1"
		},
		{
			"title": "regexp_escape",
			"text": "regexp_escape(string) - escapes special patterns to turn string into a regular expression similarly to python's re.escape function",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "regexp_escape(string) - Escapes special patterns to turn string into a regular expression similarly to Python's..."
		},
		{
			"title": "regexp_extract ;",
			"text": "regexp_extract(string, pattern [, idx ]) ; - if string contains the regexp pattern , returns the capturing group specified by optional parameter idx",
			"category": "Patternmatching Functions",
			"url": "/docs/sql/functions/patternmatching",
			"blurb": "regexp_extract(string, pattern [, idx ]) ; - If string contains the regexp pattern , returns the capturing group..."
		},
		{
			"title": "regexp_extract_all",
			"text": "regexp_extract_all(string, regex [, group = 0]) - split the string along the regex and extract all occurrences of group",
			"category": "Patternmatching Functions",
			"url": "/docs/sql/functions/patternmatching",
			"blurb": "regexp_extract_all(string, regex [, group = 0]) - Split the string along the regex and extract all occurrences of group"
		},
		{
			"title": "regexp_full_match",
			"text": "regexp_full_match(string, regex) - returns true if the entire string matches the regex",
			"category": "Patternmatching Functions",
			"url": "/docs/sql/functions/patternmatching",
			"blurb": "regexp_full_match(string, regex) - Returns true if the entire string matches the regex"
		},
		{
			"title": "regexp_matches",
			"text": "regexp_matches(string, pattern) - returns true if string contains the regexp pattern , false otherwise",
			"category": "Patternmatching Functions",
			"url": "/docs/sql/functions/patternmatching",
			"blurb": "regexp_matches(string, pattern) - Returns true if string contains the regexp pattern , false otherwise"
		},
		{
			"title": "regexp_replace ;",
			"text": "regexp_replace(string, pattern, replacement) ; - if string contains the regexp pattern , replaces the matching part with replacement",
			"category": "Patternmatching Functions",
			"url": "/docs/sql/functions/patternmatching",
			"blurb": "regexp_replace(string, pattern, replacement) ; - If string contains the regexp pattern , replaces the matching part..."
		},
		{
			"title": "regexp_split_to_array",
			"text": "regexp_split_to_array(string, regex) - splits the string along the regex",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "regexp_split_to_array(string, regex) - Splits the string along the regex"
		},
		{
			"title": "repeat",
			"text": "repeat(string, count) - repeats the string count number of times",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "repeat(string, count) - Repeats the string count number of times"
		},
		{
			"title": "repeat_row",
			"text": "repeat_row(varargs, num_rows) - returns a table with num_rows rows, each containing the fields defined in varargs",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "repeat_row(varargs, num_rows) - Returns a table with num_rows rows, each containing the fields defined in varargs"
		},
		{
			"title": "replace",
			"text": "replace(string, source, target) - replaces any occurrences of the source with target in string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "replace(string, source, target) - Replaces any occurrences of the source with target in string"
		},
		{
			"title": "reverse",
			"text": "reverse(string) - reverses the string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "reverse(string) - Reverses the string"
		},
		{
			"title": "right",
			"text": "right(string, count) - extract the right-most count characters",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "right(string, count) - Extract the right-most count characters"
		},
		{
			"title": "right_grapheme",
			"text": "right_grapheme(string, count) - extract the right-most count grapheme clusters",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "right_grapheme(string, count) - Extract the right-most count grapheme clusters"
		},
		{
			"title": "round",
			"text": "round(v numeric, s int) - round to s decimal places. values s < 0 are allowed",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "round(v NUMERIC, s INT) - Round to s decimal places. Values s < 0 are allowed"
		},
		{
			"title": "row",
			"text": "row(any, ...) - create an unnamed struct (tuple) containing the argument values.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "row(any, ...) - Create an unnamed STRUCT (tuple) containing the argument values."
		},
		{
			"title": "rpad",
			"text": "rpad(string, count, character) - pads the string with the character from the right until it has count characters",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "rpad(string, count, character) - Pads the string with the character from the right until it has count characters"
		},
		{
			"title": "rtrim",
			"text": "rtrim(string) - removes any spaces from the right side of the string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "rtrim(string) - Removes any spaces from the right side of the string"
		},
		{
			"title": "second",
			"text": "second(date) - seconds",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "second(date) - Seconds"
		},
		{
			"title": "set_bit",
			"text": "set_bit(bitstring, index, new_value) - sets the nth bit in bitstring to newvalue; the first (leftmost) bit is indexed 0. returns a new bitstring.",
			"category": "Bitstring Functions",
			"url": "/docs/sql/functions/bitstring",
			"blurb": "set_bit(bitstring, index, new_value) - Sets the nth bit in bitstring to newvalue; the first (leftmost) bit is indexed..."
		},
		{
			"title": "setseed",
			"text": "setseed(x) - sets the seed to be used for the random function",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "setseed(x) - Sets the seed to be used for the random function"
		},
		{
			"title": "sha256",
			"text": "sha256(value) - returns a varchar with the sha-256 hash of the value",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "sha256(value) - Returns a VARCHAR with the SHA-256 hash of the value"
		},
		{
			"title": "sign",
			"text": "sign(x) - returns the sign of x as -1, 0 or 1",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "sign(x) - Returns the sign of x as -1, 0 or 1"
		},
		{
			"title": "signbit",
			"text": "signbit(x) - returns whether the signbit is set or not",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "signbit(x) - Returns whether the signbit is set or not"
		},
		{
			"title": "sin",
			"text": "sin(x) - computes the sin of x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "sin(x) - Computes the sin of x"
		},
		{
			"title": "split_part",
			"text": "split_part(string, separator, index) - split the string along the separator and return the data at the (1-based) index of the list. if the index is outside the bounds of the list, return an empty string (to match postgresql's behavior).",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "split_part(string, separator, index) - Split the string along the separator and return the data at the (1-based)..."
		},
		{
			"title": "sqrt",
			"text": "sqrt(x) - returns the square root of the number",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "sqrt(x) - Returns the square root of the number"
		},
		{
			"title": "starts_with",
			"text": "starts_with(string, search_string) - return true if string begins with search_string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "starts_with(string, search_string) - Return true if string begins with search_string"
		},
		{
			"title": "stats",
			"text": "stats(expression) - returns a string with statistics about the expression. expression can be a column, constant, or sql expression.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "stats(expression) - Returns a string with statistics about the expression. Expression can be a column, constant, or..."
		},
		{
			"title": "str_split_regex",
			"text": "str_split_regex(string, regex) - splits the string along the regex",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "str_split_regex(string, regex) - Splits the string along the regex"
		},
		{
			"title": "strftime",
			"text": "strftime(timestamptz, format) - converts timestamp with time zone to string according to the format string",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "strftime(timestamptz, format) - Converts timestamp with time zone to string according to the format string"
		},
		{
			"title": "string LIKE target",
			"text": "string like target - returns true if the string matches the like specifier (see pattern matching )",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "string LIKE target - Returns true if the string matches the like specifier (see Pattern Matching )"
		},
		{
			"title": "string SIMILAR TO regex",
			"text": "string similar to regex - returns true if the string matches the regex ; identical to regexp_full_match (see pattern matching )",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "string SIMILAR TO regex - Returns true if the string matches the regex ; identical to regexp_full_match (see Pattern..."
		},
		{
			"title": "string_split",
			"text": "string_split(string, separator) - splits the string along the separator",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "string_split(string, separator) - Splits the string along the separator"
		},
		{
			"title": "string_split_regex",
			"text": "string_split_regex(string, regex) - splits the string along the regex",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "string_split_regex(string, regex) - Splits the string along the regex"
		},
		{
			"title": "strip_accents",
			"text": "strip_accents(string) - strips accents from string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "strip_accents(string) - Strips accents from string"
		},
		{
			"title": "strlen",
			"text": "strlen(string) - number of bytes in string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "strlen(string) - Number of bytes in string"
		},
		{
			"title": "strptime",
			"text": "strptime(text, format) - converts string to timestamp with time zone according to the format string if %z is specified.",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "strptime(text, format) - Converts string to timestamp with time zone according to the format string if %Z is specified."
		},
		{
			"title": "struct_extract",
			"text": "struct_extract(struct, idx) - extract the entry from an unnamed struct (tuple) using an index (1-based).",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "struct_extract(struct, idx) - Extract the entry from an unnamed STRUCT (tuple) using an index (1-based)."
		},
		{
			"title": "struct_insert",
			"text": "struct_insert(struct, name := any, ...) - add field(s)/value(s) to an existing struct with the argument values. the entry name(s) will be the bound variable name(s).",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "struct_insert(struct, name := any, ...) - Add field(s)/value(s) to an existing STRUCT with the argument values. The..."
		},
		{
			"title": "struct_pack",
			"text": "struct_pack(name := any, ...) - create a struct containing the argument values. the entry name will be the bound variable name.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "struct_pack(name := any, ...) - Create a STRUCT containing the argument values. The entry name will be the bound..."
		},
		{
			"title": "substring",
			"text": "substring(string, start, length) - extract substring of length characters starting from character start. note that a start value of 1 refers to the first character of the string.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "substring(string, start, length) - Extract substring of length characters starting from character start. Note that a..."
		},
		{
			"title": "substring_grapheme",
			"text": "substring_grapheme(string, start, length) - extract substring of length grapheme clusters starting from character start. note that a start value of 1 refers to the first character of the string.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "substring_grapheme(string, start, length) - Extract substring of length grapheme clusters starting from character..."
		},
		{
			"title": "tan",
			"text": "tan(x) - computes the tangent of x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "tan(x) - Computes the tangent of x"
		},
		{
			"title": "time_bucket",
			"text": "time_bucket(bucket_width, timestamptz [, timezone ]) - truncate timestamptz by the specified interval bucket_width. bucket starts and ends are calculated using timezone. timezone is a varchar and defaults to utc.",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "time_bucket(bucket_width, timestamptz [, timezone ]) - Truncate timestamptz by the specified interval bucket_width...."
		},
		{
			"title": "timezone",
			"text": "timezone(text, timestamptz) - use the date parts of the timestamp in the given time zone to construct a timestamp. effectively, the result is a \"local\" time.",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "timezone(text, timestamptz) - Use the date parts of the timestamp in the given time zone to construct a timestamp...."
		},
		{
			"title": "timezone_hour",
			"text": "timezone_hour(date) - time zone offset hour portion",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "timezone_hour(date) - Time zone offset hour portion"
		},
		{
			"title": "timezone_minute",
			"text": "timezone_minute(date) - time zone offset minutes portion",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "timezone_minute(date) - Time zone offset minutes portion"
		},
		{
			"title": "to_base64",
			"text": "to_base64(blob) - convert a blob to a base64 encoded string.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "to_base64(blob) - Convert a blob to a base64 encoded string."
		},
		{
			"title": "to_centuries",
			"text": "to_centuries(integer) - construct a century interval",
			"category": "Interval Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "to_centuries(integer) - Construct a century interval"
		},
		{
			"title": "to_days",
			"text": "to_days(integer) - construct a day interval",
			"category": "Interval Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "to_days(integer) - Construct a day interval"
		},
		{
			"title": "to_decades",
			"text": "to_decades(integer) - construct a decade interval",
			"category": "Interval Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "to_decades(integer) - Construct a decade interval"
		},
		{
			"title": "to_hours",
			"text": "to_hours(integer) - construct a hour interval",
			"category": "Interval Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "to_hours(integer) - Construct a hour interval"
		},
		{
			"title": "to_microseconds",
			"text": "to_microseconds(integer) - construct a microsecond interval",
			"category": "Interval Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "to_microseconds(integer) - Construct a microsecond interval"
		},
		{
			"title": "to_millennia",
			"text": "to_millennia(integer) - construct a millenium interval",
			"category": "Interval Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "to_millennia(integer) - Construct a millenium interval"
		},
		{
			"title": "to_milliseconds",
			"text": "to_milliseconds(integer) - construct a millisecond interval",
			"category": "Interval Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "to_milliseconds(integer) - Construct a millisecond interval"
		},
		{
			"title": "to_minutes",
			"text": "to_minutes(integer) - construct a minute interval",
			"category": "Interval Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "to_minutes(integer) - Construct a minute interval"
		},
		{
			"title": "to_months",
			"text": "to_months(integer) - construct a month interval",
			"category": "Interval Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "to_months(integer) - Construct a month interval"
		},
		{
			"title": "to_seconds",
			"text": "to_seconds(integer) - construct a second interval",
			"category": "Interval Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "to_seconds(integer) - Construct a second interval"
		},
		{
			"title": "to_timestamp",
			"text": "to_timestamp(double) - converts seconds since the epoch to a timestamp with time zone",
			"category": "Timestamp Functions",
			"url": "/docs/sql/functions/timestamp",
			"blurb": "to_timestamp(double) - Converts seconds since the epoch to a timestamp with time zone"
		},
		{
			"title": "to_weeks",
			"text": "to_weeks(integer) - construct a week interval",
			"category": "Interval Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "to_weeks(integer) - Construct a week interval"
		},
		{
			"title": "to_years",
			"text": "to_years(integer) - construct a year interval",
			"category": "Interval Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "to_years(integer) - Construct a year interval"
		},
		{
			"title": "today",
			"text": "today() - current date (start of current transaction)",
			"category": "Date Functions",
			"url": "/docs/sql/functions/date",
			"blurb": "today() - Current date (start of current transaction)"
		},
		{
			"title": "transaction_timestamp",
			"text": "transaction_timestamp() - current date and time (start of current transaction)",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "transaction_timestamp() - Current date and time (start of current transaction)"
		},
		{
			"title": "trim",
			"text": "trim(string) - removes any spaces from either side of the string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "trim(string) - Removes any spaces from either side of the string"
		},
		{
			"title": "trunc",
			"text": "trunc(x) - truncates the number",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "trunc(x) - Truncates the number"
		},
		{
			"title": "try_strptime",
			"text": "try_strptime(text, format) - converts string to timestamp according to the format string. returns null on failure.",
			"category": "Timestamp Functions",
			"url": "/docs/sql/functions/timestamp",
			"blurb": "try_strptime(text, format) - Converts string to timestamp according to the format string. Returns NULL on failure."
		},
		{
			"title": "txid_current",
			"text": "txid_current() - returns the current transaction's id (a bigint ). it will assign a new one if the current transaction does not have one already.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "txid_current() - Returns the current transaction's ID (a BIGINT ). It will assign a new one if the current..."
		},
		{
			"title": "typeof",
			"text": "typeof(expression) - returns the name of the data type of the result of the expression.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "typeof(expression) - Returns the name of the data type of the result of the expression."
		},
		{
			"title": "unicode",
			"text": "unicode(string) - returns the unicode code of the first character of the string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "unicode(string) - Returns the unicode code of the first character of the string"
		},
		{
			"title": "union_extract",
			"text": "union_extract(union, tag) - extract the value with the named tags from the union. null if the tag is not currently selected",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "union_extract(union, tag) - Extract the value with the named tags from the union. NULL if the tag is not currently..."
		},
		{
			"title": "union_tag",
			"text": "union_tag(union) - retrieve the currently selected tag of the union as an enum.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "union_tag(union) - Retrieve the currently selected tag of the union as an Enum."
		},
		{
			"title": "union_value",
			"text": "union_value(tag := any) - create a single member union containing the argument value. the tag of the value will be the bound variable name.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "union_value(tag := any) - Create a single member UNION containing the argument value. The tag of the value will be..."
		},
		{
			"title": "unnest",
			"text": "unnest(list) -",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "unnest(list) -"
		},
		{
			"title": "upper",
			"text": "upper(string) - convert string to upper case",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "upper(string) - Convert string to upper case"
		},
		{
			"title": "uuid",
			"text": "uuid() - return a random uuid similar to this: eeccb8c5-9943-b2bb-bb5e-222f4e14b687.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "uuid() - Return a random uuid similar to this: eeccb8c5-9943-b2bb-bb5e-222f4e14b687."
		},
		{
			"title": "version",
			"text": "version() - return the currently active version of duckdb in this format",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "version() - Return the currently active version of DuckDB in this format"
		},
		{
			"title": "week",
			"text": "week(date) - iso week",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "week(date) - ISO Week"
		},
		{
			"title": "weekday",
			"text": "weekday(date) - numeric weekday synonym (sunday = 0, saturday = 6)",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "weekday(date) - Numeric weekday synonym (Sunday = 0, Saturday = 6)"
		},
		{
			"title": "weekofyear",
			"text": "weekofyear(date) - iso week (synonym)",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "weekofyear(date) - ISO Week (synonym)"
		},
		{
			"title": "xor",
			"text": "xor(x) - bitwise xor",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "xor(x) - Bitwise XOR"
		},
		{
			"title": "year",
			"text": "year(date) - year",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "year(date) - Year"
		},
		{
			"title": "yearweek",
			"text": "yearweek(date) - bigint of combined iso year number and 2-digit version of iso week number",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "yearweek(date) - BIGINT of combined ISO Year number and 2-digit version of ISO Week number"
		},
		{
			"title": "~",
			"text": "~ - bitwise negation",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "~ - bitwise negation"
		}
	]
}