{
	"data": [
		{
			"title": "",
			"text": "-",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "-"
		},
		{
			"title": "!",
			"text": "! - factorial of x. computes the product of the current integer and all integers below it",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "! - factorial of x. Computes the product of the current integer and all integers below it"
		},
		{
			"title": "%",
			"text": "% - modulo (remainder)",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "% - modulo (remainder)"
		},
		{
			"title": "&",
			"text": "& - bitwise and",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "& - bitwise AND"
		},
		{
			"title": "*",
			"text": "* - multiplication",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "* - multiplication"
		},
		{
			"title": "**",
			"text": "** - exponent",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "** - exponent"
		},
		{
			"title": "+",
			"text": "+ - addition of an interval",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "+ - addition of an INTERVAL"
		},
		{
			"title": "-",
			"text": "- - subtraction of an interval",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "- - subtraction of an INTERVAL"
		},
		{
			"title": "/",
			"text": "/ - float division",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "/ - float division"
		},
		{
			"title": "//",
			"text": "// - division",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "// - division"
		},
		{
			"title": "<<",
			"text": "<< - bitwise shift left",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "<< - bitwise shift left"
		},
		{
			"title": ">>",
			"text": ">> - bitwise shift right",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": ">> - bitwise shift right"
		},
		{
			"title": "@",
			"text": "@ - absolute value (parentheses optional if operating on a column)",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "@ - absolute value (parentheses optional if operating on a column)"
		},
		{
			"title": "ADBC API",
			"text": "arrow database connectivity adbc similarly to odbc and jdbc is a c-style api that enables code portability between different database systems this allows developers to effortlessly build applications that communicate with database systems without using code specific to that system the main difference between adbc and odbc jdbc is that adbc uses arrow to transfer data between the database system and the application duckdb has an adbc driver which takes advantage of the zero-copy integration between duckdb and arrow to efficiently transfer data duckdb s adbc driver currently supports version 0 5 1 of adbc please refer to the adbc documentation page for a more extensive discussion on adbc and a detailed api explanation implemented functionality the duckdb-adbc driver implements the full adbc specification with the exception of the connectionreadpartition and statementexecutepartitions functions both of these functions exist to support systems that internally partition the query results which does not apply to duckdb in this section we will describe the main functions that exist in adbc along with the arguments they take and provide examples for each function database set of functions that operate on a database function name description arguments example --- --- --- --- databasenew allocate a new but uninitialized database adbcdatabase database adbcerror error adbcdatabasenew adbc_database adbc_error databasesetoption set a char option adbcdatabase database const char key const char value adbcerror error adbcdatabasesetoption adbc_database path test db adbc_error databaseinit finish setting options and initialize the database adbcdatabase database adbcerror error adbcdatabaseinit adbc_database adbc_error databaserelease destroy the database adbcdatabase database adbcerror error adbcdatabaserelease adbc_database adbc_error connection a set of functions that create and destroy a connection to interact with a database function name description arguments example --- --- --- --- connectionnew allocate a new but uninitialized connection adbcconnection adbcerror adbcconnectionnew adbc_connection adbc_error connectionsetoption options may be set before connectioninit adbcconnection const char const char adbcerror adbcconnectionsetoption adbc_connection adbc_connection_option_autocommit adbc_option_value_disabled adbc_error connectioninit finish setting options and initialize the connection adbcconnection adbcdatabase adbcerror adbcconnectioninit adbc_connection adbc_database adbc_error connectionrelease destroy this connection adbcconnection adbcerror adbcconnectionrelease adbc_connection adbc_error a set of functions that retrieve metadata about the database in general these functions will return arrow objects specifically an arrowarraystream function name description arguments example --- --- --- --- connectiongetobjects get a hierarchical view of all catalogs database schemas tables and columns adbcconnection int const char const char const char const char const char arrowarraystream adbcerror adbcdatabaseinit adbc_database adbc_error connectiongettableschema get the arrow schema of a table adbcconnection const char const char const char arrowschema adbcerror adbcdatabaserelease adbc_database adbc_error connectiongettabletypes get a list of table types in the database adbcconnection arrowarraystream adbcerror adbcdatabasenew adbc_database adbc_error a set of functions with transaction semantics for the connection by default all connections start with auto-commit mode on but this can be turned off via the connectionsetoption function function name description arguments example --- --- --- --- connectioncommit commit any pending transactions adbcconnection adbcerror adbcconnectioncommit adbc_connection adbc_error connectionrollback rollback any pending transactions adbcconnection adbcerror adbcconnectionrollback adbc_connection adbc_error statement statements hold state related to query execution they represent both one-off queries and prepared statements they can be reused however doing so will invalidate prior result sets from that statement the functions used to create destroy and set options for a statement function name description arguments example --- --- --- --- statementnew create a new statement for a given connection adbcconnection adbcstatement adbcerror adbcstatementnew adbc_connection adbc_statement adbc_error statementrelease destroy a statement adbcstatement adbcerror adbcstatementrelease adbc_statement adbc_error statementsetoption set a string option on a statement adbcstatement const char const char adbcerror statementsetoption adbc_statement adbc_ingest_option_target_table table_name adbc_error functions related to query execution function name description arguments example --- --- --- --- statementsetsqlquery set the sql query to execute the query can then be executed with statementexecutequery adbcstatement const char adbcerror adbcstatementsetsqlquery adbc_statement select from table adbc_error statementsetsubstraitplan set a substrait plan to execute the query can then be executed with statementexecutequery adbcstatement const uint8_t size_t adbcerror adbcstatementsetsubstraitplan adbc_statement substrait_plan length adbc_error statementexecutequery execute a statement and get the results adbcstatement arrowarraystream int64_t adbcerror adbcstatementexecutequery adbc_statement arrow_stream rows_affected adbc_error statementprepare turn this statement into a prepared statement to be executed multiple times adbcstatement adbcerror adbcstatementprepare adbc_statement adbc_error functions related to binding used for bulk insertion or in prepared statements function name description arguments example --- --- --- --- statementbindstream bind arrow stream this can be used for bulk inserts or prepared statements adbcstatement arrowarraystream adbcerror statementbindstream adbc_statement input_data adbc_error c example we begin our example by declaring the essential variables for querying data through adbc these variables include error database connection statement handling and an arrow stream to transfer data between duckdb and the application adbcerror adbc_error adbcdatabase adbc_database adbcconnection adbc_connection adbcstatement adbc_statement arrowarraystream arrow_stream we can then initialize our database variable before initializing the database we need to set two database options the first one is the driver which takes a path to the duckdb library the second option is the entrypoint which is an exported function from the duckdb-adbc driver that initializes all the adbc functions once we have configured these two options we can optionally set the path option providing a path on disk to store our duckdb database if not set a memory database is created after configuring all the necessary options we can proceed to initialize our database adbcdatabasenew adbc_database adbc_error adbcdatabasesetoption adbc_database driver path to libduckdb dylib adbc_error adbcdatabasesetoption adbc_database entrypoint duckdb_adbc_init adbc_error by default we start an in-memory database but you can optionally define a path to store it on disk adbcdatabasesetoption adbc_database path test db adbc_error adbcdatabaseinit adbc_database adbc_error after initializing the database we must create and initialize a connection to it adbcconnectionnew adbc_connection adbc_error adbcconnectioninit adbc_connection adbc_database adbc_error we can now initialize our statement and run queries through our connection after the adbcstatementexecutequery the arrow_stream is populated with the result adbcstatementnew adbc_connection adbc_statement adbc_error adbcstatementsetsqlquery adbc_statement select 42 adbc_error int64_t rows_affected adbcstatementexecutequery adbc_statement arrow_stream rows_affected adbc_error arrow_stream release arrow_stream besides running queries we can also ingest data via arrow_streams for this we need to set an option with the table name we want to insert to bind the stream and then execute the query statementsetoption adbc_statement adbc_ingest_option_target_table answertoeverything adbc_error statementbindstream adbc_statement arrow_stream adbc_error statementexecutequery adbc_statement nullptr nullptr adbc_error",
			"category": "Api",
			"url": "/docs/api/adbc",
			"blurb": "Arrow Database Connectivity (ADBC) , similarly to ODBC and JDBC, is a C-style API that enables code portability..."
		},
		{
			"title": "Aggregate Functions",
			"text": "examples -- produce a single row containing the sum of the amount column select sum amount from sales -- produce one row per unique region containing the sum of amount for each group select region sum amount from sales group by region -- return only the regions that have a sum of amount higher than 100 select region from sales group by region having sum amount 100 -- return the number of unique values in the region column select count distinct region from sales -- return two values the total sum of amount and the sum of amount minus columns where the region is north select sum amount sum amount filter region north from sales -- returns a list of all regions in order of the amount column select list region order by amount desc from sales syntax aggregates are functions that combine multiple rows into a single value aggregates are different from scalar functions and window functions because they change the cardinality of the result as such aggregates can only be used in the select and having clauses of a sql query when the distinct clause is provided only distinct values are considered in the computation of the aggregate this is typically used in combination with the count aggregate to get the number of distinct elements but it can be used together with any aggregate function in the system when the order by clause is provided the values being aggregated are sorted before applying the function usually this is not important but there are some order-sensitive aggregates that can have indeterminate results e g first last list and string_agg these can be made deterministic by ordering the arguments for order-insensitive aggregates this clause is parsed and applied which is inefficient but still produces the same result general aggregate functions the table below shows the available general aggregate functions function description example alias es --- --- --- --- any_value arg returns the first non-null value from arg any_value a - arg_max arg val finds the row with the maximum val calculates the arg expression at that row arg_max a b argmax a b max_by a b arg_min arg val finds the row with the minimum val calculates the arg expression at that row arg_min a b argmin a b min_by a b avg arg calculates the average value for all tuples in arg avg a - bit_and arg returns the bitwise and of all bits in a given expression bit_and a - bit_or arg returns the bitwise or of all bits in a given expression bit_or a - bit_xor arg returns the bitwise xor of all bits in a given expression bit_xor a - bitstring_agg arg returns a bitstring with bits set for each distinct value bitstring_agg a - bool_and arg returns true if every input value is true otherwise false bool_and a - bool_or arg returns true if any input value is true otherwise false bool_or a - count arg calculates the number of tuples tuples in arg count a - favg arg calculates the average using a more accurate floating point summation kahan sum favg a - first arg returns the first value of a column first a arbitrary a fsum arg calculates the sum using a more accurate floating point summation kahan sum fsum a sumkahan kahan_sum geomean arg calculates the geometric mean for all tuples in arg geomean a geometric_mean a histogram arg returns a list of struct s with the fields bucket and count histogram a - last arg returns the last value of a column last a - list arg returns a list containing all the values of a column list a array_agg max arg returns the maximum value present in arg max a - min arg returns the minimum value present in arg min a - product arg calculates the product of all tuples in arg product a - string_agg arg sep concatenates the column string values with a separator string_agg s group_concat sum arg calculates the sum value for all tuples in arg sum a - approximate aggregates the table below shows the available approximate aggregate functions function description example --- --- --- approx_count_distinct x gives the approximate count of distinct elements using hyperloglog approx_count_distinct a approx_quantile x pos gives the approximate quantile using t-digest approx_quantile a 0 5 reservoir_quantile x quantile sample_size 8192 gives the approximate quantile using reservoir sampling the sample size is optional and uses 8192 as a default size reservoir_quantile a 0 5 1024 statistical aggregates the table below shows the available statistical aggregate functions function description formula alias --- --- --- --- corr y x returns the correlation coefficient for non-null pairs in a group covar_pop y x stddev_pop x stddev_pop y - covar_pop y x returns the population covariance of input values sum x y - sum x sum y count count - covar_samp y x returns the sample covariance for non-null pairs in a group sum x y - sum x sum y count count - 1 - entropy x returns the log-2 entropy of count input-values - - kurtosis x returns the excess kurtosis fisher s definition of all input values with a bias correction according to the sample size - - mad x returns the median absolute deviation for the values within x null values are ignored temporal types return a positive interval median abs x-median x - median x returns the middle value of the set null values are ignored for even value counts quantitative values are averaged and ordinal values return the lower value quantile_cont x 0 5 - mode x returns the most frequent value for the values within x null values are ignored - - quantile_cont x pos returns the interpolated quantile number between 0 and 1 if pos is a list of float s then the result is a list of the corresponding interpolated quantiles - - quantile_disc x pos returns the exact quantile number between 0 and 1 if pos is a list of float s then the result is a list of the corresponding exact quantiles - quantile regr_avgx y x returns the average of the independent variable for non-null pairs in a group where x is the independent variable and y is the dependent variable - - regr_avgy y x returns the average of the dependent variable for non-null pairs in a group where x is the independent variable and y is the dependent variable - - regr_count y x returns the number of non-null number pairs in a group sum x y - sum x sum y count count - regr_intercept y x returns the intercept of the univariate linear regression line for non-null pairs in a group avg y -regr_slope y x avg x - regr_r2 y x returns the coefficient of determination for non-null pairs in a group - - regr_slope y x returns the slope of the linear regression line for non-null pairs in a group covar_pop x y var_pop x - regr_sxx y x - regr_count y x var_pop x - regr_sxy y x returns the population covariance of input values regr_count y x covar_pop y x - regr_syy y x - regr_count y x var_pop y f - skewness x returns the skewness of all input values - - stddev_pop x returns the population standard deviation sqrt var_pop x - stddev_samp x returns the sample standard deviation sqrt var_samp x stddev x var_pop x returns the population variance - - var_samp x returns the sample variance of all input values sum x 2 - sum x 2 count x count x - 1 variance arg val ordered set aggregate functions the table below shows the available ordered set aggregate functions these functions are specified using the within group order by sort_expression syntax and they are converted to an equivalent aggregate function that takes the ordering expression as the first argument function equivalent --- --- mode within group order by sort_expression mode sort_expression percentile_cont fraction within group order by sort_expression quantile_cont sort_expression fraction percentile_cont fractions within group order by sort_expression quantile_cont sort_expression fractions percentile_disc fraction within group order by sort_expression quantile_disc sort_expression fraction percentile_disc fractions within group order by sort_expression quantile_disc sort_expression fractions",
			"category": "SQL",
			"url": "/docs/sql/aggregates",
			"blurb": "Examples -- produce a single row containing the sum of the amount column SELECT SUM(amount) FROM sales; -- produce..."
		},
		{
			"title": "Alter Table",
			"text": "the alter table statement changes the schema of an existing table in the catalog examples -- add a new column with name k to the table integers it will be filled with the default value null alter table integers add column k integer -- add a new column with name l to the table integers it will be filled with the default value 10 alter table integers add column l integer default 10 -- drop the column k from the table integers alter table integers drop k -- change the type of the column i to the type varchar using a standard cast alter table integers alter i type varchar -- change the type of the column i to the type varchar using the specified expression to convert the data for each row alter table integers alter i set data type varchar using concat i _ j -- set the default value of a column alter table integers alter column i set default 10 -- drop the default value of a column alter table integers alter column i drop default -- rename a table alter table integers rename to integers_old -- rename a column of a table alter table integers rename i to j syntax alter table changes the schema of an existing table all the changes made by alter table fully respect the transactional semantics - that is - they will not be visible to other transactions until committed and can be fully reverted through a rollback rename table -- rename a table alter table integers rename to integers_old the rename to clause renames an entire table changing its name in the schema note that any views that rely on the table are not automatically updated rename column -- rename a column of a table alter table integers rename i to j alter table integers rename column j to k the rename column clause renames a single column within a table any constraints that rely on this name e g check constraints are automatically updated however note that any views that rely on this column name are not automatically updated add column -- add a new column with name k to the table integers it will be filled with the default value null alter table integers add column k integer -- add a new column with name l to the table integers it will be filled with the default value 10 alter table integers add column l integer default 10 the add column clause can be used to add a new column of a specified type to a table the new column will be filled with the specified default value or null if none is specified drop column -- drop the column k from the table integers alter table integers drop k the drop column clause can be used to remove a column from a table note that columns can only be removed if they do not have any indexes that rely on them this includes any indexes created as part of a primary key or unique constraint columns that are part of multi-column check constraints cannot be dropped either alter type -- change the type of the column i to the type varchar using a standard cast alter table integers alter i type varchar -- change the type of the column i to the type varchar using the specified expression to convert the data for each row alter table integers alter i set data type varchar using concat i _ j the set data type clause changes the type of a column in a table any data present in the column is converted according to the provided expression in the using clause or if the using clause is absent cast to the new data type note that columns can only have their type changed if they do not have any indexes that rely on them and are not part of any check constraints set drop default -- set the default value of a column alter table integers alter column i set default 10 -- drop the default value of a column alter table integers alter column i drop default the set drop default clause modifies the default value of an existing column note that this does not modify any existing data in the column dropping the default is equivalent to setting the default value to null at the moment duckdb will not allow you to alter a table if there are any dependencies that means that if you have an index on a column you will first need to drop the index alter the table and then recreate the index otherwise you will get a dependency error add drop constraint the add constraint and drop constraint clauses are not yet supported in duckdb",
			"category": "Statements",
			"url": "/docs/sql/statements/alter_table",
			"blurb": "The ALTER TABLE statement changes the schema of an existing table in the catalog. Examples -- add a new column with..."
		},
		{
			"title": "Appender",
			"text": "the c appender can be used to load bulk data into a duckdb database the appender is tied to a connection and will use the transaction context of that connection when appending an appender always appends to a single table in the database file duckdb db connection con db create the table con query create table people id integer name varchar initialize the appender appender appender con people the appendrow function is the easiest way of appending data it uses recursive templates to allow you to put all the values of a single row within one function call as follows appender appendrow 1 mark rows can also be individually constructed using the beginrow endrow and append methods this is done internally by appendrow and hence has the same performance characteristics appender beginrow appender append int32_t 2 appender append string hannes appender endrow any values added to the appender are cached prior to being inserted into the database system for performance reasons that means that while appending the rows might not be immediately visible in the system the cache is automatically flushed when the appender goes out of scope or when appender close is called the cache can also be manually flushed using the appender flush method after either flush or close is called all the data has been written to the database system date time and timestamps while numbers and strings are rather self-explanatory dates times and timestamps require some explanation they can be directly appended using the methods provided by duckdb date duckdb time or duckdb timestamp they can also be appended using the internal duckdb value type however this adds some additional overheads and should be avoided if possible below is a short example con query create table dates d date t time ts timestamp appender appender con dates construct the values using the date time timestamp types - this is the most efficient appender appendrow date fromdate 1992 1 1 time fromtime 1 1 1 0 timestamp fromdatetime date fromdate 1992 1 1 time fromtime 1 1 1 0 construct duckdb value objects appender appendrow value date 1992 1 1 value time 1 1 1 0 value timestamp 1992 1 1 1 1 1 0",
			"category": "Data",
			"url": "/docs/data/appender",
			"blurb": "The C++ Appender can be used to load bulk data into a DuckDB database. The Appender is tied to a connection, and will..."
		},
		{
			"title": "Attach/Detach",
			"text": "the attach statement adds a new database file to the catalog that can be read from and written to examples -- attach the database file db with the alias inferred from the name file attach file db -- attach the database file db with an explicit alias file_db attach file db as file_db -- attach the database file db in read only mode attach file db read_only -- attach a sqlite database for reading and writing see sqlite extension for more information attach sqlite_file db as sqlite type sqlite -- create a table in the attached database with alias file create table file new_table i integer -- detach the database with alias file detach file -- show a list of all attached databases show databases -- change the default database that is used to the database file use file syntax attach allows duckdb to operate on multiple database files and allows for transfer of data between different database files detach the detach statement allows previously attached database files to be closed and detached releasing any locks held on the database file name qualification the fully qualified name of catalog objects contains the catalog the schema and the name of the object for example -- attach the database new_db attach new_db db -- create the schema my_schema in the database new_db create schema new_db my_schema -- create the table my_table in the schema my_schema create table new_db my_schema my_table col integer -- refer to the column col inside the table my_table select new_db my_schema my_table col from new_db my_schema my_table note that often the fully qualified name is not required when a name is not fully qualified the system looks for which entries to reference using the catalog search path the default catalog search path includes the system catalog the temporary catalog and the initially attached database together with the main schema when a table is created without any qualifications the table is created in the default schema of the default database the default database is the database that is launched when the system is created - and the default schema is main -- create the table my_table in the default database create table my_table col integer the default database and schema can be changed using the use command -- set the default database schema to new_db main use new_db -- set the default database schema to new_db my_schema use new_db my_schema when providing only a single qualification the system can interpret this as either a catalog or a schema as long as there are no conflicts for example attach new_db db create schema my_schema -- creates the table new_db main tbl create table new_db tbl i integer -- creates the table default_db my_schema tbl create table my_schema tbl i integer if we create a conflict i e we have both a schema and a catalog with the same name the system requests that a fully qualified path is used instead create schema new_db create table new_db tbl i integer -- error binder error ambiguous reference to catalog or schema new_db - use a fully qualified path like memory new_db transactional semantics when running queries on multiple databases the system opens separate transactions per database the transactions are started lazily by default - when a given database is referenced for the first time in a query a transaction for that database will be started set immediate_transaction_mode true can be toggled to change this behavior to eagerly start transactions in all attached databases instead while multiple transactions can be active at a time - the system only supports writing to a single attached database in a single transaction if you try to write to multiple attached databases in a single transaction the following error will be thrown attempting to write to database db2 in a transaction that has already modified database db1 - a single transaction can only write to a single attached database the reason for this restriction is that the system does not maintain atomicity for transactions across attached databases transactions are only atomic within each database file by restricting the global transaction to write to only a single database file the atomicity guarantees are maintained",
			"category": "Statements",
			"url": "/docs/sql/statements/attach",
			"blurb": "The ATTACH statement adds a new database file to the catalog that can be read from and written to. Examples -- attach..."
		},
		{
			"title": "Bitstring Functions",
			"text": "this section describes functions and operators for examining and manipulating bit values bitstrings must be of equal length when performing the bitwise operands and or and xor when bit shifting the original length of the string is preserved bitstring operators the table below shows the available mathematical operators for bit type operator description example result --- --- --- --- bitwise and 10101 bit 10001 bit 10001 bitwise or 1011 bit 0001 bit 1011 xor bitwise xor xor 101 bit 001 bit 100 bitwise not 101 bit 010 bitwise shift left 1001011 bit 3 1011000 bitwise shift right 1001011 bit 3 0001001 bitstring functions the table below shows the available scalar functions for bit type function description example result --- --- --- --- bit_count bitstring returns the number of set bits in the bitstring bit_count 1101011 bit 5 bit_length bitstring returns the number of bits in the bitstring bit_length 1101011 bit 7 bit_position substring bitstring returns first starting index of the specified substring within bits or zero if it s not present the first leftmost bit is indexed 1 bit_position 010 bit 1110101 bit 4 bitstring bitstring length returns a bitstring of determined length bitstring 1010 bit 7 0001010 get_bit bitstring index extracts the nth bit from bitstring the first leftmost bit is indexed 0 get_bit 0110010 bit 2 1 length bitstring alias for bit_length length 1101011 bit 7 octet_length bitstring returns the number of bytes in the bitstring octet_length 1101011 bit 1 set_bit bitstring index new_value sets the nth bit in bitstring to newvalue the first leftmost bit is indexed 0 returns a new bitstring set_bit 0110010 bit 2 0 0100010 bitstring aggregate functions these aggregate functions are available for bit type function description example --- --- --- bit_and arg returns the bitwise and operation performed on all bitstrings in a given expression bit_and a bit_or arg returns the bitwise or operation performed on all bitstrings in a given expression bit_or a bit_xor arg returns the bitwise xor operation performed on all bitstrings in a given expression bit_xor a bitstring_agg arg returns a bitstring with bits set for each distinct value bitstring_agg a bitstring_agg arg min max returns a bitstring with bits set for each distinct value bitstring_agg a 1 42 bitstring aggregation the bitstring_agg function takes any integer type as input and returns a bitstring with bits set for each distinct value the left-most bit represents the smallest value in the column and the right-most bit the maximum value if possible the min and max are retrieved from the column statistics otherwise it is also possible to provide the min and max values the combination of bit_count and bitstring_agg could be used as an alternative to count distinct with possible performance improvements in cases of low cardinality and dense values",
			"category": "Functions",
			"url": "/docs/sql/functions/bitstring",
			"blurb": "This section describes functions and operators for examining and manipulating bit values. Bitstrings must be of equal..."
		},
		{
			"title": "Bitstring Type",
			"text": "name aliases description --- --- --- bit bitstring variable-length strings of 1 s and 0 s bitstrings are strings of 1 s and 0 s the bit type data is of variable length a bitstring value requires 1 byte for each group of 8 bits plus a fixed amount to store some metadata by default bitstrings will not be padded with zeroes bitstrings can be very large having the same size restrictions as blob s -- create a bitstring select 101010 bit -- create a bitstring with predefined length -- the resulting bitstring will be left-padded with zeroes this returns 000000101011 select bitstring 0101011 12 functions see bitstring functions",
			"category": "Data Types",
			"url": "/docs/sql/data_types/bitstring",
			"blurb": "The bitstring type are strings of 1's and 0's."
		},
		{
			"title": "Blob Functions",
			"text": "this section describes functions and operators for examining and manipulating blob values function description example result --- --- --- --- blob blob blob concatenation xaa blob xbb blob xaa xbb decode blob convert blob to varchar fails if blob is not valid utf-8 decode xc3 xbc blob \u00fc encode string convert varchar to blob converts utf-8 characters into literal encoding encode my_string_with_\u00fc my_string_with_ xc3 xbc octet_length blob number of bytes in blob octet_length xaa xbb blob 2",
			"category": "Functions",
			"url": "/docs/sql/functions/blob",
			"blurb": "This section describes functions and operators for examining and manipulating blob values. | Function | Description |..."
		},
		{
			"title": "Blob Type",
			"text": "name aliases description --- --- --- blob bytea variable-length binary data the blob b inary l arge ob ject type represents an arbitrary binary object stored in the database system the blob type can contain any type of binary data with no restrictions what the actual bytes represent is opaque to the database system -- create a blob value with a single byte 170 select xaa blob -- create a blob value with three bytes 170 171 172 select xaa xab xac blob -- create a blob value with two bytes 65 66 select ab blob blobs are typically used to store non-textual objects that the database does not provide explicit support for such as images while blobs can hold objects up to 4gb in size typically it is not recommended to store very large objects within the database system in many situations it is better to store the large file on the file system and store the path to the file in the database system in a varchar field functions see blob functions",
			"category": "Data Types",
			"url": "/docs/sql/data_types/blob",
			"blurb": "The blob (Binary Large OBject) type represents an arbitrary binary object stored in the database system."
		},
		{
			"title": "Boolean Type",
			"text": "name aliases description --- --- --- boolean bool logical boolean true false the boolean type represents a statement of truth true or false in sql the boolean field can also have a third state unknown which is represented by the sql null value -- select the three possible values of a boolean column select true false null boolean boolean values can be explicitly created using the literals true and false however they are most often created as a result of comparisons or conjunctions for example the comparison i 10 results in a boolean value boolean values can be used in the where and having clauses of a sql statement to filter out tuples from the result in this case tuples for which the predicate evaluates to true will pass the filter and tuples for which the predicate evaluates to false or null will be filtered out consider the following example -- create a table with the value 5 15 and null create table integers i integer insert into integers values 5 15 null -- select all entries where i 10 select from integers where i 10 -- in this case 5 and null are filtered out -- 5 10 false -- null 10 null -- the result is 15 conjunctions the and or conjunctions can be used to combine boolean values below is a truth table for the and conjunction i e x and y x x and true x and false x and null ------- ------- ------- ------- true true false null false false false false null null false null below is a truth table for the or conjunction i e x or y x x or true x or false x or null ------- ------ ------- ------ true true true true false true false null null true null null expressions see logical operators and comparison operators",
			"category": "Data Types",
			"url": "/docs/sql/data_types/boolean",
			"blurb": "The BOOLEAN type represents a statement of truth (true or false)."
		},
		{
			"title": "C API - Appender",
			"text": "appenders are the most efficient way of loading data into duckdb from within the c interface and are recommended for fast data loading the appender is much faster than using prepared statements or individual insert into statements appends are made in row-wise format for every column a duckdb_append_ type call should be made after which the row should be finished by calling duckdb_appender_end_row after all rows have been appended duckdb_appender_destroy should be used to finalize the appender and clean up the resulting memory note that duckdb_appender_destroy should always be called on the resulting appender even if the function returns duckdberror example duckdb_query con create table people id integer name varchar null duckdb_appender appender if duckdb_appender_create con null people appender duckdberror handle error append the first row 1 mark duckdb_append_int32 appender 1 duckdb_append_varchar appender mark duckdb_appender_end_row appender append the second row 2 hannes duckdb_append_int32 appender 2 duckdb_append_varchar appender hannes duckdb_appender_end_row appender finish appending and flush all the rows to the table duckdb_appender_destroy appender api reference syntax the connection context to create the appender in schema the schema of the table to append to or nullptr for the default schema table the table name to append to out_appender the resulting appender object returns duckdbsuccess on success or duckdberror on failure duckdb_appender_error returns the error message associated with the given appender if the appender has no error message this returns nullptr instead the error message should not be freed it will be de-allocated when duckdb_appender_destroy is called syntax the appender to get the error from returns the error message or nullptr if there is none duckdb_appender_flush flush the appender to the table forcing the cache of the appender to be cleared and the data to be appended to the base table this should generally not be used unless you know what you are doing instead call duckdb_appender_destroy when you are done with the appender syntax the appender to flush returns duckdbsuccess on success or duckdberror on failure duckdb_appender_close close the appender flushing all intermediate state in the appender to the table and closing it for further appends this is generally not necessary call duckdb_appender_destroy instead syntax the appender to flush and close returns duckdbsuccess on success or duckdberror on failure duckdb_appender_destroy close the appender and destroy it flushing all intermediate state in the appender to the table and de-allocating all memory associated with the appender syntax the appender to flush close and destroy returns duckdbsuccess on success or duckdberror on failure duckdb_appender_begin_row a nop function provided for backwards compatibility reasons does nothing only duckdb_appender_end_row is required syntax duckdb_appender_end_row finish the current row of appends after end_row is called the next row can be appended syntax the appender returns duckdbsuccess on success or duckdberror on failure duckdb_append_bool append a bool value to the appender syntax duckdb_append_int8 append an int8_t value to the appender syntax duckdb_append_int16 append an int16_t value to the appender syntax duckdb_append_int32 append an int32_t value to the appender syntax duckdb_append_int64 append an int64_t value to the appender syntax duckdb_append_hugeint append a duckdb_hugeint value to the appender syntax duckdb_append_uint8 append a uint8_t value to the appender syntax duckdb_append_uint16 append a uint16_t value to the appender syntax duckdb_append_uint32 append a uint32_t value to the appender syntax duckdb_append_uint64 append a uint64_t value to the appender syntax duckdb_append_float append a float value to the appender syntax duckdb_append_double append a double value to the appender syntax duckdb_append_date append a duckdb_date value to the appender syntax duckdb_append_time append a duckdb_time value to the appender syntax duckdb_append_timestamp append a duckdb_timestamp value to the appender syntax duckdb_append_interval append a duckdb_interval value to the appender syntax duckdb_append_varchar append a varchar value to the appender syntax duckdb_append_varchar_length append a varchar value to the appender syntax duckdb_append_blob append a blob value to the appender syntax duckdb_append_null append a null value to the appender of any type syntax duckdb_append_data_chunk appends a pre-filled data chunk to the specified appender the types of the data chunk must exactly match the types of the table no casting is performed if the types do not match or the appender is in an invalid state duckdberror is returned if the append is successful duckdbsuccess is returned syntax the appender to append to chunk the data chunk to append returns the return state",
			"category": "C",
			"url": "/docs/api/c/appender",
			"blurb": "Appenders are the most efficient way of loading data into DuckDB from within the C interface, and are recommended for..."
		},
		{
			"title": "C API - Complete API",
			"text": "api reference open connect syntax path to the database file on disk or nullptr or memory to open an in-memory database out_database the result database object returns duckdbsuccess on success or duckdberror on failure duckdb_open_ext extended version of duckdb_open creates a new database or opens an existing database file stored at the the given path syntax path to the database file on disk or nullptr or memory to open an in-memory database out_database the result database object config optional configuration used to start up the database system out_error if set and the function returns duckdberror this will contain the reason why the start-up failed note that the error must be freed using duckdb_free returns duckdbsuccess on success or duckdberror on failure duckdb_close closes the specified database and de-allocates all memory allocated for that database this should be called after you are done with any database allocated through duckdb_open note that failing to call duckdb_close in case of e g a program crash will not cause data corruption still it is recommended to always correctly close a database object after you are done with it syntax the database object to shut down duckdb_connect opens a connection to a database connections are required to query the database and store transactional state associated with the connection the instantiated connection should be closed using duckdb_disconnect syntax the database file to connect to out_connection the result connection object returns duckdbsuccess on success or duckdberror on failure duckdb_interrupt interrupt running query syntax the connection to interruot duckdb_query_progress get progress of the running query syntax the working connection returns -1 if no progress or a percentage of the progress duckdb_disconnect closes the specified connection and de-allocates all memory allocated for that connection syntax the connection to close duckdb_library_version returns the version of the linked duckdb with a version postfix for dev versions usually used for developing c extensions that must return this for a compatibility check syntax duckdb_create_config initializes an empty configuration object that can be used to provide start-up options for the duckdb instance through duckdb_open_ext this will always succeed unless there is a malloc failure syntax the result configuration object returns duckdbsuccess on success or duckdberror on failure duckdb_config_count this returns the total amount of configuration options available for usage with duckdb_get_config_flag this should not be called in a loop as it internally loops over all the options syntax the amount of config options available duckdb_get_config_flag obtains a human-readable name and description of a specific configuration option this can be used to e g display configuration options this will succeed unless index is out of range i e duckdb_config_count the result name or description must not be freed syntax the index of the configuration option between 0 and duckdb_config_count out_name a name of the configuration flag out_description a description of the configuration flag returns duckdbsuccess on success or duckdberror on failure duckdb_set_config sets the specified option for the specified configuration the configuration option is indicated by name to obtain a list of config options see duckdb_get_config_flag in the source code configuration options are defined in config cpp this can fail if either the name is invalid or if the value provided for the option is invalid syntax the configuration object to set the option on name the name of the configuration flag to set option the value to set the configuration flag to returns duckdbsuccess on success or duckdberror on failure duckdb_destroy_config destroys the specified configuration option and de-allocates all memory allocated for the object syntax the configuration object to destroy duckdb_query executes a sql query within a connection and stores the full materialized result in the out_result pointer if the query fails to execute duckdberror is returned and the error message can be retrieved by calling duckdb_result_error note that after running duckdb_query duckdb_destroy_result must be called on the result object even if the query fails otherwise the error stored within the result will not be freed correctly syntax the connection to perform the query in query the sql query to run out_result the query result returns duckdbsuccess on success or duckdberror on failure duckdb_destroy_result closes the result and de-allocates all memory allocated for that connection syntax the result to destroy duckdb_column_name returns the column name of the specified column the result should not need be freed the column names will automatically be destroyed when the result is destroyed returns null if the column is out of range syntax the result object to fetch the column name from col the column index returns the column name of the specified column duckdb_column_type returns the column type of the specified column returns duckdb_type_invalid if the column is out of range syntax the result object to fetch the column type from col the column index returns the column type of the specified column duckdb_column_logical_type returns the logical column type of the specified column the return type of this call should be destroyed with duckdb_destroy_logical_type returns null if the column is out of range syntax the result object to fetch the column type from col the column index returns the logical column type of the specified column duckdb_column_count returns the number of columns present in a the result object syntax the result object returns the number of columns present in the result object duckdb_row_count returns the number of rows present in a the result object syntax the result object returns the number of rows present in the result object duckdb_rows_changed returns the number of rows changed by the query stored in the result this is relevant only for insert update delete queries for other queries the rows_changed will be 0 syntax the result object returns the number of rows changed duckdb_column_data deprecated prefer using duckdb_result_get_chunk instead returns the data of a specific column of a result in columnar format the function returns a dense array which contains the result data the exact type stored in the array depends on the corresponding duckdb_type as provided by duckdb_column_type for the exact type by which the data should be accessed see the comments in the types section or the duckdb_type enum for example for a column of type duckdb_type_integer rows can be accessed in the following manner int32_t data int32_t duckdb_column_data result 0 printf data for row d d n row data row syntax the result object to fetch the column data from col the column index returns the column data of the specified column duckdb_nullmask_data deprecated prefer using duckdb_result_get_chunk instead returns the nullmask of a specific column of a result in columnar format the nullmask indicates for every row whether or not the corresponding row is null if a row is null the values present in the array provided by duckdb_column_data are undefined int32_t data int32_t duckdb_column_data result 0 bool nullmask duckdb_nullmask_data result 0 if nullmask row printf data for row d null n row else printf data for row d d n row data row syntax the result object to fetch the nullmask from col the column index returns the nullmask of the specified column duckdb_result_error returns the error message contained within the result the error is only set if duckdb_query returns duckdberror the result of this function must not be freed it will be cleaned up when duckdb_destroy_result is called syntax the result object to fetch the error from returns the error of the result duckdb_result_get_chunk fetches a data chunk from the duckdb_result this function should be called repeatedly until the result is exhausted the result must be destroyed with duckdb_destroy_data_chunk this function supersedes all duckdb_value functions as well as the duckdb_column_data and duckdb_nullmask_data functions it results in significantly better performance and should be preferred in newer code-bases if this function is used none of the other result functions can be used and vice versa i e this function cannot be mixed with the legacy result functions use duckdb_result_chunk_count to figure out how many chunks there are in the result syntax the result object to fetch the data chunk from chunk_index the chunk index to fetch from returns the resulting data chunk returns null if the chunk index is out of bounds duckdb_result_is_streaming checks if the type of the internal result is streamqueryresult syntax the result object to check returns whether or not the result object is of the type streamqueryresult duckdb_result_chunk_count returns the number of data chunks present in the result syntax the result object returns number of data chunks present in the result duckdb_value_boolean syntax the boolean value at the specified location or false if the value cannot be converted duckdb_value_int8 syntax the int8_t value at the specified location or 0 if the value cannot be converted duckdb_value_int16 syntax the int16_t value at the specified location or 0 if the value cannot be converted duckdb_value_int32 syntax the int32_t value at the specified location or 0 if the value cannot be converted duckdb_value_int64 syntax the int64_t value at the specified location or 0 if the value cannot be converted duckdb_value_hugeint syntax the duckdb_hugeint value at the specified location or 0 if the value cannot be converted duckdb_value_decimal syntax the duckdb_decimal value at the specified location or 0 if the value cannot be converted duckdb_value_uint8 syntax the uint8_t value at the specified location or 0 if the value cannot be converted duckdb_value_uint16 syntax the uint16_t value at the specified location or 0 if the value cannot be converted duckdb_value_uint32 syntax the uint32_t value at the specified location or 0 if the value cannot be converted duckdb_value_uint64 syntax the uint64_t value at the specified location or 0 if the value cannot be converted duckdb_value_float syntax the float value at the specified location or 0 if the value cannot be converted duckdb_value_double syntax the double value at the specified location or 0 if the value cannot be converted duckdb_value_date syntax the duckdb_date value at the specified location or 0 if the value cannot be converted duckdb_value_time syntax the duckdb_time value at the specified location or 0 if the value cannot be converted duckdb_value_timestamp syntax the duckdb_timestamp value at the specified location or 0 if the value cannot be converted duckdb_value_interval syntax the duckdb_interval value at the specified location or 0 if the value cannot be converted duckdb_value_varchar syntax use duckdb_value_string instead this function does not work correctly if the string contains null bytes returns the text value at the specified location as a null-terminated string or nullptr if the value cannot be converted the result must be freed with duckdb_free duckdb_value_varchar_internal syntax use duckdb_value_string_internal instead this function does not work correctly if the string contains null bytes returns the char value at the specified location only works on varchar columns and does not auto-cast if the column is not a varchar column this function will return null the result must not be freed duckdb_value_string_internal syntax use duckdb_value_string_internal instead this function does not work correctly if the string contains null bytes returns the char value at the specified location only works on varchar columns and does not auto-cast if the column is not a varchar column this function will return null the result must not be freed duckdb_value_blob syntax the duckdb_blob value at the specified location returns a blob with blob data set to nullptr if the value cannot be converted the resulting blob data must be freed with duckdb_free duckdb_value_is_null syntax returns true if the value at the specified index is null and false otherwise duckdb_malloc allocate size bytes of memory using the duckdb internal malloc function any memory allocated in this manner should be freed using duckdb_free syntax the number of bytes to allocate returns a pointer to the allocated memory region duckdb_free free a value returned from duckdb_malloc duckdb_value_varchar or duckdb_value_blob syntax the memory region to de-allocate duckdb_vector_size the internal vector size used by duckdb this is the amount of tuples that will fit into a data chunk created by duckdb_create_data_chunk syntax the vector size duckdb_string_is_inlined whether or not the duckdb_string_t value is inlined this means that the data of the string does not have a separate allocation syntax duckdb_from_date decompose a duckdb_date object into year month and date stored as duckdb_date_struct syntax the date object as obtained from a duckdb_type_date column returns the duckdb_date_struct with the decomposed elements duckdb_to_date re-compose a duckdb_date from year month and date duckdb_date_struct syntax the year month and date stored in a duckdb_date_struct returns the duckdb_date element duckdb_from_time decompose a duckdb_time object into hour minute second and microsecond stored as duckdb_time_struct syntax the time object as obtained from a duckdb_type_time column returns the duckdb_time_struct with the decomposed elements duckdb_to_time re-compose a duckdb_time from hour minute second and microsecond duckdb_time_struct syntax the hour minute second and microsecond in a duckdb_time_struct returns the duckdb_time element duckdb_from_timestamp decompose a duckdb_timestamp object into a duckdb_timestamp_struct syntax the ts object as obtained from a duckdb_type_timestamp column returns the duckdb_timestamp_struct with the decomposed elements duckdb_to_timestamp re-compose a duckdb_timestamp from a duckdb_timestamp_struct syntax the de-composed elements in a duckdb_timestamp_struct returns the duckdb_timestamp element duckdb_hugeint_to_double converts a duckdb_hugeint object as obtained from a duckdb_type_hugeint column into a double syntax the hugeint value returns the converted double element duckdb_double_to_hugeint converts a double value to a duckdb_hugeint object if the conversion fails because the double value is too big the result will be 0 syntax the double value returns the converted duckdb_hugeint element duckdb_double_to_decimal converts a double value to a duckdb_decimal object if the conversion fails because the double value is too big or the width scale are invalid the result will be 0 syntax the double value returns the converted duckdb_decimal element duckdb_decimal_to_double converts a duckdb_decimal object as obtained from a duckdb_type_decimal column into a double syntax the decimal value returns the converted double element duckdb_prepare create a prepared statement object from a query note that after calling duckdb_prepare the prepared statement should always be destroyed using duckdb_destroy_prepare even if the prepare fails if the prepare fails duckdb_prepare_error can be called to obtain the reason why the prepare failed syntax the connection object query the sql query to prepare out_prepared_statement the resulting prepared statement object returns duckdbsuccess on success or duckdberror on failure duckdb_destroy_prepare closes the prepared statement and de-allocates all memory allocated for the statement syntax the prepared statement to destroy duckdb_prepare_error returns the error message associated with the given prepared statement if the prepared statement has no error message this returns nullptr instead the error message should not be freed it will be de-allocated when duckdb_destroy_prepare is called syntax the prepared statement to obtain the error from returns the error message or nullptr if there is none duckdb_nparams returns the number of parameters that can be provided to the given prepared statement returns 0 if the query was not successfully prepared syntax the prepared statement to obtain the number of parameters for duckdb_param_type returns the parameter type for the parameter at the given index returns duckdb_type_invalid if the parameter index is out of range or the statement was not successfully prepared syntax the prepared statement param_idx the parameter index returns the parameter type duckdb_clear_bindings clear the params bind to the prepared statement syntax duckdb_bind_parameter_index retrieve the index of the parameter for the prepared statement identified by name syntax duckdb_bind_boolean binds a bool value to the prepared statement at the specified index syntax duckdb_bind_int8 binds an int8_t value to the prepared statement at the specified index syntax duckdb_bind_int16 binds an int16_t value to the prepared statement at the specified index syntax duckdb_bind_int32 binds an int32_t value to the prepared statement at the specified index syntax duckdb_bind_int64 binds an int64_t value to the prepared statement at the specified index syntax duckdb_bind_hugeint binds an duckdb_hugeint value to the prepared statement at the specified index syntax duckdb_bind_decimal binds a duckdb_decimal value to the prepared statement at the specified index syntax duckdb_bind_uint8 binds an uint8_t value to the prepared statement at the specified index syntax duckdb_bind_uint16 binds an uint16_t value to the prepared statement at the specified index syntax duckdb_bind_uint32 binds an uint32_t value to the prepared statement at the specified index syntax duckdb_bind_uint64 binds an uint64_t value to the prepared statement at the specified index syntax duckdb_bind_float binds an float value to the prepared statement at the specified index syntax duckdb_bind_double binds an double value to the prepared statement at the specified index syntax duckdb_bind_date binds a duckdb_date value to the prepared statement at the specified index syntax duckdb_bind_time binds a duckdb_time value to the prepared statement at the specified index syntax duckdb_bind_timestamp binds a duckdb_timestamp value to the prepared statement at the specified index syntax duckdb_bind_interval binds a duckdb_interval value to the prepared statement at the specified index syntax duckdb_bind_varchar binds a null-terminated varchar value to the prepared statement at the specified index syntax duckdb_bind_varchar_length binds a varchar value to the prepared statement at the specified index syntax duckdb_bind_blob binds a blob value to the prepared statement at the specified index syntax duckdb_bind_null binds a null value to the prepared statement at the specified index syntax duckdb_execute_prepared executes the prepared statement with the given bound parameters and returns a materialized query result this method can be called multiple times for each prepared statement and the parameters can be modified between calls to this function syntax the prepared statement to execute out_result the query result returns duckdbsuccess on success or duckdberror on failure duckdb_execute_prepared_arrow executes the prepared statement with the given bound parameters and returns an arrow query result syntax the prepared statement to execute out_result the query result returns duckdbsuccess on success or duckdberror on failure duckdb_arrow_scan scans the arrow stream and creates a view with the given name syntax the connection on which to execute the scan table_name name of the temporary view to create arrow arrow stream wrapper returns duckdbsuccess on success or duckdberror on failure duckdb_arrow_array_scan scans the arrow array and creates a view with the given name syntax the connection on which to execute the scan table_name name of the temporary view to create arrow_schema arrow schema wrapper arrow_array arrow array wrapper out_stream output array stream that wraps around the passed schema for releasing deleting once done returns duckdbsuccess on success or duckdberror on failure duckdb_extract_statements extract all statements from a query note that after calling duckdb_extract_statements the extracted statements should always be destroyed using duckdb_destroy_extracted even if no statements were extracted if the extract fails duckdb_extract_statements_error can be called to obtain the reason why the extract failed syntax the connection object query the sql query to extract out_extracted_statements the resulting extracted statements object returns the number of extracted statements or 0 on failure duckdb_prepare_extracted_statement prepare an extracted statement note that after calling duckdb_prepare_extracted_statement the prepared statement should always be destroyed using duckdb_destroy_prepare even if the prepare fails if the prepare fails duckdb_prepare_error can be called to obtain the reason why the prepare failed syntax the connection object extracted_statements the extracted statements object index the index of the extracted statement to prepare out_prepared_statement the resulting prepared statement object returns duckdbsuccess on success or duckdberror on failure duckdb_extract_statements_error returns the error message contained within the extracted statements the result of this function must not be freed it will be cleaned up when duckdb_destroy_extracted is called syntax the extracted statements to fetch the error from returns the error of the extracted statements duckdb_destroy_extracted de-allocates all memory allocated for the extracted statements syntax the extracted statements to destroy duckdb_pending_prepared executes the prepared statement with the given bound parameters and returns a pending result the pending result represents an intermediate structure for a query that is not yet fully executed the pending result can be used to incrementally execute a query returning control to the client between tasks note that after calling duckdb_pending_prepared the pending result should always be destroyed using duckdb_destroy_pending even if this function returns duckdberror syntax the prepared statement to execute out_result the pending query result returns duckdbsuccess on success or duckdberror on failure duckdb_pending_prepared_streaming executes the prepared statement with the given bound parameters and returns a pending result this pending result will create a streaming duckdb_result when executed the pending result represents an intermediate structure for a query that is not yet fully executed note that after calling duckdb_pending_prepared_streaming the pending result should always be destroyed using duckdb_destroy_pending even if this function returns duckdberror syntax the prepared statement to execute out_result the pending query result returns duckdbsuccess on success or duckdberror on failure duckdb_destroy_pending closes the pending result and de-allocates all memory allocated for the result syntax the pending result to destroy duckdb_pending_error returns the error message contained within the pending result the result of this function must not be freed it will be cleaned up when duckdb_destroy_pending is called syntax the pending result to fetch the error from returns the error of the pending result duckdb_pending_execute_task executes a single task within the query returning whether or not the query is ready if this returns duckdb_pending_result_ready the duckdb_execute_pending function can be called to obtain the result if this returns duckdb_pending_result_not_ready the duckdb_pending_execute_task function should be called again if this returns duckdb_pending_error an error occurred during execution the error message can be obtained by calling duckdb_pending_error on the pending_result syntax the pending result to execute a task within returns the state of the pending result after the execution duckdb_execute_pending fully execute a pending query result returning the final query result if duckdb_pending_execute_task has been called until duckdb_pending_result_ready was returned this will return fast otherwise all remaining tasks must be executed first syntax the pending result to execute out_result the result object returns duckdbsuccess on success or duckdberror on failure duckdb_pending_execution_is_finished returns whether a duckdb_pending_state is finished executing for example if pending_state is duckdb_pending_result_ready this function will return true syntax the pending state on which to decide whether to finish execution returns boolean indicating pending execution should be considered finished duckdb_destroy_value destroys the value and de-allocates all memory allocated for that type syntax the value to destroy duckdb_create_varchar creates a value from a null-terminated string syntax the null-terminated string returns the value this must be destroyed with duckdb_destroy_value duckdb_create_varchar_length creates a value from a string syntax the text length the length of the text returns the value this must be destroyed with duckdb_destroy_value duckdb_create_int64 creates a value from an int64 syntax the bigint value returns the value this must be destroyed with duckdb_destroy_value duckdb_get_varchar obtains a string representation of the given value the result must be destroyed with duckdb_free syntax the value returns the string value this must be destroyed with duckdb_free duckdb_get_int64 obtains an int64 of the given value syntax the value returns the int64 value or 0 if no conversion is possible duckdb_create_logical_type creates a duckdb_logical_type from a standard primitive type the resulting type should be destroyed with duckdb_destroy_logical_type this should not be used with duckdb_type_decimal syntax the primitive type to create returns the logical type duckdb_create_list_type creates a list type from its child type the resulting type should be destroyed with duckdb_destroy_logical_type syntax the child type of list type to create returns the logical type duckdb_create_map_type creates a map type from its key type and value type the resulting type should be destroyed with duckdb_destroy_logical_type syntax the key type and value type of map type to create returns the logical type duckdb_create_union_type creates a union type from the passed types array the resulting type should be destroyed with duckdb_destroy_logical_type syntax the array of types that the union should consist of type_amount the size of the types array returns the logical type duckdb_create_decimal_type creates a duckdb_logical_type of type decimal with the specified width and scale the resulting type should be destroyed with duckdb_destroy_logical_type syntax the width of the decimal type scale the scale of the decimal type returns the logical type duckdb_get_type_id retrieves the type class of a duckdb_logical_type syntax the logical type object returns the type id duckdb_decimal_width retrieves the width of a decimal type syntax the logical type object returns the width of the decimal type duckdb_decimal_scale retrieves the scale of a decimal type syntax the logical type object returns the scale of the decimal type duckdb_decimal_internal_type retrieves the internal storage type of a decimal type syntax the logical type object returns the internal type of the decimal type duckdb_enum_internal_type retrieves the internal storage type of an enum type syntax the logical type object returns the internal type of the enum type duckdb_enum_dictionary_size retrieves the dictionary size of the enum type syntax the logical type object returns the dictionary size of the enum type duckdb_enum_dictionary_value retrieves the dictionary value at the specified position from the enum the result must be freed with duckdb_free syntax the logical type object index the index in the dictionary returns the string value of the enum type must be freed with duckdb_free duckdb_list_type_child_type retrieves the child type of the given list type the result must be freed with duckdb_destroy_logical_type syntax the logical type object returns the child type of the list type must be destroyed with duckdb_destroy_logical_type duckdb_map_type_key_type retrieves the key type of the given map type the result must be freed with duckdb_destroy_logical_type syntax the logical type object returns the key type of the map type must be destroyed with duckdb_destroy_logical_type duckdb_map_type_value_type retrieves the value type of the given map type the result must be freed with duckdb_destroy_logical_type syntax the logical type object returns the value type of the map type must be destroyed with duckdb_destroy_logical_type duckdb_struct_type_child_count returns the number of children of a struct type syntax the logical type object returns the number of children of a struct type duckdb_struct_type_child_name retrieves the name of the struct child the result must be freed with duckdb_free syntax the logical type object index the child index returns the name of the struct type must be freed with duckdb_free duckdb_struct_type_child_type retrieves the child type of the given struct type at the specified index the result must be freed with duckdb_destroy_logical_type syntax the logical type object index the child index returns the child type of the struct type must be destroyed with duckdb_destroy_logical_type duckdb_union_type_member_count returns the number of members that the union type has syntax the logical type union object returns the number of members of a union type duckdb_union_type_member_name retrieves the name of the union member the result must be freed with duckdb_free syntax the logical type object index the child index returns the name of the union member must be freed with duckdb_free duckdb_union_type_member_type retrieves the child type of the given union member at the specified index the result must be freed with duckdb_destroy_logical_type syntax the logical type object index the child index returns the child type of the union member must be destroyed with duckdb_destroy_logical_type duckdb_destroy_logical_type destroys the logical type and de-allocates all memory allocated for that type syntax the logical type to destroy duckdb_create_data_chunk creates an empty datachunk with the specified set of types syntax an array of types of the data chunk column_count the number of columns returns the data chunk duckdb_destroy_data_chunk destroys the data chunk and de-allocates all memory allocated for that chunk syntax the data chunk to destroy duckdb_data_chunk_reset resets a data chunk clearing the validity masks and setting the cardinality of the data chunk to 0 syntax the data chunk to reset duckdb_data_chunk_get_column_count retrieves the number of columns in a data chunk syntax the data chunk to get the data from returns the number of columns in the data chunk duckdb_data_chunk_get_vector retrieves the vector at the specified column index in the data chunk the pointer to the vector is valid for as long as the chunk is alive it does not need to be destroyed syntax the data chunk to get the data from returns the vector duckdb_data_chunk_get_size retrieves the current number of tuples in a data chunk syntax the data chunk to get the data from returns the number of tuples in the data chunk duckdb_data_chunk_set_size sets the current number of tuples in a data chunk syntax the data chunk to set the size in size the number of tuples in the data chunk duckdb_vector_get_column_type retrieves the column type of the specified vector the result must be destroyed with duckdb_destroy_logical_type syntax the vector get the data from returns the type of the vector duckdb_vector_get_data retrieves the data pointer of the vector the data pointer can be used to read or write values from the vector how to read or write values depends on the type of the vector syntax the vector to get the data from returns the data pointer duckdb_vector_get_validity retrieves the validity mask pointer of the specified vector if all values are valid this function might return null the validity mask is a bitset that signifies null-ness within the data chunk it is a series of uint64_t values where each uint64_t value contains validity for 64 tuples the bit is set to 1 if the value is valid i e not null or 0 if the value is invalid i e null validity of a specific value can be obtained like this idx_t entry_idx row_idx 64 idx_t idx_in_entry row_idx 64 bool is_valid validity_mask entry_idx 1 idx_in_entry alternatively the slower duckdb_validity_row_is_valid function can be used syntax the vector to get the data from returns the pointer to the validity mask or null if no validity mask is present duckdb_vector_ensure_validity_writable ensures the validity mask is writable by allocating it after this function is called duckdb_vector_get_validity will always return non-null this allows null values to be written to the vector regardless of whether a validity mask was present before syntax the vector to alter duckdb_vector_assign_string_element assigns a string element in the vector at the specified location syntax the vector to alter index the row position in the vector to assign the string to str the null-terminated string duckdb_vector_assign_string_element_len assigns a string element in the vector at the specified location syntax the vector to alter index the row position in the vector to assign the string to str the string str_len the length of the string in bytes duckdb_list_vector_get_child retrieves the child vector of a list vector the resulting vector is valid as long as the parent vector is valid syntax the vector returns the child vector duckdb_list_vector_get_size returns the size of the child vector of the list syntax the vector returns the size of the child list duckdb_list_vector_set_size sets the total size of the underlying child-vector of a list vector syntax the list vector size the size of the child list returns the duckdb state returns duckdberror if the vector is nullptr duckdb_list_vector_reserve sets the total capacity of the underlying child-vector of a list syntax the list vector required_capacity the total capacity to reserve return the duckdb state returns duckdberror if the vector is nullptr duckdb_struct_vector_get_child retrieves the child vector of a struct vector the resulting vector is valid as long as the parent vector is valid syntax the vector index the child index returns the child vector duckdb_validity_row_is_valid returns whether or not a row is valid i e not null in the given validity mask syntax the validity mask as obtained through duckdb_data_chunk_get_validity row the row index returns true if the row is valid false otherwise duckdb_validity_set_row_validity in a validity mask sets a specific row to either valid or invalid note that duckdb_data_chunk_ensure_validity_writable should be called before calling duckdb_data_chunk_get_validity to ensure that there is a validity mask to write to syntax the validity mask as obtained through duckdb_data_chunk_get_validity row the row index valid whether or not to set the row to valid or invalid duckdb_validity_set_row_invalid in a validity mask sets a specific row to invalid equivalent to duckdb_validity_set_row_validity with valid set to false syntax the validity mask row the row index duckdb_validity_set_row_valid in a validity mask sets a specific row to valid equivalent to duckdb_validity_set_row_validity with valid set to true syntax the validity mask row the row index duckdb_create_table_function creates a new empty table function the return value should be destroyed with duckdb_destroy_table_function syntax the table function object duckdb_destroy_table_function destroys the given table function object syntax the table function to destroy duckdb_table_function_set_name sets the name of the given table function syntax the table function name the name of the table function duckdb_table_function_add_parameter adds a parameter to the table function syntax the table function type the type of the parameter to add duckdb_table_function_add_named_parameter adds a named parameter to the table function syntax the table function name the name of the parameter type the type of the parameter to add duckdb_table_function_set_extra_info assigns extra information to the table function that can be fetched during binding etc syntax the table function extra_info the extra information destroy the callback that will be called to destroy the bind data if any duckdb_table_function_set_bind sets the bind function of the table function syntax the table function bind the bind function duckdb_table_function_set_init sets the init function of the table function syntax the table function init the init function duckdb_table_function_set_local_init sets the thread-local init function of the table function syntax the table function init the init function duckdb_table_function_set_function sets the main function of the table function syntax the table function function the function duckdb_table_function_supports_projection_pushdown sets whether or not the given table function supports projection pushdown if this is set to true the system will provide a list of all required columns in the init stage through the duckdb_init_get_column_count and duckdb_init_get_column_index functions if this is set to false the default the system will expect all columns to be projected syntax the table function pushdown true if the table function supports projection pushdown false otherwise duckdb_register_table_function register the table function object within the given connection the function requires at least a name a bind function an init function and a main function if the function is incomplete or a function with this name already exists duckdberror is returned syntax the connection to register it in function the function pointer returns whether or not the registration was successful duckdb_bind_get_extra_info retrieves the extra info of the function as set in duckdb_table_function_set_extra_info syntax the info object returns the extra info duckdb_bind_add_result_column adds a result column to the output of the table function syntax the info object name the name of the column type the logical type of the column duckdb_bind_get_parameter_count retrieves the number of regular non-named parameters to the function syntax the info object returns the number of parameters duckdb_bind_get_parameter retrieves the parameter at the given index the result must be destroyed with duckdb_destroy_value syntax the info object index the index of the parameter to get returns the value of the parameter must be destroyed with duckdb_destroy_value duckdb_bind_get_named_parameter retrieves a named parameter with the given name the result must be destroyed with duckdb_destroy_value syntax the info object name the name of the parameter returns the value of the parameter must be destroyed with duckdb_destroy_value duckdb_bind_set_bind_data sets the user-provided bind data in the bind object this object can be retrieved again during execution syntax the info object extra_data the bind data object destroy the callback that will be called to destroy the bind data if any duckdb_bind_set_cardinality sets the cardinality estimate for the table function used for optimization syntax the bind data object is_exact whether or not the cardinality estimate is exact or an approximation duckdb_bind_set_error report that an error has occurred while calling bind syntax the info object error the error message duckdb_init_get_extra_info retrieves the extra info of the function as set in duckdb_table_function_set_extra_info syntax the info object returns the extra info duckdb_init_get_bind_data gets the bind data set by duckdb_bind_set_bind_data during the bind note that the bind data should be considered as read-only for tracking state use the init data instead syntax the info object returns the bind data object duckdb_init_set_init_data sets the user-provided init data in the init object this object can be retrieved again during execution syntax the info object extra_data the init data object destroy the callback that will be called to destroy the init data if any duckdb_init_get_column_count returns the number of projected columns this function must be used if projection pushdown is enabled to figure out which columns to emit syntax the info object returns the number of projected columns duckdb_init_get_column_index returns the column index of the projected column at the specified position this function must be used if projection pushdown is enabled to figure out which columns to emit syntax the info object column_index the index at which to get the projected column index from 0 duckdb_init_get_column_count info returns the column index of the projected column duckdb_init_set_max_threads sets how many threads can process this table function in parallel default 1 syntax the info object max_threads the maximum amount of threads that can process this table function duckdb_init_set_error report that an error has occurred while calling init syntax the info object error the error message duckdb_function_get_extra_info retrieves the extra info of the function as set in duckdb_table_function_set_extra_info syntax the info object returns the extra info duckdb_function_get_bind_data gets the bind data set by duckdb_bind_set_bind_data during the bind note that the bind data should be considered as read-only for tracking state use the init data instead syntax the info object returns the bind data object duckdb_function_get_init_data gets the init data set by duckdb_init_set_init_data during the init syntax the info object returns the init data object duckdb_function_get_local_init_data gets the thread-local init data set by duckdb_init_set_init_data during the local_init syntax the info object returns the init data object duckdb_function_set_error report that an error has occurred while executing the function syntax the info object error the error message duckdb_add_replacement_scan add a replacement scan definition to the specified database syntax the database object to add the replacement scan to replacement the replacement scan callback extra_data extra data that is passed back into the specified callback delete_callback the delete callback to call on the extra data if any duckdb_replacement_scan_set_function_name sets the replacement function name to use if this function is called in the replacement callback the replacement scan is performed if it is not called the replacement callback is not performed syntax the info object function_name the function name to substitute duckdb_replacement_scan_add_parameter adds a parameter to the replacement scan function syntax the info object parameter the parameter to add duckdb_replacement_scan_set_error report that an error has occurred while executing the replacement scan syntax the info object error the error message duckdb_appender_create creates an appender object syntax the connection context to create the appender in schema the schema of the table to append to or nullptr for the default schema table the table name to append to out_appender the resulting appender object returns duckdbsuccess on success or duckdberror on failure duckdb_appender_error returns the error message associated with the given appender if the appender has no error message this returns nullptr instead the error message should not be freed it will be de-allocated when duckdb_appender_destroy is called syntax the appender to get the error from returns the error message or nullptr if there is none duckdb_appender_flush flush the appender to the table forcing the cache of the appender to be cleared and the data to be appended to the base table this should generally not be used unless you know what you are doing instead call duckdb_appender_destroy when you are done with the appender syntax the appender to flush returns duckdbsuccess on success or duckdberror on failure duckdb_appender_close close the appender flushing all intermediate state in the appender to the table and closing it for further appends this is generally not necessary call duckdb_appender_destroy instead syntax the appender to flush and close returns duckdbsuccess on success or duckdberror on failure duckdb_appender_destroy close the appender and destroy it flushing all intermediate state in the appender to the table and de-allocating all memory associated with the appender syntax the appender to flush close and destroy returns duckdbsuccess on success or duckdberror on failure duckdb_appender_begin_row a nop function provided for backwards compatibility reasons does nothing only duckdb_appender_end_row is required syntax duckdb_appender_end_row finish the current row of appends after end_row is called the next row can be appended syntax the appender returns duckdbsuccess on success or duckdberror on failure duckdb_append_bool append a bool value to the appender syntax duckdb_append_int8 append an int8_t value to the appender syntax duckdb_append_int16 append an int16_t value to the appender syntax duckdb_append_int32 append an int32_t value to the appender syntax duckdb_append_int64 append an int64_t value to the appender syntax duckdb_append_hugeint append a duckdb_hugeint value to the appender syntax duckdb_append_uint8 append a uint8_t value to the appender syntax duckdb_append_uint16 append a uint16_t value to the appender syntax duckdb_append_uint32 append a uint32_t value to the appender syntax duckdb_append_uint64 append a uint64_t value to the appender syntax duckdb_append_float append a float value to the appender syntax duckdb_append_double append a double value to the appender syntax duckdb_append_date append a duckdb_date value to the appender syntax duckdb_append_time append a duckdb_time value to the appender syntax duckdb_append_timestamp append a duckdb_timestamp value to the appender syntax duckdb_append_interval append a duckdb_interval value to the appender syntax duckdb_append_varchar append a varchar value to the appender syntax duckdb_append_varchar_length append a varchar value to the appender syntax duckdb_append_blob append a blob value to the appender syntax duckdb_append_null append a null value to the appender of any type syntax duckdb_append_data_chunk appends a pre-filled data chunk to the specified appender the types of the data chunk must exactly match the types of the table no casting is performed if the types do not match or the appender is in an invalid state duckdberror is returned if the append is successful duckdbsuccess is returned syntax the appender to append to chunk the data chunk to append returns the return state duckdb_query_arrow executes a sql query within a connection and stores the full materialized result in an arrow structure if the query fails to execute duckdberror is returned and the error message can be retrieved by calling duckdb_query_arrow_error note that after running duckdb_query_arrow duckdb_destroy_arrow must be called on the result object even if the query fails otherwise the error stored within the result will not be freed correctly syntax the connection to perform the query in query the sql query to run out_result the query result returns duckdbsuccess on success or duckdberror on failure duckdb_query_arrow_schema fetch the internal arrow schema from the arrow result syntax the result to fetch the schema from out_schema the output schema returns duckdbsuccess on success or duckdberror on failure duckdb_query_arrow_array fetch an internal arrow array from the arrow result this function can be called multiple time to get next chunks which will free the previous out_array so consume the out_array before calling this function again syntax the result to fetch the array from out_array the output array returns duckdbsuccess on success or duckdberror on failure duckdb_arrow_column_count returns the number of columns present in a the arrow result object syntax the result object returns the number of columns present in the result object duckdb_arrow_row_count returns the number of rows present in a the arrow result object syntax the result object returns the number of rows present in the result object duckdb_arrow_rows_changed returns the number of rows changed by the query stored in the arrow result this is relevant only for insert update delete queries for other queries the rows_changed will be 0 syntax the result object returns the number of rows changed duckdb_query_arrow_error returns the error message contained within the result the error is only set if duckdb_query_arrow returns duckdberror the error message should not be freed it will be de-allocated when duckdb_destroy_arrow is called syntax the result object to fetch the nullmask from returns the error of the result duckdb_destroy_arrow closes the result and de-allocates all memory allocated for the arrow result syntax the result to destroy duckdb_execute_tasks execute duckdb tasks on this thread will return after max_tasks have been executed or if there are no more tasks present syntax the database object to execute tasks for max_tasks the maximum amount of tasks to execute duckdb_create_task_state creates a task state that can be used with duckdb_execute_tasks_state to execute tasks until duckdb_finish_execution is called on the state duckdb_destroy_state should be called on the result in order to free memory syntax the database object to create the task state for returns the task state that can be used with duckdb_execute_tasks_state duckdb_execute_tasks_state execute duckdb tasks on this thread the thread will keep on executing tasks forever until duckdb_finish_execution is called on the state multiple threads can share the same duckdb_task_state syntax the task state of the executor duckdb_execute_n_tasks_state execute duckdb tasks on this thread the thread will keep on executing tasks until either duckdb_finish_execution is called on the state max_tasks tasks have been executed or there are no more tasks to be executed multiple threads can share the same duckdb_task_state syntax the task state of the executor max_tasks the maximum amount of tasks to execute returns the amount of tasks that have actually been executed duckdb_finish_execution finish execution on a specific task syntax the task state to finish execution duckdb_task_state_is_finished check if the provided duckdb_task_state has finished execution syntax the task state to inspect returns whether or not duckdb_finish_execution has been called on the task state duckdb_destroy_task_state destroys the task state returned from duckdb_create_task_state note that this should not be called while there is an active duckdb_execute_tasks_state running on the task state syntax the task state to clean up duckdb_execution_is_finished returns true if execution of the current query is finished syntax the connection on which to check duckdb_stream_fetch_chunk fetches a data chunk from the streaming duckdb_result this function should be called repeatedly until the result is exhausted the result must be destroyed with duckdb_destroy_data_chunk this function can only be used on duckdb_results created with duckdb_pending_prepared_streaming if this function is used none of the other result functions can be used and vice versa i e this function cannot be mixed with the legacy result functions or the materialized result functions it is not known beforehand how many chunks will be returned by this result syntax the result object to fetch the data chunk from returns the resulting data chunk returns null if the result has an error",
			"category": "C",
			"url": "/docs/api/c/api",
			"blurb": "API Reference Open/Connect Syntax Path to the database file on disk, or nullptr or :memory: to open an in-memory..."
		},
		{
			"title": "C API - Configuration",
			"text": "configuration options can be provided to change different settings of the database system note that many of these settings can be changed later on using pragma statements as well the configuration object should be created filled with values and passed to duckdb_open_ext example duckdb_database db duckdb_config config create the configuration object if duckdb_create_config config duckdberror handle error set some configuration options duckdb_set_config config access_mode read_write or read_only duckdb_set_config config threads 8 duckdb_set_config config max_memory 8gb duckdb_set_config config default_order desc open the database using the configuration if duckdb_open_ext null db config null duckdberror handle error cleanup the configuration object duckdb_destroy_config config run queries cleanup duckdb_close db api reference this will always succeed unless there is a malloc failure syntax the result configuration object returns duckdbsuccess on success or duckdberror on failure duckdb_config_count this returns the total amount of configuration options available for usage with duckdb_get_config_flag this should not be called in a loop as it internally loops over all the options syntax the amount of config options available duckdb_get_config_flag obtains a human-readable name and description of a specific configuration option this can be used to e g display configuration options this will succeed unless index is out of range i e duckdb_config_count the result name or description must not be freed syntax the index of the configuration option between 0 and duckdb_config_count out_name a name of the configuration flag out_description a description of the configuration flag returns duckdbsuccess on success or duckdberror on failure duckdb_set_config sets the specified option for the specified configuration the configuration option is indicated by name to obtain a list of config options see duckdb_get_config_flag in the source code configuration options are defined in config cpp this can fail if either the name is invalid or if the value provided for the option is invalid syntax the configuration object to set the option on name the name of the configuration flag to set option the value to set the configuration flag to returns duckdbsuccess on success or duckdberror on failure duckdb_destroy_config destroys the specified configuration option and de-allocates all memory allocated for the object syntax the configuration object to destroy",
			"category": "C",
			"url": "/docs/api/c/config",
			"blurb": "Configuration options can be provided to change different settings of the database system. Note that many of these..."
		},
		{
			"title": "C API - Data Chunks",
			"text": "data chunks represent a horizontal slice of a table they hold a number of vectors that can each hold up to the vector_size rows the vector size can be obtained through the duckdb_vector_size function and is configurable but is usually set to 2048 data chunks and vectors are what duckdb uses natively to store and represent data for this reason the data chunk interface is the most efficient way of interfacing with duckdb be aware however that correctly interfacing with duckdb using the data chunk api does require knowledge of duckdb s internal vector format the primary manner of interfacing with data chunks is by obtaining the internal vectors of the data chunk using the duckdb_data_chunk_get_vector method and subsequently using the duckdb_vector_get_data and duckdb_vector_get_validity methods to read the internal data and the validity mask of the vector for composite types list and struct vectors duckdb_list_vector_get_child and duckdb_struct_vector_get_child should be used to read child vectors api reference syntax an array of types of the data chunk column_count the number of columns returns the data chunk duckdb_destroy_data_chunk destroys the data chunk and de-allocates all memory allocated for that chunk syntax the data chunk to destroy duckdb_data_chunk_reset resets a data chunk clearing the validity masks and setting the cardinality of the data chunk to 0 syntax the data chunk to reset duckdb_data_chunk_get_column_count retrieves the number of columns in a data chunk syntax the data chunk to get the data from returns the number of columns in the data chunk duckdb_data_chunk_get_vector retrieves the vector at the specified column index in the data chunk the pointer to the vector is valid for as long as the chunk is alive it does not need to be destroyed syntax the data chunk to get the data from returns the vector duckdb_data_chunk_get_size retrieves the current number of tuples in a data chunk syntax the data chunk to get the data from returns the number of tuples in the data chunk duckdb_data_chunk_set_size sets the current number of tuples in a data chunk syntax the data chunk to set the size in size the number of tuples in the data chunk duckdb_vector_get_column_type retrieves the column type of the specified vector the result must be destroyed with duckdb_destroy_logical_type syntax the vector get the data from returns the type of the vector duckdb_vector_get_data retrieves the data pointer of the vector the data pointer can be used to read or write values from the vector how to read or write values depends on the type of the vector syntax the vector to get the data from returns the data pointer duckdb_vector_get_validity retrieves the validity mask pointer of the specified vector if all values are valid this function might return null the validity mask is a bitset that signifies null-ness within the data chunk it is a series of uint64_t values where each uint64_t value contains validity for 64 tuples the bit is set to 1 if the value is valid i e not null or 0 if the value is invalid i e null validity of a specific value can be obtained like this idx_t entry_idx row_idx 64 idx_t idx_in_entry row_idx 64 bool is_valid validity_mask entry_idx 1 idx_in_entry alternatively the slower duckdb_validity_row_is_valid function can be used syntax the vector to get the data from returns the pointer to the validity mask or null if no validity mask is present duckdb_vector_ensure_validity_writable ensures the validity mask is writable by allocating it after this function is called duckdb_vector_get_validity will always return non-null this allows null values to be written to the vector regardless of whether a validity mask was present before syntax the vector to alter duckdb_vector_assign_string_element assigns a string element in the vector at the specified location syntax the vector to alter index the row position in the vector to assign the string to str the null-terminated string duckdb_vector_assign_string_element_len assigns a string element in the vector at the specified location syntax the vector to alter index the row position in the vector to assign the string to str the string str_len the length of the string in bytes duckdb_list_vector_get_child retrieves the child vector of a list vector the resulting vector is valid as long as the parent vector is valid syntax the vector returns the child vector duckdb_list_vector_get_size returns the size of the child vector of the list syntax the vector returns the size of the child list duckdb_list_vector_set_size sets the total size of the underlying child-vector of a list vector syntax the list vector size the size of the child list returns the duckdb state returns duckdberror if the vector is nullptr duckdb_list_vector_reserve sets the total capacity of the underlying child-vector of a list syntax the list vector required_capacity the total capacity to reserve return the duckdb state returns duckdberror if the vector is nullptr duckdb_struct_vector_get_child retrieves the child vector of a struct vector the resulting vector is valid as long as the parent vector is valid syntax the vector index the child index returns the child vector duckdb_validity_row_is_valid returns whether or not a row is valid i e not null in the given validity mask syntax the validity mask as obtained through duckdb_data_chunk_get_validity row the row index returns true if the row is valid false otherwise duckdb_validity_set_row_validity in a validity mask sets a specific row to either valid or invalid note that duckdb_data_chunk_ensure_validity_writable should be called before calling duckdb_data_chunk_get_validity to ensure that there is a validity mask to write to syntax the validity mask as obtained through duckdb_data_chunk_get_validity row the row index valid whether or not to set the row to valid or invalid duckdb_validity_set_row_invalid in a validity mask sets a specific row to invalid equivalent to duckdb_validity_set_row_validity with valid set to false syntax the validity mask row the row index duckdb_validity_set_row_valid in a validity mask sets a specific row to valid equivalent to duckdb_validity_set_row_validity with valid set to true syntax the validity mask row the row index",
			"category": "C",
			"url": "/docs/api/c/data_chunk",
			"blurb": "Data chunks represent a horizontal slice of a table. They hold a number of vectors, that can each hold up to the..."
		},
		{
			"title": "C API - Overview",
			"text": "duckdb implements a custom c api modelled somewhat following the sqlite c api the api is contained in the duckdb h header continue to startup shutdown to get started or check out the full api overview we also provide a sqlite api wrapper which means that if your applications is programmed against the sqlite c api you can re-link to duckdb and it should continue working see the sqlite_api_wrapper folder in our source repository for more information installation the duckdb c api can be installed as part of the libduckdb packages please see the installation page for details pages in this section",
			"category": "C",
			"url": "/docs/api/c/overview",
			"blurb": "DuckDB implements a custom C API modelled somewhat following the SQLite C API. The API is contained in the duckdb.h..."
		},
		{
			"title": "C API - Prepared Statements",
			"text": "a prepared statement is a parameterized query the query is prepared with question marks or dollar symbols 1 indicating the parameters of the query values can then be bound to these parameters after which the prepared statement can be executed using those parameters a single query can be prepared once and executed many times prepared statements are useful to easily supply parameters to functions while avoiding string concatenation sql injection attacks speeding up queries that will be executed many times with different parameters duckdb supports prepared statements in the c api with the duckdb_prepare method the duckdb_bind family of functions is used to supply values for subsequent execution of the prepared statement using duckdb_execute_prepared after we are done with the prepared statement it can be cleaned up using the duckdb_destroy_prepare method example duckdb_prepared_statement stmt duckdb_result result if duckdb_prepare con insert into integers values 1 2 stmt duckdberror handle error duckdb_bind_int32 stmt 1 42 the parameter index starts counting at 1 duckdb_bind_int32 stmt 2 43 null as second parameter means no result set is requested duckdb_execute_prepared stmt null duckdb_destroy_prepare stmt we can also query result sets using prepared statements if duckdb_prepare con select from integers where i stmt duckdberror handle error duckdb_bind_int32 stmt 1 42 duckdb_execute_prepared stmt result do something with result clean up duckdb_destroy_result result duckdb_destroy_prepare stmt after calling duckdb_prepare the prepared statement parameters can be inspected using duckdb_nparams and duckdb_param_type in case the prepare fails the error can be obtained through duckdb_prepare_error it is not required that the duckdb_bind family of functions matches the prepared statement parameter type exactly the values will be auto-cast to the required value as required for example calling duckdb_bind_int8 on a parameter type of duckdb_type_integer will work as expected do not use prepared statements to insert large amounts of data into duckdb instead it is recommended to use the appender api reference note that after calling duckdb_prepare the prepared statement should always be destroyed using duckdb_destroy_prepare even if the prepare fails if the prepare fails duckdb_prepare_error can be called to obtain the reason why the prepare failed syntax the connection object query the sql query to prepare out_prepared_statement the resulting prepared statement object returns duckdbsuccess on success or duckdberror on failure duckdb_destroy_prepare closes the prepared statement and de-allocates all memory allocated for the statement syntax the prepared statement to destroy duckdb_prepare_error returns the error message associated with the given prepared statement if the prepared statement has no error message this returns nullptr instead the error message should not be freed it will be de-allocated when duckdb_destroy_prepare is called syntax the prepared statement to obtain the error from returns the error message or nullptr if there is none duckdb_nparams returns the number of parameters that can be provided to the given prepared statement returns 0 if the query was not successfully prepared syntax the prepared statement to obtain the number of parameters for duckdb_param_type returns the parameter type for the parameter at the given index returns duckdb_type_invalid if the parameter index is out of range or the statement was not successfully prepared syntax the prepared statement param_idx the parameter index returns the parameter type duckdb_clear_bindings clear the params bind to the prepared statement syntax duckdb_bind_parameter_index retrieve the index of the parameter for the prepared statement identified by name syntax duckdb_bind_boolean binds a bool value to the prepared statement at the specified index syntax duckdb_bind_int8 binds an int8_t value to the prepared statement at the specified index syntax duckdb_bind_int16 binds an int16_t value to the prepared statement at the specified index syntax duckdb_bind_int32 binds an int32_t value to the prepared statement at the specified index syntax duckdb_bind_int64 binds an int64_t value to the prepared statement at the specified index syntax duckdb_bind_hugeint binds an duckdb_hugeint value to the prepared statement at the specified index syntax duckdb_bind_decimal binds a duckdb_decimal value to the prepared statement at the specified index syntax duckdb_bind_uint8 binds an uint8_t value to the prepared statement at the specified index syntax duckdb_bind_uint16 binds an uint16_t value to the prepared statement at the specified index syntax duckdb_bind_uint32 binds an uint32_t value to the prepared statement at the specified index syntax duckdb_bind_uint64 binds an uint64_t value to the prepared statement at the specified index syntax duckdb_bind_float binds an float value to the prepared statement at the specified index syntax duckdb_bind_double binds an double value to the prepared statement at the specified index syntax duckdb_bind_date binds a duckdb_date value to the prepared statement at the specified index syntax duckdb_bind_time binds a duckdb_time value to the prepared statement at the specified index syntax duckdb_bind_timestamp binds a duckdb_timestamp value to the prepared statement at the specified index syntax duckdb_bind_interval binds a duckdb_interval value to the prepared statement at the specified index syntax duckdb_bind_varchar binds a null-terminated varchar value to the prepared statement at the specified index syntax duckdb_bind_varchar_length binds a varchar value to the prepared statement at the specified index syntax duckdb_bind_blob binds a blob value to the prepared statement at the specified index syntax duckdb_bind_null binds a null value to the prepared statement at the specified index syntax duckdb_execute_prepared executes the prepared statement with the given bound parameters and returns a materialized query result this method can be called multiple times for each prepared statement and the parameters can be modified between calls to this function syntax the prepared statement to execute out_result the query result returns duckdbsuccess on success or duckdberror on failure duckdb_execute_prepared_arrow executes the prepared statement with the given bound parameters and returns an arrow query result syntax the prepared statement to execute out_result the query result returns duckdbsuccess on success or duckdberror on failure duckdb_arrow_scan scans the arrow stream and creates a view with the given name syntax the connection on which to execute the scan table_name name of the temporary view to create arrow arrow stream wrapper returns duckdbsuccess on success or duckdberror on failure duckdb_arrow_array_scan scans the arrow array and creates a view with the given name syntax the connection on which to execute the scan table_name name of the temporary view to create arrow_schema arrow schema wrapper arrow_array arrow array wrapper out_stream output array stream that wraps around the passed schema for releasing deleting once done returns duckdbsuccess on success or duckdberror on failure",
			"category": "C",
			"url": "/docs/api/c/prepared",
			"blurb": "A prepared statement is a parameterized query. The query is prepared with question marks ( ? ) or dollar symbols ( $1..."
		},
		{
			"title": "C API - Query",
			"text": "the duckdb_query method allows sql queries to be run in duckdb from c this method takes two parameters a null-terminated sql query string and a duckdb_result result pointer the result pointer may be null if the application is not interested in the result set or if the query produces no result after the result is consumed the duckdb_destroy_result method should be used to clean up the result elements can be extracted from the duckdb_result object using a variety of methods the duckdb_column_count and duckdb_row_count methods can be used to extract the number of columns and the number of rows respectively duckdb_column_name and duckdb_column_type can be used to extract the names and types of individual columns example duckdb_state state duckdb_result result create a table state duckdb_query con create table integers i integer j integer null if state duckdberror handle error insert three rows into the table state duckdb_query con insert into integers values 3 4 5 6 7 null null if state duckdberror handle error query rows again state duckdb_query con select from integers result if state duckdberror handle error handle the result destroy the result after we are done with it duckdb_destroy_result result value extraction values can be extracted using either the duckdb_column_data duckdb_nullmask_data functions or using the duckdb_value convenience functions the duckdb_column_data duckdb_nullmask_data functions directly hand you a pointer to the result arrays in columnar format and can therefore be very fast the duckdb_value functions perform bounds- and type-checking and will automatically cast values to the desired type this makes them more convenient and easier to use at the expense of being slower see the types page for more information for optimal performance use duckdb_column_data and duckdb_nullmask_data to extract data from the query result the duckdb_value functions perform internal type-checking bounds-checking and casting which makes them slower duckdb_value below is an example that prints the above result to csv format using the duckdb_value_varchar function note that the function is generic we do not need to know about the types of the individual result columns print the above result to csv format using duckdb_value_varchar idx_t row_count duckdb_row_count result idx_t column_count duckdb_column_count result for idx_t row 0 row row_count row for idx_t col 0 col column_count col if col 0 printf auto str_val duckdb_value_varchar result col row printf s str_val duckdb_free str_val printf n duckdb_column_data below is an example that prints the above result to csv format using the duckdb_column_data function note that the function is not generic we do need to know exactly what the types of the result columns are int32_t i_data int32_t duckdb_column_data result 0 int32_t j_data int32_t duckdb_column_data result 1 bool i_mask duckdb_nullmask_data result 0 bool j_mask duckdb_nullmask_data result 1 idx_t row_count duckdb_row_count result for idx_t row 0 row row_count row if i_mask row printf null else printf d i_data row printf if j_mask row printf null else printf d j_data row printf n when using duckdb_column_data be careful that the type matches exactly what you expect it to be as the code directly accesses an internal array there is no type-checking accessing a duckdb_type_integer column as if it was a duckdb_type_bigint column will provide unpredictable results api reference note that after running duckdb_query duckdb_destroy_result must be called on the result object even if the query fails otherwise the error stored within the result will not be freed correctly syntax the connection to perform the query in query the sql query to run out_result the query result returns duckdbsuccess on success or duckdberror on failure duckdb_destroy_result closes the result and de-allocates all memory allocated for that connection syntax the result to destroy duckdb_column_name returns the column name of the specified column the result should not need be freed the column names will automatically be destroyed when the result is destroyed returns null if the column is out of range syntax the result object to fetch the column name from col the column index returns the column name of the specified column duckdb_column_type returns the column type of the specified column returns duckdb_type_invalid if the column is out of range syntax the result object to fetch the column type from col the column index returns the column type of the specified column duckdb_column_logical_type returns the logical column type of the specified column the return type of this call should be destroyed with duckdb_destroy_logical_type returns null if the column is out of range syntax the result object to fetch the column type from col the column index returns the logical column type of the specified column duckdb_column_count returns the number of columns present in a the result object syntax the result object returns the number of columns present in the result object duckdb_row_count returns the number of rows present in a the result object syntax the result object returns the number of rows present in the result object duckdb_rows_changed returns the number of rows changed by the query stored in the result this is relevant only for insert update delete queries for other queries the rows_changed will be 0 syntax the result object returns the number of rows changed duckdb_column_data deprecated prefer using duckdb_result_get_chunk instead returns the data of a specific column of a result in columnar format the function returns a dense array which contains the result data the exact type stored in the array depends on the corresponding duckdb_type as provided by duckdb_column_type for the exact type by which the data should be accessed see the comments in the types section or the duckdb_type enum for example for a column of type duckdb_type_integer rows can be accessed in the following manner int32_t data int32_t duckdb_column_data result 0 printf data for row d d n row data row syntax the result object to fetch the column data from col the column index returns the column data of the specified column duckdb_nullmask_data deprecated prefer using duckdb_result_get_chunk instead returns the nullmask of a specific column of a result in columnar format the nullmask indicates for every row whether or not the corresponding row is null if a row is null the values present in the array provided by duckdb_column_data are undefined int32_t data int32_t duckdb_column_data result 0 bool nullmask duckdb_nullmask_data result 0 if nullmask row printf data for row d null n row else printf data for row d d n row data row syntax the result object to fetch the nullmask from col the column index returns the nullmask of the specified column duckdb_result_error returns the error message contained within the result the error is only set if duckdb_query returns duckdberror the result of this function must not be freed it will be cleaned up when duckdb_destroy_result is called syntax the result object to fetch the error from returns the error of the result",
			"category": "C",
			"url": "/docs/api/c/query",
			"blurb": "The duckdb_query method allows SQL queries to be run in DuckDB from C. This method takes two parameters, a (null-..."
		},
		{
			"title": "C API - Replacement Scans",
			"text": "the replacement scan api can be used to register a callback that is called when a table is read that does not exist in the catalog for example when a query such as select from my_table is executed and my_table does not exist the replacement scan callback will be called with my_table as parameter the replacement scan can then insert a table function with a specific parameter to replace the read of the table api reference syntax the database object to add the replacement scan to replacement the replacement scan callback extra_data extra data that is passed back into the specified callback delete_callback the delete callback to call on the extra data if any duckdb_replacement_scan_set_function_name sets the replacement function name to use if this function is called in the replacement callback the replacement scan is performed if it is not called the replacement callback is not performed syntax the info object function_name the function name to substitute duckdb_replacement_scan_add_parameter adds a parameter to the replacement scan function syntax the info object parameter the parameter to add duckdb_replacement_scan_set_error report that an error has occurred while executing the replacement scan syntax the info object error the error message",
			"category": "C",
			"url": "/docs/api/c/replacement_scans",
			"blurb": "The replacement scan API can be used to register a callback that is called when a table is read that does not exist..."
		},
		{
			"title": "C API - Startup & Shutdown",
			"text": "to use duckdb you must first initialize a duckdb_database handle using duckdb_open duckdb_open takes as parameter the database file to read and write from the special value null nullptr can be used to create an in-memory database note that for an in-memory database no data is persisted to disk i e all data is lost when you exit the process with the duckdb_database handle you can create one or many duckdb_connection using duckdb_connect while individual connections are thread-safe they will be locked during querying it is therefore recommended that each thread uses its own connection to allow for the best parallel performance all duckdb_connection s have to explicitly be disconnected with duckdb_disconnect and the duckdb_database has to be explicitly closed with duckdb_close to avoid memory and file handle leaking example duckdb_database db duckdb_connection con if duckdb_open null db duckdberror handle error if duckdb_connect db con duckdberror handle error run queries cleanup duckdb_disconnect con duckdb_close db api reference syntax path to the database file on disk or nullptr or memory to open an in-memory database out_database the result database object returns duckdbsuccess on success or duckdberror on failure duckdb_open_ext extended version of duckdb_open creates a new database or opens an existing database file stored at the the given path syntax path to the database file on disk or nullptr or memory to open an in-memory database out_database the result database object config optional configuration used to start up the database system out_error if set and the function returns duckdberror this will contain the reason why the start-up failed note that the error must be freed using duckdb_free returns duckdbsuccess on success or duckdberror on failure duckdb_close closes the specified database and de-allocates all memory allocated for that database this should be called after you are done with any database allocated through duckdb_open note that failing to call duckdb_close in case of e g a program crash will not cause data corruption still it is recommended to always correctly close a database object after you are done with it syntax the database object to shut down duckdb_connect opens a connection to a database connections are required to query the database and store transactional state associated with the connection the instantiated connection should be closed using duckdb_disconnect syntax the database file to connect to out_connection the result connection object returns duckdbsuccess on success or duckdberror on failure duckdb_interrupt interrupt running query syntax the connection to interruot duckdb_query_progress get progress of the running query syntax the working connection returns -1 if no progress or a percentage of the progress duckdb_disconnect closes the specified connection and de-allocates all memory allocated for that connection syntax the connection to close duckdb_library_version returns the version of the linked duckdb with a version postfix for dev versions usually used for developing c extensions that must return this for a compatibility check syntax",
			"category": "C",
			"url": "/docs/api/c/connect",
			"blurb": "To use DuckDB, you must first initialize a duckdb_database handle using duckdb_open() . duckdb_open() takes as..."
		},
		{
			"title": "C API - Table Functions",
			"text": "the table function api can be used to define a table function that can then be called from within duckdb in the from clause of a query api reference the return value should be destroyed with duckdb_destroy_table_function syntax the table function object duckdb_destroy_table_function destroys the given table function object syntax the table function to destroy duckdb_table_function_set_name sets the name of the given table function syntax the table function name the name of the table function duckdb_table_function_add_parameter adds a parameter to the table function syntax the table function type the type of the parameter to add duckdb_table_function_add_named_parameter adds a named parameter to the table function syntax the table function name the name of the parameter type the type of the parameter to add duckdb_table_function_set_extra_info assigns extra information to the table function that can be fetched during binding etc syntax the table function extra_info the extra information destroy the callback that will be called to destroy the bind data if any duckdb_table_function_set_bind sets the bind function of the table function syntax the table function bind the bind function duckdb_table_function_set_init sets the init function of the table function syntax the table function init the init function duckdb_table_function_set_local_init sets the thread-local init function of the table function syntax the table function init the init function duckdb_table_function_set_function sets the main function of the table function syntax the table function function the function duckdb_table_function_supports_projection_pushdown sets whether or not the given table function supports projection pushdown if this is set to true the system will provide a list of all required columns in the init stage through the duckdb_init_get_column_count and duckdb_init_get_column_index functions if this is set to false the default the system will expect all columns to be projected syntax the table function pushdown true if the table function supports projection pushdown false otherwise duckdb_register_table_function register the table function object within the given connection the function requires at least a name a bind function an init function and a main function if the function is incomplete or a function with this name already exists duckdberror is returned syntax the connection to register it in function the function pointer returns whether or not the registration was successful duckdb_bind_get_extra_info retrieves the extra info of the function as set in duckdb_table_function_set_extra_info syntax the info object returns the extra info duckdb_bind_add_result_column adds a result column to the output of the table function syntax the info object name the name of the column type the logical type of the column duckdb_bind_get_parameter_count retrieves the number of regular non-named parameters to the function syntax the info object returns the number of parameters duckdb_bind_get_parameter retrieves the parameter at the given index the result must be destroyed with duckdb_destroy_value syntax the info object index the index of the parameter to get returns the value of the parameter must be destroyed with duckdb_destroy_value duckdb_bind_get_named_parameter retrieves a named parameter with the given name the result must be destroyed with duckdb_destroy_value syntax the info object name the name of the parameter returns the value of the parameter must be destroyed with duckdb_destroy_value duckdb_bind_set_bind_data sets the user-provided bind data in the bind object this object can be retrieved again during execution syntax the info object extra_data the bind data object destroy the callback that will be called to destroy the bind data if any duckdb_bind_set_cardinality sets the cardinality estimate for the table function used for optimization syntax the bind data object is_exact whether or not the cardinality estimate is exact or an approximation duckdb_bind_set_error report that an error has occurred while calling bind syntax the info object error the error message duckdb_init_get_extra_info retrieves the extra info of the function as set in duckdb_table_function_set_extra_info syntax the info object returns the extra info duckdb_init_get_bind_data gets the bind data set by duckdb_bind_set_bind_data during the bind note that the bind data should be considered as read-only for tracking state use the init data instead syntax the info object returns the bind data object duckdb_init_set_init_data sets the user-provided init data in the init object this object can be retrieved again during execution syntax the info object extra_data the init data object destroy the callback that will be called to destroy the init data if any duckdb_init_get_column_count returns the number of projected columns this function must be used if projection pushdown is enabled to figure out which columns to emit syntax the info object returns the number of projected columns duckdb_init_get_column_index returns the column index of the projected column at the specified position this function must be used if projection pushdown is enabled to figure out which columns to emit syntax the info object column_index the index at which to get the projected column index from 0 duckdb_init_get_column_count info returns the column index of the projected column duckdb_init_set_max_threads sets how many threads can process this table function in parallel default 1 syntax the info object max_threads the maximum amount of threads that can process this table function duckdb_init_set_error report that an error has occurred while calling init syntax the info object error the error message duckdb_function_get_extra_info retrieves the extra info of the function as set in duckdb_table_function_set_extra_info syntax the info object returns the extra info duckdb_function_get_bind_data gets the bind data set by duckdb_bind_set_bind_data during the bind note that the bind data should be considered as read-only for tracking state use the init data instead syntax the info object returns the bind data object duckdb_function_get_init_data gets the init data set by duckdb_init_set_init_data during the init syntax the info object returns the init data object duckdb_function_get_local_init_data gets the thread-local init data set by duckdb_init_set_init_data during the local_init syntax the info object returns the init data object duckdb_function_set_error report that an error has occurred while executing the function syntax the info object error the error message",
			"category": "C",
			"url": "/docs/api/c/table_functions",
			"blurb": "The table function API can be used to define a table function that can then be called from within DuckDB in the FROM..."
		},
		{
			"title": "C API - Types",
			"text": "duckdb is a strongly typed database system as such every column has a single type specified this type is constant over the entire column that is to say a column that is labeled as an integer column will only contain integer values duckdb also supports columns of composite types for example it is possible to define an array of integers int it is also possible to define types as arbitrary structs row i integer j varchar for that reason native duckdb type objects are not mere enums but a class that can potentially be nested types in the c api are modeled using an enum duckdb_type and a complex class duckdb_logical_type for most primitive types e g integers or varchars the enum is sufficient for more complex types such as lists structs or decimals the logical type must be used typedef enum duckdb_type duckdb_type_invalid duckdb_type_boolean duckdb_type_tinyint duckdb_type_smallint duckdb_type_integer duckdb_type_bigint duckdb_type_utinyint duckdb_type_usmallint duckdb_type_uinteger duckdb_type_ubigint duckdb_type_float duckdb_type_double duckdb_type_timestamp duckdb_type_date duckdb_type_time duckdb_type_interval duckdb_type_hugeint duckdb_type_varchar duckdb_type_blob duckdb_type_decimal duckdb_type_timestamp_s duckdb_type_timestamp_ms duckdb_type_timestamp_ns duckdb_type_enum duckdb_type_list duckdb_type_struct duckdb_type_map duckdb_type_uuid duckdb_type_json duckdb_type the enum type of a column in the result can be obtained using the duckdb_column_type function the logical type of a column can be obtained using the duckdb_column_logical_type function duckdb_value the duckdb_value functions will auto-cast values as required for example it is no problem to use duckdb_value_double on a column of type duckdb_value_int32 the value will be auto-cast and returned as a double note that in certain cases the cast may fail for example this can happen if we request a duckdb_value_int8 and the value does not fit within an int8 value in this case a default value will be returned usually 0 or nullptr the same default value will also be returned if the corresponding value is null the duckdb_value_is_null function can be used to check if a specific value is null or not the exception to the auto-cast rule is the duckdb_value_varchar_internal function this function does not auto-cast and only works for varchar columns the reason this function exists is that the result does not need to be freed note that duckdb_value_varchar and duckdb_value_blob require the result to be de-allocated using duckdb_free duckdb_result_get_chunk the duckdb_result_get_chunk function can be used to read data chunks from a duckdb result set and is the most efficient way of reading data from a duckdb result using the c api it is also the only way of reading data of certain types from a duckdb result for example the duckdb_value functions do not support structural reading of composite types lists or structs or more complex types like enums and decimals for more information about data chunks see the documentation on data chunks api reference the result must be destroyed with duckdb_destroy_data_chunk this function supersedes all duckdb_value functions as well as the duckdb_column_data and duckdb_nullmask_data functions it results in significantly better performance and should be preferred in newer code-bases if this function is used none of the other result functions can be used and vice versa i e this function cannot be mixed with the legacy result functions use duckdb_result_chunk_count to figure out how many chunks there are in the result syntax the result object to fetch the data chunk from chunk_index the chunk index to fetch from returns the resulting data chunk returns null if the chunk index is out of bounds duckdb_result_is_streaming checks if the type of the internal result is streamqueryresult syntax the result object to check returns whether or not the result object is of the type streamqueryresult duckdb_result_chunk_count returns the number of data chunks present in the result syntax the result object returns number of data chunks present in the result duckdb_value_boolean syntax the boolean value at the specified location or false if the value cannot be converted duckdb_value_int8 syntax the int8_t value at the specified location or 0 if the value cannot be converted duckdb_value_int16 syntax the int16_t value at the specified location or 0 if the value cannot be converted duckdb_value_int32 syntax the int32_t value at the specified location or 0 if the value cannot be converted duckdb_value_int64 syntax the int64_t value at the specified location or 0 if the value cannot be converted duckdb_value_hugeint syntax the duckdb_hugeint value at the specified location or 0 if the value cannot be converted duckdb_value_decimal syntax the duckdb_decimal value at the specified location or 0 if the value cannot be converted duckdb_value_uint8 syntax the uint8_t value at the specified location or 0 if the value cannot be converted duckdb_value_uint16 syntax the uint16_t value at the specified location or 0 if the value cannot be converted duckdb_value_uint32 syntax the uint32_t value at the specified location or 0 if the value cannot be converted duckdb_value_uint64 syntax the uint64_t value at the specified location or 0 if the value cannot be converted duckdb_value_float syntax the float value at the specified location or 0 if the value cannot be converted duckdb_value_double syntax the double value at the specified location or 0 if the value cannot be converted duckdb_value_date syntax the duckdb_date value at the specified location or 0 if the value cannot be converted duckdb_value_time syntax the duckdb_time value at the specified location or 0 if the value cannot be converted duckdb_value_timestamp syntax the duckdb_timestamp value at the specified location or 0 if the value cannot be converted duckdb_value_interval syntax the duckdb_interval value at the specified location or 0 if the value cannot be converted duckdb_value_varchar syntax use duckdb_value_string instead this function does not work correctly if the string contains null bytes returns the text value at the specified location as a null-terminated string or nullptr if the value cannot be converted the result must be freed with duckdb_free duckdb_value_varchar_internal syntax use duckdb_value_string_internal instead this function does not work correctly if the string contains null bytes returns the char value at the specified location only works on varchar columns and does not auto-cast if the column is not a varchar column this function will return null the result must not be freed duckdb_value_string_internal syntax use duckdb_value_string_internal instead this function does not work correctly if the string contains null bytes returns the char value at the specified location only works on varchar columns and does not auto-cast if the column is not a varchar column this function will return null the result must not be freed duckdb_value_blob syntax the duckdb_blob value at the specified location returns a blob with blob data set to nullptr if the value cannot be converted the resulting blob data must be freed with duckdb_free duckdb_value_is_null syntax returns true if the value at the specified index is null and false otherwise duckdb_from_date decompose a duckdb_date object into year month and date stored as duckdb_date_struct syntax the date object as obtained from a duckdb_type_date column returns the duckdb_date_struct with the decomposed elements duckdb_to_date re-compose a duckdb_date from year month and date duckdb_date_struct syntax the year month and date stored in a duckdb_date_struct returns the duckdb_date element duckdb_from_time decompose a duckdb_time object into hour minute second and microsecond stored as duckdb_time_struct syntax the time object as obtained from a duckdb_type_time column returns the duckdb_time_struct with the decomposed elements duckdb_to_time re-compose a duckdb_time from hour minute second and microsecond duckdb_time_struct syntax the hour minute second and microsecond in a duckdb_time_struct returns the duckdb_time element duckdb_from_timestamp decompose a duckdb_timestamp object into a duckdb_timestamp_struct syntax the ts object as obtained from a duckdb_type_timestamp column returns the duckdb_timestamp_struct with the decomposed elements duckdb_to_timestamp re-compose a duckdb_timestamp from a duckdb_timestamp_struct syntax the de-composed elements in a duckdb_timestamp_struct returns the duckdb_timestamp element duckdb_hugeint_to_double converts a duckdb_hugeint object as obtained from a duckdb_type_hugeint column into a double syntax the hugeint value returns the converted double element duckdb_double_to_hugeint converts a double value to a duckdb_hugeint object if the conversion fails because the double value is too big the result will be 0 syntax the double value returns the converted duckdb_hugeint element duckdb_double_to_decimal converts a double value to a duckdb_decimal object if the conversion fails because the double value is too big or the width scale are invalid the result will be 0 syntax the double value returns the converted duckdb_decimal element duckdb_decimal_to_double converts a duckdb_decimal object as obtained from a duckdb_type_decimal column into a double syntax the decimal value returns the converted double element duckdb_create_logical_type creates a duckdb_logical_type from a standard primitive type the resulting type should be destroyed with duckdb_destroy_logical_type this should not be used with duckdb_type_decimal syntax the primitive type to create returns the logical type duckdb_create_list_type creates a list type from its child type the resulting type should be destroyed with duckdb_destroy_logical_type syntax the child type of list type to create returns the logical type duckdb_create_map_type creates a map type from its key type and value type the resulting type should be destroyed with duckdb_destroy_logical_type syntax the key type and value type of map type to create returns the logical type duckdb_create_union_type creates a union type from the passed types array the resulting type should be destroyed with duckdb_destroy_logical_type syntax the array of types that the union should consist of type_amount the size of the types array returns the logical type duckdb_create_decimal_type creates a duckdb_logical_type of type decimal with the specified width and scale the resulting type should be destroyed with duckdb_destroy_logical_type syntax the width of the decimal type scale the scale of the decimal type returns the logical type duckdb_get_type_id retrieves the type class of a duckdb_logical_type syntax the logical type object returns the type id duckdb_decimal_width retrieves the width of a decimal type syntax the logical type object returns the width of the decimal type duckdb_decimal_scale retrieves the scale of a decimal type syntax the logical type object returns the scale of the decimal type duckdb_decimal_internal_type retrieves the internal storage type of a decimal type syntax the logical type object returns the internal type of the decimal type duckdb_enum_internal_type retrieves the internal storage type of an enum type syntax the logical type object returns the internal type of the enum type duckdb_enum_dictionary_size retrieves the dictionary size of the enum type syntax the logical type object returns the dictionary size of the enum type duckdb_enum_dictionary_value retrieves the dictionary value at the specified position from the enum the result must be freed with duckdb_free syntax the logical type object index the index in the dictionary returns the string value of the enum type must be freed with duckdb_free duckdb_list_type_child_type retrieves the child type of the given list type the result must be freed with duckdb_destroy_logical_type syntax the logical type object returns the child type of the list type must be destroyed with duckdb_destroy_logical_type duckdb_map_type_key_type retrieves the key type of the given map type the result must be freed with duckdb_destroy_logical_type syntax the logical type object returns the key type of the map type must be destroyed with duckdb_destroy_logical_type duckdb_map_type_value_type retrieves the value type of the given map type the result must be freed with duckdb_destroy_logical_type syntax the logical type object returns the value type of the map type must be destroyed with duckdb_destroy_logical_type duckdb_struct_type_child_count returns the number of children of a struct type syntax the logical type object returns the number of children of a struct type duckdb_struct_type_child_name retrieves the name of the struct child the result must be freed with duckdb_free syntax the logical type object index the child index returns the name of the struct type must be freed with duckdb_free duckdb_struct_type_child_type retrieves the child type of the given struct type at the specified index the result must be freed with duckdb_destroy_logical_type syntax the logical type object index the child index returns the child type of the struct type must be destroyed with duckdb_destroy_logical_type duckdb_union_type_member_count returns the number of members that the union type has syntax the logical type union object returns the number of members of a union type duckdb_union_type_member_name retrieves the name of the union member the result must be freed with duckdb_free syntax the logical type object index the child index returns the name of the union member must be freed with duckdb_free duckdb_union_type_member_type retrieves the child type of the given union member at the specified index the result must be freed with duckdb_destroy_logical_type syntax the logical type object index the child index returns the child type of the union member must be destroyed with duckdb_destroy_logical_type duckdb_destroy_logical_type destroys the logical type and de-allocates all memory allocated for that type syntax the logical type to destroy",
			"category": "C",
			"url": "/docs/api/c/types",
			"blurb": "DuckDB is a strongly typed database system. As such, every column has a single type specified. This type is constant..."
		},
		{
			"title": "C API - Values",
			"text": "the value class represents a single value of any type api reference syntax the value to destroy duckdb_create_varchar creates a value from a null-terminated string syntax the null-terminated string returns the value this must be destroyed with duckdb_destroy_value duckdb_create_varchar_length creates a value from a string syntax the text length the length of the text returns the value this must be destroyed with duckdb_destroy_value duckdb_create_int64 creates a value from an int64 syntax the bigint value returns the value this must be destroyed with duckdb_destroy_value duckdb_get_varchar obtains a string representation of the given value the result must be destroyed with duckdb_free syntax the value returns the string value this must be destroyed with duckdb_free duckdb_get_int64 obtains an int64 of the given value syntax the value returns the int64 value or 0 if no conversion is possible",
			"category": "C",
			"url": "/docs/api/c/value",
			"blurb": "The value class represents a single value of any type. API Reference Syntax The value to destroy...."
		},
		{
			"title": "C++ API",
			"text": "installation the duckdb c api can be installed as part of the libduckdb packages please see the installation page for details basic api usage duckdb implements a custom c api this is built around the abstractions of a database instance duckdb class multiple connection s to the database instance and queryresult instances as the result of queries the header file for the c api is duckdb hpp the standard source distribution of libduckdb contains an amalgamation of the duckdb sources which combine all sources into two files duckdb hpp and duckdb cpp the duckdb hpp header is much larger in this case regardless of whether you are using the amalgamation or not just include duckdb hpp startup shutdown to use duckdb you must first initialize a duckdb instance using its constructor duckdb takes as parameter the database file to read and write from the special value nullptr can be used to create an in-memory database note that for an in-memory database no data is persisted to disk i e all data is lost when you exit the process the second parameter to the duckdb constructor is an optional dbconfig object in dbconfig you can set various database parameters for example the read write mode or memory limits the duckdb constructor may throw exceptions for example if the database file is not usable with the duckdb instance you can create one or many connection instances using the connection constructor while connections should be thread-safe they will be locked during querying it is therefore recommended that each thread uses its own connection if you are in a multithreaded environment duckdb db nullptr connection con db querying connections expose the query method to send a sql query string to duckdb from c query fully materializes the query result as a materializedqueryresult in memory before returning at which point the query result can be consumed there is also a streaming api for queries see further below create a table con query create table integers i integer j integer insert three rows into the table con query insert into integers values 3 4 5 6 7 null materializedqueryresult result con query select from integers if result- success cerr result- error the materializedqueryresult instance contains firstly two fields that indicate whether the query was successful query will not throw exceptions under normal circumstances instead invalid queries or other issues will lead to the success boolean field in the query result instance to be set to false in this case an error message may be available in error as a string if successful other fields are set the type of statement that was just executed e g statementtype insert_statement is contained in statement_type the high-level sql type types of the result set columns are in sql_types and the low-level data representation types are in types the names of the result columns are in the names string vector in case multiple result sets are returned for example because the result set contained multiple statements the result set can be chained using the next field todo duckdb also supports prepared statements in the c api with the prepare method this returns an instance of preparedstatement this instance can be used to execute the prepared statement with parameters below is an example std unique_ptr preparedstatement prepare con prepare select count from a where i 1 std unique_ptr queryresult result prepare- execute 12 do not use prepared statements to insert large amounts of data into duckdb see the data import documentation for better options streaming queries udf api the udf api is exposed in duckdb connection through the methods createscalarfunction and createvectorizedfunction and variants these methods created udfs into the temporary schema temp_schema of the owner connection that is the only one allowed to use and change them createscalarfunction the user can code an ordinary scalar function and invoke the createscalarfunction to register and afterward use the udf in a select statement for instance bool bigger_than_four int value return value 4 connection createscalarfunction bool int bigger_than_four bigger_than_four connection query select bigger_than_four i from values 3 5 tbl i - print the createscalarfunction methods automatically creates vectorized scalar udfs so they are as efficient as built-in functions we have two variants of this method interface as follows 1 template typename tr typename args nbsp nbsp nbsp void createscalarfunction string name tr udf_func args template parameters tr is the return type of the udf function args are the arguments up to 3 for the udf function this method only supports until ternary functions name is the name to register the udf function udf_func is a pointer to the udf function this method automatically discovers from the template typenames the corresponding sqltypes bool sqltype boolean int8_t sqltype tinyint int16_t sqltype smallint int32_t sqltype integer int64_t sqltype bigint float sqltype float double sqltype double string_t sqltype varchar in duckdb some primitive types e g int32_t are mapped to the same sqltype integer time and date then for disambiguation the users can use the following overloaded method 2 template typename tr typename args nbsp nbsp nbsp void createscalarfunction string name vector sqltype args sqltype ret_type tr udf_func args an example of use would be int32_t udf_date int32_t a return a con query create table dates d date con query insert into dates values 1992-01-01 con createscalarfunction int32_t int32_t udf_date sqltype date sqltype date udf_date con query select udf_date d from dates - print template parameters tr is the return type of the udf function args are the arguments up to 3 for the udf function this method only supports until ternary functions name is the name to register the udf function args are the sqltype arguments that the function uses which should match with the template args types ret_type is the sqltype of return of the function which should match with the template tr type udf_func is a pointer to the udf function this function checks the template types against the sqltypes passed as arguments and they must match as follow sqltypeid boolean bool sqltypeid tinyint int8_t sqltypeid smallint int16_t sqltypeid date sqltypeid time sqltypeid integer int32_t sqltypeid bigint sqltypeid timestamp int64_t sqltypeid float sqltypeid double sqltypeid decimal double sqltypeid varchar sqltypeid char sqltypeid blob string_t sqltypeid varbinary blob_t createvectorizedfunction the createvectorizedfunction methods register a vectorized udf such as this vectorized function copies the input values to the result vector template typename type static void udf_vectorized datachunk args expressionstate state vector result set the result vector type result vector_type vectortype flat_vector get a raw array from the result auto result_data flatvector getdata type result get the solely input vector auto input args data 0 now get an orrified vector vectordata vdata input orrify args size vdata get a raw array from the orrified input auto input_data type vdata data handling the data for idx_t i 0 i args size i auto idx vdata sel- get_index i if vdata nullmask idx continue result_data i input_data idx con query create table integers i integer con query insert into integers values 1 2 3 999 con createvectorizedfunction int int udf_vectorized_int udf_vectorized int con query select udf_vectorized_int i from integers - print the vectorized udf is a pointer of the type scalar_function_t typedef std function void datachunk args expressionstate expr vector result scalar_function_t args is a datachunk that holds a set of input vectors for the udf that all have the same length expr is an expressionstate that provides information to the query s expression state result is a vector to store the result values there are different vector types to handle in a vectorized udf constantvector dictionaryvector flatvector listvector stringvector structvector sequencevector the general api of the createvectorizedfunction method is as follows 1 template typename tr typename args nbsp nbsp nbsp void createvectorizedfunction string name scalar_function_t udf_func sqltype varargs sqltype invalid template parameters tr is the return type of the udf function args are the arguments up to 3 for the udf function name is the name to register the udf function udf_func is a vectorized udf function varargs the type of varargs to support or sqltypeid invalid default value if the function does not accept variable length arguments this method automatically discovers from the template typenames the corresponding sqltypes bool sqltype boolean int8_t sqltype tinyint int16_t sqltype smallint int32_t sqltype integer int64_t sqltype bigint float sqltype float double sqltype double string_t sqltype varchar 2 template typename tr typename args nbsp nbsp nbsp void createvectorizedfunction string name vector sqltype args sqltype ret_type scalar_function_t udf_func sqltype varargs sqltype invalid todo",
			"category": "Api",
			"url": "/docs/api/cpp",
			"blurb": "Installation The DuckDB C++ API can be installed as part of the libduckdb packages. Please see the installation page..."
		},
		{
			"title": "CLI API",
			"text": "installation the duckdb cli command line interface is a single dependency free executable it is precompiled for windows mac and linux please see the installation page under the cli tab or download the version for your environment from the duckdb github releases page in the assets section for pre-release versions you may compile from source or download the executable file that is produced from github actions linux mac windows the duckdb cli is based on the sqlite command line shell so cli-client-specific functionality is similar to what is described in the sqlite documentation although duckdb s sql syntax follows postgresql conventions getting started once the cli executable has been downloaded unzip it and save it to any directory navigate to that directory in a terminal and enter the command duckdb to run the executable if in a powershell or posix shell environment use the command duckdb instead to see additional command line options to use when starting the cli use the command duckdb --help by default the cli will open a temporary in-memory database to open or create a persistent database simply include a path as a command line argument like duckdb path to my_database duckdb this path can point to an existing database or to a file that does not yet exist and duckdb will open or create a database at that location as needed the file may have any arbitrary extension but db or duckdb are two common choices you will see a prompt like the below with a d on the final line v0 3 4 662041e2b enter help for usage hints connected to a transient in-memory database use open filename to reopen on a persistent database d once the cli has been opened enter a sql statement followed by a semicolon then hit enter and it will be executed results will be displayed in a table in the terminal if a semicolon is omitted hitting enter will allow for multi-line sql statements to be entered d select quack as my_column my_column ----------- quack d select nicely formatted quack as my_column excited quacking as another_column my_column another_column ------------------------ ------------------ nicely formatted quack excited quacking the cli supports all of duckdb s rich sql syntax including select create and alter statements etc to exit the cli press ctrl - d if your platform supports it otherwise press ctrl - c if using a persistent database it will automatically checkpoint save the latest edits to disk and close this will remove the wal file the write-ahead-log and consolidate all of your data into the single file database special commands dot commands in addition to sql syntax special dot commands may be entered that are specific to the cli client to use one of these commands begin the line with a period immediately followed by the name of the command you wish to execute additional arguments to the command are entered space separated after the command if an argument must contain a space either single or double quotes may be used to wrap that parameter dot commands must be entered on a single line and no whitespace may occur before the period no semicolon is required at the end of the line to see available commands use the help command d help bail on off stop after hitting an error default off binary on off turn binary output on or off default off cd directory change the working directory to directory changes on off show number of rows changed by sql check glob fail if output since testcase does not match clone newdb clone data into newdb from the existing database columns column-wise rendering of query results constant color sets the syntax highlighting color used for constant values constantcode code sets the syntax highlighting terminal code used for constant values databases list names and files of attached databases dump table render database content as sql echo on off turn command echo on or off excel display the output of next command in spreadsheet exit code exit this program with return-code code explain on off auto change the explain formatting mode default auto fullschema --indent show schema and the content of sqlite_stat tables headers on off turn display of headers on or off help -all pattern show help text for pattern highlight on off toggle syntax highlighting in the shell on off import file table import data from file into table indexes table show names of indexes keyword color sets the syntax highlighting color used for keywords keywordcode code sets the syntax highlighting terminal code used for keywords lint options report potential schema issues log file off turn logging on or off file can be stderr stdout maxrows count sets the maximum number of rows for display only for duckbox mode maxwidth count sets the maximum width in characters 0 defaults to terminal width only for duckbox mode mode mode table set output mode nullvalue string use string in place of null values once options file output for the next sql command only to file open options file close existing database and reopen file output file send output to file or stdout if file is omitted parameter cmd manage sql parameter bindings print string print literal string prompt main continue replace the standard prompts quit exit this program read file read input from file rows row-wise rendering of query results default schema pattern show the create statements matching pattern separator col row change the column and row separators sha3sum compute a sha3 hash of database content shell cmd args run cmd args in a system shell show show the current values for various settings system cmd args run cmd args in a system shell tables table list names of tables matching like pattern table testcase name begin redirecting output to testcase-out txt timer on off turn sql timer on or off width num1 num2 set minimum column widths for columnar output note that the above list of methods is extensive and duckdb supports only a subset of the commands that are displayed please file a github issue if a command that is central to your workflow is not yet supported as an example of passing an argument to a dot command the help text may be filtered by passing in a text string as the second argument d help sh sha3sum compute a sha3 hash of database content shell cmd args run cmd args in a system shell show show the current values for various settings syntax highlighting by default the shell includes support for syntax highlighting syntax highlighting can be disabled using the highlight off command the colors of the syntax highlighting can also be configured using the following commands d constant error expected usage constant red green yellow blue magenta cyan white brightblack brightred brightgreen brightyellow brightblue brightmagenta brightcyan brightwhite d keyword error expected usage keyword red green yellow blue magenta cyan white brightblack brightred brightgreen brightyellow brightblue brightmagenta brightcyan brightwhite d keywordcode error expected usage keywordcode terminal_code d constantcode error expected usage constantcode terminal_code auto-complete the shell offers context-aware auto-complete of sql queries auto-complete is triggered by pressing the tab character the shell auto-completes four different groups 1 keywords 2 table names table functions 3 column names scalar functions and 4 file names the shell looks at the position in the sql statement to determine which of these auto-completions to trigger for example s - select select s - student_id select student_id f - from select student_id from g - grades select student_id from d - data select student_id from data - data grades csv output formats the mode command may be used to change the appearance of the tables returned in the terminal output in addition to customizing the appearance these modes have additional benefits this can be useful for presenting duckdb output elsewhere by redirecting the terminal output to a file for example see writing results to a file section below using the insert mode will build a series of sql statements that can be used to insert the data at a later point the markdown mode is particularly useful for building documentation mode description ------------ ---------------------------------------------- ascii columns rows delimited by 0x1f and 0x1e box tables using unicode box-drawing characters csv comma-separated values column output in columns see width duckbox tables with extensive features html html table code insert sql insert statements for table json results in a json array jsonlines results in a ndjson latex latex tabular environment code line one value per line list values delimited by markdown markdown table format quote escape answers as for sql table ascii-art table tabs tab-separated values tcl tcl list elements trash no output d mode markdown d select quacking intensifies as incoming_ducks incoming_ducks ---------------------- quacking intensifies the output appearance can also be adjusted with the separator command if using an export mode that relies on a separator csv or tabs for example the separator will be reset when the mode is changed for example mode csv will set the separator to a comma using separator will then convert the output to be pipe separated d mode csv d select 1 as col_1 2 as col_2 union all select 10 as col1 20 as col_2 col_1 col_2 1 2 10 20 d separator d select 1 as col_1 2 as col_2 union all select 10 as col1 20 as col_2 col_1 col_2 1 2 10 20 prepared statements the duckdb cli supports executing prepared statements in addition to normal select statements to create a prepared statement use the prepare statement d prepare s1 as select from my_table where my_column 1 or my_column 2 to run the prepared statement with parameters use the execute statement d execute s1 42 101 querying the database schema all duckdb clients support querying the database schema with sql but the cli has additional dot commands that can make it easier to understand the contents of a database the tables command will return a list of tables in the database it has an optional argument that will filter the results according to a like pattern d create table swimmers as select duck as animal d create table fliers as select duck as animal d create table walkers as select duck as animal d tables fliers swimmers walkers for example to filter to only tables that contain an l use the like pattern l d tables l fliers walkers the schema command will show all of the sql statements used to define the schema of the database d schema create table fliers animal varchar create table swimmers animal varchar create table walkers animal varchar opening database files in addition to connecting to a database when opening the cli a new database connection can be made by using the open command if no additional parameters are supplied a new in-memory database connection is created this database will not be persisted when the cli connection is closed d open the open command optionally accepts several options but the final parameter can be used to indicate a path to a persistent database or where one should be created the special string memory can also be used to open a temporary in-memory database d open persistent duckdb one important option accepted by open is the --readonly flag this disallows any editing of the database to open in read only mode the database must already exist this also means that a new in-memory database can t be opened in read only mode since in-memory databases are created upon connection d open --readonly preexisting duckdb writing results to a file by default the duckdb cli sends results to the terminal s standard output however this can be modified using either the output or once commands pass in the desired output file location as a parameter the once command will only output the next set of results and then revert to standard out but output will redirect all subsequent output to that file location note that each result will overwrite the entire file at that destination to revert back to standard output enter output with no file parameter in this example the output format is changed to markdown the destination is identified as a markdown file and then duckdb will write the output of the sql statement to that file output is then reverted to standard output using output with no parameter d mode markdown d output my_results md d select taking flight as output_column d output d select back to the terminal as displayed_column the file my_results md will then contain output_column --------------- taking flight the terminal will then display displayed_column ---------------------- back to the terminal a common output format is csv or comma separated values duckdb supports sql syntax to export data as csv or parquet but the cli-specific commands may be used to write a csv instead if desired d mode csv d once my_output_file csv d select 1 as col_1 2 as col_2 union all select 10 as col1 20 as col_2 the file my_output_file csv will then contain col_1 col_2 1 2 10 20 by passing special options flags to the once command query results can also be sent to a temporary file and automatically opened in the user s default program use either the -e flag for a text file opened in the default text editor or the -x flag for a csv file opened in the default spreadsheet editor this is useful for more detailed inspection of query results especially if there is a relatively large result set the excel command is equivalent to once -x d once -e d select quack as hello the results then open in the default text file editor of the system for example import data from csv duckdb supports sql syntax to directly query or import csv files but the cli-specific commands may be used to import a csv instead if desired the import command takes two arguments and also supports several options the first argument is the path to the csv file and the second is the name of the duckdb table to create since duckdb requires stricter typing than sqlite upon which the duckdb cli is based the destination table must be created before using the import command to automatically detect the schema and create a table from a csv see the read_csv_auto examples in the import docs in this example a csv file is generated by changing to csv mode and setting an output file location d mode csv d output import_example csv d select 1 as col_1 2 as col_2 union all select 10 as col1 20 as col_2 now that the csv has been written a table can be created with the desired schema and the csv can be imported the output is reset to the terminal to avoid continuing to edit the output file specified above the --skip n option is used to ignore the first row of data since it is a header row and the table has already been created with the correct column names d mode csv d output d create table test_table col_1 int col_2 int d import import_example csv test_table --skip 1 note that the import command utilizes the current mode and separator settings when identifying the structure of the data to import the --csv option can be used to override that behavior d import import_example csv test_table --skip 1 --csv reading sql from a file the duckdb cli can read both sql commands and dot commands from an external file instead of the terminal using the read command this allows for a number of commands to be run in sequence and allows command sequences to be saved and reused the read command requires only one argument the path to the file containing the sql and or commands to execute after running the commands in the file control will revert back to the terminal output from the execution of that file is governed by the same output and once commands that have been discussed previously this allows the output to be displayed back to the terminal as in the first example below or out to another file as in the second example in this example the file select_example sql is located in the same directory as duckdb exe and contains the following sql statement select from generate_series 5 to execute it from the cli the read command is used d read select_example sql the output below is returned to the terminal by default but can be adjusted using the output or once commands generate_series ----------------- 0 1 2 3 4 5 multiple commands including both sql and dot commands can also be run in a single read command in this example the file write_markdown_to_file sql is located in the same directory as duckdb exe and contains the following commands mode markdown output series md select from generate_series 5 to execute it from the cli the read command is used as before d read write_markdown_to_file sql in this case no output is returned to the terminal instead the file series md is created or replaced if it already existed with the markdown-formatted results shown here generate_series ----------------- 0 1 2 3 4 5 configuring the cli the various dot commands above can be used to configure the cli on start-up the cli reads and executes all commands in the file duckdbrc this allows you to store the configuration state of the cli this file is passed to a read command at startup so any series of dot commands and sql commands may be included you may also point to a different initialization file using the -init switch as an example a file in the same directory as the duckdb cli named select_example will change the duckdb prompt to be a duck head and run a sql statement note that the duck head is built with unicode characters and does not always work in all terminal environments like windows unless running with wsl and using the windows terminal -- duck head prompt prompt -- example sql statement select begin quacking as ready set to invoke that file on initialization use this command duckdb -init select_example this outputs -- loading resources from home user duckdbrc ready set varchar begin quacking v version git hash enter help for usage hints connected to a transient in-memory database use open filename to reopen on a persistent database non-interactive usage to read process a file and exit immediately pipe the file contents in to duckdb duckdb select_example sql generate_series ----------------- 0 1 2 3 4 5 to execute a command with sql text passed in directly from the command line call duckdb with two arguments the database location or memory and a string with the sql statement to execute duckdb memory select 42 as the_answer the_answer int32 42 loading extensions the cli does not use the sqlite shell s load command instead directly execute duckdb s sql install and load commands as you would other sql statements see the extension docs for details d install fts d load fts reading from stdin and writing to stdout when in a unix environment it can be useful to pipe data between multiple commands duckdb is able to read data from stdin as well as write to stdout using the file location of stdin dev stdin and stdout dev stdout within sql commands as pipes act very similarly to file handles this command will create an example csv copy select 42 as woot union all select 43 as woot to test csv header first read a file and pipe it to the duckdb cli executable as arguments to the duckdb cli pass in the location of the database to open in this case an in memory database and a sql command that utilizes dev stdin as a file location cat test csv duckdb memory select from read_csv_auto dev stdin woot int32 42 43 to write back to stdout the copy command can be used with the dev stdout file location cat test csv duckdb memory copy select from read_csv_auto dev stdin to dev stdout with format csv header woot 42 43",
			"category": "Api",
			"url": "/docs/api/cli",
			"blurb": "Installation The DuckDB CLI (Command Line Interface) is a single, dependency free executable. It is precompiled for..."
		},
		{
			"title": "CLI Charting - Using DuckDB with CLI tools",
			"text": "duckdb can be used with cli graphing tools to quickly pipe input to stdout to graph your data in one line youplot is a ruby-based cli tool for drawing visually pleasing plots on the terminal it can accept input from other programs by piping data from stdin it takes tab-separated or delimiter of your choice data and can easily generate various types of plots including bar line histogram and scatter with duckdb you can write to the console stdout by using the to dev stdout command and you can also write comma-separated values by using with format csv header installing youplot installation instructions for youplot can be found on the main youplot repository if you re on a mac you can use brew install youplot run uplot --help to ensure you ve installed it successfully piping duckdb queries to stdout by combining the copy to function with a csv output file data can be read from any format supported by duckdb and piped to youplot there are three important steps to doing this as an example this is how to read all data from input json duckdb -s select from read_json_auto input json to prepare the data for youplot write a simple aggregate duckdb -s select date sum purchases as total_purchases from read_json_auto input json group by 1 order by 2 desc limit 10 finally wrap the select in the copy to function with an output location of dev stdout the syntax looks like this copy your_select_query to dev stdout with format csv header the full duckdb command below outputs the query in csv format with a header duckdb -s copy select date sum purchases as total_purchases from read_json_auto input json group by 1 order by 2 desc limit 10 to dev stdout with format csv header connecting duckdb to youplot finally the data can now be piped to youplot let s assume we have an input json file with dates and number of purchases made by somebody on that date using the query above we ll pipe the data to the uplot command to draw a plot of the top 10 purchase dates duckdb -s copy select date sum purchases as total_purchases from read_json_auto input json group by 1 order by 2 desc limit 10 to dev stdout with format csv header uplot bar -d -h -t top 10 purchase dates this tells uplot to draw a bar plot use a comma-seperated delimiter -d that the data has a header -h and give the plot a title -t youplot-top-10 bonus round stdin stdout maybe you re piping some data through jq maybe you re downloading a json file from somewhere you can also tell duckdb to read the data from another process by changing the filename to dev stdin let s combine this with a quick curl from github to see what a certain user has been up to lately curl -sl https api github com users dacort events per_page 100 duckdb -s copy select type count as event_count from read_json_auto dev stdin group by 1 order by 2 desc limit 10 to dev stdout with format csv header uplot bar -d -h -t github events for dacort github-events",
			"category": "Data Viewers",
			"url": "/docs/guides/data_viewers/youplot",
			"blurb": "DuckDB can be used with CLI graphing tools to quickly pipe input to stdout to graph your data in one line. YouPlot is..."
		},
		{
			"title": "CSV Auto Detection",
			"text": "when using read_csv_auto or reading a csv file with the auto_detect flag set the system tries to automatically infer how to read the csv file this step is necessary because csv files are not self-describing and come in many different dialects the auto-detection works roughly as follows detect the dialect of the csv file delimiter quoting rule escape detect the types of each of the columns detect whether or not the file has a header row by default the system will try to auto-detect all options however options can be individually overridden by the user this can be useful in case the system makes a mistake for example if the delimiter is chosen incorrectly we can override it by calling the read_csv_auto with an explicit delimiter e g read_csv_auto file csv delim the detection works by operating on a sample of the file the size of the sample can be modified by setting the sample_size parameter the default sample size is 20480 rows setting the sample_size parameter to -1 means the entire file is read for sampling the way sampling is performed depends on the type of file if we are reading from a regular file on disk we will jump into the file and try to sample from different locations in the file if we are reading from a file in which we cannot jump - such as a gz compressed csv file or stdin - samples are taken only from the beginning of the file dialect detection dialect detection works by attempting to parse the samples using the set of considered values the detected dialect is the dialect that has 1 a consistent number of columns for each row and 2 the highest number of columns for each row the following dialects are considered for automatic dialect detection parameters considered values ------------ ------------------- delim t quote empty escape empty consider the following example file flights csv flightdate uniquecarrier origincityname destcityname 1988-01-01 aa new york ny los angeles ca 1988-01-02 aa new york ny los angeles ca 1988-01-03 aa new york ny los angeles ca in this file the dialect detection works as follows if we split by a every row is split into 4 columns if we split by a rows 2-4 are split into 3 columns while the first row is split into 1 column if we split by every row is split into 1 column if we split by t every row is split into 1 column in this example - the system selects the as the delimiter all rows are split into the same amount of columns and there is more than one column per row meaning the delimiter was actually found in the csv file type detection after detecting the dialect the system will attempt to figure out the types of each of the columns note that this step is only performed if we are calling read_csv_auto in case of the copy statement the types of the table that we are copying into will be used instead the type detection works by attempting to convert the values in each column to the candidate types if the conversion is unsuccessful the candidate type is removed from the set of candidate types for that column after all samples have been handled - the remaining candidate type with the highest priority is chosen the set of considered candidate types in order of priority is given below types ------------- boolean bigint double time date timestamp varchar note everything can be cast to varchar this type has the lowest priority - i e columns are converted to varchar if they cannot be cast to anything else in flights csv the flightdate column will be cast to a date while the other columns will be cast to varchar the detected types can be individually overridden using the types option this option takes either a list of types e g types int varchar date which overrides the types of the columns in-order of occurrence in the csv file alternatively types takes a name - type map which overrides options of individual columns e g types quarter int the type detection can be entirely disabled by using the all_varchar option if this is set all columns will remain as varchar as they originally occur in the csv file header detection header detection works by checking if the candidate header row deviates from the other rows in the file in terms of types for example in flights csv we can see that the header row consists of only varchar columns - whereas the values contain a date value for the flightdate column as such - the system defines the first row as the header row and extracts the column names from the header row in files that do not have a header row the column names are generated as column0 column1 etc note that headers cannot be detected correctly if all columns are of type varchar - as in this case the system cannot distinguish the header row from the other rows in the file in this case the system assumes the file has no header this can be overridden using the header option dates and timestamps duckdb supports the iso 8601 format format by default for timestamps dates and times unfortunately not all dates and times are formatted using this standard for that reason the csv reader also supports the dateformat and timestampformat options using this format the user can specify a format string that specifies how the date or timestamp should be read as part of the auto-detection the system tries to figure out if dates and times are stored in a different representation this is not always possible - as there are ambiguities in the representation for example the date 01-02-2000 can be parsed as either january 2nd or february 1st often these ambiguities can be resolved for example if we later encounter the date 21-02-2000 then we know that the format must have been dd-mm-yyyy mm-dd-yyyy is no longer possible as there is no 21nd month if the ambiguities cannot be resolved by looking at the data the system has a list of preferences for which date format to use if the system choses incorrectly the user can specify the dateformat and timestampformat options manually the system considers the following formats for dates dateformat higher entries are chosen over lower entries in case of ambiguities i e iso 8601 is preferred over mm-dd-yyyy dateformat ------------ iso 8601 y- m- d y- m- d d- m- y d- m- y m- d- y m- d- y the system considers the following formats for timestamps timestampformat higher entries are chosen over lower entries in case of ambiguities timestampformat ---------------------- iso 8601 y- m- d h m s y- m- d h m s d- m- y h m s d- m- y h m s m- d- y i m s p m- d- y i m s p y- m- d h m s f",
			"category": "Csv",
			"url": "/docs/data/csv/auto_detection",
			"blurb": "When using read_csv_auto , or reading a CSV file with the auto_detect flag set, the system tries to automatically..."
		},
		{
			"title": "CSV Export",
			"text": "to export the data from a table to a csv file use the copy statement copy tbl to output csv header delimiter the result of queries can also be directly exported to a csv file copy select from tbl to output csv header delimiter for additional options see the copy statement documentation",
			"category": "Import",
			"url": "/docs/guides/import/csv_export",
			"blurb": "To export the data from a table to a CSV file, use the COPY statement. COPY tbl TO 'output.csv' (HEADER, DELIMITER..."
		},
		{
			"title": "CSV Import",
			"text": "to read data from a csv file use the read_csv_auto function in the from clause of a query select from read_csv_auto input csv to create a new table using the result from a query use create table as from a select statement create table new_tbl as select from read_csv_auto input csv we can use duckdb s optional from -first syntax to omit select create table new_tbl as from read_csv_auto input csv to load data into an existing table from a query use insert into from a select statement insert into tbl select from read_csv_auto input csv alternatively the copy statement can also be used to load data from a csv file into an existing table copy tbl from input csv for additional options see the csv loading reference and the copy statement documentation",
			"category": "Import",
			"url": "/docs/guides/import/csv_import",
			"blurb": "To read data from a CSV file, use the read_csv_auto function in the FROM clause of a query. SELECT * FROM..."
		},
		{
			"title": "CSV Import/Export",
			"text": "copy moves data between duckdb tables and external comma separated value csv files csv import copy from imports data into duckdb from an external csv file into an existing table the data is appended to whatever data is in the table already the amount of columns inside the file must match the amount of columns in the table table_name and the contents of the columns must be convertible to the column types of the table in case this is not possible an error will be thrown if a list of columns is specified copy will only copy the data in the specified columns from the file if there are any columns in the table that are not in the column list copy from will insert the default values for those columns -- copy the contents of a comma-separated file test csv without a header into the table test copy test from test csv -- copy the contents of a comma-separated file with a header into the category table copy category from categories csv header -- copy the contents of lineitem tbl into the lineitem table where the contents are delimited by a pipe character copy lineitem from lineitem tbl delimiter -- read the contents of a comma-separated file names csv into the name column of the category table any other columns of this table are filled with their default value copy category name from names csv syntax csv export copy to exports data from duckdb to an external csv file it has mostly the same set of options as copy from however in the case of copy to the options specify how the csv file should be written to disk any csv file created by copy to can be copied back into the database by using copy from with the same set of options the copy to function can be called specifying either a table name or a query when a table name is specified the contents of the entire table will be written into the resulting csv file when a query is specified the query is executed and the result of the query is written to the resulting file -- copy the contents of the lineitem table to the file lineitem tbl where the columns are delimited by a pipe character including a header line copy lineitem to lineitem tbl delimiter header -- copy the l_orderkey column of the lineitem table to the file orderkey tbl copy lineitem l_orderkey to orderkey tbl delimiter -- copy the result of a query to the file query csv including a header with column names copy select 42 as a hello as b to query csv with header 1 delimiter syntax parameters name description --- --- table_name the name optionally schema-qualified of an existing table column_name an optional list of columns to be copied if no column list is specified all columns of the table will be copied filename the path and name of the input or output file boolean specifies whether the selected option should be turned on or off you can write true on or 1 to enable the option and false off or 0 to disable it the boolean value can also be omitted in which case true is assumed format if this option is used its value must be csv with any other format an error will be thrown delimiter specifies the string that separates columns within each row line of the file the default value is a comma null specifies the string that represents a null value the default is an empty string please note that in copy from both an unquoted empty string and a quoted empty string represent a null value if any other null string is specified again both its quoted and its unquoted appearance represent a null value copy to does not quote null values on output even if force_quote is true header specifies that the file contains a header line with the names of each column in the file thus copy from ignores the first line when importing data whereas on output copy to the first line of the file contains the column names of the exported columns quote specifies the quoting string to be used when a data value is quoted the default is double-quote escape specifies the string that should appear before a data character sequence that matches the quote value the default is the same as the quote value so that the quoting string is doubled if it appears in the data force_quote forces quoting to be used for all non-null values in each specified column null output is never quoted if is specified non-null values will be quoted in all columns this option is allowed only in copy to force_not_null do not match the specified columns values against the null string in the default case where the null string is empty this means that empty values will be read as zero-length strings rather than nulls this option is allowed only in copy from encoding if this option is used its value must be utf8 with any other encoding an error will be thrown ignore_errors if this option is used all errors related to the data will be ignored and the problematic rows are skipped notes it is recommended that the file name used in copy always be specified as an absolute path the values in each record are separated by the delimiter string if the value contains the delimiter string the quote string the null string a carriage return or line feed character then the whole value is prefixed and suffixed by the quote string and any occurrence within the value of a quote string or the escape string is preceded by the escape string you can also use force_quote to force quotes when outputting non-null values in specific columns the csv format has no standard way to distinguish a null value from an empty string you can use force_not_null to prevent null input comparisons for specific columns in csv format all characters are significant a quoted value surrounded by white space or any characters other than delimiter will include those characters this can cause errors if you import data from a system that pads csv lines with white space out to some fixed width if such a situation arises you might need to preprocess the csv file to remove the trailing white space before importing the data into duckdb",
			"category": "Docs",
			"url": "/docs/csv_import",
			"blurb": "COPY moves data between DuckDB tables and external Comma Separated Value (CSV) files. CSV Import COPY ... FROM..."
		},
		{
			"title": "CSV Loading",
			"text": "examples -- read a csv file from disk auto-infer options select from flights csv -- read_csv with custom options select from read_csv flights csv delim header true columns flightdate date uniquecarrier varchar origincityname varchar destcityname varchar -- read a csv from stdin auto-infer options cat data csv issue2471 csv duckdb -c select from read_csv_auto dev stdin -- read a csv file into a table create table ontime flightdate date uniquecarrier varchar origincityname varchar destcityname varchar copy ontime from flights csv auto_detect true -- alternatively create a table without specifying the schema manually create table ontime as select from flights csv -- we can use the from-first syntax to omit select create table ontime as from flights csv -- write the result of a query to a csv file copy select from ontime to flights csv with header 1 delimiter -- we can use the from-first syntax to omit select copy from ontime to flights csv with header 1 delimiter csv loading csv loading is a very common and yet surprisingly tricky task while csvs seem simple on the surface there are a lot of inconsistencies found within csv files that can make loading them a challenge csv files come in many different varieties are often corrupt and do not have a schema the csv reader needs to cope with all of these different situations the duckdb csv reader can automatically infer which configuration flags to use by analyzing the csv file this will work correctly in most situations and should be the first option attempted in rare situations where the csv reader cannot figure out the correct configuration it is possible to manually configure the csv reader to correctly parse the csv file see the auto detection page for more information below are parameters that can be passed in to the csv reader parameters name description type default ------ -------- --- --- all_varchar option to skip type detection for csv parsing and assume all columns to be of type varchar bool false auto_detect enables auto detection of parameters bool true columns a struct that specifies the column names and column types contained within the csv file e g col1 integer col2 varchar struct empty compression the compression type for the file by default this will be detected automatically from the file extension e g t csv gz will use gzip t csv will use none options are none gzip zstd varchar auto dateformat specifies the date format to use when parsing dates see date format varchar empty decimal_separator the decimal separator of numbers varchar delim or sep specifies the string that separates columns within each row line of the file varchar escape specifies the string that should appear before a data character sequence that matches the quote value varchar filename whether or not an extra filename column should be included in the result bool false force_not_null do not match the specified columns values against the null string in the default case where the null string is empty this means that empty values will be read as zero-length strings rather than nulls varchar header specifies that the file contains a header line with the names of each column in the file bool false hive_partitioning whether or not to interpret the path as a hive partitioned path bool false ignore_errors option to ignore any parsing errors encountered - and instead ignore rows with errors bool false max_line_size the maximum line size in bytes bigint 2097152 names the column names as a list example here varchar empty new_line set the new line character s in the file options are r n or r n varchar empty normalize_names boolean value that specifies whether or not column names should be normalized removing any non-alphanumeric characters from them bool false nullstr specifies the string that represents a null value varchar empty parallel whether or not the parallel csv reader is used bool true quote specifies the quoting string to be used when a data value is quoted varchar sample_size the number of sample rows for auto detection of parameters bigint 20480 skip the number of lines at the top of the file to skip bigint 0 timestampformat specifies the date format to use when parsing timestamps see date format varchar empty types or dtypes the column types as either a list by position or a struct by name example here varchar or struct empty union_by_name whether the columns of multiple schemas should be unified by name rather than by position bool false writing the contents of tables or the result of queries can be written directly to a csv file using the copy statement see the copy documentation for more information read_csv_auto function the read_csv_auto is the simplest method of loading csv files it automatically attempts to figure out the correct configuration of the csv reader it also automatically deduces types of columns if the csv file has a header it will use the names found in that header to name the columns otherwise the columns will be named column0 column1 column2 select from read_csv_auto flights csv flightdate uniquecarrier origincityname destcityname ---------- ------------- ----------------- --------------- 1988-01-01 aa new york ny los angeles ca 1988-01-02 aa new york ny los angeles ca 1988-01-03 aa new york ny los angeles ca the path can either be a relative path relative to the current working directory or an absolute path we can use read_csv_auto to create a persistent table as well create table ontime as select from read_csv_auto flights csv describe ontime field type null key default extra -------------- ------- ---- ---- ------- ----- flightdate date yes null null null uniquecarrier varchar yes null null null origincityname varchar yes null null null destcityname varchar yes null null null select from read_csv_auto flights csv sample_size 20000 if we set delim sep quote escape or header explicitly we can bypass the automatic detection of this particular parameter select from read_csv_auto flights csv header true multiple files can be read at once by providing a glob or a list of files refer to the multiple files section for more information copy statement the copy statement can be used to load data from a csv file into a table this statement has the same syntax as the copy statement supported by postgresql for the copy statement we must first create a table with the correct schema to load the data into we then specify the csv file to load from plus any configuration options separately create table ontime flightdate date uniquecarrier varchar origincityname varchar destcityname varchar copy ontime from flights csv delimiter header select from ontime flightdate uniquecarrier origincityname destcityname ---------- ------------- ----------------- --------------- 1988-01-01 aa new york ny los angeles ca 1988-01-02 aa new york ny los angeles ca 1988-01-03 aa new york ny los angeles ca if we want to use the automatic format detection we can set auto_detect to true and omit the otherwise required configuration options create table ontime flightdate date uniquecarrier varchar origincityname varchar destcityname varchar copy ontime from flights csv auto_detect true select from ontime more on the copy statement can be found here",
			"category": "Csv",
			"url": "/docs/data/csv/overview",
			"blurb": "Examples -- read a CSV file from disk, auto-infer options SELECT * FROM 'flights.csv'; -- read_csv with custom..."
		},
		{
			"title": "CSV Loading Tips",
			"text": "below is a collection of tips to help when attempting to process especially gnarly csv files override the header flag if the header is not correctly detected if a file contains only string columns the header auto-detection might fail provide the header option to override this behavior select from read_csv_auto flights csv header true provide names if the file does not contain a header if the file does not contain a header names will be auto-generated by default you can provide your own names with the names option select from read_csv_auto flights csv names flightdate uniquecarrier override the types of specific columns the types flag can be used to override types of only certain columns by providing a struct of name - type mappings select from read_csv_auto flights csv types flightdate date use copy when loading data into a table the copy statement copies data directly into a table the csv reader uses the schema of the table instead of auto-detecting types from the file this speeds up the auto-detection and prevents mistakes from being made during auto-detection copy tbl from test csv auto_detect 1 use union_by_name when loading files with different schemas the union_by_name option can be used to unify the schema of files that have different or missing columns for files that do not have certain columns null values are filled in select from read_csv_auto flights csv union_by_name true",
			"category": "Csv",
			"url": "/docs/data/csv/tips",
			"blurb": "Below is a collection of tips to help when attempting to process especially gnarly CSV files. Override the header..."
		},
		{
			"title": "Call",
			"text": "the call statement invokes the given table function and returns the results examples -- invoke the duckdb_functions table function call duckdb_functions -- invoke the pragma_table_info table function call pragma_table_info pg_am syntax",
			"category": "Statements",
			"url": "/docs/sql/statements/call",
			"blurb": "The CALL statement invokes the given table function and returns the results. Examples -- Invoke the..."
		},
		{
			"title": "Case Statement",
			"text": "the case statement performs a switch based on a condition the basic form is identical to the ternary condition used in many programming languages case when cond then a else b end is equivalent to cond a b with a single condition this can be expressed with if cond a b create or replace table integers as select unnest 1 2 3 as i select i case when i 2 then 1 else 0 end as test from integers -- 1 2 3 -- 0 0 1 -- this is equivalent to select i if i 2 1 0 as test from integers -- 1 2 3 -- 0 0 1 the when cond then expr part of the case statement can be chained whenever any of the conditions returns true for a single tuple the corresponding expression is evaluated and returned create or replace table integers as select unnest 1 2 3 as i select i case when i 1 then 10 when i 2 then 20 else 0 end as test from integers -- 1 2 3 -- 10 20 0 the else part of the case statement is optional if no else statement is provided and none of the conditions match the case statement will return null create or replace table integers as select unnest 1 2 3 as i select i case when i 1 then 10 end as test from integers -- 1 2 3 -- 10 null null after the case but before the when an individual expression can also be provided when this is done the case statement is essentially transformed into a switch statement create or replace table integers as select unnest 1 2 3 as i select i case i when 1 then 10 when 2 then 20 when 3 then 30 end as test from integers -- 1 2 3 -- 10 20 30 -- this is equivalent to select i case when i 1 then 10 when i 2 then 20 when i 3 then 30 end as test from integers",
			"category": "Expressions",
			"url": "/docs/sql/expressions/case",
			"blurb": "The CASE statement performs a switch based on a condition. The basic form is identical to the ternary condition used..."
		},
		{
			"title": "Casting",
			"text": "casting refers to the process of changing the type of a row from one type to another the standard sql syntax for this is cast expr as typename duckdb also supports the easier to type shorthand expr typename which is also present in postgresql select cast i as varchar from generate_series 1 3 tbl i -- 1 2 3 select i double from generate_series 1 3 tbl i -- 1 0 2 0 3 0 select cast hello as integer -- conversion error could not convert string hello to int32 select try_cast hello as integer -- null the exact behavior of the cast depends on the source and destination types for example when casting from varchar to any other type the string will be attempted to be converted not all casts are possible for example it is not possible to convert an integer to a date casts may also throw errors when the cast could not be successfully performed for example trying trying to cast the string hello to an integer will result in an error being thrown try_cast can be used when the preferred behavior is not to throw an error but instead to return a null value try_cast will never throw an error and will instead return null if a cast is not possible implicit casting in many situations the system will add casts by itself this is called implicit casting this happens for example when a function is called with an argument that does not match the type of the function but can be casted to the desired type consider the function sin double this function takes as input argument a column of type double however it can be called with an integer as well sin 1 the integer is converted into a double before being passed to the sin function generally implicit casts only cast upwards that is to say we can implicitly cast an integer to a bigint but not the other way around",
			"category": "Expressions",
			"url": "/docs/sql/expressions/cast",
			"blurb": "Casting refers to the process of changing the type of a row from one type to another. The standard SQL syntax for..."
		},
		{
			"title": "Checkpoint",
			"text": "the checkpoint statement synchronizes data in the write-ahead log wal to the database data file for in-memory databases this statement will succeed with no effect examples -- synchronize data in the default database checkpoint -- synchronize data in the specified database checkpoint file_db -- abort any in-progress transactions to synchronize the data force checkpoint syntax checkpoint operations happen automatically based on the wal size see configuration this statement is for manual checkpoint actions behavior the default checkpoint command will fail if there are any running transactions including force will abort any transactions and execute the checkpoint operation also see the related pragma for further behavior modification",
			"category": "Statements",
			"url": "/docs/sql/statements/checkpoint",
			"blurb": "The CHECKPOINT statement synchronizes data in the write-ahead log (WAL) to the database data file. For in-memory..."
		},
		{
			"title": "Client APIs Overview",
			"text": "there are various client apis for duckdb duckdb s native api is c with official wrappers available for c python r java node js webassembly wasm odbc api julia and a command line interface cli there are also contributed third-party duckdb wrappers for c by giorgi common lisp by ak-coram crystal by amauryt go by marcboeker ruby by suketa rust by wangfenjin pages in this section",
			"category": "Api",
			"url": "/docs/api/overview",
			"blurb": "There are various client APIs for DuckDB. DuckDB's native API is C++ , with official wrappers available for C ,..."
		},
		{
			"title": "Collations",
			"text": "collations provide rules for how text should be sorted or compared in the execution engine collations are useful for localization as the rules for how text should be ordered are different for different languages or for different countries these orderings are often incompatible with one another for example in english the letter y comes between x and z however in lithuanian the letter y comes between the i and j for that reason different collations are supported the user must choose which collation they want to use when performing sorting and comparison operations by default the binary collation is used that means that strings are ordered and compared based only on their binary contents this makes sense for standard ascii characters i e the letters a-z and numbers 0-9 but generally does not make much sense for special unicode characters it is however by far the fastest method of performing ordering and comparisons hence it is recommended to stick with the binary collation unless required otherwise using collations in the stand-alone installation of duckdb three collations are included nocase noaccent and nfc the nocase collation compares characters as equal regardless of their casing the noaccent collation compares characters as equal regardless of their accents the nfc collation performs nfc-normalized comparisons see here for more information select hello hello -- false select hello collate nocase hello -- true select hello h\u00ebllo -- false select hello collate noaccent h\u00ebllo -- true collations can be combined by chaining them using the dot operator note however that not all collations can be combined together in general the nocase collation can be combined with any other collator but most other collations cannot be combined select hello collate nocase hell\u00f6 -- false select hello collate noaccent hell\u00f6 -- false select hello collate nocase noaccent hell\u00f6 -- true default collations the collations we have seen so far have all been specified per expression it is also possible to specify a default collator either on the global database level or on a base table column the pragma default_collation can be used to specify the global default collator this is the collator that will be used if no other one is specified pragma default_collation nocase select hello hello -- true collations can also be specified per-column when creating a table when that column is then used in a comparison the per-column collation is used to perform that comparison create table names name varchar collate noaccent insert into names values h\u00e4nnes select name from names where name hannes -- h\u00e4nnes be careful here however as different collations cannot be combined this can be problematic when you want to compare columns that have a different collation specified select name from names where name hannes collate nocase -- error cannot combine types with different collation create table other_names name varchar collate nocase insert into other_names values h\u00e4nnes select from names other_names where names name other_names name -- error cannot combine types with different collation -- need to manually overwrite the collation select from names other_names where names name collate noaccent nocase other_names name collate noaccent nocase -- h\u00e4nnes h\u00e4nnes icu collations the collations we have seen so far are not region dependent and do not follow any specific regional rules if you wish to follow the rules of a specific region or language you will need to use one of the icu collations for that you need to include the icu extension this can be found in the extension icu folder in the project using the c api the extension can be loaded as follows duckdb db db loadextension icuextension loading this extension will add a number of language and region specific collations to your database these can be queried using pragma collations command or by querying the pragma_collations function pragma collations select from pragma_collations -- af am ar as az be bg bn bo bs bs ca ceb chr cs cy da de de_at dsb dz ee el en en_us en_us eo es et fa fa_af fi fil fo fr fr_ca ga gl gu ha haw he he_il hi hr hsb hu hy id id_id ig is it ja ka kk kl km kn ko kok ku ky lb lkt ln lo lt lv mk ml mn mr ms mt my nb nb_no ne nl nn om or pa pa pa_in pl ps pt ro ru se si sk sl smn sq sr sr sr_ba sr_me sr_rs sr sr_ba sr_rs sv sw ta te th tk to tr ug uk ur uz vi wae wo xh yi yo zh zh zh_cn zh_sg zh zh_hk zh_mo zh_tw zu these collations can then be used as the other collations would be used before they can also be combined with the nocase collation for example to use the german collation rules you could use the following code snippet create table strings s varchar collate de insert into strings values gabel g\u00f6bel goethe goldmann g\u00f6the g\u00f6tz select from strings order by s -- gabel g\u00f6bel goethe goldmann g\u00f6the g\u00f6tz",
			"category": "Expressions",
			"url": "/docs/sql/expressions/collations",
			"blurb": "Collations provide rules for how text should be sorted or compared in the execution engine. Collations are useful for..."
		},
		{
			"title": "Combining Schemas",
			"text": "examples -- read a set of csv files combining columns by position select from read_csv_auto flights csv -- read a set of csv files combining columns by name select from read_csv_auto flights csv union_by_name true combining schemas when reading from multiple files we have to combine schemas from those files that is because each file has its own schema that can differ from the other files duckdb offers two ways of unifying schemas of multiple files by column position and by column name by default duckdb reads the schema of the first file provided and then unifies columns in subsequent files by column position this works correctly as long as all files have the same schema if the schema of the files differs you might want to use the union_by_name option to allow duckdb to construct the schema by reading all of the names instead below is an example of how both methods work union by position by default duckdb unifies the columns of these different files by position this means that the first column in each file is combined together as well as the second column in each file etc for example consider the following two files flights1 csv flightdate uniquecarrier origincityname destcityname 1988-01-01 aa new york ny los angeles ca 1988-01-02 aa new york ny los angeles ca flights2 csv flightdate uniquecarrier origincityname destcityname 1988-01-03 aa new york ny los angeles ca reading the two files at the same time will produce the following result set flightdate uniquecarrier origincityname destcityname ------------ --------------- ---------------- ----------------- 1988-01-01 aa new york ny los angeles ca 1988-01-02 aa new york ny los angeles ca 1988-01-03 aa new york ny los angeles ca this is equivalent to the sql construct union all union by name if you are processing multiple files that have different schemas perhaps because columns have been added or renamed it might be desirable to unify the columns of different files by name instead this can be done by providing the union_by_name option for example consider the following two files where flights2 csv has an extra column uniquecarrier flights1 csv flightdate origincityname destcityname 1988-01-01 new york ny los angeles ca 1988-01-02 new york ny los angeles ca flights2 csv flightdate uniquecarrier origincityname destcityname 1988-01-03 aa new york ny los angeles ca reading these when unifying column names by position results in an error - as the two files have a different number of columns when specifying the union_by_name option the columns are correctly unified and any missing values are set to null select from read_csv_auto flights1 csv flights2 csv union_by_name true flightdate origincityname destcityname uniquecarrier ------------ ---------------- ----------------- --------------- 1988-01-01 new york ny los angeles ca null 1988-01-02 new york ny los angeles ca null 1988-01-03 new york ny los angeles ca aa this is equivalent to the sql construct union all by name",
			"category": "Multiple Files",
			"url": "/docs/data/multiple_files/combining_schemas",
			"blurb": "Examples -- read a set of CSV files combining columns by position SELECT * FROM read_csv_auto('flights*.csv') -- read..."
		},
		{
			"title": "Comparisons",
			"text": "comparison operators the table below shows the standard comparison operators whenever either of the input arguments is null the output of the comparison is null operator description example result --- --- --- --- less than 2 3 true greater than 2 3 false less than or equal to 2 3 true greater than or equal to 4 null null equal null null null or not equal 2 2 false the table below shows the standard distinction operators these operators treat null values as equal operator description example result --- --- --- --- is distinct from equal including null 2 is distinct from null true is not distinct from not equal including null null is not distinct from null true between and is not null besides the standard comparison operators there are also the between and is not null operators these behave much like operators but have special syntax mandated by the sql standard they are shown in the table below note that between and not between are only equivalent to the examples below in the cases where both a x and y are of the same type as between will cast all of its inputs to the same type predicate description --- --- a between x and y equivalent to a x and a y a not between x and y equivalent to a x or a y expression is null true if expression is null false otherwise expression isnull alias for is null non-standard expression is not null false if expression is null true otherwise expression notnull alias for is not null non-standard",
			"category": "Expressions",
			"url": "/docs/sql/expressions/comparison_operators",
			"blurb": "Comparison Operators The table below shows the standard comparison operators. Whenever either of the input arguments..."
		},
		{
			"title": "Configuration",
			"text": "duckdb has a number of configuration options that can be used to change the behavior of the system the configuration options can be set using either the set statement or the pragma statement they can also be reset to their original values using the reset statement examples -- set the memory limit of the system to 10gb set memory_limit 10gb -- configure the system to use 1 thread set threads to 1 -- enable printing of a progress bar during long-running queries set enable_progress_bar true -- set the default null order to nulls last pragma default_null_order nulls_last -- show a list of all available settings select from duckdb_settings -- return the current value of a specific setting -- this example returns automatic select current_setting access_mode -- reset the memory limit of the system back to the default reset memory_limit configuration reference below is a list of all available settings name description input_type default_value ------------------------------------------ --------------------------------------------------------------------------------------------------------------------------------------------------------- ------------ -------------------------- calendar the current calendar varchar system locale calendar timezone the current time zone varchar system locale timezone access_mode access mode of the database automatic read_only or read_write varchar automatic allocator_flush_threshold peak allocation threshold at which to flush the allocator after completing a task varchar 134 2mb allow_unsigned_extensions allow to load extensions with invalid or missing signatures boolean false arrow_large_buffer_size if arrow buffers for strings blobs uuids and bits should be exported using large buffers boolean false binary_as_string in parquet files interpret binary data as a string boolean checkpoint_threshold wal_autocheckpoint the wal size threshold at which to automatically trigger a checkpoint e g 1gb varchar 16 7mb custom_extension_repository overrides the custom endpoint for remote extension installation varchar default_collation the collation setting used when none is specified varchar default_null_order null_order null ordering used when none is specified nulls_first or nulls_last varchar nulls_last default_order the order type used when none is specified asc or desc varchar asc disabled_filesystems disable specific file systems preventing access e g localfilesystem varchar enable_external_access allow the database to access external state through e g loading installing modules copy to from csv readers pandas replacement scans etc boolean true enable_fsst_vectors allow scans on fsst compressed segments to emit compressed vectors to utilize late decompression boolean false enable_http_metadata_cache whether or not the global http metadata is used to cache http metadata boolean false enable_object_cache whether or not object cache is used to cache e g parquet metadata boolean false enable_profiling enables profiling and sets the output format json query_tree query_tree_optimizer varchar null enable_progress_bar enables the progress bar printing progress to the terminal for long queries boolean false enable_progress_bar_print controls the printing of the progress bar when enable_progress_bar is true boolean true experimental_parallel_csv whether or not to use the experimental parallel csv reader boolean null explain_output output of explain statements all optimized_only physical_only varchar physical_only extension_directory set the directory to store extensions in varchar external_threads the number of external threads that work on duckdb tasks bigint 0 file_search_path a comma separated list of directories to search for input files varchar force_download forces upfront download of file boolean 0 home_directory sets the home directory used by the system varchar http_retries http retries on i o error default 3 ubigint 3 http_retry_backoff backoff factor for exponentially increasing retry wait time default 4 float 4 http_retry_wait_ms time between retries default 100ms ubigint 100 http_timeout http timeout read write connection retry default 30000ms ubigint 30000 immediate_transaction_mode whether transactions should be started lazily when needed or immediately when begin transaction is called boolean false integer_division whether or not the operator defaults to integer division or to floating point division boolean 0 lock_configuration whether or not the configuration can be altered boolean false log_query_path specifies the path to which queries should be logged default empty string queries are not logged varchar null max_expression_depth the maximum expression depth limit in the parser warning increasing this setting and using very deep expressions might lead to stack overflow errors ubigint 1000 max_memory memory_limit the maximum memory of the system e g 1gb varchar 75 of ram ordered_aggregate_threshold the number of rows to accumulate before sorting used for tuning ubigint 262144 password the password to use ignored for legacy compatibility varchar null perfect_ht_threshold threshold in bytes for when to use a perfect hash table default 12 bigint 12 pivot_filter_threshold the threshold to switch from using filtered aggregates to list with a dedicated pivot operator bigint 10 pivot_limit the maximum number of pivot columns in a pivot statement default 100000 bigint 100000 prefer_range_joins force use of range joins with mixed predicates boolean false preserve_identifier_case whether or not to preserve the identifier case instead of always lowercasing all non-quoted identifiers boolean true preserve_insertion_order whether or not to preserve insertion order if set to false the system is allowed to re-order any results that do not contain order by clauses boolean true profile_output profiling_output the file to which profile output should be saved or empty to print to the terminal varchar profiler_history_size sets the profiler history size bigint null profiling_mode the profiling mode standard or detailed varchar null progress_bar_time sets the time in milliseconds how long a query needs to take before we start printing a progress bar bigint 2000 s3_access_key_id s3 access key id varchar s3_endpoint s3 endpoint default s3 amazonaws com varchar s3 amazonaws com s3_region s3 region varchar s3_secret_access_key s3 access key varchar s3_session_token s3 session token varchar s3_uploader_max_filesize s3 uploader max filesize between 50gb and 5tb default 800gb varchar 800gb s3_uploader_max_parts_per_file s3 uploader max parts per file between 1 and 10000 default 10000 ubigint 10000 s3_uploader_thread_limit s3 uploader global thread limit default 50 ubigint 50 s3_url_compatibility_mode disable globs and query parameters on s3 urls boolean 0 s3_url_style s3 url style vhost default or path varchar vhost s3_use_ssl s3 use ssl default true boolean 1 schema sets the default search schema equivalent to setting search_path to a single value varchar main search_path sets the default search search path as a comma-separated list of values varchar temp_directory set the directory to which to write temp files varchar threads worker_threads the number of total threads used by the system bigint cores username user the username to use ignored for legacy compatibility varchar null",
			"category": "SQL",
			"url": "/docs/sql/configuration",
			"blurb": "DuckDB has a number of configuration options that can be used to change the behavior of the system. The configuration..."
		},
		{
			"title": "Connect",
			"text": "connect or create a database to use duckdb you must first create a connection to a database the exact process varies by client most clients take a parameter pointing to a database file to read and write from the file extension may be anything e g db duckdb etc if the database file does not exist it will be created the special value memory can be used to create an in-memory database where no data is persisted to disk i e all data is lost when you exit the process see the api docs for client-specific details",
			"category": "Docs",
			"url": "/docs/connect",
			"blurb": "Connect or Create a Database To use DuckDB, you must first create a connection to a database. The exact process..."
		},
		{
			"title": "Constraints",
			"text": "in sql constraints can be specified for tables constraints enforce certain properties over data that is inserted into a table constraints can be specified along with the schema of the table as part of the create table statement in certain cases constraints can also be added to a table using the alter table statement but this is not currently supported for all constraints syntax check check constraints allow you to specify an arbitrary boolean expression any columns that do not satisfy this expression violate the constraint for example we could enforce that the name column does not contain spaces using the following check constraint create table students name varchar check not contains name insert into students values this name contains spaces -- constraint error check constraint failed students not null a not-null constraint specifies that the column cannot contain any null values by default all columns in tables are nullable adding not null to a column definition enforces that a column cannot contain null values create table students name varchar not null insert into students values null -- constraint error not null constraint failed students name primary key unique primary key or unique constraints define a column or set of columns that are a unique identifier for a row in the table the constraint enforces that the specified columns are unique within a table i e that at most one row contains the given values for the set of columns create table students id integer primary key name varchar insert into students values 1 student 1 insert into students values 1 student 2 -- constraint error duplicate key id 1 violates primary key constraint in order to enforce this property efficiently an art index is automatically created for every primary key or unique constraint that is defined in the table primary key constraints and unique constraints are identical except for two points a table can only have one primary key constraint defined but many unique constraints a primary key constraint also enforces the keys to not be null indexes have certain limitations that might result in constraints being evaluated too eagerly see the indexes section for more details foreign key foreign keys define a column or set of columns that refer to a primary key or unique constraint from another table the constraint enforces that the key exists in the other table create table students id integer primary key name varchar create table exams exam_id integer references students id grade integer insert into students values 1 student 1 insert into exams values 1 10 insert into exams values 2 10 -- constraint error violates foreign key constraint because key id 2 does not exist in the referenced table in order to enforce this property efficiently an art index is automatically created for every foreign key constraint that is defined in the table indexes have certain limitations that might result in constraints being evaluated too eagerly see the indexes section for more details",
			"category": "SQL",
			"url": "/docs/sql/constraints",
			"blurb": "In SQL, constraints can be specified for tables. Constraints enforce certain properties over data that is inserted..."
		},
		{
			"title": "Copy",
			"text": "examples -- read a csv file into the lineitem table - using auto-detected options copy lineitem from lineitem csv auto_detect true -- read a parquet file into the lineitem table copy lineitem from lineitem pq format parquet -- read a json file into the lineitem table - using auto-detected options copy lineitem from lineitem json format json auto_detect true -- write a table to a csv file copy lineitem to lineitem csv format csv delimiter header -- write the result of a query to a parquet file copy select l_orderkey l_partkey from lineitem to lineitem parquet compression zstd copy statements copy moves data between duckdb and external files copy from imports data into duckdb from an external file copy to writes data from duckdb to an external file the copy command can be used for csv parquet and json files copy from copy from imports data from an external file into an existing table the data is appended to whatever data is in the table already the amount of columns inside the file must match the amount of columns in the table table_name and the contents of the columns must be convertible to the column types of the table in case this is not possible an error will be thrown if a list of columns is specified copy will only copy the data in the specified columns from the file if there are any columns in the table that are not in the column list copy from will insert the default values for those columns -- copy the contents of a comma-separated file test csv without a header into the table test copy test from test csv -- copy the contents of a comma-separated file with a header into the category table copy category from categories csv header -- copy the contents of lineitem tbl into the lineitem table where the contents are delimited by a pipe character copy lineitem from lineitem tbl delimiter -- copy the contents of lineitem tbl into the lineitem table where the delimiter quote character and presence of a header are automatically detected copy lineitem from lineitem tbl auto_detect true -- read the contents of a comma-separated file names csv into the name column of the category table any other columns of this table are filled with their default value copy category name from names csv -- read the contents of a parquet file lineitem parquet into the lineitem table copy lineitem from lineitem parquet format parquet -- read the contents of a newline-delimited json file lineitem ndjson into the lineitem table copy lineitem from lineitem ndjson format json -- read the contents of a json file lineitem json into the lineitem table copy lineitem from lineitem json format json array true syntax copy to copy to exports data from duckdb to an external csv or parquet file it has mostly the same set of options as copy from however in the case of copy to the options specify how the file should be written to disk any file created by copy to can be copied back into the database by using copy from with a similar set of options the copy to function can be called specifying either a table name or a query when a table name is specified the contents of the entire table will be written into the resulting file when a query is specified the query is executed and the result of the query is written to the resulting file -- copy the contents of the lineitem table to the file lineitem tbl where the columns are delimited by a pipe character including a header line copy lineitem to lineitem tbl delimiter header -- copy the l_orderkey column of the lineitem table to the file orderkey tbl copy lineitem l_orderkey to orderkey tbl delimiter -- copy the result of a query to the file query csv including a header with column names copy select 42 as a hello as b to query csv with header 1 delimiter -- copy the result of a query to the parquet file query parquet copy select 42 as a hello as b to query parquet format parquet -- copy the result of a query to the newline-delimited json file query ndjson copy select 42 as a hello as b to query ndjson format json -- copy the result of a query to the json file query json copy select 42 as a hello as b to query json format json array true syntax copy options zero or more copy options may be provided as a part of the copy operation the with specifier is optional but if any options are specified the parentheses are required parameter values can be passed in with or without wrapping in single quotes any option that is a boolean can be enabled or disabled in multiple ways you can write true on or 1 to enable the option and false off or 0 to disable it the boolean value can also be omitted in which case true is assumed the below options are applicable to all formats written with copy name description type default --- --- ---- ---- allow_overwrite whether or not to allow overwriting a directory if one already exists only has an effect when used with partition_by bool false format specifies the copy function to use the default is selected from the file extension e g parquet results in a parquet file being written read if the file extension is unknown csv is selected available options are csv parquet and json varchar auto partition_by the columns to partition by using a hive partitioning scheme see the partitioned writes section varchar empty per_thread_output generate one file per thread rather than one file in total this allows for faster parallel writing bool false use_tmp_file whether or not to write to a temporary file first if the original file exists target csv tmp this prevents overwriting an existing file with a broken file in case the writing is cancelled bool auto csv options the below options are applicable when writing csv files name description type default --- --- ---- ---- compression the compression type for the file by default this will be detected automatically from the file extension e g file csv gz will use gzip file csv will use none options are none gzip zstd varchar auto force_quote the list of columns to always add quotes to even if not required varchar dateformat specifies the date format to use when writing dates see date format varchar empty delim or sep the character that is written to separate columns within each row varchar escape the character that should appear before a character that matches the quote value varchar header whether or not to write a header for the csv file bool false nullstr the string that is written to represent a null value varchar empty quote the quoting character to be used when a data value is quoted varchar timestampformat specifies the date format to use when writing timestamps see date format varchar empty parquet options the below options are applicable when writing parquet files name description type default --- --- ---- ---- compression the compression format to use uncompressed snappy gzip or zstd varchar snappy row_group_size the target size i e number of rows of each row-group bigint 122880 row_group_size_bytes the target size of each row-group you can pass either a human-readable string e g 2mb or an integer i e the number of bytes this option is only used when you have issued set preserve_insertion_order false otherwise it is ignored bigint row_group_size 1024 field_ids the field_id for each column pass auto to attempt to infer automatically struct empty some examples of field_ids are -- assign field_ids automatically copy select 128 as i to my parquet field_ids auto -- sets the field_id of column i to 42 copy select 128 as i to my parquet field_ids i 42 -- sets the field_id of column i to 42 and column j to 43 copy select 128 as i 256 as j to my parquet field_ids i 42 j 43 -- sets the field_id of column my_struct to 43 -- and column i nested inside my_struct to 43 copy select i 128 as my_struct to my parquet field_ids my_struct __duckdb_field_id 42 i 43 -- sets the field_id of column my_list to 42 -- and column element default name of list child to 43 copy select 128 256 as my_list to my parquet field_ids my_list __duckdb_field_id 42 element 43 -- sets the field_id of colum my_map to 42 -- and columns key and value default names of map children to 43 and 44 copy select map key1 128 key2 256 my_map to my parquet field_ids my_map __duckdb_field_id 42 key 43 value 44 json options the below options are applicable when writing json files name description type default --- --- ---- ---- compression the compression type for the file by default this will be detected automatically from the file extension e g file csv gz will use gzip file csv will use none options are none gzip zstd varchar auto dateformat specifies the date format to use when writing dates see date format varchar empty timestampformat specifies the date format to use when writing timestamps see date format varchar empty array whether to write a json array if true a json array of records is written if false newline-delimited json is written bool false",
			"category": "Statements",
			"url": "/docs/sql/statements/copy",
			"blurb": "Examples -- read a CSV file into the lineitem table - using auto-detected options COPY lineitem FROM 'lineitem.csv'..."
		},
		{
			"title": "Create Macro",
			"text": "the create macro statement can create a scalar or table macro function in the catalog a macro may only be a single select statement similar to a view but it has the benefit of accepting parameters for a scalar macro create macro is followed by the name of the macro and optionally parameters within a set of parentheses the keyword as is next followed by the text of the macro by design a scalar macro may only return a single value for a table macro the syntax is similar to a scalar macro except as is replaced with as table a table macro may return a table of arbitrary size and shape if a macro is temporary it is only usable within the same database connection and is deleted when the connection is closed examples -- create a macro that adds two expressions a and b create macro add a b as a b -- create a macro for a case expression create macro ifelse a b c as case when a then b else c end -- create a macro that does a subquery create macro one as select 1 -- create a macro with a common table expression -- parameter names get priority over column names disambiguate using the table name create macro plus_one a as with cte as select 1 as a select cte a a from cte -- macro s are schema-dependent and have an alias function create function main myavg x as sum x count x -- create a macro with default constant parameters create macro add_default a b 5 as a b -- create a macro arr_append with a functionality equivalent to array_append create macro arr_append l e as list_concat l list_value e -- table macros -- create a table macro without parameters create macro static_table as table select hello as column1 world as column2 -- create a table macro with parameters that can be of any type create macro dynamic_table col1_value col2_value as table select col1_value as column1 col2_value as column2 -- create a table macro that returns multiple rows -- it will be replaced if it already exists and it is temporary will be automatically deleted when the connection ends create or replace temp macro dynamic_table col1_value col2_value as table select col1_value as column1 col2_value as column2 union all select hello as col1_value 456 as col2_value syntax macros allow you to create shortcuts for combinations of expressions -- failure cannot find column b create macro add a as a b -- this works create macro add a b as a b -- error cannot bind varchar integer select add hello 3 -- success select add 1 2 -- 3 macro s can have default parameters -- b is a default parameter create macro add_default a b 5 as a b -- the following will result in 42 select add_default 37 -- error add_default only has one positional parameter select add_default 40 2 -- success default parameters are used by assigning them like so select add_default 40 b 2 -- error default parameters must come after positional parameters select add_default b 2 40 -- the order of default parameters does not matter create macro triple_add a b 5 c 10 as a b c -- success select triple_add 40 c 1 b 1 -- 42 when macro s are used they are expanded i e replaced with the original expression and the parameters within the expanded expression are replaced with the supplied arguments step by step -- the add macro we defined above is used in a query select add 40 2 -- internally add is replaced with its definition of a b select a b -- then the parameters are replaced by the supplied arguments select 40 2 -- 42",
			"category": "Statements",
			"url": "/docs/sql/statements/create_macro",
			"blurb": "The CREATE MACRO statement can create a scalar or table macro (function) in the catalog. A macro may only be a single..."
		},
		{
			"title": "Create Schema",
			"text": "the create schema statement creates a schema in the catalog the default schema is main examples -- create a schema create schema s1 -- create a schema if it does not exist yet create schema if not exists s2 -- create table in the schemas create table s1 t id integer primary key other_id integer create table s2 t id integer primary key j varchar -- compute a join between tables from two schemas select from s1 t s1t s2 t s2t where s1t other_id s2t id syntax",
			"category": "Statements",
			"url": "/docs/sql/statements/create_schema",
			"blurb": "The CREATE SCHEMA statement creates a schema in the catalog. The default schema is main . Examples -- create a schema..."
		},
		{
			"title": "Create Sequence",
			"text": "the create sequence statement creates a new sequence number generator examples create an ascending sequence called serial starting at 101 create sequence serial start 101 select the next number from this sequence select nextval serial nextval --------- 101 optionally you may also view the current number from this sequence note that the nextval function must have already been called before calling currval select currval serial currval --------- 101 use this sequence in an insert command insert into distributors values nextval serial nothing syntax create sequence creates a new sequence number generator if a schema name is given then the sequence is created in the specified schema otherwise it is created in the current schema temporary sequences exist in a special schema so a schema name may not be given when creating a temporary sequence the sequence name must be distinct from the name of any other sequence in the same schema after a sequence is created you use the function nextval to operate on the sequence parameters name description --- --- temporary or temp if specified the sequence object is created only for this session and is automatically dropped on session exit existing permanent sequences with the same name are not visible in this session while the temporary sequence exists unless they are referenced with schema-qualified names name the name optionally schema-qualified of the sequence to be created increment the optional clause increment by increment specifies which value is added to the current sequence value to create a new value a positive value will make an ascending sequence a negative one a descending sequence the default value is 1 minvalue the optional clause minvalue minvalue determines the minimum value a sequence can generate if this clause is not supplied or no minvalue is specified then defaults will be used the defaults are 1 and - 2 63 - 1 for ascending and descending sequences respectively maxvalue the optional clause maxvalue maxvalue determines the maximum value for the sequence if this clause is not supplied or no maxvalue is specified then default values will be used the defaults are 2 63 - 1 and -1 for ascending and descending sequences respectively start the optional clause start with start allows the sequence to begin anywhere the default starting value is minvalue for ascending sequences and maxvalue for descending ones cycle or no cycle the cycle option allows the sequence to wrap around when the maxvalue or minvalue has been reached by an ascending or descending sequence respectively if the limit is reached the next number generated will be the minvalue or maxvalue respectively if no cycle is specified any calls to nextval after the sequence has reached its maximum value will return an error if neither cycle or no cycle are specified no cycle is the default use drop sequence to remove a sequence sequences are based on bigint arithmetic so the range cannot exceed the range of an eight-byte integer -9223372036854775808 to 9223372036854775807",
			"category": "Statements",
			"url": "/docs/sql/statements/create_sequence",
			"blurb": "The CREATE SEQUENCE statement creates a new sequence number generator. Examples Create an ascending sequence called..."
		},
		{
			"title": "Create Table",
			"text": "the create table statement creates a table in the catalog examples -- create a table with two integer columns i and j create table t1 i integer j integer -- create a table with a primary key create table t1 id integer primary key j varchar -- create a table with a composite primary key create table t1 id integer j varchar primary key id j -- create a table with various different types and constraints create table t1 i integer not null decimalnr double check decimalnr 10 date date unique time timestamp -- create a table from the result of a query create table t1 as select 42 as i 84 as j -- create a table from a csv file using auto-detect i e automatically detecting column names and types create table t1 as select from read_csv_auto path file csv -- we can use the from-first syntax to omit select create table t1 as from read_csv_auto path file csv temporary tables temporary tables can be created using a create temp table statement see diagram below temporary tables are session scoped similar to postgres for example meaning that only the specific connection that created them can access them and once the connection to duckdb is closed they will be automatically dropped temporary tables reside in memory rather than on disk even when connecting to a persistent duckdb but if the temp_directory configuration is set when connecting or with a set command data will be spilled to disk if memory becomes constrained -- create a temporary table from a csv file using auto-detect i e automatically detecting column names and types create temp table t1 as select from read_csv_auto path file csv -- allow temporary tables to off-load excess memory to disk set temp_directory path to directory create or replace the create or replace syntax allows a new table to be created or for an existing table to be overwritten by the new table this is shorthand for dropping the existing table and then creating the new one -- create a table with two integer columns i and j even if t1 already exists create or replace table t1 i integer j integer if not exists the if not exists syntax will only proceed with the creation of the table if it does not already exist if the table already exists no action will be taken and the existing table will remain in the database -- create a table with two integer columns i and j only if t1 does not exist yet create table if not exists t1 i integer j integer check constraints a check constraint is an expression that must be satisfied by the values of every row in the table create table t1 id integer primary key percentage integer check 0 percentage and percentage 100 insert into t1 values 1 5 insert into t1 values 2 -1 -- error constraint error check constraint failed t1 insert into t1 values 3 101 -- error constraint error check constraint failed t1 create table t2 id integer primary key x integer y integer check x y insert into t2 values 1 5 10 insert into t2 values 2 5 3 -- error constraint error check constraint failed t2 check constraints can also be added as part of the constraints clause create table t3 id integer primary key x integer y integer constraint x_smaller_than_y check x y insert into t3 values 1 5 10 insert into t3 values 2 5 3 -- error constraint error check constraint failed t3 foreign key constraints a foreign key is a column or set of columns that references another table s primary key foreign keys check referential integrity i e the referred primary key must exist in the other table upon insertion create table t1 id integer primary key j varchar create table t2 id integer primary key t1_id integer foreign key t1_id references t1 id -- example insert into t1 values 1 a insert into t2 values 1 1 insert into t2 values 2 2 -- error constraint error violates foreign key constraint because key id 2 does not exist in the referenced table foreign keys can be defined on composite primary keys create table t3 id integer j varchar primary key id j create table t4 id integer primary key t3_id integer t3_j varchar foreign key t3_id t3_j references t3 id j -- example insert into t3 values 1 a insert into t4 values 1 1 a insert into t4 values 2 1 b -- error constraint error violates foreign key constraint because key id 1 j b does not exist in the referenced table foreign keys can also be defined on unique columns create table t5 id integer unique j varchar create table t6 id integer primary key t5_id integer foreign key t5_id references t5 id foreign keys with cascading deletes on delete cascade can be defined in create table statements but they are not enforced i e an attempt to delete a row that has a foreign key pointing to it will result in a constraint error generated columns the type generated always as expr virtual stored syntax will create a generated column the data in this kind of column is generated from its expression which can reference other regular or generated columns of the table since they are produced by calculations these columns can not be inserted into directly duckdb can infer the type of the generated column based on the expression s return type this allows you to leave out the type when declaring a generated column it is possible to explicitly set a type but insertions into the referenced columns might fail if the type can not be cast to the type of the generated column generated columns come in two varieties virtual and stored the data of virtual generated columns is not stored on disk instead it is computed from the expression every time the column is referenced through a select statement the data of stored generated columns is stored on disk and is computed every time the data of their dependencies change through an insert update drop statement currently only the virtual kind is supported and it is also the default option if the last field is left blank -- the simplest syntax for a generated column -- the type is derived from the expression and the variant defaults to virtual create table t1 x float two_x as 2 x -- fully specifying the same generated column for completeness create table t1 x float two_x float generated always as 2 x virtual syntax",
			"category": "Statements",
			"url": "/docs/sql/statements/create_table",
			"blurb": "The CREATE TABLE statement creates a table in the catalog. Examples -- create a table with two integer columns (i and..."
		},
		{
			"title": "Create Type",
			"text": "the create type statement defines a new type in the catalog examples -- create a simple enum type create type mood as enum happy sad curious -- create a simple struct type create type many_things as struct k integer l varchar -- create a simple union type create type one_thing as union number integer string varchar -- create a type alias create type x_index as integer syntax create type defines a new data type available to this duckdb instance these new types can then be inspected in the duckdb_types table extending these custom types to support custom operators such as the postgres operator would require c development but that can be done in an extension",
			"category": "Statements",
			"url": "/docs/sql/statements/create_type",
			"blurb": "The CREATE Type statement defines a new type in the catalog. Examples -- create a simple enum type CREATE TYPE mood..."
		},
		{
			"title": "Create View",
			"text": "the create view statement defines a new view in the catalog examples -- create a simple view create view v1 as select from tbl -- create a view or replace it if a view with that name already exists create or replace view v1 as select 42 -- create a view and replace the column names create view v1 a as select 42 syntax create view defines a view of a query the view is not physically materialized instead the query is run every time the view is referenced in a query create or replace view is similar but if a view of the same name already exists it is replaced if a schema name is given then the view is created in the specified schema otherwise it is created in the current schema temporary views exist in a special schema so a schema name cannot be given when creating a temporary view the name of the view must be distinct from the name of any other view or table in the same schema",
			"category": "Statements",
			"url": "/docs/sql/statements/create_view",
			"blurb": "The CREATE VIEW statement defines a new view in the catalog. Examples -- create a simple view CREATE VIEW v1 AS..."
		},
		{
			"title": "DBeaver SQL IDE",
			"text": "dbeaver is a powerful and popular desktop sql editor and integrated development environment ide it has both an open source and enterprise version it is useful for visually inspecting the available tables in duckdb and for quickly building complex queries duckdb s jdbc connector allows dbeaver to query duckdb files and by extension any other files that duckdb can access like parquet files install dbeaver using the download links and instructions found at their download page open dbeaver and create a new connection either click on the new database connector button or go to database new database connection in the menu bar img src images guides dbeaver_new_database_connection png alt dbeaver new database connection title dbeaver new database connection img src images guides dbeaver_new_database_connection_menu png alt dbeaver new database connection menu title dbeaver new database connection menu search for duckdb select it and click next img src images guides dbeaver_select_database_driver png alt dbeaver select database driver title dbeaver select database driver enter the path or browse to the duckdb database file you wish to query to use an in-memory duckdb useful primarily if just interested in querying parquet files or for testing enter memory as the path img src images guides dbeaver_connection_settings_path png alt dbeaver set path title dbeaver set path click test connection this will then prompt you to install the duckdb jdbc driver if you are not prompted see alternative driver installation instructions below img src images guides dbeaver_connection_settings_test_connection png alt dbeaver test connection title dbeaver test connection click download to download duckdb s jdbc driver from maven once download is complete click ok then click finish note if you are in a corporate environment or behind a firewall before clicking download click the download configuration link to configure your proxy settings img src images guides dbeaver_download_driver_files png alt dbeaver download driver files title dbeaver download driver files you should now see a database connection to your duckdb database in the left hand database navigator pane expand it to see the tables and views in your database right click on that connection and create a new sql script img src images guides dbeaver_new_sql_script png alt dbeaver new sql script title dbeaver new sql script write some sql and click the execute button img src images guides dbeaver_execute_query png alt dbeaver execute query title dbeaver execute query now you re ready to fly with duckdb and dbeaver img src images guides dbeaver_query_results png alt dbeaver query results title dbeaver query results alternative driver installation if not prompted to install the duckdb driver when testing your connection return to the connect to a database dialog and click edit driver settings img src images guides dbeaver_edit_driver_settings png alt dbeaver edit driver settings title dbeaver edit driver settings alternate you may also access the driver settings menu by returning to the main dbeaver window and clicking database driver manager in the menu bar then select duckdb then click edit img src images guides dbeaver_driver_manager png alt dbeaver driver manager title dbeaver driver manager img src images guides dbeaver_driver_manager_edit png alt dbeaver driver manager edit title dbeaver driver manager edit go to the libraries tab then click on the duckdb driver and click download update if you do not see the duckdb driver first click on reset to defaults img src images guides dbeaver_edit_driver_duckdb png alt dbeaver edit driver title dbeaver edit driver click download to download duckdb s jdbc driver from maven once download is complete click ok then return to the main dbeaver window and continue with step 7 above note if you are in a corporate environment or behind a firewall before clicking download click the download configuration link to configure your proxy settings img src images guides dbeaver_download_driver_files_from_driver_settings png alt dbeaver download driver files 2 title dbeaver download driver files 2",
			"category": "Sql Editors",
			"url": "/docs/guides/sql_editors/dbeaver",
			"blurb": "DBeaver is a powerful and popular desktop sql editor and integrated development environment (IDE). It has both an..."
		},
		{
			"title": "Data Ingestion",
			"text": "csv files csv files can be read using the read_csv function called either from within python or directly from within sql by default the read_csv function attempts to auto-detect the csv settings by sampling from the provided file import duckdb read from a file using fully auto-detected settings duckdb read_csv example csv read multiple csv files from a folder duckdb read_csv folder csv specify options on how the csv is formatted internally duckdb read_csv example csv header false sep override types of the first two columns duckdb read_csv example csv dtype int varchar use the experimental parallel csv reader duckdb read_csv example csv parallel true directly read a csv file from within sql duckdb sql select from example csv call read_csv from within sql duckdb sql select from read_csv_auto example csv see the csv loading page for more information parquet files parquet files can be read using the read_parquet function called either from within python or directly from within sql import duckdb read from a single parquet file duckdb read_parquet example parquet read multiple parquet files from a folder duckdb read_parquet folder parquet directly read a parquet file from within sql duckdb sql select from example parquet call read_parquet from within sql duckdb sql select from read_parquet example parquet see the parquet loading page for more information json files json files can be read using the read_json function called either from within python or directly from within sql by default the read_json function will automatically detect if a file contains newline-delimited json or regular json and will detect the schema of the objects stored within the json file import duckdb read from a single json file duckdb read_json example json read multiple json files from a folder duckdb read_json folder json directly read a json file from within sql duckdb sql select from example json call read_json from within sql duckdb sql select from read_json_auto example json dataframes arrow tables duckdb is automatically able to query a pandas dataframe polars dataframe or arrow object that is stored in a python variable by name duckdb supports querying multiple types of apache arrow objects including tables datasets recordbatchreaders and scanners see the python guides for more examples import duckdb import pandas as pd test_df pd dataframe from_dict i 1 2 3 4 j one two three four duckdb sql select from test_df fetchall 1 one 2 two 3 three 4 four duckdb also supports registering a dataframe or arrow object as a virtual table comparable to a sql view this is useful when querying a dataframe arrow object that is stored in another way as a class variable or a value in a dictionary below is a pandas example if your pandas dataframe is stored in another location here is an example of manually registering it import duckdb import pandas as pd my_dictionary my_dictionary test_df pd dataframe from_dict i 1 2 3 4 j one two three four duckdb register test_df_view my_dictionary test_df duckdb sql select from test_df_view fetchall 1 one 2 two 3 three 4 four you can also create a persistent table in duckdb from the contents of the dataframe or the view create a new table from the contents of a dataframe con execute create table test_df_table as select from test_df insert into an existing table from the contents of a dataframe con execute insert into test_df_table select from test_df pandas dataframes - object columns pandas dataframe columns of an object dtype require some special care since this stores values of arbitrary type to convert these columns to duckdb we first go through an analyze phase before converting the values in this analyze phase a sample of all the rows of the column are analyzed to determine the target type this sample size is by default set to 1000 if the type picked during the analyze step is wrong this will result in a failed to cast value error in which case you will need to increase the sample size the sample size can be changed by setting the pandas_analyze_sample config option example setting the sample size to 100000 duckdb default_connection execute set global pandas_analyze_sample 100000",
			"category": "Python",
			"url": "/docs/api/python/data_ingestion",
			"blurb": "CSV Files CSV files can be read using the read_csv function, called either from within Python or directly from within..."
		},
		{
			"title": "Data Ingestion",
			"text": "duckdb-wasm has multiple ways to import data depending on the format of the data there are two steps to import data into duckdb first the data file is imported into a local file system using register functions registeremptyfilebuffer registerfilebuffer registerfilehandle registerfiletext registerfileurl then the data file is imported into duckdb using insert functions insertarrowfromipcstream insertarrowtable insertcsvfrompath insertjsonfrompath or directly using from sql query using extensions like parquet or httpfs insert statements can also be used to import data data import open close connection create a new connection const c await db connect import data close the connection to release memory await c close apache arrow data can be inserted from an existing arrow table more example https arrow apache org docs js import tablefromarrays from apache-arrow const arrowtable tablefromarrays id 1 2 3 name john jane jack age 20 21 22 await c insertarrowtable arrowtable name arrow_table from a raw arrow ipc stream const streamresponse await fetch someapi const streamreader streamresponse body getreader const streaminserts while true const value done await streamreader read if done break streaminserts push c insertarrowfromipcstream value name streamed await promise all streaminserts csv from csv files interchangeable registerfile text buffer url handle const csvcontent 1 foo n2 bar n await db registerfiletext data csv csvcontent with typed insert options await db insertcsvfrompath data csv schema main name foo detect false header false delimiter columns col1 new arrow int32 col2 new arrow utf8 json from json documents in row-major format const jsonrowcontent col1 1 col2 foo col1 2 col2 bar await db registerfiletext rows json json stringify jsonrowcontent await c insertjsonfrompath rows json name rows or column-major format const jsoncolcontent col1 1 2 col2 foo bar await db registerfiletext columns json json stringify jsoncolcontent await c insertjsonfrompath columns json name columns from api const streamresponse await fetch someapi content json await db registerfilebuffer file json new uint8array await streamresponse arraybuffer await c insertjsonfrompath file json name jsoncontent parquet from parquet files local const pickedfile file letuserpickfile await db registerfilehandle local parquet pickedfile duckdbdataprotocol browser_filereader true remote await db registerfileurl remote parquet https origin remote parquet duckdbdataprotocol http false using fetch const res await fetch https origin remote parquet await db registerfilebuffer buffer parquet new uint8array await res arraybuffer by specifying urls in the sql text await c query create table direct as select from https origin remote parquet or by executing raw insert statements await c query insert into existing_table values 1 foo 2 bar httpfs by specifying urls in the sql text await c query create table direct as select from https origin remote parquet insert statement or by executing raw insert statements await c query insert into existing_table values 1 foo 2 bar",
			"category": "Wasm",
			"url": "/docs/api/wasm/data_ingestion",
			"blurb": "DuckDB-Wasm has multiple ways to import data, depending on the format of the data. There are two steps to import data..."
		},
		{
			"title": "Data Types",
			"text": "general-purpose data types the table below shows all the built-in general-purpose data types the alternatives listed in the aliases column can be used to refer to these types as well however note that the aliases are not part of the sql standard and hence might not be accepted by other database engines name aliases description --- --- --- bigint int8 long signed eight-byte integer bit bitstring string of 1 s and 0 s boolean bool logical logical boolean true false blob bytea binary varbinary variable-length binary data date calendar date year month day double float8 numeric decimal double precision floating-point number 8 bytes decimal prec scale fixed-precision number with the given width precision and scale hugeint signed sixteen-byte integer integer int4 int signed signed four-byte integer interval date time delta real float4 float single precision floating-point number 4 bytes smallint int2 short signed two-byte integer time time of day no time zone timestamp datetime combination of time and date timestamp with time zone timestamptz combination of time and date that uses the current time zone tinyint int1 signed one-byte integer ubigint unsigned eight-byte integer uinteger unsigned four-byte integer usmallint unsigned two-byte integer utinyint unsigned one-byte integer uuid uuid data type varchar char bpchar text string variable-length character string nested composite types duckdb supports four nested data types list struct map and union each supports different use cases and has a different structure name description rules when used in a column build from values define in ddl create --- --- --- --- --- list an ordered sequence of data values of the same type each row must have the same data type within each list but can have any number of elements 1 2 3 int struct a dictionary of multiple named values where each key is a string but the value can be a different type for each key each row must have the same keys i 42 j a struct i int j varchar map a dictionary of multiple named values each key having the same type and each value having the same type keys and values can be any type and can be different types from one another rows may have different keys map 1 2 a b map int varchar union a union of multiple alternative data types storing one of them in each value at a time a union also contains a discriminator tag value to inspect and access the currently set member type rows may be set to different member types of the union union_value num 2 union num int text varchar nesting list s struct s map s and union s can be arbitrarily nested to any depth so long as the type rules are observed -- struct with lists select birds duck goose heron aliens null amphibians frog toad -- struct with list of maps select test map 1 5 42 1 45 map 1 5 42 1 45 -- a list of unions select union_value num 2 union_value str abc union str varchar num integer links to detailed documentation",
			"category": "Data Types",
			"url": "/docs/sql/data_types/overview",
			"blurb": "The table below shows all the built-in general-purpose data types."
		},
		{
			"title": "Date Format",
			"text": "the strftime and strptime functions can be used to convert between dates timestamps and strings this is often required when parsing csv files displaying output to the user or transferring information between programs because there are many possible date representations these functions accept a format string that describes how the date or timestamp should be structured strftime examples strftime timestamp format converts timestamps or dates to strings according to the specified pattern select strftime date 1992-03-02 d m y -- 02 03 1992 select strftime timestamp 1992-03-02 20 32 45 a -d b y - i m s p -- monday 2 march 1992 - 08 32 45 pm strptime examples strptime string format converts strings to timestamps according to the specified pattern select strptime 02 03 1992 d m y -- 1992-03-02 00 00 00 select strptime monday 2 march 1992 - 08 32 45 pm a -d b y - i m s p -- 1992-03-02 20 32 45 csv parsing the date formats can also be specified during csv parsing either in the copy statement or in the read_csv function this can be done by either specifying a dateformat or a timestampformat or both dateformat will be used for converting dates and timestampformat will be used for converting timestamps below are some examples for how to use this -- in copy statement copy dates from test csv dateformat d m y timestampformat a -d b y - i m s p -- in read_csv function select from read_csv test csv dateformat m d y format specifiers below is a full list of all available format specifiers specifier description example --- --- --- --- a abbreviated weekday name sun mon a full weekday name sunday monday w weekday as a decimal number 0 1 6 d day of the month as a zero-padded decimal 01 02 31 -d day of the month as a decimal number 1 2 30 b abbreviated month name jan feb dec b full month name january february m month as a zero-padded decimal number 01 02 12 -m month as a decimal number 1 2 12 y year without century as a zero-padded decimal number 00 01 99 -y year without century as a decimal number 0 1 99 y year with century as a decimal number 2013 2019 etc h hour 24-hour clock as a zero-padded decimal number 00 01 23 -h hour 24-hour clock as a decimal number 0 1 23 i hour 12-hour clock as a zero-padded decimal number 01 02 12 -i hour 12-hour clock as a decimal number 1 2 12 p locale s am or pm am pm m minute as a zero-padded decimal number 00 01 59 -m minute as a decimal number 0 1 59 s second as a zero-padded decimal number 00 01 59 -s second as a decimal number 0 1 59 g millisecond as a decimal number zero-padded on the left 000 - 999 f microsecond as a decimal number zero-padded on the left 000000 - 999999 z time offset from utc in the form hh mm hhmm or hh -0700 z time zone name europe amsterdam j day of the year as a zero-padded decimal number 001 002 366 -j day of the year as a decimal number 1 2 366 u week number of the year sunday as the first day of the week 00 01 53 w week number of the year monday as the first day of the week 00 01 53 c iso date and time representation 1992-03-02 10 30 20 x iso date representation 1992-03-02 x iso time representation 10 30 20 a literal character",
			"category": "Functions",
			"url": "/docs/sql/functions/dateformat",
			"blurb": "The strftime and strptime functions can be used to convert between dates/timestamps and strings. This is often..."
		},
		{
			"title": "Date Functions",
			"text": "this section describes functions and operators for examining and manipulating date values date operators the table below shows the available mathematical operators for date types operator description example result --- --- --- --- addition of days integers date 1992-03-22 5 1992-03-27 addition of an interval date 1992-03-22 interval 5 day 1992-03-27 addition of a variable interval select date 1992-03-22 interval 1 day d days from values 5 11 as d days 1992-03-27 1992-04-02 - subtraction of date s date 1992-03-27 - date 1992-03-22 5 - subtraction of an interval date 1992-03-27 - interval 5 day 1992-03-22 - subtraction of a variable interval select date 1992-03-27 - interval 1 day d days from values 5 11 as d days 1992-03-22 1992-03-16 adding to or subtracting from infinite values produces the same infinite value date functions the table below shows the available functions for date types dates can also be manipulated with the timestamp functions through type promotion function description example result --- --- --- --- current_date current date at start of current transaction current_date 2022-10-08 date_diff part startdate enddate the number of partition boundaries between the dates date_diff month date 1992-09-15 date 1992-11-14 2 datediff part startdate enddate alias of date_diff the number of partition boundaries between the dates datediff month date 1992-09-15 date 1992-11-14 2 date_part part date get the subfield equivalent to extract date_part year date 1992-09-20 1992 datepart part date alias of date_part get the subfield equivalent to extract datepart year date 1992-09-20 1992 date_sub part startdate enddate the number of complete partitions between the dates date_sub month date 1992-09-15 date 1992-11-14 1 datesub part startdate enddate alias of date_sub the number of complete partitions between the dates datesub month date 1992-09-15 date 1992-11-14 1 date_trunc part date truncate to specified precision date_trunc month date 1992-03-07 1992-03-01 datetrunc part date alias of date_trunc truncate to specified precision datetrunc month date 1992-03-07 1992-03-01 dayname date the english name of the weekday dayname date 1992-09-20 sunday isfinite date returns true if the date is finite false otherwise isfinite date 1992-03-07 true isinf date returns true if the date is infinite false otherwise isinf date -infinity true extract part from date get subfield from a date extract year from date 1992-09-20 1992 greatest date date the later of two dates greatest date 1992-09-20 date 1992-03-07 1992-09-20 last_day date the last day of the corresponding month in the date last_day date 1992-09-20 1992-09-30 least date date the earlier of two dates least date 1992-09-20 date 1992-03-07 1992-03-07 make_date bigint bigint bigint the date for the given parts make_date 1992 9 20 1992-09-20 monthname date the english name of the month monthname date 1992-09-20 september strftime date format converts a date to a string according to the format string strftime date 1992-01-01 a -d b y wed 1 january 1992 time_bucket bucket_width date origin truncate date by the specified interval bucket_width buckets are aligned relative to origin date origin defaults to 2000-01-03 for buckets that don t include a month or year interval and to 2000-01-01 for month and year buckets time_bucket interval 2 weeks date 1992-04-20 date 1992-04-01 1992-04-15 time_bucket bucket_width date offset truncate date by the specified interval bucket_width buckets are offset by offset interval time_bucket interval 2 months date 1992-04-20 interval 1 month 1992-04-01 today current date start of current transaction today 2022-10-08 there are also dedicated extraction functions to get the subfields a few examples include extracting the day from a date or the day of the week from a date functions applied to infinite dates will either return the same infinite dates e g greatest or null e g date_part depending on what makes sense in general if the function needs to examine the parts of the infinite date the result will be null",
			"category": "Functions",
			"url": "/docs/sql/functions/date",
			"blurb": "This section describes functions and operators for examining and manipulating date values. Date Operators The table..."
		},
		{
			"title": "Date Parts",
			"text": "the date_part and date_diff and date_trunc functions can be used to manipulate the fields of temporal types the fields are specified as strings that contain the part name of the field part specifiers below is a full list of all available date part specifiers the examples are the corresponding parts of the timestamp 2021-08-03 11 59 44 123456 usable as date part specifiers and in intervals specifier description synonyms example --- --- --- --- century gregorian century cent centuries c 21 day gregorian day days d dayofmonth 3 decade gregorian decade dec decades decs 202 hour hours hr hours hrs h 11 microseconds sub-minute microseconds microsecond us usec usecs usecond useconds 44123456 millennium gregorian millennium mil millenniums millenia mils millenium 3 milliseconds sub-minute milliseconds millisecond ms msec msecs msecond mseconds 44123 minute minutes min minutes mins m 59 month gregorian month mon months mons 8 quarter quarter of the year 1-4 quarters 3 second seconds sec seconds secs s 44 year gregorian year yr y years yrs 2021 usable in date part specifiers only specifier description synonyms example --- --- --- --- dayofweek day of the week sunday 0 saturday 6 weekday dow 2 dayofyear day of the year 1-365 366 doy 215 epoch seconds since 1970-01-01 1627991984 era gregorian era ce ad bce bc 1 isodow iso day of the week monday 1 sunday 7 2 isoyear iso year number starts on monday of week containing jan 4th 2021 timezone time zone offset in seconds 0 timezone_hour time zone offset hour portion 0 timezone_minute time zone offset minute portion 0 week week number weeks w 31 yearweek iso year and week number in yyyyww format 202131 note that the time zone parts are all zero unless a time zone plugin such as icu has been installed to support timestamp with time zone part functions there are dedicated extraction functions to get certain subfields function description example result --- --- --- --- century date century century date 1992-02-15 20 day date day day date 1992-02-15 15 dayofmonth date day synonym dayofmonth date 1992-02-15 15 dayofweek date numeric weekday sunday 0 saturday 6 dayofweek date 1992-02-15 6 dayofyear date numeric iso weekday monday 1 sunday 7 isodow date 1992-02-15 46 decade date decade year 10 decade date 1992-02-15 199 epoch date seconds since 1970-01-01 epoch date 1992-02-15 698112000 era date calendar era era date 0044-03-15 bc 0 hour date hours hour timestamp 2021-08-03 11 59 44 123456 11 isodow date numeric iso weekday monday 1 sunday 7 isodow date 1992-02-15 6 isoyear date iso year number starts on monday of week containing jan 4th isoyear date 2022-01-01 2021 microsecond date sub-minute microseconds microsecond timestamp 2021-08-03 11 59 44 123456 44123456 millennium date millennium millennium date 1992-02-15 2 millisecond date sub-minute milliseconds millisecond timestamp 2021-08-03 11 59 44 123456 44123 minute date minutes minute timestamp 2021-08-03 11 59 44 123456 59 month date month month date 1992-02-15 2 quarter date quarter quarter date 1992-02-15 1 second date seconds second timestamp 2021-08-03 11 59 44 123456 44 timezone date time zone offset in minutes timezone date 1992-02-15 0 timezone_hour date time zone offset hour portion timezone_hour date 1992-02-15 0 timezone_minute date time zone offset minutes portion timezone_minute date 1992-02-15 0 week date iso week week date 1992-02-15 7 weekday date numeric weekday synonym sunday 0 saturday 6 weekday date 1992-02-15 6 weekofyear date iso week synonym weekofyear date 1992-02-15 7 year date year year date 1992-02-15 1992 yearweek date bigint of combined iso year number and 2-digit version of iso week number yearweek date 1992-02-15 199207",
			"category": "Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "The date_part and date_diff and date_trunc functions can be used to manipulate the fields of temporal types. The..."
		},
		{
			"title": "Date Types",
			"text": "name aliases description ------- -------- -------------------------------- date calendar date year month day a date specifies a combination of year month and day duckdb follows the sql standard s lead by counting dates exclusively in the gregorian calendar even for years before that calendar was in use dates can be created using the date keyword where the data must be formatted according to the iso 8601 format yyyy-mm-dd -- 20 september 1992 select date 1992-09-20 special values there are also three special date values that can be used on input input string description ------------- ---------------------------------- epoch 1970-01-01 unix system day zero infinity later than all other dates -infinity earlier than all other dates the values infinity and -infinity are specially represented inside the system and will be displayed unchanged but epoch is simply a notational shorthand that will be converted to the date value when read select -infinity date epoch date infinity date negative epoch positive ---------- ----------- --------- -infinity 1970-01-01 infinity functions see date functions",
			"category": "Data Types",
			"url": "/docs/sql/data_types/date",
			"blurb": "A date specifies a combination of year, month and day."
		},
		{
			"title": "Delete Statement",
			"text": "the delete statement removes rows from the table identified by the table-name examples -- remove the rows matching the condition i 2 from the database delete from tbl where i 2 -- delete all rows in the table tbl delete from tbl syntax the delete statement removes rows from the table identified by the table-name if the where clause is not present all records in the table are deleted if a where clause is supplied then only those rows for which the where clause results in true are deleted rows for which the expression is false or null are retained the using clause allows deleting based on the content of other tables or subqueries",
			"category": "Statements",
			"url": "/docs/sql/statements/delete",
			"blurb": "The DELETE statement removes rows from the table identified by the table-name. Examples -- remove the rows matching..."
		},
		{
			"title": "Describe",
			"text": "in order to view the schema of the result of a query prepend describe to a query describe select from tbl in order to view the schema of a table use describe followed by the table name describe tbl below is an example of describe on the lineitem table of tpc-h column_name column_type null key default extra l_orderkey integer no null null null l_partkey integer no null null null l_suppkey integer no null null null l_linenumber integer no null null null l_quantity integer no null null null l_extendedprice decimal 15 2 no null null null l_discount decimal 15 2 no null null null l_tax decimal 15 2 no null null null l_returnflag varchar no null null null l_linestatus varchar no null null null l_shipdate date no null null null l_commitdate date no null null null l_receiptdate date no null null null l_shipinstruct varchar no null null null l_shipmode varchar no null null null l_comment varchar no null null null",
			"category": "Meta",
			"url": "/docs/guides/meta/describe",
			"blurb": "In order to view the schema of the result of a query, prepend DESCRIBE to a query. DESCRIBE SELECT * FROM tbl; In..."
		},
		{
			"title": "Drop Statement",
			"text": "the drop statement removes a catalog entry added previously with the create command examples -- delete the table with the name tbl drop table tbl -- drop the view with the name v1 do not throw an error if the view does not exist drop view if exists v1 syntax the optional if exists clause suppresses the error that would normally result if the table does not exist by default or if the restrict clause is provided the entry will not be dropped if there are any other objects that depend on it if the cascade clause is provided then all the objects that are dependent on the object will be dropped as well create schema myschema create table myschema t1 i integer -- error cannot drop myschema because the table myschema t1 depends on it drop schema myschema -- cascade drops both myschema and myschema t1 drop schema myschema cascade",
			"category": "Statements",
			"url": "/docs/sql/statements/drop",
			"blurb": "The DROP statement removes a catalog entry added previously with the CREATE command. Examples -- delete the table..."
		},
		{
			"title": "DuckDB ASOF Join",
			"text": "problem we have a time-based price table traditional joins against this table get null results if there is a time which does not exactly match solution asof join picks a good value for in the gap values first we create a price table and sales table create table prices as select 2001-01-01 00 16 00 timestamp interval v minute as ticker_time v as unit_price from range 0 5 vals v create table sales item text sale_time timestamp quantity int insert into sales values a 2001-01-01 00 18 00 10 insert into sales values b 2001-01-01 00 18 30 20 insert into sales values c 2001-01-01 00 19 00 30 we can see that we have a unit_price defined for each hour but not for half hours select from prices ticker_time unit_price timestamp int64 2001-01-01 00 16 00 0 2001-01-01 00 17 00 1 2001-01-01 00 18 00 2 no unit_price for 18 30 2001-01-01 00 19 00 3 2001-01-01 00 20 00 4 select from sales item sale_time quantity varchar timestamp int32 a 2001-01-01 00 18 00 10 b 2001-01-01 00 18 30 20 a sale time of 18 30 c 2001-01-01 00 19 00 30 with a normal left join there is a problem for the 18 30 sale since there is not a sale_time of 18 30 a join against that time will be null -- no price value for 18 30 so item b s unit_price and total are null select s p unit_price s quantity p unit_price as total from sales s left join prices p on s sale_time p ticker_time item sale_time quantity unit_price total varchar timestamp int32 int64 int64 a 2001-01-01 00 18 00 10 2 20 c 2001-01-01 00 19 00 30 3 90 b 2001-01-01 00 18 30 20 null null null result the asof join picks a good price for the 18 30 sale the on s sale_time pp ticker_time will cause the nearest lower value in this case for 18 00 to be used -- using asof 18 30 rounds down to use the 18 00 unit_price select s p unit_price s quantity p unit_price as total_cost from sales s asof left join prices p on s sale_time p ticker_time item sale_time quantity unit_price total_cost varchar timestamp int32 int64 int64 a 2001-01-01 00 18 00 10 2 20 b 2001-01-01 00 18 30 20 2 40 good result c 2001-01-01 00 19 00 30 3 90",
			"category": "Sql Features",
			"url": "/docs/guides/sql_features/asof_join",
			"blurb": "Problem: we have a time-based price table; Traditional joins against this table get NULL results if there is a time..."
		},
		{
			"title": "DuckDB Full Text Search",
			"text": "a full text index allows for a query to quickly search for all occurrences of individual words within longer text strings here s an example of building a full text index of shakespeare s plays create table corpus as select from read_parquet https github com marhar duckdb_tools raw main full-text-shakespeare shakespeare parquet describe corpus column_name column_type null line_id varchar yes play_name varchar yes line_number varchar yes speaker varchar yes text_entry varchar yes the text of each line is in text_entry and a unique key for each line is in line_id first we create the index specifying the table name the unique id column and the column s to index we will just index the single column text_entry which contains the text of the lines in the play install fts load fts pragma create_fts_index corpus line_id text_entry the table is now ready to query using the okapi bm25 ranking function rows with no match return a null score what does shakespeare say about butter select fts_main_corpus match_bm25 line_id butter as score line_id play_name speaker text_entry from corpus where score is not null order by score score line_id play_name speaker text_entry double varchar varchar varchar varchar 2 683490686835495 h4 2 4 115 henry iv prince henry didst thou never see titan kiss a dish of 3 781282331450016 h4 1 2 21 henry iv falstaff prologue to an egg and butter 3 781282331450016 h4 2 1 55 henry iv chamberlain they are up already and call for eggs and 3 781282331450016 h4 4 2 21 henry iv falstaff toasts-and-butter with hearts in their be 3 781282331450016 h4 4 2 62 henry iv prince henry already made thee butter but tell me jac 3 781282331450016 aww 4 1 40 alls well that end parolles butter-womans mouth and buy myself another 3 781282331450016 aww 5 2 9 alls well that end clown henceforth eat no fish of fortunes butteri 3 781282331450016 ayli 3 2 93 as you like it touchstone right butter-womens rank to market 3 781282331450016 kl 2 4 132 king lear fool kindness to his horse buttered his hay 3 781282331450016 mww 2 2 260 merry wives of win falstaff hang him mechanical salt-butter rogue i 3 781282331450016 mww 2 2 284 merry wives of win ford rather trust a fleming with my butter par 3 781282331450016 mww 3 5 7 merry wives of win falstaff ill have my brains taen out and buttered 3 781282331450016 mww 3 5 102 merry wives of win falstaff to heat as butter a man of continual diss 6 399093176300027 h4 2 4 494 henry iv carrier as fat as butter 14 rows 5 columns unlike standard indexes full text indexes don t auto-update as the underlying data is changed so you need to pragma drop_fts_index my_fts_index and recreate it when appropriate note on generating the corpus table details are here the columns are line_id play_name line_number speaker text_entry we need a unique key for each row in order for full text searching to work the line_id kl 2 4 132 means king lear act 2 scene 4 line 132",
			"category": "Sql Features",
			"url": "/docs/guides/sql_features/full_text_search",
			"blurb": "A full text index allows for a query to quickly search for all occurrences of individual words within longer text..."
		},
		{
			"title": "DuckDB Wasm",
			"text": "duckdb has been compiled to webassembly so it can run inside any browser on any device include iframe html src https shell duckdb org duckdb-wasm offers a layered api it can be embedded as a javascript webassembly library as a web shell or built from source according to your needs getting started with duckdb-wasm a great starting point is to read the duckdb-wasm launch blog post another great resource is the github repository for details see the full duckdb-wasm api documentation pages in this section",
			"category": "Wasm",
			"url": "/docs/api/wasm/overview",
			"blurb": "DuckDB has been compiled to WebAssembly, so it can run inside any browser on any device. {% include iframe.html..."
		},
		{
			"title": "DuckDB with DataFusion",
			"text": "datafusion is a dataframe and sql library built in rust with bindings for python it uses apache arrow s columnar format as its memory model datafusion can output results as apache arrow and duckdb can read those results directly duckdb can also rapidly output results to apache arrow which can be easily converted to a datafusion dataframe due to the interoperability of apache arrow workflows can alternate between duckdb and datafusion with ease this example workflow is also available as a google collab notebook installation pip install --quiet duckdb datafusion pyarrow datafusion to duckdb to convert from datafusion to duckdb first save datafusion results into arrow batches using the collect function and then create an arrow table using pyarrow s table from_batches function then include that arrow table in the from clause of a duckdb query as a note pandas is not required as a first step prior to using datafusion but was helpful for generating example data to reuse in the second example below import the libraries create an example pandas dataframe then convert to datafusion import duckdb import pyarrow as pa import pandas as pd import datafusion as df from datafusion import functions as f pandas_df pd dataframe a 1 2 3 4 5 fruits banana banana apple apple banana b 5 4 3 2 1 cars beetle audi beetle beetle beetle arrow_table table pa table from_pandas pandas_df arrow_batches table to_batches ctx sessioncontext datafusion_df ctx create_dataframe arrow_batches datafusion_df calculate a new datafusion dataframe and output it to a variable as an apache arrow table arrow_batches datafusion_df aggregate df col fruits f sum df col a alias sum_a_by_fruits sort df col fruits sort ascending true collect datafusion_to_arrow pa table from_batches arrow_batches datafusion_to_arrow then query the apache arrow table using duckdb and output the results as another apache arrow table for use in a subsequent duckdb or datafusion operation output duckdb query select fruits first sum_a_by_fruits as sum_a from datafusion_to_arrow group by all order by all arrow duckdb to datafusion duckdb can output results as apache arrow tables which can be imported into datafusion with the datafusion dataframe constructor the same approach could be used with pandas dataframes but arrow is a faster way to pass data between duckdb and datafusion this example reuses the original pandas dataframe created above as a starting point as a note pandas is not required as a first step but was only used to generate example data after the import statements and example dataframe creation above query the pandas dataframe using duckdb and output the results as an arrow table duckdb_to_arrow duckdb query select fruits cars fruits as literal_string_fruits sum b filter cars beetle over as b sum a filter b 2 over partition by cars as sum_a_by_cars sum a over partition by fruits as sum_a_by_fruits from df order by fruits df b arrow load the apache arrow table into datafusion using the datafusion dataframe constructor datafusion_df_2 ctx create_dataframe duckdb_to_arrow to_batches datafusion_df_2 complete a calculation using datafusion then output the results as another apache arrow table for use in a subsequent duckdb or datafusion operation output_2 datafusion_df_2 aggregate df col fruits f sum df col sum_a_by_fruits collect output_2 to learn more about datafusion feel free to explore their github repository",
			"category": "Python",
			"url": "/docs/guides/python/datafusion",
			"blurb": "DataFusion is a DataFrame and SQL library built in Rust with bindings for Python. It uses Apache Arrow's columnar..."
		},
		{
			"title": "DuckDB with Fugue",
			"text": "fugue is a unified interface for distributed computing fugue executes python pandas and sql code on top of spark dask and ray the focus of this tutorial will be on fuguesql an enhanced sql interface that allows to define end-to-end workflows in sql rather than juggling between python and sql code there are three main use cases for fuguesql with duckdb simplified syntax and additional operators with a notebook extension running python pandas code alongside sql code seamlessly testing code on small data and then running it on sparksql or dask-sql when ready to scale for any questions see the fuguesql tutorials or message in the fugue slack you can also follow along with this collab notebook installation pip install -u fugue duckdb this will install duckdb and fugue together simplified sql syntax fugue is compatible with standard sql but also extends it with additional keywords and syntax for example the load and save keywords can be used to load and save files and the operator can be used for intermediate tables the code below shows an example setup import pandas as pd df pd dataframe a 1 2 3 b 2 3 4 df to_parquet tmp f parquet here the file is loaded processed and then saved in the query import fugue api as fa query df load tmp f parquet res select from df where a 1 save res overwrite tmp f2 parquet fa fugue_sql_flow query engine duckdb for other available keywords check the sql operators available there is also a jupyter extension for fuguesql to be used inside a notebook with syntax highlighting to use it with duckdb simply use fsql duckdb as the cell magic running python pandas functions there will be operations that python or pandas can express more succinctly than sql for example we can use the pandas cumsum method to get the cumulative sum of a column annotated python or pandas code can be invoked using the transform keyword in the example below a python function is defined and then invoked in the fuguesql query schema b_cumsum int def new_col df pd dataframe - pd dataframe df df sort_values b return df assign b_cumsum df b cumsum query df load tmp f parquet df2 select from df where a 3 df3 transform df2 using new_col select a b as c from df3 pandas_df fa as_pandas fa fugue_sql query engine duckdb the fugue_sql function automatically returns the last dataframe of the query when using the transform function fuguesql will bring the duckdb table to pandas to execute the python code by using transform there is no more need to break up the sql to invoke python code fuguesql is a first-class interface for defining the end-to-end logic because we used the duckdb engine the output of the query will be a duckdb dataframe it can be converted to pandas by using as_pandas also notice fuguesql can handle multiple select statements in one query distributing with sparksql one of the features of fugue is that the same code will be able to run on top of spark and dask just by changing the execution engine this allows users to prototype locally with duckdb on smaller data and then bring the execution to a spark cluster when ready to execute on the full-sized data developing on spark can be cumbersome even if using the local version of spark duckdb will significantly speed up iterations because of its fast execution query df load tmp f parquet df2 select from df where a 3 df3 transform df2 using new_col select a b as c from df3 spark_df fa fugue_sql query engine spark the output of the code above will be a spark dataframe more information on fugue can be found on their github",
			"category": "Python",
			"url": "/docs/guides/python/fugue",
			"blurb": "Fugue is a unified interface for distributed computing. Fugue executes Python, Pandas, and SQL code on top of Spark,..."
		},
		{
			"title": "DuckDB with Ibis",
			"text": "ibis is a python library that allows queries to be written in a pythonic relational style and then be compiled into sql ibis supports multiple database backends including duckdb by using duckdb s sqlalchemy driver ibis expressions can also be combined with sql statements installation to install only the duckdb backend for ibis use the commands below see the ibis duckdb installation instructions for a conda alternative note that duckdb support was added in ibis version 3 0 0 pip install ibis-framework duckdb duckdb sqlalchemy duckdb_engine and more are installed as dependencies querying duckdb with ibis the following example is loosely borrowed from the introduction to ibis tutorial which uses sqlite first we import ibis set it to interactive mode just for demo purposes - it is faster to not use this option and then connect to an in-memory duckdb instance we can then inspect the tables in our database import ibis ibis options interactive true use eager evaluation use only for demo purposes connection ibis duckdb connect memory use an in memory duckdb connection ibis duckdb connect path to my_db db use or create a physical duckdb at this path print connection list_tables output pragma_database_list duckdb_tables duckdb_views duckdb_indexes sqlite_master sqlite_schema sqlite_temp_master sqlite_temp_schema duckdb_constraints duckdb_columns duckdb_schemas duckdb_types we then create a handler to a specific table to be able to explore it further here we use a built in table called duckdb_types for simplicity the first thing we want to see is a list of columns duckdb_types_table connection table duckdb_types print duckdb_types_table columns output schema_name schema_oid type_oid type_name type_size type_category internal to access only certain columns use bracket syntax on the table handler we can also apply functions to transform the data for example to show only distinct values use the compile function to see the sql query that ibis generates print duckdb_types_table type_category type_size distinct print duckdb_types_table type_category type_size distinct compile type_category type_size --- --- boolean 1 numeric 1 numeric 2 numeric 4 numeric 8 datetime 4 datetime 8 string 16 nan 16 datetime 16 numeric 16 composite 16 composite 0 numeric nan select distinct t0 type_category t0 type_size from duckdb_types as t0 multiple methods can be chained together to build up more complex expressions this statement selects a subset of columns filters to rows containing a specific value in one column and sorts by another column the ibis-generated sql is shown below note that it uses a parameter as a part of the filter function print duckdb_types_table type_name type_category type_size filter duckdb_types_table type_category numeric sort_by type_size print duckdb_types_table type_name type_category type_size filter duckdb_types_table type_category numeric sort_by type_size compile type_name type_category type_size --- --- --- decimal numeric nan tinyint numeric 1 utinyint numeric 1 smallint numeric 2 usmallint numeric 2 integer numeric 4 float numeric 4 uinteger numeric 4 bigint numeric 8 double numeric 8 ubigint numeric 8 hugeint numeric 16 select t0 type_name t0 type_category t0 type_size from select t1 type_name as type_name t1 type_category as type_category t1 type_size as type_size from duckdb_types as t1 where t1 type_category cast as text as t0 order by t0 type_size combining sql and ibis expressions ibis can also be used to combine sql and relational operators sql can precede or follow ibis relational operations print duckdb_types_table sql select dense_rank over order by type_size as size_rank from duckdb_types group_by type_category aggregate avg_size_rank lambda t t size_rank mean print duckdb_types_table sql select dense_rank over order by type_size as size_rank from duckdb_types group_by type_category aggregate avg_size_rank lambda t t size_rank mean compile type_category avg_size_rank --- --- numeric 4 583333333333333 composite 3 6666666666666665 boolean 3 0 datetime 6 0 string 7 0 nan 7 0 with _ibis_view_11 as select dense_rank over order by type_size as size_rank from duckdb_types select t0 type_category avg t0 size_rank as avg_size_rank from _ibis_view_11 as t0 group by t0 type_category to learn more about ibis feel free to continue with the ibis introductory tutorial",
			"category": "Python",
			"url": "/docs/guides/python/ibis",
			"blurb": "Ibis is a Python library that allows queries to be written in a pythonic relational style and then be compiled into..."
		},
		{
			"title": "DuckDB with Polars",
			"text": "polars is a dataframes library built in rust with bindings for python and node js it uses apache arrow s columnar format as its memory model duckdb can read polars dataframes and convert query results to polars dataframes it does this internally using the efficient apache arrow integration note that the pyarrow library must be installed for the integration to work installation pip install duckdb pip install -u polars pyarrow polars to duckdb duckdb can natively query polars dataframes by referring to the name of polars dataframes as they exist in the current scope import duckdb import polars as pl df pl dataframe a 1 2 3 4 5 fruits banana banana apple apple banana b 5 4 3 2 1 cars beetle audi beetle beetle beetle duckdb sql select from df show duckdb to polars duckdb can output results as polars dataframes using the pl result-conversion method df duckdb sql select 1 as id banana as fruit union all select 2 apple union all select 3 mango pl print df to learn more about polars feel free to explore their python api reference",
			"category": "Python",
			"url": "/docs/guides/python/polars",
			"blurb": "Polars is a DataFrames library built in Rust with bindings for Python and Node.js. It uses Apache Arrow's columnar..."
		},
		{
			"title": "DuckDB with Vaex",
			"text": "vaex is a high performance dataframe library in python vaex is a hybrid dataframe as it supports both numpy s and apache arrow s data structures vaex dataframes can export data as apache arrow table which can be directly used by duckdb since duckdb can output results as an apache arrow table which can be easily turned into a vaex dataframe one can easily alternate between duckdb and vaex the following example shows how one can use both duckdb and vaex dataframe for a simple exploratory work installation pip install duckdb pip install vaex vaex dataframe to duckdb a vaex dataframe can be exported as an arrow table via the to_arrow_table method this operation does not take extra memory if the data being exported is already in memory or memory-mapped the exported arrow table can be queried directly via duckdb let s use the well known titanic dataset that also ships with vaex to do some operations like filling missing values and creating new columns then we will export the dataframe to an arrow table import duckdb import vaex df vaex datasets titanic df age df age fillna df age mean df fare df age fillna df fare mean df family_size df sibsp df parch 1 df fare_per_family_member df fare df family_size df name_title df name str replace a-z a-z 1 regex true arrow_table df to_arrow_table now we can directly query the arrow table using duckdb the output of which can be another arrow table which can be used for subsequent duckdb queries or it can be converted to a vaex dataframe query_result_arrow_table duckdb query select pclass mean age as age mean family_size as family_size mean fare_per_family_member as fare_per_family_member count distinct name_title as distinct_titles list distinct name_title from arrow_table group by pclass order by pclass arrow duckdb to vaex dataframe the output of a duckdb query can be an arrow table which can be easily converted to a vaex dataframe via the vaex from_arrow_table method one can also pass data around via pandas dataframes but arrow is faster we can use the query result from above and convert it to a vaex dataframe df_from_duckdb vaex from_arrow_table query_result_arrow_table one can then continue to use vaex and also export the data or part of it to an arrow table to be used with duckdb as needed to learn more about vaex feel free to explore their documentation",
			"category": "Python",
			"url": "/docs/guides/python/vaex",
			"blurb": "Vaex is a high performance DataFrame library in Python. Vaex is a hybrid DataFrame, as it supports both Numpy's and..."
		},
		{
			"title": "DuckDB_% Metadata Functions",
			"text": "duckdb offers a collection of table functions that provide metadata about the current database these functions reside in the main schema and their names are prefixed with duckdb_ the resultset returned by a duckdb_ table function may be used just like an ordinary table or view for example you can use a duckdb_ function call in the from clause of a select statement and you may refer to the columns of its returned resultset elsewhere in the statement for example in the where clause table functions are still functions and you should write parenthesis after the function name to call it to obtain its returned resultset select from duckdb_settings alternatively you may execute table functions also using the call -syntax call duckdb_settings in this case too the parentheses are mandatory note for some of the duckdb_ functions there is also a identically named view available which also resides in the main schema typically these views do a select on the duckdb_ table function with the same name while filtering out those objects that are marked as internal we mention it here because if you accidentally omit the parentheses in your duckdb_ table function call you might still get a result but from the identically named view example -- duckdb_views table function returns all views including those marked internal select from duckdb_views -- duckdb_views view returns views that are not marked as internal select from duckdb_views duckdb_columns the duckdb_columns function provides metadata about the columns available in the duckdb instance column description type --- --- --- database_name the name of the database that contains the column object varchar database_oid internal identifier of the database that contains the column object bigint schema_name the sql name of the schema that contains the table object that defines this column varchar schema_oid internal identifier of the schema object that contains the table of the column bigint table_name the sql name of the table that defines the column varchar table_oid internal identifier name of the table object that defines the column bigint column_name the sql name of the column varchar column_index the unique position of the column within its table integer internal true if this column built-in false if it is user-defined boolean column_default the default value of the column expressed in sql varchar is_nullable true if the column can hold null values false if the column cannot hold null -values boolean data_type the name of the column datatype varchar data_type_id the internal identifier of the column data type bigint character_maximum_length always null duckdb text types do not enforce a value length restriction based on a length type parameter integer numeric_precision the number of units in the base indicated by numeric_precision_radix used for storing column values for integral and approximate numeric types this is the number of bits for decimal types this is the number of digits positions integer numeric_precision_radix the number-base of the units in the numeric_precision column for integral and approximate numeric types this is 2 indicating the precision is expressed as a number of bits for the decimal type this is 10 indicating the precision is expressed as a number of decimal positions integer numeric_scale applicable to decimal type indicates the maximum number of fractional digits i e the number of digits that may appear after the decimal separator integer the information_schema columns system view provides a more standardized way to obtain metadata about database columns but the duckdb_columns function also returns metadata about duckdb internal objects in fact information_schema columns is implemented as a query on top of duckdb_columns duckdb_constraints the duckdb_constraints function provides metadata about the constraints available in the duckdb instance column description type --- --- --- database_name the name of the database that contains the constraint varchar database_oid internal identifier of the database that contains the constraint bigint schema_name the sql name of the schema that contains the table on which the constraint is defined varchar schema_oid internal identifier of the schema object that contains the table on which the constraint is defined bigint table_name the sql name of the table on which the constraint is defined varchar table_oid internal identifier name of the table object on which the constraint is defined bigint constraint_index indicates the position of the constraint as it appears in its table definition bigint constraint_type indicates the type of constraint applicable values are check foreign key primary key not null unique varchar constraint_text the definition of the constraint expressed as a sql-phrase not necessarily a complete or syntactically valid ddl-statement varchar expression if constraint is a check constraint the definition of the condition being checked otherwise null varchar constraint_column_indexes an array of table column indexes referring to the columns that appear in the constraint definition bigint constraint_column_names an array of table column names appearing in the constraint definition varchar duckdb_databases the duckdb_databases function lists the databases that are accessible from within the current duckdb process apart from the database associated at startup the list also includes databases that were attached later on to the duckdb process column description type --- --- --- database_name the name of the database or the alias if the database was attached using an alias-clause varchar database_oid the internal identifier of the database varchar path the file path associated with the database varchar internal true indicates a system or built-in database false indicates a user-defined database boolean type the type indicates the type of rdbms implemented by the attached database for duckdb databases that value is duckdb duckdb_dependencies the duckdb_dependencies function provides metadata about the dependencies available in the duckdb instance column description type --- --- --- classid always 0 bigint objid the internal id of the object bigint objsubid always 0 integer refclassid always 0 bigint s refobjid the internal id of the dependent object bigint refobjsubid always 0 integer deptype the type of dependency either regular n or automatic a varchar duckdb_extensions the duckdb_extensions function provides metadata about the extensions available in the duckdb instance column description type --- --- --- extension_name the name of the extension varchar loaded true if the extension is loaded false if it s not loaded boolean installed true if the extension is installed false if it s not installed boolean install_path built-in if the extension is built-in otherwise the filesystem path where binary that implements the extension resides varchar description human readable text that describes the extension s functionality varchar aliases list of alternative names for this extension varchar duckdb_functions the duckdb_functions function provides metadata about the functions available in the duckdb instance column description type --- --- --- database_name the name of the database that contains this function varchar schema_name the sql name of the schema where the function resides varchar function_name the sql name of the function varchar function_type the function kind value is one of table scalar aggregate pragma macro varchar description description of this function always null varchar return_type the logical data type name of the returned value applicable for scalar and aggregate functions varchar parameters if the function has parameters the list of parameter names varchar parameter_types if the function has parameters a list of logical data type names corresponding to the parameter list varchar varargs the name of the data type in case the function has a variable number of arguments or null if the function does not have a variable number of arguments varchar macro_definition if this is a macro the sql expression that defines it varchar has_side_effects false if this is a pure function true if this function changes the database state like sequence functions nextval and curval boolean function_oid the internal identifier for this function bigint duckdb_indexes the duckdb_indexes function provides metadata about secondary indexes available in the duckdb instance column description type --- --- --- database_name the name of the database that contains this index varchar database_oid internal identifier of the database containing the index bigint schema_name the sql name of the schema that contains the table with the secondary index varchar schema_oid internal identifier of the schema object bigint index_name the sql name of this secondary index varchar index_oid the object identifier of this index bigint table_name the name of the table with the index varchar table_oid internal identifier name of the table object bigint is_unique true if the index was created with the unique modifier false if it was not boolean is_primary always false boolean expressions always null varchar sql the definition of the index expressed as a create index sql statement varchar note that duckdb_indexes only provides metadata about secondary indexes - i e those indexes created by explicit create index statements primary keys are maintained using indexes but their details are included in the duckdb_constraints function duckdb_keywords the duckdb_keywords function provides metadata about duckdb s keywords and reserved words column description type --- --- --- keyword_name the keyword varchar keyword_category indicates the category of the keyword values are column_name reserved type_function and unreserved varchar duckdb_schemas the duckdb_schemas function provides metadata about the schemas available in the duckdb instance column description type --- --- --- oid internal identifier of the schema object bigint database_name the name of the database that contains this schema varchar database_oid internal identifier of the database containing the schema bigint schema_name the sql name of the schema varchar internal true if this is an internal built-in schema false if this is a user-defined schema boolean sql always null varchar the information_schema schemata system view provides a more standardized way to obtain metadata about database schemas duckdb_sequences the duckdb_sequences function provides metadata about the sequences available in the duckdb instance column description type --- --- --- database_name the name of the database that contains this sequence varchar database_oid internal identifier of the database containing the sequence bigint schema_name the sql name of the schema that contains the sequence object varchar schema_oid internal identifier of the schema object that contains the sequence object bigint sequence_name the sql name that identifies the sequence within the schema varchar sequence_oid the internal identifier of this sequence object bigint temporary whether this sequence is temporary temporary sequences are transient and only visible within the current connection boolean start_value the initial value of the sequence this value will be returned when nextval is called for the very first time on this sequence bigint min_value the minimum value of the sequence bigint max_value the maximum value of the sequence bigint increment_by the value that is added to the current value of the sequence to draw the next value from the sequence bigint cycle whether the sequence should start over when drawing the next value would result in a value outside the range boolean last_value null if no value was ever drawn from the sequence using nextval 1 if a value was drawn bigint sql the definition of this object expressed as sql ddl-statement varchar attributes like temporary start_value etc correspond to the various options available in the create sequence statement and are documented there in full note that the attributes will always be filled out in the duckdb_sequences resultset even if they were not explicitly specified in the create sequence statement note1 the column name last_value suggests that it contains the last value that was drawn from the sequence but that is not the case it s either null if a value was never drawn from the sequence or 1 when there was a value drawn ever from the sequence note2 if the sequence cycles then the sequence will start over from the boundary of its range not necessarily from the value specified as start value duckdb_settings the duckdb_settings function provides metadata about the settings available in the duckdb instance column description type --- --- --- name name of the setting varchar value current value of the setting varchar description a description of the setting varchar input_type the logical datatype of the setting s value varchar the various settings are described in the configuration page duckdb_tables the duckdb_tables function provides metadata about the base tables available in the duckdb instance column description type --- --- --- database_name the name of the database that contains this table varchar database_oid internal identifier of the database containing the table bigint schema_name the sql name of the schema that contains the base table varchar schema_oid internal identifier of the schema object that contains the base table bigint table_name the sql name of the base table varchar table_oid internal identifier of the base table object bigint internal false if this is a user-defined table boolean temporary whether this is a temporary table temporary tables are not persisted and only visible within the current connection boolean has_primary_key true if this table object defines a primary key boolean estimated_size the estimated number of rows in the table bigint column_count the number of columns defined by this object bigint index_count the number of indexes associated with this table this number includes all secondary indexes as well as internal indexes generated to maintain primary key and or unique constraints bigint check_constraint_count the number of check constraints active on columns within the table bigint sql the definition of this object expressed as sql create table -statement varchar the information_schema tables system view provides a more standardized way to obtain metadata about database tables that also includes views but the resultset returned by duckdb_tables contains a few columns that are not included in information_schema tables duckdb_types the duckdb_types function provides metadata about the data types available in the duckdb instance column description type --- --- --- database_name the name of the database that contains this schema varchar database_oid internal identifier of the database that contains- the data type bigint schema_name the sql name of the schema containing the type definition always main varchar schema_oid internal identifier of the schema object bigint type_name the name or alias of this data type varchar type_oid the internal identifier of the data type object if null then this is an alias of the type as identified by the value in the logical_type column bigint type_size the number of bytes required to represent a value of this type in memory bigint logical_type the canonical name of this data type the same logical_type may be referenced by several types having different type_name s varchar type_category the category to which this type belongs data types within the same category generally expose similar behavior when values of this type are used in expression for example the numeric type_category includes integers decimals and floating point numbers varchar internal whether this is an internal built-in or a user object boolean duckdb_views the duckdb_views function provides metadata about the views available in the duckdb instance column description type --- --- --- database_name the name of the database that contains this view varchar database_oid internal identifier of the database that contains this view bigint schema_name the sql name of the schema where the view resides varchar schema_oid internal identifier of the schema object that contains the view bigint view_name the sql name of the view object varchar view_oid the internal identifier of this view object bigint internal true if this is an internal built-in view false if this is a user-defined view boolean temporary true if this is a temporary view temporary views are not persistent and are only visible within the current connection boolean column_count the number of columns defined by this view object bigint sql the definition of this object expressed as sql ddl-statement varchar the information_schema tables system view provides a more standardized way to obtain metadata about database views that also includes base tables but the resultset returned by duckdb_views contains also definitions of internal view objects as well as a few columns that are not included in information_schema tables",
			"category": "SQL",
			"url": "/docs/sql/duckdb_table_functions",
			"blurb": "DuckDB offers a collection of table functions that provide metadata about the current database. These functions..."
		},
		{
			"title": "Enum Functions",
			"text": "this section describes functions and operators for examining and manipulating enum values the examples assume an enum type created as create type mood as enum sad ok happy anxious these functions can take null or a specific value of the type as argument s with the exception of enum_range_boundary the result depends only on the type of the argument and not on its value function description example result --- --- --- --- enum_code enum_value returns the numeric value backing the given enum value enum_code happy mood 2 enum_first enum returns the first value of the input enum type enum_first null mood sad enum_last enum returns the last value of the input enum type enum_last null mood anxious enum_range enum returns all values of the input enum type as an array enum_range null mood sad ok happy anxious enum_range_boundary enum enum returns the range between the two given enum values as an array the values must be of the same enum type when the first parameter is null the result starts with the first value of the enum type when the second parameter is null the result ends with the last value of the enum type enum_range_boundary null happy mood sad ok happy",
			"category": "Functions",
			"url": "/docs/sql/functions/enum",
			"blurb": "This section describes functions and operators for examining and manipulating ENUM values. The examples assume an..."
		},
		{
			"title": "Enum Types",
			"text": "name description --- --- enum dictionary encoding representing all possible string values of a column enums the enum type represents a dictionary data structure with all possible unique values of a column for example a column storing the days of the week can be an enum holding all possible days enums are particularly interesting for string columns with low cardinality i e fewer distinct values this is because the column only stores a numerical reference to the string in the enum dictionary resulting in immense savings in disk storage and faster query performance enum definition enum types are created from either a hardcoded set of values or from a select statement that returns a single column of varchars the set of values in the select statement will be deduplicated but if the enum is created from a hardcoded set there may not be any duplicates -- create enum using hardcoded values create type enum_name as enum value_1 value_2 -- create enum using a select statement that returns a single column of varchars create type enum_name as enum select expression for example -- creates new user defined type mood as an enum create type mood as enum sad ok happy -- this will fail since the mood type already exists create type mood as enum sad ok happy anxious -- this will fail since enums cannot hold null values create type breed as enum maltese null -- this will fail since enum values must be unique create type breed as enum maltese maltese -- create an enum from a select statement -- first create an example table of values create table my_inputs as select duck as my_varchar union all select duck as my_varchar union all select goose as my_varchar -- create an enum using the unique string values in the my_varchar column create type birds as enum select my_varchar from my_inputs -- show the available values in the birds enum using the enum_range function select enum_range null birds as my_enum_range my_enum_range --------------- duck goose enum usage after an enum has been created it can be used anywhere a standard built-in type is used for example we can create a table with a column that references the enum -- creates a table person with attributes name string type and current_mood mood type create table person name text current_mood mood -- inserts tuples in the person table insert into person values pedro happy mark null pagliacci sad mr mackey ok -- this will fail since the mood type does not have a quackity-quack value insert into person values hannes quackity-quack -- the string sad is cast to the type mood returning a numerical reference value -- this makes the comparison a numerical comparison instead of a string comparison select from person where current_mood sad ---- pagliacci -- if you are importing data from a file you can create an enum for a varchar column before importing -- the subquery select automatically selects only distinct values create type mood as enum select mood from path to file csv -- then you can create a table with the enum type and import using any data import statement create table person name text current_mood mood copy person from path to file csv auto_detect true enum vs strings duckdb enums are automatically cast to varchar types whenever necessary this characteristic allows for enum columns to be used in any varchar function in addition it also allows for comparisons between different enum columns or an enum and a varchar column for example -- regexp_matches is a function that takes a varchar hence current_mood is cast to varchar select regexp_matches current_mood a from person ---- true false true false create type new_mood as enum happy anxious create table person_2 name text current_mood mood future_mood new_mood past_mood varchar -- since current_mood and future_mood are constructed on different enums -- duckdb will cast both enums to strings and perform a string comparison select from person_2 where current_mood future_mood -- since current_mood is an enum -- duckdb will cast the current_mood enum to varchar and perform a string comparison select from person_2 where current_mood past_mood enum removal enum types are stored in the catalog and a catalog dependency is added to each table that uses them it is possible to drop an enum from the catalog using the following command drop type enum_name note that any dependent must be removed before dropping the enum or the enum must be dropped with the additional cascade parameter for example -- this will fail since person has a catalog dependency to the mood type drop type mood drop table person drop table person_2 -- this successfully removes the mood type -- another option would be to drop type mood cascade drops the type and its dependents drop type mood",
			"category": "Data Types",
			"url": "/docs/sql/data_types/enum",
			"blurb": "The ENUM type represents a dictionary data structure with all possible unique values of a column."
		},
		{
			"title": "Excel",
			"text": "this extension contrary to its name does not provide support for reading excel files instead see our guides for excel import and export it instead provides a function that wraps the number formatting functionality of the i18npool library which formats numbers per excel s formatting rules function description example result ------------------------------------------------ --------------------------------------------------------------------- ---------------------------------- ---------- text number format_string format the given number per the rules given in the format_string text 1234567 897 h am pm 9 pm excel_text number format_string alias for text text 1234567 897 h mm am pm 9 31 pm",
			"category": "Extensions",
			"url": "/docs/extensions/excel",
			"blurb": "This extension, contrary to its name, does not provide support for reading Excel files. Instead, see our guides for..."
		},
		{
			"title": "Excel Export",
			"text": "to export the data from a table to an excel file install and load the spatial extension then use the copy statement the file will contain one worksheet with the same name as the file but without the xlsx extension install spatial -- only needed once per duckdb connection load spatial -- only needed once per duckdb connection copy tbl to output xlsx with format gdal driver xlsx the result of queries can also be directly exported to an excel file install spatial -- only needed once per duckdb connection load spatial -- only needed once per duckdb connection copy select from tbl to output xlsx with format gdal driver xlsx note dates and timestamps are not supported by the xlsx writer driver cast columns of those types to varchar prior to creating the xlsx file note the output file must not already exist for additional details see the spatial extension page and the gdal xlsx driver page",
			"category": "Import",
			"url": "/docs/guides/import/excel_export",
			"blurb": "To export the data from a table to an Excel file, install and load the spatial extension, then use the COPY..."
		},
		{
			"title": "Excel Import",
			"text": "to read data from an excel file install and load the spatial extension then use the st_read function in the from clause of a query use the layer parameter to specify the excel worksheet name install spatial -- only needed once per duckdb connection load spatial -- only needed once per duckdb connection select from st_read test_excel xlsx layer sheet1 to create a new table using the result from a query use create table as from a select statement install spatial -- only needed once per duckdb connection load spatial -- only needed once per duckdb connection create table new_tbl as select from st_read test_excel xlsx layer sheet1 to load data into an existing table from a query use insert into from a select statement install spatial -- only needed once per duckdb connection load spatial -- only needed once per duckdb connection insert into tbl select from st_read test_excel xlsx layer sheet1 several configuration options are also available for the underlying gdal library that is doing the xlsx parsing set those options in an environment variable prior to executing the duckdb sql statement the options include ogr_xlsx_headers force disable auto either force the first row to be interpreted as headers disable to treat the first row as a row of data or auto to detect automatically ogr_xlsx_field_types string auto either auto detect the data types in the file or force all data types to be string for additional details see the spatial extension page the gdal xlsx driver page and the gdal configuration options page",
			"category": "Import",
			"url": "/docs/guides/import/excel_import",
			"blurb": "To read data from an Excel file, install and load the spatial extension, then use the st_read function in the FROM..."
		},
		{
			"title": "Execute SQL",
			"text": "sql queries can be executed using the duckdb sql command import duckdb duckdb sql select 42 show by default this will create a relation object the result can be converted to various formats using the result conversion functions for example the fetchall method can be used to convert the result to python objects results duckdb sql select 42 fetchall print results 42 several other result objects exist for example you can use df to convert the result to a pandas dataframe results duckdb sql select 42 df print results 42 0 42 by default a global in-memory connection will be used any data stored in files will be lost after shutting down the program a connection to a persistent database can be created using the connect function after connecting sql queries can be executed using the sql command con duckdb connect file db con sql create table integers i integer con sql insert into integers values 42 con sql select from integers show",
			"category": "Python",
			"url": "/docs/guides/python/execute_sql",
			"blurb": "SQL queries can be executed using the duckdb.sql command. import duckdb duckdb.sql(SELECT 42).show() By default this..."
		},
		{
			"title": "Explain",
			"text": "in order to view the query plan of a query prepend explain to a query explain select from tbl by default only the final physical plan is shown in order to see the unoptimized and optimized logical plans change the explain_output setting set explain_output all below is an example of running explain on q1 of the tpc-h benchmark order_by lineitem l_returnflag asc lineitem l_linestatus asc hash_group_by 0 1 sum 2 sum 3 sum 4 sum 5 avg 6 avg 7 avg 8 count_star projection l_returnflag l_linestatus l_quantity l_extendedprice 4 4 1 00 l_tax l_quantity l_extendedprice l_discount projection l_returnflag l_linestatus l_quantity l_extendedprice l_extendedprice 1 00 - l_discount l_tax l_discount seq_scan lineitem l_shipdate l_returnflag l_linestatus l_quantity l_extendedprice l_discount l_tax filters l_shipdate 1998 -09-02 and l_shipdate null",
			"category": "Meta",
			"url": "/docs/guides/meta/explain",
			"blurb": "In order to view the query plan of a query, prepend EXPLAIN to a query. EXPLAIN SELECT * FROM tbl; By default only..."
		},
		{
			"title": "Export & Import Database",
			"text": "the export database command allows you to export the contents of the database to a specific directory the import database command allows you to then read the contents again examples -- export the database to the target directory export database target_directory -- export the table contents with the given options export database target_directory format csv delimiter -- export the table contents as parquet export database target_directory format parquet -- export as parquet compressed with zstd with a row_group_size of 100000 export database target_directory format parquet compression zstd row_group_size 100000 --reload the database again import database target_directory for details regarding the writing of parquet files see the parquet files page in the data import section and the copy statement page syntax the export database command exports the full contents of the database - including schema information tables views and sequences - to a specific directory that can then be loaded again the created directory will be structured as follows target_directory schema sql target_directory load sql target_directory t_1 csv target_directory t_n csv the schema sql file contains the schema statements that are found in the database it contains any create schema create table create view and create sequence commands that are necessary to re-construct the database the load sql file contains a set of copy statements that can be used to read the data from the csv files again the file contains a single copy statement for every table found in the schema the database can be reloaded by using the import database command again or manually by running schema sql followed by load sql to re-load the data",
			"category": "Statements",
			"url": "/docs/sql/statements/export",
			"blurb": "The EXPORT DATABASE command allows you to export the contents of the database to a specific directory. The IMPORT..."
		},
		{
			"title": "Export To Apache Arrow",
			"text": "all results of a query can be exported to an apache arrow table using the arrow function alternatively results can be returned as a recordbatchreader using the fetch_record_batch function and results can be read one batch at a time in addition relations built using duckdb s relational api can also be exported export to an arrow table import duckdb import pyarrow as pa my_arrow_table pa table from_pydict i 1 2 3 4 j one two three four query the apache arrow table my_arrow_table and return as an arrow table results duckdb sql select from my_arrow_table arrow export as a recordbatchreader import duckdb import pyarrow as pa my_arrow_table pa table from_pydict i 1 2 3 4 j one two three four query the apache arrow table my_arrow_table and return as an arrow recordbatchreader chunk_size 1_000_000 results duckdb sql select from my_arrow_table fetch_record_batch chunk_size loop through the results a stopiteration exception is thrown when the recordbatchreader is empty while true try process a single chunk here just printing as an example print results read_next_batch to_pandas except stopiteration print already fetched all batches break export from relational api arrow objects can also be exported from the relational api a relation can be converted to an arrow table using the arrow or to_arrow_table functions or a record batch using record_batch a result can be exported to an arrow table with arrow or the alias fetch_arrow_table or to a recordbatchreader using fetch_arrow_reader import duckdb connect to an in-memory database con duckdb connect con execute create table integers i integer con execute insert into integers values 0 1 2 3 4 5 6 7 8 9 null create a relation from the table and export the entire relation as arrow rel con table integers relation_as_arrow rel arrow or to_arrow_table or calculate a result using that relation and export that result to arrow res rel aggregate sum i execute result_as_arrow res arrow or fetch_arrow_table",
			"category": "Python",
			"url": "/docs/guides/python/export_arrow",
			"blurb": "All results of a query can be exported to an Apache Arrow Table using the arrow function. Alternatively, results can..."
		},
		{
			"title": "Export To Pandas",
			"text": "the result of a query can be converted to a pandas dataframe using the df function import duckdb read the result of an arbitrary sql query to a pandas dataframe results duckdb sql select 42 df",
			"category": "Python",
			"url": "/docs/guides/python/export_pandas",
			"blurb": "The result of a query can be converted to a Pandas DataFrame using the df() function. import duckdb # read the result..."
		},
		{
			"title": "Expressions",
			"text": "an expression is a combination of values operators and functions expressions are highly composable and range from very simple to arbitrarily complex they can be found in many different parts of sql statements in this section we provide the different types of operators and functions that can be used within expressions more",
			"category": "Expressions",
			"url": "/docs/sql/expressions/overview",
			"blurb": "An expression is a combination of values, operators and functions. Expressions are highly composable, and range from..."
		},
		{
			"title": "Extensions",
			"text": "default extensions currently enabled in duckdb-wasm are parquet and fts httpfs is a specific re-implementation that comes bundled by default json and excel are build-time opt-in dynamic runtime extension loading dynamic extension loading is currently experimental participate in the tracking issue or try it on the experimental deployment at https shellwip duckdb org include iframe html src https shellwip duckdb org",
			"category": "Wasm",
			"url": "/docs/api/wasm/extensions",
			"blurb": "Default extensions currently enabled in DuckDB-Wasm are Parquet and FTS. HTTPFS is a specific re-implementation that..."
		},
		{
			"title": "Extensions",
			"text": "duckdb has a number of extensions available for use not all of them are included by default in every distribution but duckdb has a mechanism that allows for remote installation remote installation if a given extensions is not available with your distribution you can do the following to make it available install fts load fts if you are using the python api client you can install and load them with the load_extension name str and install_extension name str methods unsigned extensions all verified extensions are signed if you wish to load your own extensions or extensions from untrusted third-parties you ll need to enable the allow_unsigned_extensions flag to load unsigned extensions using the cli you ll need to pass the -unsigned flag to it on startup listing extensions you can check the list of core and installed extensions with the following query select from duckdb_extensions all available extensions extension name description aliases ----------------------------------------------------------------------------------------------------------------------------------- -------------------------------------------------------------------- --------------- autocomplete adds supports for autocomplete in the shell arrow github logo a zero-copy data integration between apache arrow and duckdb excel adds support for excel-like format strings fts adds support for full-text search indexes httpfs adds support for reading and writing files over a http s connection http https s3 icu adds support for time zones and collations using the icu library inet adds support for ip-related data types and functions jemalloc overwrites system allocator with jemalloc json adds support for json operations parquet adds support for reading and writing parquet files postgres_scanner github logo adds support for reading from a postgres database postgres spatial github logo adds support for geospatial data processing sqlite_scanner github logo adds support for reading sqlite database files sqlite sqlite3 substrait github logo support substrait query plans in duckdb tpcds adds tpc-ds data generation and query support tpch adds tpc-h data generation and query support visualizer downloading extensions directly from s3 downloading an extension directly could be helpful when building a lambda or container that uses duckdb duckdb extensions are stored in public s3 buckets but the directory structure of those buckets is not searchable as a result a direct url to the file must be used to directly download an extension file use the following format https extensions duckdb org v release_version_number platform_name extension_name duckdb_extension gz for example https extensions duckdb org v site currentduckdbversion windows_amd64 json duckdb_extension gz the list of supported platforms may increase over time but the current list of platforms includes linux_amd64_gcc4 linux_amd64 linux_arm64 osx_amd64 osx_arm64 windows_amd64 windows_amd64_rtools see above for a list of extension names and how to pull the latest list of extensions loading an extension from local storage extensions are stored in gzip format so they must be unzipped prior to use there are many methods to decompress gzip here is a python example import gzip import shutil with gzip open httpfs duckdb_extension gz rb as f_in with open httpfs duckdb_extension wb as f_out shutil copyfileobj f_in f_out after unzipping the install and load commands can be used with the path to the duckdb_extension file for example if the file was unzipped into the same directory as where duckdb is being executed install httpfs duckdb_extension load httpfs duckdb_extension pages in this section",
			"category": "Extensions",
			"url": "/docs/extensions/overview",
			"blurb": "DuckDB has a number of extensions available for use. Not all of them are included by default in every distribution,..."
		},
		{
			"title": "FILTER Clause",
			"text": "the filter clause may optionally follow an aggregate function in a select statement this will filter the rows of data that are fed into the aggregate function in the same way that a where clause filters rows but localized to the specific aggregate function filter s are not currently able to be used when the aggregate function is in a windowing context there are multiple types of situations where this is useful including when evaluating multiple aggregates with different filters and when creating a pivoted view of a dataset filter provides a cleaner syntax for pivoting data when compared with the more traditional case when approach discussed below some aggregate functions also do not filter out null values so using a filter clause will return valid results when at times the case when approach will not this occurs with the functions first and last which are desirable in a non-aggregating pivot operation where the goal is to simply re-orient the data into columns rather than re-aggregate it filter also improves null handling when using the list and array_agg functions as the case when approach will include null values in the list result while the filter clause will remove them examples -- compare total row count to -- the number of rows where i 5 -- the number of rows where i is odd select count as total_rows count filter where i 5 as lte_five count filter where i 2 1 as odds from generate_series 1 10 tbl i total_rows lte_five odds --- --- --- 10 5 5 -- different aggregate functions may be used and multiple where expressions are also permitted -- the sum of i for rows where i 5 -- the median of i where i is odd select sum i filter where i 5 as lte_five_sum median i filter where i 2 1 as odds_median median i filter where i 2 1 and i 5 as odds_lte_five_median from generate_series 1 10 tbl i lte_five_sum odds_median odds_lte_five_median --- --- --- 15 5 0 3 0 the filter clause can also be used to pivot data from rows into columns this is a static pivot as columns must be defined prior to runtime in sql however this kind of statement can be dynamically generated in a host programming language to leverage duckdb s sql engine for rapid larger than memory pivoting --first generate an example dataset create temp table stacked_data as select i case when i rows 0 25 then 2022 when i rows 0 5 then 2023 when i rows 0 75 then 2024 when i rows 0 875 then 2025 else null end as year from select i count over as rows from generate_series 1 100000000 tbl i tbl -- pivot the data out by year move each year out to a separate column select count i filter where year 2022 as 2022 count i filter where year 2023 as 2023 count i filter where year 2024 as 2024 count i filter where year 2025 as 2025 count i filter where year is null as nulls from stacked_data --this syntax produces the same results as the filter clauses above select count case when year 2022 then i end as 2022 count case when year 2023 then i end as 2023 count case when year 2024 then i end as 2024 count case when year 2025 then i end as 2025 count case when year is null then i end as nulls from stacked_data 2022 2023 2024 2025 nulls --- --- --- --- --- 25000000 25000000 25000000 12500000 12500000 however the case when approach will not work as expected when using an aggregate function that does not ignore null values the first function falls into this category so filter is preferred in this case -- pivot the data out by year move each year out to a separate column select first i filter where year 2022 as 2022 first i filter where year 2023 as 2023 first i filter where year 2024 as 2024 first i filter where year 2025 as 2025 first i filter where year is null as nulls from stacked_data 2022 2023 2024 2025 nulls --- --- --- --- --- 1474561 25804801 50749441 76431361 87500001 --this will produce null values whenever the first evaluation of the case when clause returns a null select first case when year 2022 then i end as 2022 first case when year 2023 then i end as 2023 first case when year 2024 then i end as 2024 first case when year 2025 then i end as 2025 first case when year is null then i end as nulls from stacked_data 2022 2023 2024 2025 nulls --- --- --- --- --- 1228801 null null null null aggregate function syntax including filter clause",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/filter",
			"blurb": "The FILTER clause may optionally follow an aggregate function in a SELECT statement. This will filter the rows of..."
		},
		{
			"title": "FROM & JOIN Clauses",
			"text": "the from clause specifies the source of the data on which the remainder of the query should operate logically the from clause is where the query starts execution the from clause can contain a single table a combination of multiple tables that are joined together using join clauses or another select query inside a subquery node duckdb also has an optional from -first syntax which enables you to also query without a select statement examples -- select all columns from the table called table_name from table_name -- select all columns from the table called table_name select from table_name -- select all columns from the table called table_name in the schema schema_name select from schema_name table_name -- select the column i from the table function range where the first column of the range function is renamed to i select t i from range 100 as t i -- select all columns from the csv file called test csv select from test csv -- select all columns from a subquery select from select from table_name -- join two tables together select from table_name join other_table on table_name key other_table key -- select a 10 sample from a table select from table_name tablesample 10 -- select a sample of 10 rows from a table select from table_name tablesample 10 rows joins joins are a fundamental relational operation used to connect two tables or relations horizontally the relations are referred to as the left and right sides of the join based on how they are written in the join clause each result row has the columns from both relations a join uses a rule to match pairs of rows from each relation often this is a predicate but there are other implied rules that may be specified outer joins rows that do not have any matches can still be returned if an outer join is specified outer joins can be one of left all rows from the left relation appear at least once right all rows from the right relation appear at least once full all rows from both relations appear at least once a join that is not outer is inner only rows that get paired are returned when an unpaired row is returned the attributes from the other table are set to null cross product joins the simplest type of join is a cross join there are no conditions for this type of join and it just returns all the possible pairs -- return all pairs of rows select a b from a cross join b conditional joins most joins are specified by a predicate that connects attributes from one side to attributes from the other side the conditions can be explicitly specified using an on clause with the join clearer or implied by the where clause old-fashioned -- return the regions for the nations select n r from l_nations n join l_regions r on n_regionkey r_regionkey if the column names are the same and are required to be equal then the simpler using syntax can be used -- return the regions for the nations select n r from l_nations n join l_regions r using regionkey the expressions to not have to be equalities - any predicate can be used -- return the pairs of jobs where one ran longer but cost less select s1 t_id s2 t_id from west s1 west s2 where s1 time s2 time and s1 cost s2 cost semi and anti joins semi joins return rows from the left table that have at least one match in the right table anti joins return rows from the left table that have no matches in the right table when using a semi or anti join the result will never have more rows than the left hand side table semi and anti joins provide the same logic as not in statements -- return a list of cars that have a valid region select cars name cars manufacturer from cars semi join region on cars region region id -- return a list of cars with no recorded safety data select cars name cars manufacturer from cars anti join safety_data on cars safety_report_id safety_data report_id positional joins when working with data frames or other embedded tables of the same size the rows may have a natural correspondence based on their physical order in scripting languages this is easily expressed using a loop for i 0 i n i f t1 a i t2 b i it is difficult to express this in standard sql because relational tables are not ordered but imported tables like data frames or disk files like csvs or parquet files do have a natural ordering connecting them using this ordering is called a positional join -- treat two data frames as a single table select df1 df2 from df1 positional join df2 positional joins are always full outer joins as-of joins a common operation when working with temporal or similarly-ordered data is to find the nearest first event in a reference table such as prices this is called an as-of join -- attach prices to stock trades select t p price from trades t asof join prices p on t symbol p symbol and t when p when the asof join requires at least one inequality condition on the ordering field the inequality can be any inequality condition on any data type but the most common form is on a temporal type any other conditions must be equalities or not distinct this means that the left right order of the tables is significant asof joins each left side row with at most one right side row it can be specified as an outer join to find unpaired rows e g trades without prices or prices which have no trades -- attach prices or nulls to stock trades select from trades t asof left join prices p on t symbol p symbol and t when p when asof joins can also specify join conditions on matching column names with the using syntax but the last attribute in the list must be the inequality which will be greater than or equal to select from trades t asof join prices p using symbol when -- returns symbol trades when price but not prices when if you combine using with a select like this the query will return the left side probe column values for the matches not the right side build column values to get the prices times in the example you will need to list the columns explicitly select t symbol t when as trade_when p when as price_when price from trades t asof left join prices p using symbol when syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/from",
			"blurb": "The FROM clause can contain a single table, a combination of multiple tables that are joined together, or another..."
		},
		{
			"title": "Filesystems",
			"text": "this feature is experimental and is subject to change duckdb support for fsspec filesystems allows querying data in filesystems that duckdb s httpfs extension does not support fsspec has a large number of inbuilt filesystems and there are also many external implementations this capability is only available in duckdb s python client because fsspec is a python library while the httpfs extension is available in many duckdb clients example the following is an example of using fsspec to query a file in google cloud storage instead of using their s3 inter-compatibility api firstly you must install duckdb and fsspec and a filesystem interface of your choice pip install duckdb fsspec gcsfs then you can register whichever filesystem you d like to query import duckdb from fsspec import filesystem this line will throw an exception if the appropriate filesystem interface is not installed duckdb register_filesystem filesystem gcs duckdb sql select from read_csv_auto gcs bucket file csv potential issues please also note that as these filesystems are not implemented in c their performance may not be comparable to the ones provided by the httpfs extension it s also worth noting that as they are third party libraries they may contain bugs that are beyond our control",
			"category": "Python",
			"url": "/docs/guides/python/filesystems",
			"blurb": "This feature is experimental, and is subject to change DuckDB support for fsspec filesystems allows querying data in..."
		},
		{
			"title": "Full Text Search",
			"text": "full text search is an extension to duckdb that allows for search through strings similar to sqlite s fts5 extension api the extension adds two pragma statements to duckdb one to create and one to drop an index additionally a scalar macro stem is added which is used internally by the extension pragma create_fts_index create_fts_index input_table input_id input_values stemmer porter stopwords english ignore a-z strip_accents 1 lower 1 overwrite 0 pragma that creates a fts index for the specified table name type description -- -- -- input_table varchar qualified name of specified table e g table_name or main table_name input_id varchar column name of document identifier e g document_identifier input_values varchar column names of the text fields to be indexed vararg e g text_field_1 text_field_2 text_field_n or for all columns in input_table of type varchar stemmer varchar the type of stemmer to be used one of arabic basque catalan danish dutch english finnish french german greek hindi hungarian indonesian irish italian lithuanian nepali norwegian porter portuguese romanian russian serbian spanish swedish tamil turkish or none if no stemming is to be used defaults to porter stopwords varchar qualified name of table containing a single varchar column containing the desired stopwords or none if no stopwords are to be used defaults to english for a pre-defined list of 571 english stopwords ignore varchar regular expression of patterns to be ignored defaults to a-z ignoring all escaped and non-alphabetic lowercase characters strip_accents boolean whether to remove accents e g convert \u00e1 to a defaults to 1 lower boolean whether to convert all text to lowercase defaults to 1 overwrite boolean whether to overwrite an existing index on a table defaults to 0 this pragma builds the index under a newly created schema the schema will be named after the input table if an index is created on table main table_name then the schema will be named fts_main_table_name pragma drop_fts_index drop_fts_index input_table drops a fts index for the specified table name type description -- -- -- input_table varchar qualified name of input table e g table_name or main table_name match_bm25 match_bm25 input_id query_string fields null k 1 2 b 0 75 conjunctive 0 when an index is built this retrieval macro is created that can be used to search the index name type description -- -- -- input_id varchar column name of document identifier e g document_identifier query_string varchar the string to search the index for fields varchar comma-separarated list of fields to search in e g text_field_2 text_field_n defaults to null to search all indexed fields k double parameter k sub 1 sub in the okapi bm25 retrieval model defaults to 1 2 b double parameter b in the okapi bm25 retrieval model defaults to 0 75 conjunctive boolean whether to make the query conjunctive i e all terms in the query string must be present in order for a document to be retrieved stem stem input_string stemmer reduces words to their base used internally by the extension name type description -- -- -- input_string varchar the column or constant to be stemmed stemmer varchar the type of stemmer to be used one of arabic basque catalan danish dutch english finnish french german greek hindi hungarian indonesian irish italian lithuanian nepali norwegian porter portuguese romanian russian serbian spanish swedish tamil turkish or none if no stemming is to be used example usage -- create a table and fill it with text data create table documents document_identifier varchar text_content varchar author varchar doc_version integer insert into documents values doc1 the mallard is a dabbling duck that breeds throughout the temperate hannes m\u00fchleisen 3 doc2 the cat is a domestic species of small carnivorous mammal laurens kuiper 2 -- build the index make both the text_content and author columns searchable pragma create_fts_index documents document_identifier text_content author -- search the author field index for documents that are written by hannes - this retrieves doc1 select text_content score from select fts_main_documents match_bm25 document_identifier muhleisen fields author as score from documents sq where score is not null and doc_version 2 order by score desc -- search for documents about small cats - this retrieves doc2 select text_content score from select fts_main_documents match_bm25 document_identifier small cats as score from documents sq where score is not null order by score desc caveats note that the fts index will not update automatically when input table changes a workaround of this limitation can be recreating the index to refresh",
			"category": "Extensions",
			"url": "/docs/extensions/full_text_search",
			"blurb": "Full Text Search is an extension to DuckDB that allows for search through strings, similar to SQLite's FTS5..."
		},
		{
			"title": "Functions",
			"text": "functions are query functions duckdb_functions table function shows the list of functions currently built into the system d select distinct on function_name function_name function_type return_type parameters parameter_types from duckdb_functions where function_type scalar limit 10 function_name function_type return_type parameters parameter_types log10 scalar double col0 double mod scalar tinyint col0 col1 tinyint tinyint date_diff scalar bigint col0 col1 col2 varchar date date writefile scalar varchar regexp_replace scalar varchar col0 col1 col2 col3 varchar varchar varchar varchar age scalar interval col0 timestamp age scalar interval col0 col1 timestamp timestamp datediff scalar bigint col0 col1 col2 varchar date date map scalar map year scalar bigint col0 timestamp with time zone currently the description and parameter names of functions are still missing more",
			"category": "Functions",
			"url": "/docs/sql/functions/overview",
			"blurb": "Functions are ... Query functions duckdb_functions table function shows the list of functions currently built into..."
		},
		{
			"title": "GROUP BY Clause",
			"text": "the group by clause specifies which grouping columns should be used to perform any aggregations in the select clause if the group by clause is specified the query is always an aggregate query even if no aggregations are present in the select clause when a group by clause is specified all tuples that have matching data in the grouping columns i e all tuples that belong to the same group will be combined the values of the grouping columns themselves are unchanged and any other columns can be combined using an aggregate function such as count sum avg etc group by all use group by all to group by all columns in the select statement that are not wrapped in aggregate functions this simplifies the syntax by allowing the columns list to be maintained in a single location and prevents bugs by keeping the select granularity aligned to the group by granularity ex prevents any duplication see examples below and additional examples in the friendlier sql with duckdb blog post multiple dimensions normally the group by clause groups along a single dimension using the grouping sets cube or rollup clauses it is possible to group along multiple dimensions see the grouping sets page for more information examples -- count the number of entries in the addresses table that belong to each different city select city count from addresses group by city -- compute the average income per city per street_name select city street_name avg income from addresses group by city street_name group by all examples -- group by city and street_name to remove any duplicate values select city street_name from addresses group by all -- group by city street_name -- compute the average income per city per street_name -- since income is wrapped in an aggregate function do not include it in the group by select city street_name avg income from addresses group by all -- group by city street_name syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/groupby",
			"blurb": "The GROUP BY clause specifies which grouping columns should be used to perform any aggregations in the SELECT clause...."
		},
		{
			"title": "GROUPING SETS",
			"text": "grouping sets rollup and cube can be used in the group by clause to perform a grouping over multiple dimensions within the same query note that this syntax is not compatible with group by all examples -- compute the average income along the provided four different dimensions -- signifies the empty set i e computing an ungrouped aggregate select city street_name avg income from addresses group by grouping sets city street_name city street_name -- compute the average income along the same dimensions select city street_name avg income from addresses group by cube city street_name -- compute the average income along the dimensions city street_name city and select city street_name avg income from addresses group by rollup city street_name description grouping sets perform the same aggregate across different group by clauses in a single query create table students course varchar type varchar insert into students course type values cs bachelor cs bachelor cs phd math masters cs null cs null math null select course type count from students group by grouping sets course type course type course type count_star cs bachelor 2 cs phd 1 math masters 1 cs null 2 math null 1 cs null 5 math null 2 null bachelor 2 null phd 1 null masters 1 null null 3 null null 7 in the above query we group across four different sets course type course type and the empty group the result contains null for a group which is not in the grouping set for the result i e the above query is equivalent to the following union statement -- group by course type select course type count from students group by course type union all -- group by type select null as course type count from students group by type union all -- group by course select course null as type count from students group by course union all -- group by nothing select null as course null as type count from students cube and rollup are syntactic sugar to easily produce commonly used grouping sets the rollup clause will produce all sub-groups of a grouping set e g rollup country city zip produces the grouping sets country city zip country city country this can be useful for producing different levels of detail of a group by clause this produces n 1 grouping sets where n is the amount of terms in the rollup clause cube produces grouping sets for all combinations of the inputs e g cube country city zip will produce country city zip country city country zip city zip country city zip this produces 2 n grouping sets grouping alias grouping_id is a special aggregate function that can be used in combination with grouping sets the grouping function takes as parameters a group and returns 0 if the group is included in the grouping for that row or 1 otherwise this is primarily useful because the grouping columns by which we do not aggregate return null which is ambiguous with groups that are actually the value null the grouping or grouping_id function can be used to distinguish these two cases syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/grouping_sets",
			"blurb": "GROUPING SETS , ROLLUP and CUBE can be used in the GROUP BY clause to perform a grouping over multiple dimensions..."
		},
		{
			"title": "Guides",
			"text": "the guides section contains compact how-to guides that are focused on achieving a single goal for an in-depth reference see the documentation section duckdb sql asof join full text search sql data import export csv files how to load a csv file into a table how to export a table to a csv file parquet files how to load a parquet file into a table how to export a table to a parquet file how to run a query directly on a parquet file json files how to load a json file into a table how to export a table to a json file excel files with the spatial extension how to load an excel file into a table how to export a table to an excel file http s3 and gcp how to load a parquet file directly from http s how to load a parquet file directly from s3 or gcs sql meta queries how to list all tables how to view the schema of the result of a query how to quickly get a feel for a dataset using summarize how to view the query plan of a query how to profile a query python client how to install the python client how to execute sql queries how to easily query duckdb in jupyter notebooks how to use multiple python threads with duckdb how to use fsspec filesystems with duckdb pandas how to execute sql on a pandas dataframe how to create a table from a pandas dataframe how to export data to a pandas dataframe apache arrow how to execute sql on apache arrow how to create a duckdb table from apache arrow how to export data to apache arrow relational api how to query pandas dataframes with the relational api python library integrations how to use ibis to query duckdb with or without sql how to use fuguesql to use duckdb with python pandas functions how to use duckdb with polars dataframes via apache arrow how to use duckdb with vaex dataframes via apache arrow how to use duckdb with datafusion via apache arrow sql editors ide s how to set up the dbeaver sql ide data viewers how to use tad to view tabular data files and duckdb databases how to visualise duckdb databases with tableau how to draw command-line plots with duckdb and youplot",
			"category": "Guides",
			"url": "/docs/guides/index",
			"blurb": "The guides section contains compact how-to guides that are focused on achieving a single goal. For an in-depth..."
		},
		{
			"title": "HAVING Clause",
			"text": "the having clause can be used after the group by clause to provide filter criteria after the grouping has been completed in terms of syntax the having clause is identical to the where clause but while the where clause occurs before the grouping the having clause occurs after the grouping examples -- count the number of entries in the addresses table that belong to each different city -- filtering out cities with a count below 50 select city count from addresses group by city having count 50 -- compute the average income per city per street_name -- filtering out cities with an average income bigger than twice the median income select city street_name avg income from addresses group by city street_name having avg income 2 median income syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/having",
			"blurb": "The HAVING clause can be used after the GROUP BY clause to provide filter criteria after the grouping has been..."
		},
		{
			"title": "HTTP Parquet Import",
			"text": "to load a parquet file over http s the httpfs extension is required this can be installed use the install sql command this only needs to be run once install httpfs to load the httpfs extension for usage use the load sql command load httpfs after the httpfs extension is set up parquet files can be read over http s using the following command select from read_parquet https domain path to file parquet",
			"category": "Import",
			"url": "/docs/guides/import/http_import",
			"blurb": "To load a Parquet file over http(s) , the HTTPFS extension is required. This can be installed use the INSTALL SQL..."
		},
		{
			"title": "HTTPFS",
			"text": "the httpfs extension is a loadable extension implementing a file system that allows reading remote writing remote files for pure http s only file reading is supported for object storage using the s3 api the httpfs extension supports reading writing globbing files some clients come prebundled with this extension in which case it s not necessary to first install or even load the extension depending on the client you use no action may be required or you might have to install httpfs on first use and use load httpfs at the start of every session http s with the httpfs extension it is possible to directly query files over http s this currently works for csv json and parquet files select from https domain tld file extension for csv files files will be downloaded entirely in most cases due to the row-based nature of the format for parquet files duckdb can use a combination of the parquet metadata and http range requests to only download the parts of the file that are actually required by the query for example the query select column_a from https domain tld file parquet will only read the parquet metadata and the data for the column_a column in some cases even no actual data needs to be read at all as they only require reading the metadata select count from https domain tld file parquet scanning multiple files over http s is also supported select from read_parquet https domain tld file1 parquet https domain tld file2 parquet -- parquet_scan is an alias of read_parquet so they are equivalent select from parquet_scan https domain tld file1 parquet https domain tld file2 parquet s3 the httpfs extension supports reading writing globbing files on object storage servers using the s3 api requirements the httpfs filesystem is tested with aws s3 minio google cloud and lakefs other services that implement the s3 api should also work but not all features may be supported below is a list of which parts of the s3 api are required for each httpfs feature feature required s3 api features --- --- public file reads http range requests private file reads secret key or session token authentication file glob listobjectv2 file writes multipart upload configuration to be able to read or write from s3 the correct region should be set set s3_region us-east-1 optionally the endpoint can be configured in case a non-aws object storage server is used set s3_endpoint domain tld port if the endpoint is not ssl-enabled then run set s3_use_ssl false switching between path-style and vhost-style urls is possible using set s3_url_style path however note that this may also require updating the endpoint for example for aws s3 it is required to change the endpoint to s3 region amazonaws com after configuring the correct endpoint and region public files can be read to also read private files authentication credentials can be added set s3_access_key_id aws access key id set s3_secret_access_key aws secret access key alternatively session tokens are also supported and can be used instead set s3_session_token aws session token per-request configuration aside from the global s3 configuration described above specific configuration values can be used on a per-request basis this allows for use of multiple sets of credentials regions etc these are used by including them on the s3 url as query parameters all the individual configuration values listed above can be set as query parameters for instance s3 bucket file parquet s3_access_key_id accesskey s3_secret_access_key secretkey or select from s3 bucket file parquet s3_region region s3_session_token session_token t1 inner join s3 bucket file csv s3_access_key_id accesskey s3_secret_access_key secretkey t2 reading reading files from s3 is now as simple as select from s3 bucket file extension multiple files are also possible for example select from read_parquet s3 bucket file1 parquet s3 bucket file2 parquet glob file globbing is implemented using the listobjectv2 api call and allows to use filesystem-like glob patterns to match multiple files for example select from read_parquet s3 bucket parquet this query matches all files in the root of the bucket with the parquet extension several features for matching are supported such as to match any number of any character for any single character or 0-9 for a single character in a range of characters select count from read_parquet s3 bucket folder 100 t 0-9 parquet a useful feature when using globs is the filename option which adds a column with the file that a row originated from select from read_parquet s3 bucket parquet filename 1 could for example result in column_a column_b filename --- --- --- 1 examplevalue1 s3 bucket file1 parquet 2 examplevalue1 s3 bucket file2 parquet hive partitioning duckdb also offers support for the hive partitioning scheme in the hive partitioning scheme data is partitioned in separate files the columns by which the data is partitioned are not actually in the files but are encoded in the file path so for example let us consider three parquet files hive paritioned by year s3 bucket year 2012 file parquet s3 bucket year 2013 file parquet s3 bucket year 2014 file parquet if scanning these files with the hive_partitioning option enabled select from read_parquet s3 bucket file parquet hive_partitioning 1 could result in column_a column_b year --- --- --- 1 examplevalue1 2012 2 examplevalue2 2013 3 examplevalue3 2014 note that the year column does not actually exist in the parquet files it is parsed from the filenames within duckdb however these columns behave just like regular columns for example filters can be applied on hive partition columns select from read_parquet s3 bucket file parquet hive_partitioning 1 where year 2013 writing writing to s3 uses the multipart upload api this allows duckdb to robustly upload files at high speed writing to s3 works for both csv and parquet copy table_name to s3 bucket file extension partioned copy to s3 also works copy table to s3 my-bucket partitioned format parquet partition_by part_col_a part_col_b an automatic check is performed for existing files directories which is currently quite conservative and on s3 will add a bit of latency to disable this check and force writing an allow_overwrite flag is added copy table to s3 my-bucket partitioned format parquet partition_by part_col_a part_col_b allow_overwrite true the naming scheme of the written files looks like this s3 my-bucket partitioned part_col_a val part_col_b val data_ thread_number parquet configuration some additional configuration options exist for the s3 upload though the default values should suffice for most use cases setting description --- --- s3_uploader_max_parts_per_file used for part size calculation see aws docs s3_uploader_max_filesize used for part size calculation see aws docs s3_uploader_thread_limit maximum number of uploader threads",
			"category": "Extensions",
			"url": "/docs/extensions/httpfs",
			"blurb": "The httpfs extension is a loadable extension implementing a file system that allows reading remote/writing remote..."
		},
		{
			"title": "Hive Partitioning",
			"text": "examples -- read data from a hive partitioned data set select from read_parquet orders parquet hive_partitioning 1 -- parquet_scan is an alias of read_parquet so they are equivalent select from parquet_scan orders parquet hive_partitioning 1 -- write a table to a hive partitioned data set copy orders to orders format parquet partition_by year month hive partitioning hive partitioning is a partitioning strategy that is used to split a table into multiple files based on partition keys the files are organized into folders within each folder the partition key has a value that is determined by the name of the folder below is an example of a hive partitioned file hierarchy the files are partitioned on two keys year and month orders year 2021 month 1 file1 parquet file2 parquet month 2 file3 parquet year 2022 month 11 file4 parquet file5 parquet month 12 file6 parquet files stored in this hierarchy can be read using the hive_partitioning flag select from read_parquet orders parquet hive_partitioning 1 when we specify the hive_partitioning flag the values of the columns will be read from the directories filter pushdown filters on the partition keys are automatically pushed down into the files this way the system skips reading files that are not necessary to answer a query for example consider the following query on the above dataset select from read_parquet orders parquet hive_partitioning 1 where year 2022 and month 11 when executing this query only the following files will be read orders year 2022 month 11 file4 parquet file5 parquet autodetection by default the system tries to infer if the provided files are in a hive partitioned hierarchy and if so the hive_partitioning flag is enabled automatically the autodetection will look at the names of the folders and search for a key value pattern this behaviour can be overridden by setting the hive_partitioning flag manually hive types hive_types is a way to specify the logical types of the hive partitions in a struct from read_parquet dir parquet hive_partitioning 1 hive_types release date orders bigint hive_types will be autodetected for the following types date timestamp and bigint to switch off the autodetection the flag hive_types_autocast 0 can be set writing partitioned files see the partitioned writes section",
			"category": "Partitioning",
			"url": "/docs/data/partitioning/hive_partitioning",
			"blurb": "Examples -- read data from a hive partitioned data set SELECT * FROM read_parquet('orders/*/*/*.parquet',..."
		},
		{
			"title": "IN Operator",
			"text": "the in operator checks containment of the left expression inside the set of expressions on the right hand side rhs the in operator returns true if the expression is present in the rhs false if the expression is not in the rhs and the rhs has no null values or null if the expression is not in the rhs and the rhs has null values select math in cs math -- true select english in cs math -- false select math in cs math null -- true select english in cs math null -- null not in can be used to check if an element is not present in the set x not in y is equivalent to not x in y the in operator can also be used with a subquery that returns a single column see the subqueries page for more information",
			"category": "Expressions",
			"url": "/docs/sql/expressions/in",
			"blurb": "The IN operator checks containment of the left expression inside the set of expressions on the right hand side (RHS)...."
		},
		{
			"title": "Import From Apache Arrow",
			"text": "create table as and insert into can be used to create a table from any query we can then create tables or insert into existing tables by referring to referring to the apache arrow object in the query this example imports from an arrow table but duckdb can query different apache arrow formats as seen in the sql on arrow guide import duckdb import pyarrow as pa connect to an in-memory database my_arrow pa table from_pydict a 42 create the table my_table from the dataframe my_arrow duckdb sql create table my_table as select from my_arrow insert into the table my_table from the dataframe my_arrow duckdb sql insert into my_table select from my_arrow",
			"category": "Python",
			"url": "/docs/guides/python/import_arrow",
			"blurb": "CREATE TABLE AS and INSERT INTO can be used to create a table from any query. We can then create tables or insert..."
		},
		{
			"title": "Import From Pandas",
			"text": "create table as and insert into can be used to create a table from any query we can then create tables or insert into existing tables by referring to referring to the pandas dataframe in the query import duckdb import pandas create a pandas dataframe my_df pandas dataframe from_dict a 42 create the table my_table from the dataframe my_df note duckdb sql connects to the default in-memory database connection duckdb sql create table my_table as select from my_df insert into the table my_table from the dataframe my_df duckdb sql insert into my_table select from my_df",
			"category": "Python",
			"url": "/docs/guides/python/import_pandas",
			"blurb": "CREATE TABLE AS and INSERT INTO can be used to create a table from any query. We can then create tables or insert..."
		},
		{
			"title": "Importing Data",
			"text": "the first step to using a database system is to insert data into that system duckdb provides several data ingestion methods that allow you to easily and efficiently fill up the database in this section we provide an overview of these methods so you can select which one is correct for you insert statements insert statements are the standard way of loading data into a database system they are suitable for quick prototyping but should be avoided for bulk loading as they have significant per-row overhead insert into people values 1 mark see here for a more detailed description of insert statements csv loading data can be efficiently loaded from csv files using the read_csv_auto function or the copy statement select from read_csv_auto test csv you can also load data from compressed e g compressed with gzip csv files for example select from read_csv_auto test csv gz see here for a detailed description of csv loading parquet loading parquet files can be efficiently loaded and queried using the read_parquet function select from read_parquet test parquet see here for a detailed description of parquet loading json loading json files can be efficiently loaded and queried using the read_json_auto function select from read_json_auto test json see here for a detailed description of json loading appender c and java in c and java the appender can be used as an alternative for bulk data loading this class can be used to efficiently add rows to the database system without needing to use sql c appender appender con people appender appendrow 1 mark appender close java con createappender main people appender beginrow appender append mark appender endrow appender close see here for a detailed description of the c appender",
			"category": "Data",
			"url": "/docs/data/overview",
			"blurb": "The first step to using a database system is to insert data into that system. DuckDB provides several data ingestion..."
		},
		{
			"title": "Indexes",
			"text": "index types duckdb currently uses two index types a min-max index is automatically created for columns of all general-purpose data types an adaptive radix tree art is mainly used to ensure primary key constraints and to speed up point and very highly selective i e 0 1 queries such an index is automatically created for columns with a unique or primary key constraint and can be defined using create index joins on columns with an art index can make use of the index join algorithm forcing index joins is possible using pragmas art indexes must currently be able to fit in-memory avoid creating art indexes if the index does not fit in memory persistence min-max indexes are persisted art indexes are persisted create index create index constructs an index on the specified column s of the specified table compound indexes on multiple columns expressions are supported currently unidimensional indexes are supported multidimensional indexes are not supported parameters name description --- --- unique causes the system to check for duplicate values in the table when the index is created if data already exist and each time data is added attempts to insert or update data that would result in duplicate entries will generate an error name the name of the index to be created table the name of the table to be indexed column the name of the column to be indexed expression an expression based on one or more columns of the table the expression usually must be written with surrounding parentheses as shown in the syntax however the parentheses can be omitted if the expression has the form of a function call examples -- create a unique index films_id_idx on the column id of table films create unique index films_id_idx on films id -- create index s_idx that allows for duplicate values on column revenue of table films create index s_idx on films revenue -- create compound index gy_idx on genre and year columns create index gy_idx on films genre year -- create index i_index on the expression of the sum of columns j and k from table integers create index i_index on integers j k drop index drop index drops an existing index from the database system parameters name description --- --- if exists do not throw an error if the index does not exist name the name of an index to remove examples -- remove the index title_idx drop index title_idx index limitations art indexes create a secondary copy of the data in a second location - this complicates processing particularly when combined with transactions certain limitations apply when it comes to modifying data that is also stored in secondary indexes updates become deletes and inserts when an update statement is executed on a column that is present in an index - the statement is transformed into a delete of the original row followed by an insert this has certain performance implications particularly for wide tables as entire rows are rewritten instead of only the affected columns over-eager unique constraint checking due to the presence of transactions data can only be removed from the index after 1 the transaction that performed the delete is committed and 2 no further transactions exist that refer to the old entry still present in the index as a result of this - transactions that perform deletions followed by insertions may trigger unexpected unique constraint violations as the deleted tuple has not actually been removed from the index yet for example create table students id integer primary key name varchar insert into students values 1 student 1 begin delete from students where id 1 insert into students values 1 student 2 -- constraint error duplicate key id 1 violates primary key constraint this combined with the fact that updates are turned into deletions and insertions within the same transaction means that updating rows in the presence of unique or primary key constraints can often lead to unexpected unique constraint violations create table students id integer primary key name varchar insert into students values 1 student 1 update students set name student 2 id 1 where id 1 -- constraint error duplicate key id 1 violates primary key constraint currently this is an expected limitation of the system - although we aim to resolve this in the future",
			"category": "SQL",
			"url": "/docs/sql/indexes",
			"blurb": "Index types DuckDB currently uses two index types: A min-max index is automatically created for columns of all..."
		},
		{
			"title": "Information Schema",
			"text": "the views in the information_schema are sql-standard views that describe the catalog entries of the database these views can be filtered to obtain information about a specific column or table database catalog and schema the top level catalog view is information_schema schemata it lists the catalogs and the schemas present in the database and has the following layout column description type example --- --- --- --- catalog_name name of the database that the schema is contained in varchar null schema_name name of the schema varchar main schema_owner name of the owner of the schema not yet implemented varchar null default_character_set_catalog applies to a feature not available in duckdb varchar null default_character_set_schema applies to a feature not available in duckdb varchar null default_character_set_name applies to a feature not available in duckdb varchar null sql_path the file system location of the database currently unimplemented varchar null tables and views the view that describes the catalog information for tables and views is information_schema tables it lists the tables present in the database and has the following layout column description type example --- --- --- --- table_catalog the catalog the table or view belongs to varchar null table_schema the schema the table or view belongs to varchar main table_name the name of the table or view varchar widgets table_type the type of table one of base table local temporary view varchar base table self_referencing_column_name applies to a feature not available in duckdb varchar null reference_generation applies to a feature not available in duckdb varchar null user_defined_type_catalog if the table is a typed table the name of the database that contains the underlying data type always the current database else null currently unimplemented varchar null user_defined_type_schema if the table is a typed table the name of the schema that contains the underlying data type else null currently unimplemented varchar null user_defined_type_name if the table is a typed table the name of the underlying data type else null currently unimplemented varchar null is_insertable_into yes if the table is insertable into no if not base tables are always insertable into views not necessarily varchar yes is_typed yes if the table is a typed table no if not varchar no commit_action not yet implemented varchar no columns the view that describes the catalog information for columns is information_schema columns it lists the column present in the database and has the following layout column description type example --- --- --- --- table_catalog name of the database containing the table varchar null table_schema name of the schema containing the table varchar main table_name name of the table varchar widgets column_name name of the column varchar price ordinal_position ordinal position of the column within the table count starts at 1 integer 5 column_default default expression of the column varchar 1 99 is_nullable yes if the column is possibly nullable no if it is known not nullable varchar yes data_type data type of the column varchar decimal 18 2 character_maximum_length if data_type identifies a character or bit string type the declared maximum length null for all other data types or if no maximum length was declared integer 255 character_octet_length if data_type identifies a character type the maximum possible length in octets bytes of a datum null for all other data types the maximum octet length depends on the declared character maximum length see above and the character encoding integer 1073741824 numeric_precision if data_type identifies a numeric type this column contains the declared or implicit precision of the type for this column the precision indicates the number of significant digits for all other data types this column is null integer 18 numeric_scale if data_type identifies a numeric type this column contains the declared or implicit scale of the type for this column the precision indicates the number of significant digits for all other data types this column is null integer 2 datetime_precision if data_type identifies a date time timestamp or interval type this column contains the declared or implicit fractional seconds precision of the type for this column that is the number of decimal digits maintained following the decimal point in the seconds value no fractional seconds are currently supported in duckdb for all other data types this column is null integer 0 catalog functions several functions are also provided to see details about the schemas that are configured in the database function description example result --- --- --- --- current_schema return the name of the currently active schema default is main current_schema main current_schemas boolean return list of schemas pass a parameter of true to include implicit schemas current_schemas true temp main pg_catalog",
			"category": "SQL",
			"url": "/docs/sql/information_schema",
			"blurb": "The views in the information_schema are SQL-standard views that describe the catalog entries of the database. These..."
		},
		{
			"title": "Insert Statement",
			"text": "the insert statement inserts new data into a table examples -- insert the values 1 2 3 into tbl insert into tbl values 1 2 3 -- insert the result of a query into a table insert into tbl select from other_tbl -- insert values into the i column inserting the default value into other columns insert into tbl i values 1 2 3 -- explicitly insert the default value into a column insert into tbl i values 1 default 3 -- assuming tbl has a primary key unique constraint do nothing on conflict insert or ignore into tbl i values 1 -- or update the table with the new values instead insert or replace into tbl i values 1 syntax insert into inserts new rows into a table one can insert one or more rows specified by value expressions or zero or more rows resulting from a query insert column order it s possible to provide an optional insert column order this can either be by position the default or by name each column not present in the explicit or implicit column list will be filled with a default value either its declared default value or null if there is none if the expression for any column is not of the correct data type automatic type conversion will be attempted by position the order that values are inserted into the columns of the table is determined by the order that the columns were declared in this can be overridden by providing column names as part of the target for example create table tbl a integer b integer insert into tbl b a values 5 42 this will insert 5 into b and 42 into a the values supplied by the values clause or query are associated with the column list left-to-right by name the names of the column list of the select statement are matched against the column names of the table to determine the order that values should be inserted into the table even if the order of the columns in the table differs from the order of the values in the select statement for example create table tbl a integer b integer insert into tbl by name select 42 as b this will insert 42 into b and insert null or its default value into a it s important to note that when using insert into table by name the column names specified in the select statement must match the column names in the table if a column name is misspelled or does not exist in the table an error will occur this is not a problem however if columns are missing from the select statement as those will be filled with the default value on conflict clause an on conflict clause can be used to perform a certain action on conflicts that arise from unique or primary key constraints a conflict_target may also be provided which is a group of columns that an index indexes on or if left out all unique or primary key constraint s on the table are targeted the conflict_target is optional unless using a do update see below and there are multiple unique primary key constraints on the table when a conflict target is provided you can further filter this with a where clause that should be met by all conflicts if a conflict does not meet this condition an error will be thrown instead and the entire operation is aborted because we need a way to refer to both the to-be-inserted tuple and the existing tuple we introduce the special excluded qualifier when the excluded qualifier is provided the reference refers to the to-be-inserted tuple otherwise it refers to the existing tuple this special qualifier can be used within the where clauses and set expressions of the on conflict clause there are two supported actions do nothing causes the error s to be ignored and the values are not inserted or updated do update causes the insert to turn into an update on the conflicting row s instead the set expressions that follow determine how these rows are updated optionally you can provide an additional where clause that can exclude certain rows from the update the conflicts that don t meet this condition are ignored instead insert or replace is a shorter syntax alternative to on conflict do update set c1 excluded c1 c2 excluded c2 it updates every column of the existing row to the new values of the to-be-inserted row insert or ignore is a shorter syntax alternative to on conflict do nothing returning clause the returning clause may be used to return the contents of the rows that were inserted this can be useful if some columns are calculated upon insert for example if the table contains an automatically incrementing primary key then the returning clause will include the automatically created primary key this is also useful in the case of generated columns some or all columns can be explicitly chosen to be returned and they may optionally be renamed using aliases arbitrary non-aggregating expressions may also be returned instead of simply returning a column all columns can be returned using the expression and columns or expressions can be returned in addition to all columns returned by the -- a basic example to show the syntax create table t1 i int insert into t1 select 1 returning i --- 1 -- a more complex example that includes an expression in the returning clause create table t2 i int j int insert into t2 select 2 as i 3 as j returning i j as i_times_j i j i_times_j --- --- ----------- 2 3 6 this example shows a situation where the returning clause is more helpful first a table is created with a primary key column then a sequence is created to allow for that primary key to be incremented as new rows are inserted when we insert into the table we do not already know the values generated by the sequence so it is valuable to return them for additional information see the create sequence documentation create table t3 i int primary key j int create sequence t3_key insert into t3 select nextval t3_key as i 42 as j union all select nextval t3_key as i 43 as j returning i j --- --- 1 42 2 43",
			"category": "Statements",
			"url": "/docs/sql/statements/insert",
			"blurb": "The INSERT statement inserts new data into a table. Examples -- insert the values (1), (2), (3) into tbl INSERT INTO..."
		},
		{
			"title": "Insert Statements",
			"text": "insert statements are the standard way of loading data into a relational database when using insert statements the values are supplied row-by-row while simple there is significant overhead involved in parsing and processing individual insert statements this makes lots of individual row-by-row insertions very inefficient for bulk insertion as a rule-of-thumb avoid using lots of individual row-by-row insert statements when inserting more than a few rows i e avoid using insert statements as part of a loop when bulk inserting data try to maximize the amount of data that is inserted per statement if you must use insert statements to load data in a loop avoid executing the statements in auto-commit mode after every commit the database is required to sync the changes made to disk to ensure no data is lost in auto-commit mode every single statement will be wrapped in a separate transaction meaning fsync will be called for every statement this is typically unnecessary when bulk loading and will significantly slow down your program if you absolutely must use insert statements in a loop to load data wrap them in calls to begin transaction and commit syntax an example of using insert into to load data in a table is as follows create table people id integer name varchar insert into people values 1 mark 2 hannes a more detailed description together with syntax diagram can be found here",
			"category": "Data",
			"url": "/docs/data/insert",
			"blurb": "Insert statements are the standard way of loading data into a relational database. When using insert statements, the..."
		},
		{
			"title": "Install the Python Client",
			"text": "the latest release of the python client can be installed using pip pip install duckdb the pre-release python client can be installed using --pre pip install duckdb --upgrade --pre the python client can be installed from source from the tools pythonpkg directory in the duckdb github repository cd tools pythonpkg python setup py install",
			"category": "Python",
			"url": "/docs/guides/python/install",
			"blurb": "The latest release of the Python client can be installed using pip . pip install duckdb The pre-release Python client..."
		},
		{
			"title": "Instantiation",
			"text": "duckdb-wasm has multiple ways to be instantiated depending on the use case instantiation cdn jsdelivr import as duckdb from duckdb duckdb-wasm const jsdelivr_bundles duckdb getjsdelivrbundles select a bundle based on browser checks const bundle await duckdb selectbundle jsdelivr_bundles const worker_url url createobjecturl new blob importscripts bundle mainworker type text javascript instantiate the asynchronus version of duckdb-wasm const worker new worker worker_url const logger new duckdb consolelogger const db new duckdb asyncduckdb logger worker await db instantiate bundle mainmodule bundle pthreadworker url revokeobjecturl worker_url webpack import as duckdb from duckdb duckdb-wasm import duckdb_wasm from duckdb duckdb-wasm dist duckdb-mvp wasm import duckdb_wasm_next from duckdb duckdb-wasm dist duckdb-eh wasm const manual_bundles duckdb duckdbbundles mvp mainmodule duckdb_wasm mainworker new url duckdb duckdb-wasm dist duckdb-browser-mvp worker js import meta url tostring eh mainmodule duckdb_wasm_next mainworker new url duckdb duckdb-wasm dist duckdb-browser-eh worker js import meta url tostring select a bundle based on browser checks const bundle await duckdb selectbundle manual_bundles instantiate the asynchronus version of duckdb-wasm const worker new worker bundle mainworker const logger new duckdb consolelogger const db new duckdb asyncduckdb logger worker await db instantiate bundle mainmodule bundle pthreadworker vite import as duckdb from duckdb duckdb-wasm import duckdb_wasm from duckdb duckdb-wasm dist duckdb-mvp wasm url import mvp_worker from duckdb duckdb-wasm dist duckdb-browser-mvp worker js url import duckdb_wasm_eh from duckdb duckdb-wasm dist duckdb-eh wasm url import eh_worker from duckdb duckdb-wasm dist duckdb-browser-eh worker js url const manual_bundles duckdb duckdbbundles mvp mainmodule duckdb_wasm mainworker mvp_worker eh mainmodule duckdb_wasm_eh mainworker eh_worker select a bundle based on browser checks const bundle await duckdb selectbundle manual_bundles instantiate the asynchronus version of duckdb-wasm const worker new worker bundle mainworker const logger new duckdb consolelogger const db new duckdb asyncduckdb logger worker await db instantiate bundle mainmodule bundle pthreadworker static served manually download the files from https cdn jsdelivr net npm duckdb duckdb-wasm dist import as duckdb from duckdb duckdb-wasm const manual_bundles duckdb duckdbbundles mvp mainmodule change me duckdb-mvp wasm mainworker change me duckdb-browser-mvp worker js eh mainmodule change m duckdb-eh wasm mainworker change m duckdb-browser-eh worker js select a bundle based on browser checks const bundle await duckdb selectbundle jsdelivr_bundles instantiate the asynchronous version of duckdb-wasm const worker new worker bundle mainworker const logger new duckdb consolelogger const db new duckdb asyncduckdb logger worker await db instantiate bundle mainmodule bundle pthreadworker",
			"category": "Wasm",
			"url": "/docs/api/wasm/instantiation",
			"blurb": "DuckDB-Wasm has multiple ways to be instantiated depending on the use case. Instantiation cdn(jsdelivr) import * as..."
		},
		{
			"title": "Interval Functions",
			"text": "this section describes functions and operators for examining and manipulating interval values interval operators the table below shows the available mathematical operators for interval types operator description example result --- --- --- --- addition of an interval interval 1 hour interval 5 hour interval 6 hour addition to a date date 1992-03-22 interval 5 day 1992-03-27 addition to a timestamp timestamp 1992-03-22 01 02 03 interval 5 day 1992-03-27 01 02 03 addition to a time time 01 02 03 interval 5 hour 06 02 03 - subtraction of an interval interval 5 hour - interval 1 hour interval 4 hour - subtraction from a date date 1992-03-27 - interval 5 day 1992-03-22 - subtraction from a timestamp timestamp 1992-03-27 01 02 03 - interval 5 day 1992-03-22 01 02 03 - subtraction from a time time 06 02 03 - interval 5 hour 01 02 03 interval functions the table below shows the available scalar functions for interval types function description example result --- --- --- --- date_part part interval get subfield equivalent to extract date_part year interval 14 months 1 datepart part interval alias of date_part get subfield equivalent to extract datepart year interval 14 months 1 extract part from interval get subfield from a date extract month from interval 14 months 2 to_days integer construct a day interval to_days 5 interval 5 day to_hours integer construct a hour interval to_hours 5 interval 5 hour to_microseconds integer construct a microsecond interval to_microseconds 5 interval 5 microsecond to_milliseconds integer construct a millisecond interval to_milliseconds 5 interval 5 millisecond to_minutes integer construct a minute interval to_minutes 5 interval 5 minute to_months integer construct a month interval to_months 5 interval 5 month to_seconds integer construct a second interval to_seconds 5 interval 5 second to_years integer construct a year interval to_years 5 interval 5 year only the documented date parts are defined for intervals",
			"category": "Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "This section describes functions and operators for examining and manipulating INTERVAL values. Interval Operators The..."
		},
		{
			"title": "Interval Type",
			"text": "intervals represent a period of time this period can be measured in a specific unit or combination of units for example years days or seconds intervals are generally used to modify timestamps or dates by either adding or subtracting them name description --- --- interval period of time an interval can be constructed by providing an amount together with a unit intervals can be added or subtracted from date or timestamp values -- 1 year select interval 1 year -- add 1 year to a specific date select date 2000-01-01 interval 1 year -- subtract 1 year from a specific date select date 2000-01-01 - interval 1 year -- construct an interval from a column instead of a constant select interval i year from range 1 5 t i -- construct an interval with mixed units select interval 1 month 1 day -- warning if a decimal value is specified it will be automatically rounded to an integer -- to use more precise values simply use a more granular date part -- in this example use 18 months instead of 1 5 years -- the statement below is equivalent to to_years cast 1 5 as integer -- 2 years select interval 1 5 years --warning this returns 2 years details the interval class represents a period of time using three distinct components the month day and microsecond these three components are required because there is no direct translation between them for example a month does not correspond to a fixed amount of days that depends on which month is referenced february has fewer days than march the division into components makes the interval class suitable for adding or subtracting specific time units to a date for example we can generate a table with the first day of every month using the following sql query select date 2000-01-01 interval i month from range 12 t i difference between dates if we subtract two timestamps from one another we obtain an interval describing the difference between the timestamps with the days and microseconds components for example select timestamp 2000-02-01 12 00 00 - timestamp 2000-01-01 11 00 00 as diff diff interval 31 days 01 00 00 the datediff function can be used to obtain the difference between two dates for a specific unit select datediff month timestamp 2000-01-01 11 00 00 timestamp 2000-02-01 12 00 00 as diff diff int64 1 functions see the date part functions docs for a list of available date parts for use with an interval see the interval operators for various functions that operate on intervals",
			"category": "Data Types",
			"url": "/docs/sql/data_types/interval",
			"blurb": "An interval specifies a period of time measured in units of a specific date part like years, days, seconds, or others."
		},
		{
			"title": "JSON",
			"text": "the json extension is a loadable extension that implements sql functions that are useful for reading values from existing json and creating new json data json type the json extension makes use of the json logical type the json logical type is interpreted as json i e parsed in json functions rather than interpreted as varchar i e a regular string all json creation functions return values of this type we also allow any of our types to be casted to json and json to be casted back to any of our types for example -- cast json to our struct type select duck 42 json struct duck integer -- duck 42 -- and back select duck 42 json -- duck 42 this works for our nested types as shown in the example but also for non-nested types select 2023-05-12 date json -- 2023-05-12 the only exception to this behavior is the cast from varchar to json which does not alter the data but instead parses and validates the contents of the varchar as json json table functions the following two table functions are used to read json function description --- --- read_json_objects filename read 1 json objects from filename where filename can also be a list of files or a glob pattern read_ndjson_objects filename alias for read_json_objects with parameter format set to newline_delimited read_json_objects_auto filename alias for read_json_objects with parameter format set to auto these functions have the following parameters name description type default --- --- --- --- maximum_object_size the maximum size of a json object in bytes uinteger 16777216 format can be one of auto unstructured newline_delimited array varchar array ignore_errors whether to ignore parse errors only possible when format is newline_delimited bool false compression the compression type for the file by default this will be detected automatically from the file extension e g t json gz will use gzip t json will use none options are none gzip zstd and auto varchar auto filename whether or not an extra filename column should be included in the result bool false hive_partitioning whether or not to interpret the path as a hive partitioned path bool false the format parameter specifies how to read the json from a file with unstructured the top-level json is read e g duck 42 goose 1 2 3 will result in two objects being read with newline_delimited ndjson is read where each json is separated by a newline n e g duck 42 goose 1 2 3 will also result in two objects being read with array each array element is read e g duck 42 goose 1 2 3 again will result in two objects being read example usage select from read_json_objects my_file1 json -- duck 42 goose 1 2 3 select from read_json_objects my_file1 json my_file2 json -- duck 42 goose 1 2 3 -- duck 43 goose 4 5 6 swan 3 3 select from read_ndjson_objects json gz -- duck 42 goose 1 2 3 -- duck 43 goose 4 5 6 swan 3 3 duckdb also supports reading json as a table using the following functions function description --- --- read_json filename read json from filename where filename can also be a list of files or a glob pattern read_ndjson filename alias for read_json with parameter format set to newline_delimited read_json_auto filename alias for read_json with all auto-detection enabled read_ndjson_auto filename alias for read_json_auto with parameter format set to newline_delimited besides the maximum_object_size format ignore_errors and compression these functions have additional parameters name description type default --- --- --- --- columns a struct that specifies the key names and value types contained within the json file e g key1 integer key2 varchar if auto_detect is enabled these will be inferred struct empty records can be one of auto true false varchar records auto_detect whether to auto-detect detect the names of the keys and data types of the values automatically bool false sample_size option to define number of sample objects for automatic json type detection set to -1 to scan the entire input file ubigint 20480 maximum_depth maximum nesting depth to which the automatic schema detection detects types set to -1 to fully detect nested json types bigint -1 dateformat specifies the date format to use when parsing dates see date format varchar iso timestampformat specifies the date format to use when parsing timestamps see date format varchar iso union_by_name whether the schema s of multiple json files should be unified bool false example usage select from read_json my_file1 json columns duck integer duck --- 42 duckdb can convert json arrays directly to its internal list type and missing keys become null select from read_json my_file1 json my_file2 json columns duck integer goose integer swan double duck goose swan --- --- --- 42 1 2 3 null 43 4 5 6 3 3 duckdb can automatically detect the types like so select goose duck from read_json_auto json gz select goose duck from json gz -- equivalent goose duck --- --- 1 2 3 42 4 5 6 43 duckdb can read and auto-detect a variety of formats specified with the format parameter querying a json file that contains an array e g duck 42 goose 4 2 duck 43 goose 4 3 can be queried exactly the same as a json file that contains unstructured json e g duck 42 goose 4 2 duck 43 goose 4 3 both can be read as the table duck goose --- --- 42 4 2 43 4 3 if your json file does not contain records i e any other type of json than objects duckdb can still read it this is specified with the records parameter the records parameter specifies whether the json contains records that should be unpacked into individual columns i e reading the following file with records duck 42 goose 1 2 3 duck 43 goose 4 5 6 results in two columns duck goose --- --- 42 1 2 3 42 4 5 6 you can read the same file with records set to false to get a single column which is a struct containing the data json --- duck 42 goose 1 2 3 duck 43 goose 4 5 6 for additional examples reading more complex data please see the shredding deeply nested json one vector at a time blog post json import export when the json extension is installed format json is supported for copy from copy to export database and import database see copy and import export by default copy expects newline-delimited json if you prefer copying data to from a json array you can specify array true i e copy select from range 5 to my json array true will create the following file range 0 range 1 range 2 range 3 range 4 this can be read like so create table test range bigint copy test from my json array true the format can be detected automatically the format like so copy test from my json auto_detect true json scalar functions the following scalar json functions can be used to gain information about the stored json values with the exception of json_valid json all json functions produce an error when invalid json is supplied we support two kinds of notations to describe locations within json json pointer and jsonpath function description --- --- json json parse and minify json json_valid json return whether json is valid json json_array_length json path return the number of elements in the json array json or 0 if it is not a json array if path is specified return the number of elements in the json array at the given path if path is a list the result will be list of array lengths json_type json path return the type of the supplied json which is one of object array bigint ubigint varchar boolean null if path is specified return the type of the element at the given path if path is a list the result will be list of types json_keys json path returns the keys of json as a list of varchar if json is a json object if path is specified return the keys of the json object at the given path if path is a list the result will be list of list of varchar json_structure json return the structure of json defaults to json the structure is inconsistent e g incompatible types in an array json_contains json_haystack json_needle returns true if json_needle is contained in json_haystack both parameters are of json type but json_needle can also be a numeric value or a string however the string must be wrapped in double quotes the jsonpointer syntax separates each field with a for example to extract the first element of the array with key duck you can do select json_extract duck 1 2 3 duck 0 -- 1 the jsonpath syntax separates fields with a and accesses array elements with i and always starts with using the same example we can do select json_extract duck 1 2 3 duck 0 -- 1 jsonpath is more expressive and can also access from the back of lists select json_extract duck 1 2 3 duck -1 -- 3 jsonpath also allows escaping syntax tokens using double quotes select json_extract duck goose 1 2 3 duck goose 1 -- 2 other examples create table example j json insert into example values family anatidae species duck goose swan null select json j from example -- family anatidae species duck goose swan null select json_valid j from example -- true select json_valid -- false select json_array_length duck goose swan null -- 4 select json_array_length j species from example -- 4 select json_array_length j species from example -- 4 select json_array_length j species from example -- 4 select json_array_length j species from example -- 4 select json_type j from example -- object select json_keys from example -- family species select json_structure j from example -- family varchar species varchar select json_structure duck family anatidae -- json select json_contains key value value -- true select json_contains key 1 1 -- true select json_contains top_key key value key value -- true json extraction functions there are two extraction functions which have their respective operators the operators can only be used if the string is stored as the json logical type these functions supports the same two location notations as the previous functions function alias operator description --- --- --- json_extract json path json_extract_path - extract json from json at the given path if path is a list the result will be a list of json json_extract_string json path json_extract_path_text - extract varchar from json at the given path if path is a list the result will be a list of varchar examples create table example j json insert into example values family anatidae species duck goose swan null select json_extract j family from example -- anatidae select j- family from example -- anatidae select j- species 0 from example -- duck select j- species - 0 from example -- duck select j- species - 0 1 from example -- duck goose select json_extract_string j family from example -- anatidae select j- family from example -- anatidae select j- species 0 from example -- duck select j- species - 0 from example -- duck select j- species - 0 1 from example -- duck goose json creation functions the following functions are used to create json function description --- --- to_json any create json from a value of any type our list is converted to a json array and our struct and map are converted to a json object json_quote any alias for to_json array_to_json list alias for to_json that only accepts list row_to_json list alias for to_json that only accepts struct json_array any create a json array from any number of values json_object key value create a json object from any number of key value pairs json_merge_patch json json merge two json documents together examples select to_json duck -- duck select to_json 1 2 3 -- 1 2 3 select to_json duck 42 -- duck 42 select to_json map duck 42 -- duck 42 select json_array 42 duck null -- 42 duck null select json_object duck 42 -- duck 42 select json_merge_patch duck 42 goose 123 -- goose 123 duck 42 json aggregate functions there are three json aggregate functions function description --- --- json_group_array any return a json array with all values of any in the aggregation json_group_object key value return a json object with all key value pairs in the aggregation json_group_structure json return the combined json_structure of all json in the aggregation examples create table example k varchar v integer insert into example values duck 42 goose 7 select json_group_array v from example -- 42 7 select json_group_object k v from example -- duck 42 goose 7 drop table example create table example j json insert into example values family anatidae species duck goose coolness 42 42 family canidae species labrador bulldog hair true select json_group_structure j from example -- family varchar species varchar coolness double hair boolean transforming json in many cases it is inefficient to extract values from json one-by-one instead we can extract all values at once transforming json to the nested types list and struct function description --- --- json_transform json structure transform json according to the specified structure from_json json structure alias for json_transform json_transform_strict json structure same as json_transform but throws an error when type casting fails from_json_strict json structure alias for json_transform_strict the structure argument is json of the same form as returned by json_structure the structure argument can be modified to transform the json into the desired structure and types it is possible to extract fewer key value pairs than are present in the json and it is also possible to extract more missing keys become null examples create table example j json insert into example values family anatidae species duck goose coolness 42 42 family canidae species labrador bulldog hair true select json_transform j family varchar coolness double from example -- family anatidae coolness 42 420000 -- family canidae coolness null select json_transform j family tinyint coolness decimal 4 2 from example -- family null coolness 42 42 -- family null coolness null select json_transform_strict j family tinyint coolness double from example -- invalid input error failed to cast value anatidae de serializing sql to json and vice versa the json extension also provides functions to serialize and deserialize select statements between sql and json as well as executing json serialized statements function type description --- --- --- json_serialize_sql varchar skip_empty boolean skip_null boolean format boolean scalar serialize a set of separated select statments to an equivalent list of json serialized statements json_deserialize_sql json scalar deserialize one or many json serialized statements back to an equivalent sql string json_execute_serialized_sql varchar table execute json serialized statements and return the resulting rows only one statement at a time is supported for now pragma json_execute_serialized_sql varchar pragma pragma version of the json_execute_serialized_sql function the json_serialize_sql varchar function takes three optional parameters skip_empty skip_null and format that can be used to control the output of the serialized statements if you run the json_execute_serialize_sql varchar table function inside of a transaction the serialized statements will not be able to see any transaction local changes this is because the statements are executed in a separate query context you can use the pragma json_execute_serialize_sql varchar pragma version to execute the statements in the same query context as the pragma although with the limitation that the serialized json must be provided as a constant string i e you cannot do pragma json_execute_serialize_sql json_serialize_sql note that these functions do not preserve syntactic sugar such as from select so a statement round-tripped through json_deserialize_sql json_serialize_sql may not be identical to the original statement but should always be semantically equivalent and produce the same output examples -- simple example select json_serialize_sql select 2 -- error false statements node type select_node modifiers cte_map map select_list class constant type value_constant alias value type id integer type_info null is_null false value 2 from_table type empty alias sample null where_clause null group_expressions group_sets aggregate_handling standard_handling having null sample null qualify null -- example with multiple statements and skip options select json_serialize_sql select 1 2 select a b from tbl1 skip_empty true skip_null true -- error false statements node type select_node select_list class function type function function_name children class constant type value_constant value type id integer is_null false value 1 class constant type value_constant value type id integer is_null false value 2 order_bys type order_modifier distinct false is_operator true export_state false from_table type empty aggregate_handling standard_handling node type select_node select_list class function type function function_name children class column_ref type column_ref column_names a class column_ref type column_ref column_names b order_bys type order_modifier distinct false is_operator true export_state false from_table type base_table table_name tbl1 aggregate_handling standard_handling -- example with a syntax error select json_serialize_sql totally not valid sql -- error true error_type parser error_message syntax error at or near totally nline 1 totally not valid sql n -- example with deserialize select json_deserialize_sql json_serialize_sql select 1 2 -- select 1 2 -- example with deserialize and syntax sugar select json_deserialize_sql json_serialize_sql from x select 1 2 -- select 1 2 from x -- example with execute select from json_execute_serialized_sql json_serialize_sql select 1 2 -- 3 -- example with error select from json_execute_serialized_sql json_serialize_sql totally not valid sql -- error parser error error parsing json parser syntax error at or near totally",
			"category": "Extensions",
			"url": "/docs/extensions/json",
			"blurb": "The json extension is a loadable extension that implements SQL functions that are useful for reading values from..."
		},
		{
			"title": "JSON Export",
			"text": "to export the data from a table to a json file use the copy statement copy tbl to output json the result of queries can also be directly exported to a json file copy select from tbl to output json for additional options see the copy statement documentation",
			"category": "Import",
			"url": "/docs/guides/import/json_export",
			"blurb": "To export the data from a table to a JSON file, use the COPY statement. COPY tbl TO 'output.json'; The result of..."
		},
		{
			"title": "JSON Import",
			"text": "to read data from a json file use the read_json_auto function in the from clause of a query select from read_json_auto input json to create a new table using the result from a query use create table as from a select statement create table new_tbl as select from read_json_auto input json to load data into an existing table from a query use insert into from a select statement insert into tbl select from read_json_auto input json alternatively the copy statement can also be used to load data from a json file into an existing table copy tbl from input json for additional options see the json loading reference and the copy statement documentation",
			"category": "Import",
			"url": "/docs/guides/import/json_import",
			"blurb": "To read data from a JSON file, use the read_json_auto function in the FROM clause of a query. SELECT * FROM..."
		},
		{
			"title": "JSON Loading",
			"text": "examples -- read a json file from disk auto-infer options select from todos json -- read_json with custom options select from read_json todos json format array columns userid ubigint id ubigint title varchar completed boolean -- read a json file from stdin auto-infer options cat data json todos json duckdb -c select from read_json_auto dev stdin -- read a json file into a table create table todos userid ubigint id ubigint title varchar completed boolean copy todos from todos json -- alternatively create a table without specifying the schema manually create table todos as select from todos json -- write the result of a query to a json file copy select from todos to todos json json loading json is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute value pairs and arrays or other serializable values while it is not a very efficient format for tabular data it is very commonly used especially as a data interchange format the duckdb json reader can automatically infer which configuration flags to use by analyzing the json file this will work correctly in most situations and should be the first option attempted in rare situations where the json reader cannot figure out the correct configuration it is possible to manually configure the json reader to correctly parse the json file below are parameters that can be passed in to the json reader parameters name description type default --- --- --- --- maximum_object_size the maximum size of a json object in bytes uinteger 16777216 format can be one of auto unstructured newline_delimited array varchar array ignore_errors whether to ignore parse errors only possible when format is newline_delimited bool false compression the compression type for the file by default this will be detected automatically from the file extension e g t json gz will use gzip t json will use none options are none gzip zstd and auto varchar auto columns a struct that specifies the key names and value types contained within the json file e g key1 integer key2 varchar if auto_detect is enabled these will be inferred struct empty records can be one of auto true false varchar records auto_detect whether to auto-detect detect the names of the keys and data types of the values automatically bool false sample_size option to define number of sample objects for automatic json type detection set to -1 to scan the entire input file ubigint 20480 maximum_depth maximum nesting depth to which the automatic schema detection detects types set to -1 to fully detect nested json types bigint -1 dateformat specifies the date format to use when parsing dates see date format varchar iso timestampformat specifies the date format to use when parsing timestamps see date format varchar iso filename whether or not an extra filename column should be included in the result bool false hive_partitioning whether or not to interpret the path as a hive partitioned path bool false union_by_name whether the schema s of multiple json files should be unified bool false when using read_json_auto every parameter that supports auto-detection is enabled examples of format settings the json extension can attempt to determine the format of a json file when setting format to auto here are some example json files and the corresponding format settings that should be used in each of the below cases the format setting was not needed as duckdb was able to infer it correctly but it is included for illustrative purposes a query of this shape would work in each case select from filename json format newline_delimited with format newline_delimited newline-delimited json can be parsed each line is a json key1 value1 key2 value1 key1 value2 key2 value2 key1 value3 key2 value3 select from read_json_auto records json format newline_delimited key1 key2 -------- -------- value1 value1 value2 value2 value3 value3 format array if the json file contains a json array of objects pretty-printed or not array_of_objects may be used key1 value1 key2 value1 key1 value2 key2 value2 key1 value3 key2 value3 select from read_json_auto array json format array key1 key2 -------- -------- value1 value1 value2 value2 value3 value3 format unstructured if the json file contains json that is not newline-delimited or an array unstructured may be used key1 value1 key2 value1 key1 value2 key2 value2 key1 value3 key2 value3 select from read_json_auto unstructured json format unstructured key1 key2 -------- -------- value1 value1 value2 value2 value3 value3 examples of records settings the json extension can attempt to determine whether a json file contains records when setting records auto when records true the json extension expects json objects and will unpack the fields of json objects into individual columns continuing with the same example file from before key1 value1 key2 value1 key1 value2 key2 value2 key1 value3 key2 value3 select from read_json_auto records json records true key1 key2 -------- -------- value1 value1 value2 value2 value3 value3 when records false the json extension will not unpack the top-level objects and create struct s instead select from read_json_auto records json records false json ---------------------------------- key1 value1 key2 value1 key1 value2 key2 value2 key1 value3 key2 value3 this is especially useful if we have non-object json for example 1 2 3 4 5 6 7 8 9 select from read_json_auto arrays json records false json ----------- 1 2 3 4 5 6 7 8 9 writing the contents of tables or the result of queries can be written directly to a json file using the copy statement see the copy documentation for more information read_json_auto function the read_json_auto is the simplest method of loading json files it automatically attempts to figure out the correct configuration of the json reader it also automatically deduces types of columns select from read_json_auto todos json limit 5 userid id title completed -------- ---- ----------------------------------------------------------------- ----------- 1 1 delectus aut autem false 1 2 quis ut nam facilis et officia qui false 1 3 fugiat veniam minus false 1 4 et porro tempora true 1 5 laboriosam mollitia et enim quasi adipisci quia provident illum false the path can either be a relative path relative to the current working directory or an absolute path we can use read_json_auto to create a persistent table as well create table todos as select from read_json_auto todos json describe todos column_name column_type null key default extra ------------- ------------- ------ ----- --------- ------- userid ubigint yes id ubigint yes title varchar yes completed boolean yes if we specify the columns we can bypass the automatic detection note that not all columns need to be specified select from read_json_auto todos json columns userid ubigint completed boolean multiple files can be read at once by providing a glob or a list of files refer to the multiple files section for more information copy statement the copy statement can be used to load data from a json file into a table for the copy statement we must first create a table with the correct schema to load the data into we then specify the json file to load from plus any configuration options separately create table todos userid ubigint id ubigint title varchar completed boolean copy todos from todos json select from todos limit 5 userid id title completed -------- ---- ----------------------------------------------------------------- ----------- 1 1 delectus aut autem false 1 2 quis ut nam facilis et officia qui false 1 3 fugiat veniam minus false 1 4 et porro tempora true 1 5 laboriosam mollitia et enim quasi adipisci quia provident illum false more on the copy statement can be found here",
			"category": "Json",
			"url": "/docs/data/json/overview",
			"blurb": "Examples -- read a JSON file from disk, auto-infer options SELECT * FROM 'todos.json'; -- read_json with custom..."
		},
		{
			"title": "Java JDBC API",
			"text": "installation the duckdb java jdbc api can be installed from maven central please see the installation page for details basic api usage duckdb s jdbc api implements the main parts of the standard java database connectivity jdbc api version 4 1 describing jdbc is beyond the scope of this page see the official documentation for details below we focus on the duckdb-specific parts refer to the externally hosted api reference for more information about our extensions to the jdbc specification or the below arrow methods startup shutdown in jdbc database connections are created through the standard java sql drivermanager class the driver should auto-register in the drivermanager if that does not work for some reason you can enforce registration like so class forname org duckdb duckdbdriver to create a duckdb connection call drivermanager with the jdbc duckdb jdbc url prefix like so connection conn drivermanager getconnection jdbc duckdb when using the jdbc duckdb url alone an in-memory database is created note that for an in-memory database no data is persisted to disk i e all data is lost when you exit the java program if you would like to access or create a persistent database append its file name after the path for example if your database is stored in tmp my_database use the jdbc url jdbc duckdb tmp my_database to create a connection to it it is possible to open a duckdb database file in read-only mode this is for example useful if multiple java processes want to read the same database file at the same time to open an existing database file in read-only mode set the connection property duckdb read_only like so properties ro_prop new properties ro_prop setproperty duckdb read_only true connection conn_ro drivermanager getconnection jdbc duckdb tmp my_database ro_prop additional connections can be created using the drivermanager a more efficient mechanism is to call the duckdbconnecttion duplicate method like so connection conn2 duckdbconnection conn duplicate multiple connections are allowed but mixing read-write and read-only connections is unsupported querying duckdb supports the standard jdbc methods to send queries and retrieve result sets first a statement object has to be created from the connection this object can then be used to send queries using execute and executequery execute is meant for queries where no results are expected like create table or update etc and executequery is meant to be used for queries that produce results e g select below two examples see also the jdbc statement and resultset documentations create a table statement stmt conn createstatement stmt execute create table items item varchar value decimal 10 2 count integer insert two items into the table stmt execute insert into items values jeans 20 0 1 hammer 42 2 2 try resultset rs stmt executequery select from items while rs next system out println rs getstring 1 system out println rs getint 3 jeans 1 hammer 2 duckdb also supports prepared statements as per the jdbc api try preparedstatement p_stmt conn preparestatement insert into items values p_stmt setstring 1 chainsaw p_stmt setdouble 2 500 0 p_stmt setint 3 42 p_stmt execute more calls to execute possible do not use prepared statements to insert large amounts of data into duckdb see the data import documentation for better options arrow methods refer to the api reference for type signatures arrow export the following demonstrates exporting an arrow stream and consuming it using the java arrow bindings import org apache arrow memory rootallocator import org apache arrow vector ipc arrowreader import org duckdb duckdbresultset try var conn drivermanager getconnection jdbc duckdb var p_stmt conn preparestatement select from generate_series 2000 var resultset duckdbresultset p_stmt executequery var allocator new rootallocator try var reader arrowreader resultset arrowexportstream allocator 256 while reader loadnextbatch system out println reader getvectorschemaroot getvector generate_series arrow import the following demonstrates consuming an arrow stream from the java arrow bindings import org apache arrow memory rootallocator import org apache arrow vector ipc arrowreader import org duckdb duckdbconnection arrow stuff try var allocator new rootallocator arrowstreamreader reader null should not be null of course var arrow_array_stream arrowarraystream allocatenew allocator data exportarraystream allocator reader arrow_array_stream duckdb stuff try var conn duckdbconnection drivermanager getconnection jdbc duckdb conn registerarrowstream adsf arrow_array_stream run a query try var stmt conn createstatement var rs duckdbresultset stmt executequery select count from adsf while rs next system out println rs getint 1",
			"category": "Api",
			"url": "/docs/api/java",
			"blurb": "Installation The DuckDB Java JDBC API can be installed from Maven Central . Please see the installation page for..."
		},
		{
			"title": "Julia Package",
			"text": "the duckdb julia package provides a high-performance front-end for duckdb much like sqlite duckdb runs in-process within the julia client and provides a dbinterface front-end the package also supports multi-threaded execution it uses julia threads tasks for this purpose if you wish to run queries in parallel you must launch julia with multi-threading support by e g setting the julia_num_threads environment variable installation pkg add duckdb julia using duckdb basics create a new in-memory database con dbinterface connect duckdb db memory create a table dbinterface execute con create table integers i integer insert data using a prepared statement stmt dbinterface prepare con insert into integers values dbinterface execute stmt 42 query the database results dbinterface execute con select 42 a print results scanning dataframes the duckdb julia package also provides support for querying julia dataframes note that the dataframes are directly read by duckdb - they are not inserted or copied into the database itself if you wish to load data from a dataframe into a duckdb table you can run a create table as or insert into query using duckdb using dataframes create a new in-memory dabase con dbinterface connect duckdb db create a dataframe df dataframe a 1 2 3 b 42 84 42 register it as a view in the database duckdb register_data_frame con df my_df run a sql query over the dataframe results dbinterface execute con select from my_df print results original julia connector credits to kimmolinna for the original duckdb julia connector",
			"category": "Api",
			"url": "/docs/api/julia",
			"blurb": "The DuckDB Julia package provides a high-performance front-end for DuckDB. Much like SQLite, DuckDB runs in-process..."
		},
		{
			"title": "Jupyter Notebooks",
			"text": "duckdb s python client can be used directly in jupyter notebooks with no additional configuration if desired however additional libraries can be used to simplify sql query development this guide will describe how to utilize those additional libraries see other guides in the python section for how to use duckdb and python together in this example we used the jupysql package which is a direct fork of ipython-sql the main difference is that jupysql is well maintained and has both newer features and bug fixes this example workflow is also available as a google collab notebook library installation four additional libraries improve the duckdb experience in jupyter notebooks jupysql convert a jupyter code cell into a sql cell duckdb_engine duckdb sqlalchemy driver used by sqlalchemy to connect to duckdb pandas clean table visualizations and compatibility with other analysis matplotlib plotting with python run these pip install commands from the command line if jupyter notebook is not yet installed otherwise see google collab link above for an in-notebook example pip install duckdb install jupyter notebook note you can also install jupyterlab pip install jupyterlab pip install notebook install supporting libraries pip install jupysql pip install duckdb-engine pip install pandas conda install pandas in case pip fails pip install matplotlib library import and configuration next open a jupyter notebook and import the relevant libraries it s possible to have sql commands and duckdb sql share the same default connection by providing duckdb default as the sqlalchemy connection string import duckdb import pandas as pd no need to import duckdb_engine jupysql will auto-detect the driver needed based on the connection string import jupysql jupyter extension to create sql cells load_ext sql set configurations on jupysql to directly output data to pandas and to simplify the output that is printed to the notebook config sqlmagic autopandas true config sqlmagic feedback false config sqlmagic displaycon false connect jupysql to duckdb using a sqlalchemy-style connection string either connect to a new in-memory duckdb the default connection or a file backed db sql duckdb default sql duckdb memory sql duckdb path to file db querying duckdb single line sql queries can be run using sql at the start of a line query results will be displayed as a pandas df sql select off and flying as a_duckdb_column an entire jupyter cell can be used as a sql cell by placing sql at the start of the cell query results will be displayed as a pandas df sql select schema_name function_name from duckdb_functions order by all desc limit 5 to return query results into a pandas dataframe for future usage use as an assignment operator this can be used with both the sql and sql jupyter magics sql my_df select off and flying as a_duckdb_column querying pandas dataframes duckdb is able to find and query any dataframe stored as a variable in the jupyter notebook input_df pd dataframe from_dict i 1 2 3 j one two three the dataframe being queried can be specified just like any other table in the from clause sql output_df select sum i as total_i from input_df visualizing duckdb data the most common way to plot datasets in python is to load them using pandas and then use matplotlib or seaborn for plotting this approach requires loading all data into memory which is highly inefficient the plotting module in jupysql runs computations in the sql engine this delegates memory management to the engine and ensures that intermediate computations do not keep eating up memory efficiently plotting massive datasets install and load duckdb httpfs extension duckdb s httpfs extension allows parquet and csv files to be queried remotely over http these examples query a parquet file that contains historical taxi data from nyc using the parquet format allows duckdb to only pull the rows and columns into memory that are needed rather than download the entire file duckdb can be used to process local parquet files as well which may be desirable if querying the entire parquet file or running multiple queries that require large subsets of the file sql install httpfs load httpfs boxplot histogram to create a boxplot call sqlplot boxplot passing the name of the table and the column to plot in this case the name of the table is the url of the remotely stored parquet file sqlplot boxplot --table https d37ci6vzurychx cloudfront net trip-data yellow_tripdata_2021-01 parquet --column trip_distance image now create a query that filters by the 90th percentile note the use of the --save and --no-execute functions this tells jupysql to store the query but skips execution it will be referenced in the next plotting call sql --save short-trips --no-execute select from https d37ci6vzurychx cloudfront net trip-data yellow_tripdata_2021-01 parquet where trip_distance 6 3 to create a histogram call sqlplot histogram and pass the name of the table the column to plot and the number of bins this uses --with short-trips so jupysql uses the query defined previously and therefore only plots a subset of the data sqlplot histogram --table short-trips --column trip_distance --bins 10 --with short-trips image summary you now have the ability to alternate between sql and pandas in a simple and highly performant way you can plot massive datasets directly through the engine avoiding both the download of the entire file and loading all of it into pandas in memory dataframes can be read as tables in sql and sql results can be output into dataframes happy analyzing",
			"category": "Python",
			"url": "/docs/guides/python/jupyter",
			"blurb": "DuckDB's Python client can be used directly in Jupyter notebooks with no additional configuration if desired...."
		},
		{
			"title": "LIMIT Clause",
			"text": "limit is an output modifier logically it is applied at the very end of the query the limit clause restricts the amount of rows fetched the offset clause indicates at which position to start reading the values i e the first offset values are ignored note that while limit can be used without an order by clause the results might not be deterministic without the order by clause this can still be useful however for example when you want to inspect a quick snapshot of the data examples -- select the first 5 rows from the addresses table select from addresses limit 5 -- select the 5 rows from the addresses table starting at position 5 i e ignoring the first 5 rows select from addresses limit 5 offset 5 -- select the top 5 cities with the highest population select city count as population from addresses group by city order by population desc limit 5 syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/limit",
			"blurb": "LIMIT is an output modifier. Logically it is applied at the very end of the query. The LIMIT clause restricts the..."
		},
		{
			"title": "List",
			"text": "list data type a list column can have values with different lengths but they must all have the same underlying type list s are typically used to store arrays of numbers but can contain any uniform data type including other list s and struct s list s are similar to postgres s array type duckdb uses the list terminology but some array functions are provided for postgres compatibility see the data types overview for a comparison between nested data types lists can be created using the list_value expr function or the equivalent bracket notation expr the expressions can be constants or arbitrary expressions creating lists -- list of integers select 1 2 3 -- list of strings with a null value select duck goose null heron -- list of lists with null values select duck goose heron null frog toad -- create a list with the list_value function select list_value 1 2 3 -- create a table with an integer list column and a varchar list column create table list_table int_list int varchar_list varchar retrieving from lists retrieving one or more values from a list can be accomplished using brackets and slicing notation or through list functions like list_extract multiple equivalent functions are provided as aliases for compatibility with systems that refer to lists as arrays for example the function array_slice -- note that we wrap the list creation in parenthesis so that it happens first -- this is only needed in our basic examples here not when working with a list column -- for example this can t be parsed select a b c 1 example result --------------------------------------------------- ------------ select a b c 3 c select a b c -1 c select a b c 2 1 c select list_extract a b c 3 c select a b c 1 2 a b select a b c 2 a b select a b c -2 b c select list_slice a b c 2 3 b c ordering the ordering is defined positionally null values compare greater than all other values and are considered equal to each other null comparisons at the top level null nested values obey standard sql null comparison rules comparing a null nested value to a non- null nested value produces a null result comparing nested value members however uses the internal nested value rules for null s and a null nested value member will compare above a non- null nested value member functions see nested functions",
			"category": "Data Types",
			"url": "/docs/sql/data_types/list",
			"blurb": "List Data Type A LIST column can have values with different lengths, but they must all have the same underlying type...."
		},
		{
			"title": "List Tables",
			"text": "the show tables command can be used to obtain a list of all tables within the selected schema create table tbl i integer show tables name ------ tbl describe show or show all tables can be used to obtain a list of all tables within all attached databases and schemas create table tbl i integer create schema s1 create table s1 tbl v varchar show all tables database schema table_name column_names column_types temporary ---------- -------- ------------ -------------- -------------- ----------- memory main tbl i integer false memory s1 tbl v varchar false to view the schema of an individual table use the describe command create table tbl i integer primary key j varchar describe tbl column_name column_type null key default extra ------------- ------------- ------ ------ --------- ------- i integer no pri null null j varchar yes null null null the sql-standard information_schema views are also defined duckdb also defines sqlite_master and many postgres system catalog tables for compatibility with sqlite and postgres respectively",
			"category": "Meta",
			"url": "/docs/guides/meta/list_tables",
			"blurb": "The SHOW TABLES command can be used to obtain a list of all tables within the selected schema. CREATE TABLE tbl(i..."
		},
		{
			"title": "Logical Operators",
			"text": "the following logical operators are available and or and not sql uses a three-valuad logic system with true false and null note that logical operators involving null do not always evaluate to null for example null and false will evaluate to false and null or true will evaluate to true below are the complete truth tables a b a and b a or b --- --- --- --- true true true true true false false true true null null true false false false false false null false null null null null null a not a --- --- true false false true null null the operators and and or are commutative that is you can switch the left and right operand without affecting the result",
			"category": "Expressions",
			"url": "/docs/sql/expressions/logical_operators",
			"blurb": "The following logical operators are available: AND , OR and NOT . SQL uses a three-valuad logic system with TRUE ,..."
		},
		{
			"title": "Map",
			"text": "map data type map s are similar to struct s in that they are an ordered list of entries where a key maps to a value however map s do not need to have the same keys present for each row and thus are suitable for other use cases map s are useful when the schema is unknown beforehand or when the schema varies per row their flexibility is a key differentiator map s must have a single type for all keys and a single type for all values keys and values can be any type and the type of the keys does not need to match the type of the values ex a map of varchar to int is valid map s may not have duplicate keys map s return an empty list if a key is not found rather than throwing an error as structs do in contrast struct s must have string keys but each key may have a value of a different type see the data types overview for a comparison between nested data types to construct a map use the bracket syntax preceded by the map keyword creating maps -- a map with varchar keys and integer values this returns key1 1 key2 5 select map key1 1 key2 5 -- alternatively use the map_from_entries function this returns key1 1 key2 5 select map_from_entries key1 1 key2 5 -- a map with integer keys and numeric values this returns 1 42 001 5 -32 100 select map 1 42 001 5 -32 1 -- keys and or values can also be nested types -- this returns a b 1 1 2 2 c d 3 3 4 4 select map a b 1 1 2 2 c d 3 3 4 4 -- create a table with a map column that has integer keys and double values create table map_table map_col map int double retrieving from maps map s use bracket notation for retrieving values selecting from a map returns a list rather than an individual value with an empty list meaning that the key was not found -- use bracket notation to retrieve a list containing the value at a key s location this returns 42 -- note that the expression in bracket notation must match the type of the map s key select map key1 5 key2 43 key1 -- to retrieve the underlying value use list selection syntax to grab the first element -- this returns 42 select map key1 5 key2 43 key1 1 -- if the element is not in the map an empty list will be returned returns -- note that the expression in bracket notation must match the type of the map s key else an error is returned select map key1 5 key2 43 key3 -- the element_at function can also be used to retrieve a map value this returns 42 select element_at map key1 5 key2 43 key1 comparison operators nested types can be compared using all the comparison operators these comparisons can be used in logical expressions for both where and having clauses as well as for creating boolean values the ordering is defined positionally in the same way that words can be ordered in a dictionary null values compare greater than all other values and are considered equal to each other at the top level null nested values obey standard sql null comparison rules comparing a null nested value to a non- null nested value produces a null result comparing nested value members however uses the internal nested value rules for null s and a null nested value member will compare above a non- null nested value member functions see nested functions",
			"category": "Data Types",
			"url": "/docs/sql/data_types/map",
			"blurb": "Map Data Type MAP s are similar to STRUCT s in that they are an ordered list of entries where a key maps to a value...."
		},
		{
			"title": "Multiple Python Threads",
			"text": "this page demonstrates how to simultaneously insert into and read from a duckdb database across multiple python threads this could be useful in scenarios where new data is flowing in and an analysis should be periodically re-run note that this is all within a single python process see the faq for details on duckdb concurrency feel free to follow along in this google collaboratory notebook setup first import duckdb and several modules from the python standard library then connect to a file-backed duckdb database and create an example table to store inserted data this table will track the name of the thread that completed the insert and automatically insert the timestamp when that insert occurred using the default expression import duckdb from threading import thread current_thread import random duckdb_con duckdb connect my_peristent_db duckdb duckdb_con duckdb connect pass in no parameters for an in memory database duckdb_con execute create or replace table my_inserts thread_name varchar insert_time timestamp default current_timestamp reader and writer functions next define functions to be executed by the writer and reader threads each thread must use the cursor method to create a thread-local connection to the same duckdb file based on the original connection this approach also works with in-memory duckdb databases def write_from_thread duckdb_con create a duckdb connection specifically for this thread local_con duckdb_con cursor insert a row with the name of the thread insert_time is auto-generated thread_name str current_thread name result local_con execute insert into my_inserts thread_name values thread_name fetchall def read_from_thread duckdb_con create a duckdb connection specifically for this thread local_con duckdb_con cursor query the current row count thread_name str current_thread name results local_con execute select as thread_name count as row_counter current_timestamp from my_inserts thread_name fetchall print results create threads we define how many writers and readers to use and define a list to track all of the threads that will be created then create first writer and then reader threads next shuffle them so that they will be kicked off in a random order to simulate simultaneous writers and readers note that the threads have not yet been executed only defined write_thread_count 50 read_thread_count 5 threads create multiple writer and reader threads in the same process pass in the same connection as an argument for i in range write_thread_count threads append thread target write_from_thread args duckdb_con name write_thread_ str i for j in range read_thread_count threads append thread target read_from_thread args duckdb_con name read_thread_ str j shuffle the threads to simulate a mix of readers and writers random seed 6 set the seed to ensure consistent results when testing random shuffle threads run threads and show results now kick off all threads to run in parallel then wait for all of them to finish before printing out the results note that the timestamps of readers and writers are interspersed as expected due to the randomization kick off all threads in parallel for thread in threads thread start ensure all threads complete before printing final results for thread in threads thread join print duckdb_con execute select from my_inserts order by insert_time df",
			"category": "Python",
			"url": "/docs/guides/python/multiple_threads",
			"blurb": "This page demonstrates how to simultaneously insert into and read from a DuckDB database across multiple Python..."
		},
		{
			"title": "NULL Values",
			"text": "null values are special values that are used to represent missing data in sql columns of any type can contain null values logically a null value can be seen as the value of this field is unknown -- insert a null value into a table create table integers i integer insert into integers values null null values have special semantics in many parts of the query as well as in many functions any comparison with a null value returns null including null null you can use is not distinct from to perform an equality comparison where null values compare equal to each other use is not null to check if a value is null select null null -- returns null select null is not distinct from null -- returns true select null is null -- returns true null and functions a function that has input argument as null usually returns null select cos null -- null coalesce is an exception to this coalesce takes any number of arguments and returns for each row the first argument that is not null if all arguments are null coalesce also returns null select coalesce null null 1 -- 1 select coalesce 10 20 -- 10 select coalesce null null -- null ifnull is a two-argument version of coalesce select ifnull null default_string -- default_string select ifnull 1 default_string -- 1 null and conjunctions null values have special semantics in and or conjunctions for the ternary logic truth tables see the boolean type documentation null and aggregate functions null values are ignored in most aggregate functions aggregate functions that do not ignore null values include first last list and array_agg to exclude null values from those aggregate functions the filter clause can be used create table integers i integer insert into integers values 1 10 null select min i from integers -- 1 select max i from integers -- 10",
			"category": "Data Types",
			"url": "/docs/sql/data_types/nulls",
			"blurb": "The NULL value represents a missing value."
		},
		{
			"title": "Nested Functions",
			"text": "this section describes functions and operators for examining and manipulating nested values there are three nested data types lists structs and maps list functions in the descriptions l is the three element list 4 5 6 function description example result -------------------------------------------------- ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ ----------------------------------------- ------------------ list index bracket notation serves as an alias for list_extract l 3 6 list_extract list index extract the index th 1-based value from the list list_extract l 3 6 list_element list index alias for list_extract list_element l 3 6 array_extract list index alias for list_extract array_extract l 3 6 list begin end bracket notation with colon is an alias for list_slice missing arguments are interpreted as null s l 2 3 5 6 list_slice list begin end extract a sublist using slice conventions null s are interpreted as the bounds of the list negative values are accepted list_slice l 2 null 5 6 array_slice list begin end alias for list_slice array_slice l 2 null 5 6 array_pop_front list returns the list without the first element array_pop_front l 5 6 array_pop_back list returns the list without the last element array_pop_back l 4 5 list_value any create a list containing the argument values list_value 4 5 6 4 5 6 list_pack any alias for list_value list_pack 4 5 6 4 5 6 len list return the length of the list len 1 2 3 3 array_length list alias for len array_length 1 2 3 3 unnest list unnests a list by one level note that this is a special function that alters the cardinality of the result see the unnest page for more details unnest 1 2 3 1 2 3 flatten list_of_lists concatenate a list of lists into a single list this only flattens one level of the list see examples flatten 1 2 3 4 1 2 3 4 list_concat list1 list2 concatenates two lists list_concat 2 3 4 5 6 2 3 4 5 6 list_cat list1 list2 alias for list_concat list_cat 2 3 4 5 6 2 3 4 5 6 array_concat list1 list2 alias for list_concat array_concat 2 3 4 5 6 2 3 4 5 6 array_cat list1 list2 alias for list_concat array_cat 2 3 4 5 6 2 3 4 5 6 list_prepend element list prepends element to list list_prepend 3 4 5 6 3 4 5 6 array_prepend element list alias for list_prepend array_prepend 3 4 5 6 3 4 5 6 array_push_front list element alias for list_prepend array_push_front l 3 3 4 5 6 list_append list element appends element to list list_append 2 3 4 2 3 4 array_append list element alias for list_append array_append 2 3 4 2 3 4 array_push_back list element alias for list_append array_push_back l 7 4 5 6 7 list_contains list element returns true if the list contains the element list_contains 1 2 null 1 true list_has list element alias for list_contains list_has 1 2 null 1 true array_contains list element alias for list_contains array_contains 1 2 null 1 true array_has list element alias for list_contains array_has 1 2 null 1 true list_position list element returns the index of the element if the list contains the element list_contains 1 2 null 2 2 list_indexof list element alias for list_position list_indexof 1 2 null 2 2 array_position list element alias for list_position array_position 1 2 null 2 2 array_indexof list element alias for list_position array_indexof 1 2 null 2 2 list_aggregate list name executes the aggregate function name on the elements of list see the list aggregates section for more details list_aggregate 1 2 null min 1 list_aggr list name alias for list_aggregate list_aggr 1 2 null min 1 array_aggregate list name alias for list_aggregate array_aggregate 1 2 null min 1 array_aggr list name alias for list_aggregate array_aggr 1 2 null min 1 list_sort list sorts the elements of the list see the sorting lists section for more details about the sorting order and the null sorting order list_sort 3 6 1 2 1 2 3 6 array_sort list alias for list_sort array_sort 3 6 1 2 1 2 3 6 list_reverse_sort list sorts the elements of the list in reverse order see the sorting lists section for more details about the null sorting order list_reverse_sort 3 6 1 2 6 3 2 1 array_reverse_sort list alias for list_reverse_sort array_reverse_sort 3 6 1 2 6 3 2 1 list_transform list lambda returns a list that is the result of applying the lambda function to each element of the input list see the lambda functions section for more details list_transform l x - x 1 5 6 7 array_transform list lambda alias for list_transform array_transform l x - x 1 5 6 7 list_apply list lambda alias for list_transform list_apply l x - x 1 5 6 7 array_apply list lambda alias for list_transform array_apply l x - x 1 5 6 7 list_filter list lambda constructs a list from those elements of the input list for which the lambda function returns true see the lambda functions section for more details list_filter l x - x 4 5 6 array_filter list lambda alias for list_filter array_filter l x - x 4 5 6 list_distinct list removes all duplicates and nulls from a list does not preserve the original order list_distinct 1 1 null -3 1 5 1 5 -3 array_distinct list alias for list_distinct array_distinct 1 1 null -3 1 5 1 5 -3 list_unique list counts the unique elements of a list list_unique 1 1 null -3 1 5 3 array_unique list alias for list_unique array_unique 1 1 null -3 1 5 3 list_any_value list returns the first non-null value in the list list_any_value null -3 -3 list_resize list size value resizes the list to contain size elements initializes new elements with value or null if value is not set list_resize 1 2 3 5 0 1 2 3 0 0 array_resize list size value alias for list_resize array_resize 1 2 3 5 0 1 2 3 0 0 list operators the following operators are supported for lists operator description example result ---------- ------------------------------------------------------------------------------------------- ---------------------------- ----------------- alias for list_intersect 1 2 3 4 5 2 5 5 6 2 5 alias for list_has_all where the list on the right of the operator is the sublist 1 2 3 4 3 4 3 true alias for list_has_all where the list on the left of the operator is the sublist 1 4 1 2 3 4 true alias for list_concat 1 2 3 4 5 6 1 2 3 4 5 6 list comprehension python-style list comprehension can be used to compute expressions over elements in a list for example select lower x for x in strings from values hello world t strings -- hello world select upper x for x in strings if len x 0 from values hello world t strings -- hello world struct functions function description example result --- --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- --- --- struct entry dot notation serves as an alias for struct_extract i 3 s string s string struct entry bracket notation serves as an alias for struct_extract i 3 s string s string row any create a struct containing the argument values if the values are column references the entry name will be the column name otherwise it will be the string vn where n is the 1-based position of the argument row i i 4 i 4 i 3 v2 3 v3 0 struct_extract struct entry extract the named entry from the struct struct_extract s i 4 struct_pack name any create a struct containing the argument values the entry name will be the bound variable name struct_pack i 4 s string i 3 s string struct_insert struct name any add field s value s to an existing struct with the argument values the entry name s will be the bound variable name s struct_insert a 1 b 2 a 1 b 2 map functions function description example result --- --- --- --- map entry alias for element_at map 100 5 a b 100 a element_at map key return a list containing the value for a given key or an empty list if the key is not contained in the map the type of the key provided in the second parameter must match the type of the map s keys else an error is returned element_at map 100 5 42 43 100 42 map_extract map key alias of element_at return a list containing the value for a given key or an empty list if the key is not contained in the map the type of the key provided in the second parameter must match the type of the map s keys else an error is returned map_extract map 100 5 42 43 100 42 cardinality map return the size of the map or the number of entries in the map cardinality map 4 2 a b 2 map_from_entries struct k v returns a map created from the entries of the array map_from_entries k 5 v val1 k 3 v val2 5 val1 3 val2 map returns an empty map map map_keys map return a list of all keys in the map map_keys map 100 5 42 43 100 42 map_values map return a list of all values in the map map_values map 100 5 42 43 5 33 map_entries map return a list of struct k v for each key-value pair in the map map_entries map 100 5 42 43 k 100 v 42 k 5 v 43 union functions function description example result --- --- --- --- union tag dot notation serves as an alias for union_extract union_value k hello k string union_extract union tag extract the value with the named tags from the union null if the tag is not currently selected union_extract s k hello union_value tag any create a single member union containing the argument value the tag of the value will be the bound variable name union_value k hello hello union k varchar union_tag union retrieve the currently selected tag of the union as an enum union_tag union_value k foo k range functions the functions range and generate_series create a list of values in the range between start and stop the start parameter is inclusive for the range function the stop parameter is exclusive while for generate_series it is inclusive based on the number of arguments the following variants exist range start stop step range start stop range stop generate_series start stop step generate_series start stop generate_series stop the default value of start is 0 and the default value of step is 1 select range 5 -- 0 1 2 3 4 select range 2 5 -- 2 3 4 select range 2 5 3 -- 2 select generate_series 5 -- 0 1 2 3 4 5 select generate_series 2 5 -- 2 3 4 5 select generate_series 2 5 3 -- 2 5 date ranges are also supported select from range date 1992-01-01 date 1992-03-01 interval 1 month range 1992-01-01 00 00 00 1992-02-01 00 00 00 list aggregates the function list_aggregate allows the execution of arbitrary existing aggregate functions on the elements of a list its first argument is the list column its second argument is the aggregate function name e g min histogram or sum list_aggregate accepts additional arguments after the aggregate function name these extra arguments are passed directly to the aggregate function which serves as the second argument of list_aggregate select list_aggregate 1 2 -4 null min -- -4 select list_aggregate 2 4 8 42 sum -- 56 select list_aggregate 1 2 null 2 10 3 last -- 2 10 3 select list_aggregate 2 4 8 42 string_agg -- 2 4 8 42 the following is a list of existing rewrites rewrites simplify the use of the list aggregate function by only taking the list column as their argument list_avg list_var_samp list_var_pop list_stddev_pop list_stddev_samp list_sem list_approx_count_distinct list_bit_xor list_bit_or list_bit_and list_bool_and list_bool_or list_count list_entropy list_last list_first list_kurtosis list_min list_max list_product list_skewness list_sum list_string_agg list_mode list_median list_mad and list_histogram select list_min 1 2 -4 null -- -4 select list_sum 2 4 8 42 -- 56 select list_last 1 2 null 2 10 3 -- 2 10 3 array_to_string concatenates list array elements using an optional delimiter select array_to_string 1 2 3 - as str -- 1-2-3 -- this is equivalent to the following sql select list_aggr 1 2 3 string_agg - as str -- 1-2-3 sorting lists the function list_sort sorts the elements of a list either in ascending or descending order in addition it allows to provide whether null values should be moved to the beginning or to the end of the list by default if no modifiers are provided duckdb sorts asc nulls first i e the values are sorted in ascending order and null values are placed first this is identical to the default sort order of sqlite the default sort order can be changed using these pragma statements list_sort leaves it open to the user whether they want to use the default sort order or a custom order list_sort takes up to two additional optional parameters the second parameter provides the sort order and can be either asc or desc the third parameter provides the null sort order and can be either nulls first or nulls last -- default sort order and default null sort order select list_sort 1 3 null 5 null -5 ---- null null -5 1 3 5 -- only providing the sort order select list_sort 1 3 null 2 asc ---- null 1 2 3 -- providing the sort order and the null sort order select list_sort 1 3 null 2 desc nulls first ---- null 3 2 1 list_reverse_sort has an optional second parameter providing the null sort order it can be either nulls first or nulls last -- default null sort order select list_sort 1 3 null 5 null -5 ---- null null -5 1 3 5 -- providing the null sort order select list_reverse_sort 1 3 null 2 nulls last ---- 3 2 1 null lambda functions parameter1 parameter2 - expression if the lambda function has only one parameter then the brackets can be omitted the parameters can have any names param - param 1 duck - contains concat duck db duck x y - x y transform list_transform list lambda returns a list that is the result of applying the lambda function to each element of the input list the lambda function must have exactly one left-hand side parameter the return type of the lambda function defines the type of the list elements -- incrementing each list element by one select list_transform 1 2 null 3 x - x 1 ---- 2 3 null 4 -- transforming strings select list_transform duck a b duck - concat duck db ---- duckdb adb bdb -- combining lambda functions with other functions select list_transform 5 null 6 x - coalesce x 0 1 ---- 6 1 7 filter list_filter list lambda constructs a list from those elements of the input list for which the lambda function returns true the lambda function must have exactly one left-hand side parameter and its return type must be of type boolean -- filter out negative values select list_filter 5 -6 null 7 x - x 0 ---- 5 7 -- divisible by 2 and 5 select list_filter list_filter 2 4 3 1 20 10 3 30 x - x 2 0 y - y 5 0 ---- 20 10 30 -- in combination with range to construct lists select list_filter 1 2 3 4 x - x 1 from range 4 ---- 1 2 3 4 2 3 4 3 4 4 lambda functions can be arbitrarily nested -- nested lambda functions to get all squares of even list elements select list_transform list_filter 0 1 2 3 4 5 x - x 2 0 y - y y ---- 0 4 16 flatten the flatten function is a scalar function that converts a list of lists into a single list by concatenating each sub-list together note that this only flattens one level at a time not all levels of sub-lists -- convert a list of lists into a single list select flatten 1 2 3 4 ---- 1 2 3 4 -- if the list has multiple levels of lists -- only the first level of sub-lists is concatenated into a single list select flatten 1 2 3 4 5 6 7 8 ---- 1 2 3 4 5 6 7 8 in general the input to the flatten function should be a list of lists not a single level list however the behavior of the flatten function has specific behavior when handling empty lists and null values -- if the input list is empty return an empty list select flatten ---- -- if the entire input to flatten is null return null select flatten null ---- null -- if a list whose only entry is null is flattened return an empty list select flatten null ---- -- if the sub-list in a list of lists only contains null -- do not modify the sub-list -- note the extra set of parentheses vs the prior example select flatten null ---- null -- even if the only contents of each sub-list is null -- still concatenate them together -- note that no de-duplication occurs when flattening -- see list_distinct function for de-duplication select flatten null null ---- null null generate_subscripts the generate_subscript arr dim function generates indexes along the dim th dimension of array arr select generate_subscripts 4 5 6 1 as i i 1 2 3 related functions there are also aggregate functions list and histogram that produces lists and lists of structs unnest is used to unnest a list by one level",
			"category": "Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "This section describes functions and operators for examining and manipulating nested values. There are three nested..."
		},
		{
			"title": "Node.js API",
			"text": "this package provides a node js api for duckdb the sqlite for analytics the api for this client is somewhat compliant to the sqlite node js client for easier transition load the package and create a database object var duckdb require duckdb var db new duckdb database memory or a file name for a persistent db then you can run a query db all select 42 as fortytwo function err res if err throw err console log res 0 fortytwo other available methods are each where the callback is invoked for each row run to execute a single statement without results and exec which can execute several sql commands at once but also does not return results all those commands can work with prepared statements taking the values for the parameters as additional arguments for example like so db all select integer as fortytwo string as hello 42 hello world function err res if err throw err console log res 0 fortytwo console log res 0 hello however these are all shorthands for something much more elegant a database can have multiple connection s those are created using db connect var con db connect you can create multiple connections each with their own transaction context connection objects also contain shorthands to directly call run all and each with parameters and callbacks respectively for example con all select 42 as fortytwo function err res if err throw err console log res 0 fortytwo from connections you can create prepared statements and only that using con prepare var stmt con prepare select integer as fortytwo to execute this statement you can call for example all on the stmt object stmt all 42 function err res if err throw err console log res 0 fortytwo you can also execute the prepared statement multiple times this is for example useful to fill a table with data con run create table a i integer var stmt con prepare insert into a values for var i 0 i 10 i stmt run i stmt finalize con all select from a function err res if err throw err console log res prepare can also take a callback which gets the prepared statement as an argument var stmt con prepare select integer as fortytwo function err stmt stmt all 42 function err res if err throw err console log res 0 fortytwo",
			"category": "Nodejs",
			"url": "/docs/api/nodejs/overview",
			"blurb": "This package provides a node.js API for DuckDB , the SQLite for Analytics. The API for this client is somewhat..."
		},
		{
			"title": "NodeJS API",
			"text": "modules typedefs a name module_duckdb a duckdb summary these jsdoc annotations are still a work in progress - feedback and suggestions are welcome duckdb connection run sql params callback code void code all sql params callback code void code arrowipcall sql params callback code void code arrowipcstream sql params callback each sql params callback code void code stream sql params register_udf name return_type fun code void code prepare sql params callback code statement code exec sql params callback code void code register_udf_bulk name return_type callback code void code unregister_udf name return_type callback code void code register_buffer name array force callback code void code unregister_buffer name callback code void code statement sql get run sql params callback code void code all sql params callback code void code arrowipcall sql params callback code void code each sql params callback code void code finalize sql params callback code void code stream sql params columns code array lt columninfo gt code queryresult nextchunk nextipcbuffer asynciterator database close callback code void code close_internal callback code void code wait callback code void code serialize callback code void code parallelize callback code void code connect path code connection code interrupt callback code void code prepare sql code statement code run sql params callback code void code scanarrowipc sql params callback code void code each sql params callback code void code all sql params callback code void code arrowipcall sql params callback code void code arrowipcstream sql params callback code void code exec sql params callback code void code register_udf name return_type fun code this code register_buffer name code this code unregister_buffer name code this code unregister_udf name code this code registerreplacementscan fun code this code get error code number code open_readonly code number code open_readwrite code number code open_create code number code open_fullmutex code number code open_sharedcache code number code open_privatecache code number code a name module_duckdb connection a duckdb connection kind inner class of code duckdb code connection run sql params callback code void code all sql params callback code void code arrowipcall sql params callback code void code arrowipcstream sql params callback each sql params callback code void code stream sql params register_udf name return_type fun code void code prepare sql params callback code statement code exec sql params callback code void code register_udf_bulk name return_type callback code void code unregister_udf name return_type callback code void code register_buffer name array force callback code void code unregister_buffer name callback code void code a name module_duckdb connection run a connection run sql params callback code void code run a sql statement and trigger a callback when done kind instance method of code connection code param type --- --- sql params code code callback a name module_duckdb connection all a connection all sql params callback code void code run a sql query and triggers the callback once for all result rows kind instance method of code connection code param type --- --- sql params code code callback a name module_duckdb connection arrowipcall a connection arrowipcall sql params callback code void code run a sql query and serialize the result into the apache arrow ipc format requires arrow extension to be loaded kind instance method of code connection code param type --- --- sql params code code callback a name module_duckdb connection arrowipcstream a connection arrowipcstream sql params callback run a sql query returns a ipcresultstreamiterator that allows streaming the result into the apache arrow ipc format requires arrow extension to be loaded kind instance method of code connection code returns promise ipcresultstreamiterator param type --- --- sql params code code callback a name module_duckdb connection each a connection each sql params callback code void code runs a sql query and triggers the callback for each result row kind instance method of code connection code param type --- --- sql params code code callback a name module_duckdb connection stream a connection stream sql params kind instance method of code connection code param type --- --- sql params code code a name module_duckdb connection register_udf a connection register _ udf name return_type fun code void code register a user defined function kind instance method of code connection code note this follows the wasm udfs somewhat but is simpler because we can pass data much more cleanly param --- name return_type fun a name module_duckdb connection prepare a connection prepare sql params callback code statement code prepare a sql query for execution kind instance method of code connection code param type --- --- sql params code code callback a name module_duckdb connection exec a connection exec sql params callback code void code execute a sql query kind instance method of code connection code param type --- --- sql params code code callback a name module_duckdb connection register_udf_bulk a connection register _ udf _ bulk name return_type callback code void code register a user defined function kind instance method of code connection code param --- name return_type callback a name module_duckdb connection unregister_udf a connection unregister _ udf name return_type callback code void code unregister a user defined function kind instance method of code connection code param --- name return_type callback a name module_duckdb connection register_buffer a connection register _ buffer name array force callback code void code register a buffer to be scanned using the apache arrow ipc scanner requires arrow extension to be loaded kind instance method of code connection code param --- name array force callback a name module_duckdb connection unregister_buffer a connection unregister _ buffer name callback code void code unregister the buffer kind instance method of code connection code param --- name callback a name module_duckdb statement a duckdb statement kind inner class of code duckdb code statement sql get run sql params callback code void code all sql params callback code void code arrowipcall sql params callback code void code each sql params callback code void code finalize sql params callback code void code stream sql params columns code array lt columninfo gt code a name module_duckdb statement sql a statement sql kind instance property of code statement code returns sql contained in statement field a name module_duckdb statement get a statement get not implemented kind instance method of code statement code a name module_duckdb statement run a statement run sql params callback code void code kind instance method of code statement code param type --- --- sql params code code callback a name module_duckdb statement all a statement all sql params callback code void code kind instance method of code statement code param type --- --- sql params code code callback a name module_duckdb statement arrowipcall a statement arrowipcall sql params callback code void code kind instance method of code statement code param type --- --- sql params code code callback a name module_duckdb statement each a statement each sql params callback code void code kind instance method of code statement code param type --- --- sql params code code callback a name module_duckdb statement finalize a statement finalize sql params callback code void code kind instance method of code statement code param type --- --- sql params code code callback a name module_duckdb statement stream a statement stream sql params kind instance method of code statement code param type --- --- sql params code code a name module_duckdb statement columns a statement columns code array lt columninfo gt code kind instance method of code statement code returns code array lt columninfo gt code - - array of column names and types a name module_duckdb queryresult a duckdb queryresult kind inner class of code duckdb code queryresult nextchunk nextipcbuffer asynciterator a name module_duckdb queryresult nextchunk a queryresult nextchunk kind instance method of code queryresult code returns data chunk a name module_duckdb queryresult nextipcbuffer a queryresult nextipcbuffer function to fetch the next result blob of an arrow ipc stream in a zero-copy way requires arrow extension to be loaded kind instance method of code queryresult code returns data chunk a name module_duckdb queryresult asynciterator a queryresult asynciterator kind instance method of code queryresult code a name module_duckdb database a duckdb database main database interface kind inner property of code duckdb code param description --- --- path path to database file or memory for in-memory database access_mode access mode config the configuration object callback callback function database close callback code void code close_internal callback code void code wait callback code void code serialize callback code void code parallelize callback code void code connect path code connection code interrupt callback code void code prepare sql code statement code run sql params callback code void code scanarrowipc sql params callback code void code each sql params callback code void code all sql params callback code void code arrowipcall sql params callback code void code arrowipcstream sql params callback code void code exec sql params callback code void code register_udf name return_type fun code this code register_buffer name code this code unregister_buffer name code this code unregister_udf name code this code registerreplacementscan fun code this code get a name module_duckdb database close a database close callback code void code closes database instance kind instance method of code database code param --- callback a name module_duckdb database close_internal a database close _ internal callback code void code internal method do not use call connection close instead kind instance method of code database code param --- callback a name module_duckdb database wait a database wait callback code void code triggers callback when all scheduled database tasks have completed kind instance method of code database code param --- callback a name module_duckdb database serialize a database serialize callback code void code todo what does this do kind instance method of code database code param --- callback a name module_duckdb database parallelize a database parallelize callback code void code todo what does this do kind instance method of code database code param --- callback a name module_duckdb database connect a database connect path code connection code create a new database connection kind instance method of code database code param description --- --- path the database to connect to either a file path or memory a name module_duckdb database interrupt a database interrupt callback code void code supposedly interrupt queries but currently does not do anything kind instance method of code database code param --- callback a name module_duckdb database prepare a database prepare sql code statement code prepare a sql query for execution kind instance method of code database code param --- sql a name module_duckdb database run a database run sql params callback code void code convenience method for connection run using a built-in default connection kind instance method of code database code param type --- --- sql params code code callback a name module_duckdb database scanarrowipc a database scanarrowipc sql params callback code void code convenience method for connection scanarrowipc using a built-in default connection kind instance method of code database code param type --- --- sql params code code callback a name module_duckdb database each a database each sql params callback code void code kind instance method of code database code param type --- --- sql params code code callback a name module_duckdb database all a database all sql params callback code void code convenience method for connection apply using a built-in default connection kind instance method of code database code param type --- --- sql params code code callback a name module_duckdb database arrowipcall a database arrowipcall sql params callback code void code convenience method for connection arrowipcall using a built-in default connection kind instance method of code database code param type --- --- sql params code code callback a name module_duckdb database arrowipcstream a database arrowipcstream sql params callback code void code convenience method for connection arrowipcstream using a built-in default connection kind instance method of code database code param type --- --- sql params code code callback a name module_duckdb database exec a database exec sql params callback code void code kind instance method of code database code param type --- --- sql params code code callback a name module_duckdb database register_udf a database register _ udf name return_type fun code this code register a user defined function convenience method for connection register_udf kind instance method of code database code param --- name return_type fun a name module_duckdb database register_buffer a database register _ buffer name code this code register a buffer containing serialized data to be scanned from duckdb convenience method for connection unregister_buffer kind instance method of code database code param --- name a name module_duckdb database unregister_buffer a database unregister _ buffer name code this code unregister a buffer convenience method for connection unregister_buffer kind instance method of code database code param --- name a name module_duckdb database unregister_udf a database unregister _ udf name code this code unregister a udf convenience method for connection unregister_udf kind instance method of code database code param --- name a name module_duckdb database registerreplacementscan a database registerreplacementscan fun code this code register a table replace scan function kind instance method of code database code param description --- --- fun replacement scan function a name module_duckdb database get a database get not implemented kind instance method of code database code a name module_duckdb error a duckdb error code number code check that errno attribute equals this to check for a duckdb error kind inner constant of code duckdb code a name module_duckdb open_readonly a duckdb open _ readonly code number code open database in readonly mode kind inner constant of code duckdb code a name module_duckdb open_readwrite a duckdb open _ readwrite code number code currently ignored kind inner constant of code duckdb code a name module_duckdb open_create a duckdb open _ create code number code currently ignored kind inner constant of code duckdb code a name module_duckdb open_fullmutex a duckdb open _ fullmutex code number code currently ignored kind inner constant of code duckdb code a name module_duckdb open_sharedcache a duckdb open _ sharedcache code number code currently ignored kind inner constant of code duckdb code a name module_duckdb open_privatecache a duckdb open _ privatecache code number code currently ignored kind inner constant of code duckdb code a name columninfo a columninfo code object code kind global typedef properties name type description --- --- --- name code string code column name type code typeinfo code column type a name typeinfo a typeinfo code object code kind global typedef properties name type description --- --- --- id code string code type id alias code string code sql type alias sql_type code string code sql type name a name duckdberror a duckdberror code object code kind global typedef properties name type description --- --- --- errno code number code -1 for duckdb errors message code string code error message code code string code duckdb_nodejs_error for duckdb errors errortype code string code duckdb error type code eg http io catalog a name httperror a httperror code object code kind global typedef extends code duckdberror code properties name type description --- --- --- statuscode code number code http response status code reason code string code http response reason response code string code http response body headers code object code http headers",
			"category": "Nodejs",
			"url": "/docs/api/nodejs/reference",
			"blurb": "Modules Typedefs <a name=module_duckdb> </a> duckdb Summary : these jsdoc annotations are still a work in progress -..."
		},
		{
			"title": "Numeric Functions",
			"text": "numeric operators the table below shows the available mathematical operators for numeric types operator description example result --- --- --- --- addition 2 3 5 - subtraction 2 - 3 -1 multiplication 2 3 6 float division 5 2 2 5 division 5 2 2 modulo remainder 5 4 1 exponent 3 4 81 exponent alias for 3 4 81 bitwise and 91 15 11 bitwise or 32 3 35 bitwise shift left 1 4 16 bitwise shift right 8 2 2 bitwise negation 15 -16 factorial of x computes the product of the current integer and all integers below it 4 24 there are two division operators and they are equivalent when at least one of the operands is a float or a double when both operands are integers performs floating points division 5 2 2 5 while performs integer division 5 2 2 the modulo bitwise and negation and factorial operators work only on integral data types whereas the others are available for all numeric data types numeric functions the table below shows the available mathematical functions function description example result --- --- --- --- abs x absolute value abs -17 4 17 4 acos x computes the arccosine of x acos 0 5 1 0471975511965976 asin x computes the arcsine of x asin 0 5 0 5235987755982989 atan x computes the arctangent of x atan 0 5 0 4636476090008061 atan2 y x computes the arctangent y x atan2 0 5 0 5 0 7853981633974483 bit_count x returns the number of bits that are set bit_count 31 5 cbrt x returns the cube root of the number cbrt 8 2 ceil x rounds the number up ceil 17 4 18 ceiling x rounds the number up alias of ceil ceiling 17 4 18 cos x computes the cosine of x cos 90 -0 4480736161291701 cot x computes the cotangent of x cot 0 5 1 830487721712452 degrees x converts radians to degrees degrees pi 180 even x round to next even number by rounding away from zero even 2 9 4 exp x computes e x exp 0 693 2 factorial x see operator computes the product of the current integer and all integers below it factorial 4 24 floor x rounds the number down floor 17 4 17 gamma x interpolation of x-1 factorial so decimal inputs are allowed gamma 5 5 52 34277778455352 gcd x y computes the greatest common divisor of x and y gcd 42 57 3 greatest_common_divisor x y computes the greatest common divisor of x and y greatest_common_divisor 42 57 3 greatest x1 x2 selects the largest value greatest 3 2 4 4 4 isfinite x returns true if the floating point value is finite false otherwise isfinite 5 5 true isinf x returns true if the floating point value is infinite false otherwise isinf infinity float true isnan x returns true if the floating point value is not a number false otherwise isnan nan float true lcm x y computes the least common multiple of x and y lcm 42 57 798 least_common_multiple x y computes the least common multiple of x and y least_common_multiple 42 57 798 least x1 x2 selects the smallest value least 3 2 4 4 2 lgamma x computes the log of the gamma function lgamma 2 0 ln x computes the natural logarithm of x ln 2 0 693 log x computes the 10-log of x log 100 2 log2 x computes the 2-log of x log2 8 3 log10 x alias of log computes the 10-log of x log10 1000 3 nextafter x y return the next floating point value after x in the direction of y nextafter 1 float 2 float 1 0000001 pi returns the value of pi pi 3 141592653589793 pow x y computes x to the power of y pow 2 3 8 power x y alias of pow computes x to the power of y power 2 3 8 radians x converts degrees to radians radians 90 1 5707963267948966 random returns a random number between 0 and 1 random various round v numeric s int round to s decimal places values s 0 are allowed round 42 4332 2 42 43 setseed x sets the seed to be used for the random function setseed 0 42 sin x computes the sin of x sin 90 0 8939966636005579 sign x returns the sign of x as -1 0 or 1 sign -349 -1 signbit x returns whether the signbit is set or not signbit -0 0 true sqrt x returns the square root of the number sqrt 9 3 xor x bitwise xor xor 17 5 20 tan x computes the tangent of x tan 90 -1 995200412208242 absolute value parentheses optional if operating on a column -2 2",
			"category": "Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "Numeric Operators The table below shows the available mathematical operators for numeric types. | Operator |..."
		},
		{
			"title": "Numeric Types",
			"text": "integer types the types tinyint smallint integer bigint and hugeint store whole numbers that is numbers without fractional components of various ranges attempts to store values outside of the allowed range will result in an error the types utinyint usmallint uinteger ubigint store whole unsigned numbers attempts to store negative numbers or values outside of the allowed range will result in an error name aliases min max --- --- --- --- tinyint int1 -128 127 smallint int2 short -32768 32767 integer int4 int signed -2147483648 2147483647 bigint int8 long -9223372036854775808 9223372036854775807 hugeint -170141183460469231731687303715884105727 170141183460469231731687303715884105727 utinyint - 0 255 usmallint - 0 65535 uinteger - 0 4294967295 ubigint - 0 18446744073709551615 the type integer is the common choice as it offers the best balance between range storage size and performance the smallint type is generally only used if disk space is at a premium the bigint and hugeint types are designed to be used when the range of the integer type is insufficient -170141183460469231731687303715884105728 -1 127 is not representable by the internal structure fixed-point decimals the data type decimal width scale represents an exact fixed-point decimal value when creating a value of type decimal the width and scale can be specified to define which size of decimal values can be held in the field the width field determines how many digits can be held and the scale determines the amount of digits after the decimal point for example the type decimal 3 2 can fit the value 1 23 but cannot fit the value 12 3 or the value 1 234 the default width and scale is decimal 18 3 if none are specified internally decimals are represented as integers depending on their specified width width internal size bytes --- --- --- 1-4 int16 2 5-9 int32 4 10-18 int64 8 19-38 int128 16 performance can be impacted by using too large decimals when not required in particular decimal values with a width above 19 are very slow as arithmetic involving the int128 type is much more expensive than operations involving the int32 or int64 types it is therefore recommended to stick with a width of 18 or below unless there is a good reason for why this is insufficient floating-point types the data types real and double precision are inexact variable-precision numeric types in practice these types are usually implementations of ieee standard 754 for binary floating-point arithmetic single and double precision respectively to the extent that the underlying processor operating system and compiler support it name aliases description --- --- --- real float4 float single precision floating-point number 4 bytes double float8 double precision floating-point number 8 bytes inexact means that some values cannot be converted exactly to the internal format and are stored as approximations so that storing and retrieving a value might show slight discrepancies managing these errors and how they propagate through calculations is the subject of an entire branch of mathematics and computer science and will not be discussed here except for the following points if you require exact storage and calculations such as for monetary amounts use the numeric type instead if you want to do complicated calculations with these types for anything important especially if you rely on certain behavior in boundary cases infinity underflow you should evaluate the implementation carefully comparing two floating-point values for equality might not always work as expected on most platforms the real type has a range of at least 1e-37 to 1e 37 with a precision of at least 6 decimal digits the double type typically has a range of around 1e-307 to 1e 308 with a precision of at least 15 digits values that are too large or too small will cause an error rounding might take place if the precision of an input number is too high numbers too close to zero that are not representable as distinct from zero will cause an underflow error in addition to ordinary numeric values the floating-point types have several special values infinity -infinity nan these represent the ieee 754 special values infinity negative infinity and not-a-number respectively on a machine whose floating-point arithmetic does not follow ieee 754 these values will probably not work as expected when writing these values as constants in an sql command you must put quotes around them for example update table set x -infinity on input these strings are recognized in a case-insensitive manner functions see numeric functions and operators",
			"category": "Data Types",
			"url": "/docs/sql/data_types/numeric",
			"blurb": "Numeric types are used to store numbers, and come in different shapes and sizes."
		},
		{
			"title": "ODBC API - Linux",
			"text": "a driver manager is required to manage communication between applications and the odbc driver we tested and support unixodbc that is a complete odbc driver manager for linux users can install it from the command line debian so flavors sudo apt get install unixodbc fedora so flavors sudo yum install unixodbc or sudo dnf install unixodbc step 1 download odbc driver duckdb releases the odbc driver as asset for linux download it from a href https github com duckdb duckdb releases download v site currentduckdbversion duckdb_odbc-linux-amd64 zip odbc linux asset a that contains the following artifacts libduckdb_odbc so the duckdb driver compiled to ubuntu 16 04 unixodbc_setup sh a setup script to aid the configuration on linux step 2 extracting odbc artifacts run unzip to extract the files to a permanent directory mkdir duckdb_odbc unzip duckdb_odbc-linux-amd64 zip -d duckdb_odbc step 3 configuring with unixodbc the unixodbc_setup sh script aids the configuration of the duckdb odbc driver it is based on the unixodbc package that provides some commands to handle the odbc setup and test like odbcinst and isql in a terminal window change to the duckdb_odbc permanent directory and run the following commands with level options -u or -s either to configure duckdb odbc user-level odbc setup -u the -u option based on the user home directory to setup the odbc init files unixodbc_setup sh -u p s the default configuration consists of a database memory system-level odbc setup -s the -s changes the system level files that will be visible for all users because of that it requires root privileges sudo unixodbc_setup sh -s p s the default configuration consists of a database memory show usage --help the option --help shows the usage of unixodbc_setup sh that provides alternative options for a customer configuration like -db and -d unixodbc_setup sh --help usage unixodbc_setup sh level options example unixodbc_setup sh -u -db database_path -d driver_path libduckdb_odbc so level -s system-level using sudo to configure duckdb odbc at the system-level changing the files etc odbc inst ini -u user-level configuring the duckdb odbc at the user-level changing the files odbc inst ini options -db database_path the duckdb database file path the default is memory if not provided -d driver_path the driver file path i e the path for libduckdb_odbc so the default is using the base script directory step 4 optional configure the odbc driver the odbc setup on linux is based on files the well-known odbc ini and odbcinst ini these files can be placed at the system etc directory or at the user home directory home user shortcut as the dm prioritizes the user configuration files and then the system files the odbc ini file the odbc ini contains the dsns for the drivers which can have specific knobs an example of odbc ini with duckdb would be duckdb driver duckdb driver database memory duckdb between the brackets is a dsn for the duckdb driver it describes the driver s name and other configurations will be placed at the odbcinst ini database it describes the database name used by duckdb and it can also be a file path to a db in the system the odbcinst ini file the odbcinst ini contains general configurations for the odbc installed drivers in the system a driver section starts with the driver name between brackets and then it follows specific configuration knobs belonging to that driver an example of odbcinst ini with the duckdb driver would be odbc trace yes tracefile tmp odbctrace duckdb driver driver home user duckdb_odbc libduckdb_odbc so odbc it is the dm configuration section trace it enables the odbc trace file using the option yes tracefile the absolute system file path for the odbc trace file duckdb driver the section of the duckdb installed driver driver the absolute system file path of the duckdb driver",
			"category": "Odbc",
			"url": "/docs/api/odbc/linux",
			"blurb": "A driver manager is required to manage communication between applications and the ODBC driver. We tested and support..."
		},
		{
			"title": "ODBC API - MacOS",
			"text": "a driver manager is required to manage communication between applications and the odbc driver we tested and support unixodbc that is a complete odbc driver manager for macos and linux users can install it from the command line brew brew install unixodbc step 1 download odbc driver duckdb releases the odbc driver as asset for macos download it from a href https github com duckdb duckdb releases download v site currentduckdbversion duckdb_odbc-osx-universal zip odbc linux asset a that contains the following artifacts libduckdb_odbc dylib the duckdb driver compiled to macos with intel and apple m1 support step 2 extracting odbc artifacts run unzip to extract the files to a permanent directory mkdir duckdb_odbc unzip duckdb_odbc-osx-universal zip -d duckdb_odbc step 3 configure the odbc driver the odbc ini or odbc ini file the odbc ini contains the dsns for the drivers which can have specific knobs an example of odbc ini with duckdb would be duckdb driver duckdb driver database memory duckdb between the brackets is a dsn for the duckdb driver it describes the driver s name and other configurations will be placed at the odbcinst ini database it describes the database name used by duckdb and it can also be a file path to a db in the system the odbcinst ini file the odbcinst ini contains general configurations for the odbc installed drivers in the system a driver section starts with the driver name between brackets and then it follows specific configuration knobs belonging to that driver an example of odbcinst ini with the duckdb driver would be odbc trace yes tracefile tmp odbctrace duckdb driver driver user user duckdb_odbc libduckdb_odbc dylib odbc it is the dm configuration section trace it enables the odbc trace file using the option yes tracefile the absolute system file path for the odbc trace file duckdb driver the section of the duckdb installed driver driver the absolute system file path of the duckdb driver step 4 optionnal test the odbc driver after the configuration for validate the installation it is possible to use a odbc client unixodbc use a command line tool called isql use the dsn defined in odbc ini as a parameter of isql isql duckdb --------------------------------------- connected sql-statement help tablename echo string quit --------------------------------------- sql select 42 ------------ 42 ------------ 42 ------------ sqlrowcount returns -1 1 rows fetched",
			"category": "Odbc",
			"url": "/docs/api/odbc/macos",
			"blurb": "A driver manager is required to manage communication between applications and the ODBC driver. We tested and support..."
		},
		{
			"title": "ODBC API - Overview",
			"text": "the odbc open database connectivity is a c-style api that provides access to different flavors of database management systems dbmss the odbc api consists of the driver manager dm and the odbc drivers the dm is part of the system library e g unixodbc which manages the communications between the user applications and the odbc drivers typically applications are linked against the dm which uses data source name dsn to look up the correct odbc driver the odbc driver is a dbms implementation of the odbc api which handles all the internals of that dbms the dm maps user application calls of odbc functions to the correct odbc driver that performs the specified function and returns the proper values duckdb odbc driver duckdb supports the odbc version 3 0 according to the core interface conformance we release the odbc driver as assets for linux and windows users can download them from the latest release of duckdb operating system pages in this section",
			"category": "Odbc",
			"url": "/docs/api/odbc/overview",
			"blurb": "The ODBC (Open Database Connectivity) is a C-style API that provides access to different flavors of Database..."
		},
		{
			"title": "ODBC API - Windows",
			"text": "the microsoft windows requires an odbc driver manager to manage communication between applications and the odbc drivers the dm on windows is provided in a dll file odbccp32 dll and other files and tools for detailed information checkout out the common odbc component files step 1 download odbc driver duckdb releases the odbc driver as asset for windows download it from a href https github com duckdb duckdb releases download v site currentduckdbversion duckdb_odbc-windows-amd64 zip windows asset a that contains the following artifacts duckdb_odbc dll the duckdb driver compiled for windows duckdb_odbc_setup dll a setup dll used by the windows odbc data source administrator tool odbc_install exe a installation script to aid the configuration on windows step 2 extracting odbc artifacts unzip the file to a permanent directory e g duckdb_odbc an example with powershell and unzip command would be mkdir duckdb_odbc unzip duckdb_odbc-linux-amd64 zip -d duckdb_odbc step 3 odbc windows installer the odbc_install exe aids the configuration of the duckdb odbc driver on windows it depends on the odbccp32 dll that provides functions to configure the odbc registry entries inside the permanent directory e g duckdb_odbc double-click on the odbc_install exe windows administrator privileges is required in case of a non-administrator a user account control shall display step 4 configure the odbc driver the odbc_install exe adds a default dsn configuration into the odbc registries with a default database memory dsn windows setup after the installation it is possible to change the default dsn configuration or add a new one using the windows odbc data source administrator tool odbcad32 exe it also can be launched thought the windows start default duckdb dsn in the windows odbc data source administrator tool at system dsn tab is placed the default installed dsn for duckdb windows odbc config tool changing duckdb dsn selecting the default dsn i e duckdb or add a new configuration the following setup window will display duckdb windows dsn setup for now it is possible to set the dsn and the database file path associated with that dsn more detailed windows setup the odbc setup on windows is based on registry keys see registry entries for odbc components the odbc entries can be placed at the current user registry key hkcu or the system registry key hklm we have tested and used the system entries based on hklm- software- odbc the odbc_install exe changes this entry that has two subkeys odbc ini and odbcinst ini the odbc ini is where users usually insert dsn registry entries for the drivers for example the dsn registry for duckdb would look like this hklm- software- odbc- odbc ini- duckdb the odbcinst ini contains one entry for each odbc driver and other keys predefined for windows odbc configuration",
			"category": "Odbc",
			"url": "/docs/api/odbc/windows",
			"blurb": "The Microsoft Windows requires an ODBC Driver Manager to manage communication between applications and the ODBC..."
		},
		{
			"title": "ORDER BY Clause",
			"text": "order by is an output modifier logically it is applied near the very end of the query just prior to limit or offset if present the order by clause sorts the rows on the sorting criteria in either ascending or descending order in addition every order clause can specify whether null values should be moved to the beginning or to the end the order by clause may contain one or more expressions separated by commas an error will be thrown if no expressions are included since the order by clause should be removed in that situation the expressions may begin with either an arbitrary scalar expression which could be a column name a column position number ex 1 note that it is 1-indexed or the keyword all each expression can optionally be followed by an order modifier asc or desc default is asc and or a null order modifier nulls first or nulls last default is nulls last order by all the all keyword indicates that the output should be sorted by every column in order from left to right the direction of this sort may be modified using either order by all asc or order by all desc and or nulls first or nulls last note that all may not be used in combination with other expressions in the order by clause - it must be by itself see examples below null order modifier by default if no modifiers are provided duckdb sorts asc nulls last i e the values are sorted in ascending order and null values are placed last this is identical to the default sort order of postgresql note that this was a breaking change in version 0 8 0 prior to 0 8 0 duckdb sorted using asc nulls first the default sort order can be changed using the following pragma statements -- change the default null sorting order to either nulls first and nulls last pragma default_null_order nulls first -- change the default sorting order to either desc or asc pragma default_order desc collations text is sorted using the binary comparison collation by default which means values are sorted on their binary utf8 values while this works well for ascii text e g for english language data the sorting order can be incorrect for other languages for this purpose duckdb provides collations for more information on collations see the collation page examples all examples use this example table create or replace table addresses as select 123 quack blvd as address ducktown as city 11111 as zip union all select 111 duck duck goose ln ducktown 11111 union all select 111 duck duck goose ln duck town 11111 union all select 111 duck duck goose ln duck town 11111-0001 -- select the addresses ordered by city name using the default null order and default order select from addresses order by city -- select the addresses ordered by city name in descending order with nulls at the end select from addresses order by city desc nulls last -- order by city and then by zip code both using the default orderings select from addresses order by city zip -- order by city using german collation rules select from addresses order by city collate de order by all examples -- order from left to right by address then by city then by zip in ascending order select from addresses order by all address city zip ------------------------ ----------- ------------ 111 duck duck goose ln duck town 11111 111 duck duck goose ln duck town 11111-0001 111 duck duck goose ln ducktown 11111 123 quack blvd ducktown 11111 -- order from left to right by address then by city then by zip in descending order select from addresses order by all desc address city zip ------------------------ ----------- ------------ 123 quack blvd ducktown 11111 111 duck duck goose ln ducktown 11111 111 duck duck goose ln duck town 11111-0001 111 duck duck goose ln duck town 11111 syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/orderby",
			"blurb": "ORDER BY is an output modifier. Logically it is applied near the very end of the query (just prior to LIMIT or OFFSET..."
		},
		{
			"title": "Parquet Export",
			"text": "to export the data from a table to a parquet file use the copy statement copy tbl to output parquet format parquet the result of queries can also be directly exported to a parquet file copy select from tbl to output parquet format parquet",
			"category": "Import",
			"url": "/docs/guides/import/parquet_export",
			"blurb": "To export the data from a table to a Parquet file, use the COPY statement. COPY tbl TO 'output.parquet' (FORMAT..."
		},
		{
			"title": "Parquet Import",
			"text": "to read data from a parquet file use the read_parquet function in the from clause of a query select from read_parquet input parquet to create a new table using the result from a query use create table as from a select statement create table new_tbl as select from read_parquet input parquet to load data into an existing table from a query use insert into from a select statement insert into tbl select from read_parquet input parquet alternatively the copy statement can also be used to load data from a parquet file into an existing table copy tbl from input parquet format parquet for additional options see the parquet loading reference",
			"category": "Import",
			"url": "/docs/guides/import/parquet_import",
			"blurb": "To read data from a Parquet file, use the read_parquet function in the FROM clause of a query. SELECT * FROM..."
		},
		{
			"title": "Parquet Import",
			"text": "to run a query directly on a parquet file use the read_parquet function in the from clause of a query select from read_parquet input parquet the parquet file will be processed in parallel filters will be automatically pushed down into the parquet scan and only the relevant columns will be read automatically for more information see the blog post querying parquet with precision using duckdb",
			"category": "Import",
			"url": "/docs/guides/import/query_parquet",
			"blurb": "To run a query directly on a Parquet file, use the read_parquet function in the FROM clause of a query. SELECT * FROM..."
		},
		{
			"title": "Parquet Loading",
			"text": "examples -- read a single parquet file select from test parquet -- figure out which columns types are in a parquet file describe select from test parquet -- create a table from a parquet file create table test as select from test parquet -- if the file does not end in parquet use the read_parquet function select from read_parquet test parq -- use list parameter to read 3 parquet files and treat them as a single table select from read_parquet file1 parquet file2 parquet file3 parquet -- read all files that match the glob pattern select from test parquet -- read all files that match the glob pattern and include a filename column that specifies which file each row came from select from read_parquet test parquet filename true -- use a list of globs to read all parquet files from 2 specific folders select from read_parquet folder1 parquet folder2 parquet -- query the metadata of a parquet file select from parquet_metadata test parquet -- query the schema of a parquet file select from parquet_schema test parquet -- write the results of a query to a parquet file copy select from tbl to result-snappy parquet format parquet -- write the results from a query to a parquet file with specific compression and row_group_size copy from generate_series 100000 to test parquet format parquet compression zstd row_group_size 100000 -- export the table contents of the entire database as parquet export database target_directory format parquet parquet files parquet files are compressed columnar files that are efficient to load and process duckdb provides support for both reading and writing parquet files in an efficient manner as well as support for pushing filters and projections into the parquet file scans parameters parquet files are self-describing as such far fewer parameters are required than with csv files nevertheless there are a number of options exposed that can be passed to the read_parquet function or the copy statement name description type default --- --- ---- ---- binary_as_string parquet files generated by legacy writers do not correctly set the utf8 flag for strings causing string columns to be loaded as blob instead set this to true to load binary columns as strings bool false filename whether or not an extra filename column should be included in the result bool false file_row_number whether or not to include the file_row_number column bool false hive_partitioning whether or not to interpret the path as a hive partitioned path bool false union_by_name whether the columns of multiple schemas should be unified by name rather than by position bool false read_parquet function function description example -------------------------------------------------- ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ ----------------------------------------- read_parquet path s read parquet file s select from read_parquet test parquet parquet_scan path s alias for read_parquet select from parquet_scan test parquet if your file ends in parquet the function syntax is optional the system will automatically infer that you are reading a parquet file select from test parquet multiple files can be read at once by providing a glob or a list of files refer to the multiple files section for more information partial reading duckdb supports projection pushdown into the parquet file itself that is to say when querying a parquet file only the columns required for the query are read this allows you to read only the part of the parquet file that you are interested in this will be done automatically by the system duckdb also supports filter pushdown into the parquet reader when you apply a filter to a column that is scanned from a parquet file the filter will be pushed down into the scan and can even be used to skip parts of the file using the built-in zonemaps note that this will depend on whether or not your parquet file contains zonemaps filter and projection pushdown provide significant performance benefits see our blog post on this for more information inserts and views you can also insert the data into a table or create a table from the parquet file directly this will load the data from the parquet file and insert it into the database -- insert the data from the parquet file in the table insert into people select from read_parquet test parquet -- create a table directly from a parquet file create table people as select from read_parquet test parquet if you wish to keep the data stored inside the parquet file but want to query the parquet file directly you can create a view over the read_parquet function you can then query the parquet file as if it were a built-in table -- create a view over the parquet file create view people as select from read_parquet test parquet -- query the parquet file select from people writing to parquet files duckdb also has support for writing to parquet files using the copy statement syntax see the copy statement page for details including all possible parameters for the copy statement -- write a query to a snappy compressed parquet file copy select from tbl to result-snappy parquet format parquet -- write tbl to a zstd compressed parquet file copy tbl to result-zstd parquet format parquet codec zstd -- write a csv file to an uncompressed parquet file copy test csv to result-uncompressed parquet format parquet codec uncompressed -- write a query to a parquet file with zstd compression same as codec and row_group_size copy from generate_series 100000 to row-groups-zstd parquet format parquet compression zstd row_group_size 100000 duckdb s export command can be used to export an entire database to a series of parquet files see the export statement documentation for more details -- export the table contents of the entire database as parquet export database target_directory format parquet installing and loading parquet extension the support for parquet files is enabled via extension the extension is bundled with almost all clients however if your client does not bundle the parquet extension the extension must be installed and loaded separately -- run once install parquet -- run before usage load parquet",
			"category": "Parquet",
			"url": "/docs/data/parquet/overview",
			"blurb": "Examples -- read a single parquet file SELECT * FROM 'test.parquet'; -- figure out which columns/types are in a..."
		},
		{
			"title": "Parquet Tips",
			"text": "below is a collection of tips to help when dealing with parquet files tips for reading parquet files use union_by_name when loading files with different schemas the union_by_name option can be used to unify the schema of files that have different or missing columns for files that do not have certain columns null values are filled in select from read_parquet flights parquet union_by_name true tips for writing parquet files enabling per_thread_output if the final number of parquet files is not important writing one file per thread can significantly improve performance using a glob pattern upon read or a hive partitioning structure are good ways to transparently handle multiple files copy from generate_series 10000000 to test parquet format parquet per_thread_output true selecting a row_group_size the row_group_size parameter specifies the minimum number of rows in a parquet row group with a minimum value equal to duckdb s vector size currently 2048 but adjustable when compiling duckdb and a default of 122880 a parquet row group is a partition of rows consisting of a column chunk for each column in the dataset compression algorithms are only applied per row group so the larger the row group size the more opportunities to compress the data duckdb can read parquet row groups in parallel even within the same file and uses predicate pushdown to only scan the row groups whose metadata ranges match the where clause of the query however there is some overhead associated with reading the metadata in each group a good approach would be to ensure that within each file the total number of row groups is at least as large as the number of cpu threads used to query that file more row groups beyond the thread count would improve the speed of highly selective queries but slow down queries that must scan the whole file like aggregations -- write a query to a parquet file with a different row_group_size copy from generate_series 100000 to row-groups parquet format parquet row_group_size 100000",
			"category": "Parquet",
			"url": "/docs/data/parquet/tips",
			"blurb": "Below is a collection of tips to help when dealing with Parquet files. Tips for reading Parquet files Use..."
		},
		{
			"title": "Partitioned Writes",
			"text": "examples -- write a table to a hive partitioned data set of parquet files copy orders to orders format parquet partition_by year month -- write a table to a hive partitioned data set of csv files allowing overwrites copy orders to orders format csv partition_by year month overwrite_or_ignore 1 partitioned writes when the partition_by clause is specified for the copy statement the files are written in a hive partitioned folder hierarchy the target is the name of the root directory in the example above orders the files are written in-order in the file hierarchy currently one file is written per thread to each directory orders year 2021 month 1 data_1 parquet data_2 parquet month 2 data_1 parquet year 2022 month 11 data_1 parquet data_2 parquet month 12 data_1 parquet the values of the partitions are automatically extracted from the data note that it can be very expensive to write many partitions as many files will be created the ideal partition count depends on how large your data set is writing data into many small partitions is expensive it is generally recommended to have at least 100mb of data per partition overwriting by default the partitioned write will not allow overwriting existing directories use the overwrite_or_ignore option to allow overwriting an existing directory filename pattern by default files will be named data_0 parquet or data_0 csv with the flag filename_pattern a pattern with i or uuid can be defined to create specific filenames i will be replaced by an index uuid will be replaced by a 128 bits long uuid -- write a table to a hive partitioned data set of parquet files with an index in the filename copy orders to orders format parquet partition_by year month overwrite_or_ignore filename_pattern orders_ i -- write a table to a hive partitioned data set of parquet files with unique filenames copy orders to orders format parquet partition_by year month overwrite_or_ignore filename_pattern file_ uuid",
			"category": "Partitioning",
			"url": "/docs/data/partitioning/partitioned_writes",
			"blurb": "Examples -- write a table to a hive partitioned data set of parquet files COPY orders TO 'orders' (FORMAT PARQUET,..."
		},
		{
			"title": "Pattern Matching",
			"text": "pattern matching there are four separate approaches to pattern matching provided by duckdb the traditional sql like operator the more recent similar to operator added in sql 1999 a glob operator and posix-style regular expressions like the like expression returns true if the string matches the supplied pattern as expected the not like expression returns false if like returns true and vice versa an equivalent expression is not string like pattern if pattern does not contain percent signs or underscores then the pattern only represents the string itself in that case like acts like the equals operator an underscore _ in pattern stands for matches any single character a percent sign matches any sequence of zero or more characters like pattern matching always covers the entire string therefore if it s desired to match a sequence anywhere within a string the pattern must start and end with a percent sign some examples abc like abc -- true abc like a -- true abc like _b_ -- true abc like c -- false abc like c -- false abc like c -- true abc not like c -- false the keyword ilike can be used instead of like to make the match case-insensitive according to the active locale abc ilike c -- true abc not ilike c -- false to search within a string for a character that is a wildcard or _ the pattern must use an escape clause and an escape character to indicate the wildcard should be treated as a literal character instead of a wildcard see an example below additionally the function like_escape has the same functionality as a like expression with an escape clause but using function syntax see the text functions docs for details --search for strings with a then a literal percent sign then c a c like a c escape -- true azc like a c escape -- false --case insensitive ilike with escape a c ilike a c escape --true there are also alternative characters that can be used as keywords in place of like expressions these enhance postgres compatibility like-style postgres-style --- --- like not like ilike not ilike similar to the similar to operator returns true or false depending on whether its pattern matches the given string it is similar to like except that it interprets the pattern using a regular expression like like the similar to operator succeeds only if its pattern matches the entire string this is unlike common regular expression behavior where the pattern can match any part of the string a regular expression is a character sequence that is an abbreviated definition of a set of strings a regular set a string is said to match a regular expression if it is a member of the regular set described by the regular expression as with like pattern characters match string characters exactly unless they are special characters in the regular expression language but regular expressions use different special characters than like does some examples abc similar to abc -- true abc similar to a -- false abc similar to b d -- true abc similar to b c -- false abc not similar to abc -- false there are also alternative characters that can be used as keywords in place of similar to expressions these follow posix syntax similar to-style posix-style --- --- similar to not similar to glob the glob operator returns true or false if the string matches the glob pattern the glob operator is most commonly used when searching for filenames that follow a specific pattern for example a specific file extension use the question mark wildcard to match any single character and use the asterisk to match zero or more characters in addition use bracket syntax to match any single character contained within the brackets or within the character range specified by the brackets an exclamation mark may be used inside the first bracket to search for a character that is not contained within the brackets to learn more visit the glob programming wikipedia page some examples best txt glob txt -- true best txt glob txt -- true best txt glob txt -- false best txt glob abc est txt -- true best txt glob a-z est txt -- true -- the bracket syntax is case sensitive best txt glob a-z est txt -- false best txt glob a-za-z est txt -- true -- the applies to all characters within the brackets best txt glob a-za-z est txt -- false -- to negate a glob operator negate the entire expression -- not glob is not valid syntax not best txt glob txt -- false three tildes may also be used in place of the glob keyword glob-style symbolic-style --- --- glob glob function to find filenames the glob pattern matching syntax can also be used to search for filenames using the glob table function it accepts one parameter the path to search which may include glob patterns -- search the current directory for all files select from glob file --------------- duckdb exe test csv test json test parquet test2 csv test2 parquet todos json regular expressions function description example result --- --- --- --- regexp_full_match string regex returns true if the entire string matches the regex regexp_full_match anabanana an false regexp_matches string pattern returns true if string contains the regexp pattern false otherwise regexp_matches anabanana an true regexp_replace string pattern replacement if string contains the regexp pattern replaces the matching part with replacement select regexp_replace hello lo - he-lo regexp_split_to_array string regex alias of string_split_regex splits the string along the regex regexp_split_to_array hello world 42 hello world 42 regexp_extract string pattern idx if string contains the regexp pattern returns the capturing group specified by optional parameter idx regexp_extract hello_world a-z _ 1 hello regexp_extract string pattern name_list if string contains the regexp pattern returns the capturing groups as a struct with corresponding names from name_list regexp_extract 2023-04-15 d - d - d y m d y 2023 m 04 d 15 regexp_extract_all string regex group 0 split the string along the regex and extract all occurrences of group regexp_extract_all hello_world a-z _ 1 hello world the regexp_matches function is similar to the similar to operator however it does not require the entire string to match instead regexp_matches returns true if the string merely contains the pattern unless the special tokens and are used to anchor the regular expression to the start and end of the string below are some examples regexp_matches abc abc -- true regexp_matches abc abc -- true regexp_matches abc a -- true regexp_matches abc a -- false regexp_matches abc b d -- true regexp_matches abc b c -- true regexp_matches abc b c -- false regexp_matches abc i a -- true the regexp_matches function also supports the following options option description --- --- c case-sensitive matching i case-insensitive matching l match literals instead of regular expression tokens m n p newline sensitive matching s non-newline sensitive matching g global replace only available for regexp_replace regexp_matches abcd abc c -- false regexp_matches abcd abc i -- true regexp_matches ab cd l -- true regexp_matches hello nworld hello world p -- false regexp_matches hello nworld hello world s -- true the regexp_matches operator will be optimized to the like operator when possible to achieve the best results the s option should be passed by default the re2 library doesn t match to newline original optimized equivalent --- --- regexp_matches hello world hello s prefix hello world hello regexp_matches hello world world s suffix hello world world regexp_matches hello world hello world s like hello_world regexp_matches hello world he rld s like he rld the regexp_replace function can be used to replace the part of a string that matches the regexp pattern with a replacement string the notation d where d is a number indicating the group can be used to refer to groups captured in the regular expression in the replacement string below are some examples regexp_replace abc b c x -- axc regexp_replace abc b c 1 1 1 1 -- abbbbc regexp_replace abc c 1e -- abe regexp_replace abc a b 2 1 -- bac the regexp_extract function is used to extract a part of a string that matches the regexp pattern a specific capturing group within the pattern can be extracted using the idx parameter if idx is not specified it defaults to 0 extracting the first match with the whole pattern regexp_extract abc b -- abc regexp_extract abc b 0 -- abc regexp_extract abc b 1 -- empty regexp_extract abc a-z b 1 -- a regexp_extract abc a-z b 2 -- b if ids is a list of strings then regexp_extract will return the corresponding capture groups as fields of a struct regexp_extract 2023-04-15 d - d - d y m d -- y 2023 m 04 d 15 regexp_extract 2023-04-15 07 59 56 d - d - d d d d y m d -- y 2023 m 04 d 15 regexp_extract duckdb_0_7_1 w _ d _ d tool major minor fix -- error if the number of column names is less than the number of capture groups then only the first groups are returned if the number of column names is greater then an error is generated duckdb uses re2 as its regex engine for more information see the re2 docs",
			"category": "Functions",
			"url": "/docs/sql/functions/patternmatching",
			"blurb": "Pattern Matching There are four separate approaches to pattern matching provided by DuckDB: the traditional SQL LIKE..."
		},
		{
			"title": "Pivot Statement",
			"text": "the pivot statement allows distinct values within a column to be separated into their own columns the values within those new columns are calculated using an aggregate function on the subset of rows that match each distinct value duckdb implements both the sql standard pivot syntax and a simplified pivot syntax that automatically detects the columns to create while pivoting pivot_wider may also be used in place of the pivot keyword simplified pivot syntax the full syntax diagram is below but the simplified pivot syntax can be summarized using spreadsheet pivot table naming conventions as pivot dataset on column s using value s group by row s the on using and group by clauses are each optional but they may not all be omitted example data all examples use the dataset produced by the queries below create table cities country varchar name varchar year int population int insert into cities values nl amsterdam 2000 1005 insert into cities values nl amsterdam 2010 1065 insert into cities values nl amsterdam 2020 1158 insert into cities values us seattle 2000 564 insert into cities values us seattle 2010 608 insert into cities values us seattle 2020 738 insert into cities values us new york city 2000 8015 insert into cities values us new york city 2010 8175 insert into cities values us new york city 2020 8772 from cities country name year population --------- --------------- ------ ------------ nl amsterdam 2000 1005 nl amsterdam 2010 1065 nl amsterdam 2020 1158 us seattle 2000 564 us seattle 2010 608 us seattle 2020 738 us new york city 2000 8015 us new york city 2010 8175 us new york city 2020 8772 pivot on and using use the pivot statement below to create a separate column for each year and calculate the total population in each the on clause specifies which column s to split into separate columns it is equivalent to the columns parameter in a spreadsheet pivot table the using clause determines how to aggregate the values that are split into separate columns this is equivalent to the values parameter in a spreadsheet pivot table if the using clause is not included it defaults to count pivot cities on year using sum population country name 2000 2010 2020 --------- --------------- ------ ------ ------ nl amsterdam 1005 1065 1158 us seattle 564 608 738 us new york city 8015 8175 8772 in the above example the sum aggregate is always operating on a single value if we only want to change the orientation of how the data is displayed without aggregating use the first aggregate function in this example we are pivoting numeric values but the first function works very well for pivoting out a text column this is something that is difficult to do in an spreadsheet pivot table but easy in duckdb this query produces a result that is identical to the one above pivot cities on year using first population pivot on using and group by by default the pivot statement retains all columns not specified in the on or using clauses to include only certain columns and further aggregate specify columns in the group by clause this is equivalent to the rows parameter of a spreadsheet pivot table in the below example the name column is no longer included in the output and the data is aggregated up to the country level pivot cities on year using sum population group by country country 2000 2010 2020 --------- ------ ------ ------ nl 1005 1065 1158 us 8579 8783 9510 in filter for on clause to only create a separate column for specific values within a column in the on clause use an optional in expression let s say for example that we wanted to forget about the year 2020 for no particular reason pivot cities on year in 2000 2010 using sum population group by country country 2000 2010 --------- ------ ------ nl 1005 1065 us 8579 8783 multiple expressions per clause multiple columns can be specified in the on and group by clauses and multiple aggregate expressions can be included in the using clause multiple on columns and on expressions multiple columns can be pivoted out into their own columns duckdb will find the distinct values in each on clause column and create one new column for all combinations of those values a cartesian product in the below example all combinations of unique countries and unique cities receive their own column some combinations may not be present in the underlying data so those columns are populated with null values pivot cities on country name using sum population year nl_amsterdam nl_new york city nl_seattle us_amsterdam us_new york city us_seattle ------ -------------- ------------------ ------------ -------------- ------------------ ------------ 2000 1005 null null null 8015 564 2010 1065 null null null 8175 608 2020 1158 null null null 8772 738 to pivot only the combinations of values that are present in the underlying data use an expression in the on clause multiple expressions and or columns may be provided here country and name are concatenated together and the resulting concatenations each receive their own column any arbitrary non-aggregating expression may be used in this case concatenating with an underscore is used to imitate the naming convention the pivot clause uses when multiple on columns are provided like in the prior example pivot cities on country _ name using sum population year nl_amsterdam us_new york city us_seattle ------ -------------- ------------------ ------------ 2000 1005 8015 564 2010 1065 8175 608 2020 1158 8772 738 multiple using expressions an alias may also be included for each expression in the using clause it will be appended to the generated column names after an underscore _ this makes the column naming convention much cleaner when multiple expressions are included in the using clause in this example both the sum and max of the population column are calculated for each year and are split into separate columns pivot cities on year using sum population as total max population as max group by country country 2000_total 2000_max 2010_total 2010_max 2020_total 2020_max --------- ------------ ---------- ------------ ---------- ------------ ---------- nl 1005 1005 1065 1065 1158 1158 us 8579 8015 8783 8175 9510 8772 multiple group by columns multiple group by columns may also be provided note that column names must be used rather than column positions 1 2 etc and that expressions are not supported in the group by clause pivot cities on year using sum population group by country name country name 2000 2010 2020 --------- --------------- ------ ------ ------ nl amsterdam 1005 1065 1158 us seattle 564 608 738 us new york city 8015 8175 8772 using pivot within a select statement the pivot statement may be included within a select statement as a cte a common table expression or with clause or a subquery this allows for a pivot to be used alongside other sql logic as well as for multiple pivot s to be used in one query no select is needed within the cte the pivot keyword can be thought of as taking its place with pivot_alias as pivot cities on year using sum population group by country select from pivot_alias a pivot may be used in a subquery and must be wrapped in parentheses note that this behavior is different than the sql standard pivot as illustrated in subsequent examples select from pivot cities on year using sum population group by country pivot_alias multiple pivots each pivot can be treated as if it were a select node so they can be joined together or manipulated in other ways for example if two pivot statements share the same group by expression they can be joined together using the columns in the group by clause into a wider pivot from pivot cities on year using sum population group by country year_pivot join pivot cities on name using sum population group by country name_pivot using country country 2000 2010 2020 amsterdam new york city seattle --------- ------ ------ ------ ----------- --------------- --------- nl 1005 1065 1158 3228 null null us 8579 8783 9510 null 24962 1910 internals pivoting is implemented as a combination of sql query re-writing and a dedicated physicalpivot operator for higher performance each pivot is implemented as set of aggregations into lists and then the dedicated physicalpivot operator converts those lists into column names and values additional pre-processing steps are required if the columns to be created when pivoting are detected dynamically which occurs when the in clause is not in use duckdb like most sql engines requires that all column names and types be known at the start of a query in order to automatically detect the columns that should be created as a result of a pivot statement it must be translated into multiple queries enum types are used to find the distinct values that should become columns each enum is then injected into one of the pivot statement s in clauses after the in clauses have been populated with enum s the query is re-written again into a set of aggregations into lists for example pivot cities on year using sum population is initially translated into create temporary type __pivot_enum_0_0 as enum select distinct year varchar from cities order by year pivot cities on year in __pivot_enum_0_0 using sum population and finally translated into select country name list year list population_sum from select country name year sum population as population_sum from cities group by all group by all this produces the result country name list year list population_sum --------- --------------- -------------------- ---------------------- nl amsterdam 2000 2010 2020 1005 1065 1158 us seattle 2000 2010 2020 564 608 738 us new york city 2000 2010 2020 8015 8175 8772 the physicalpivot operator converts those lists into column names and values to return this result country name 2000 2010 2020 --------- --------------- ------ ------ ------ nl amsterdam 1005 1065 1158 us seattle 564 608 738 us new york city 8015 8175 8772 simplified pivot full syntax diagram below is the full syntax diagram of the pivot statement sql standard pivot syntax the full syntax diagram is below but the sql standard pivot syntax can be summarized as from dataset pivot values s for column_1 in in_list column_2 in in_list group by rows s unlike the simplified syntax the in clause must be specified for each column to be pivoted if you are interested in dynamic pivoting the simplified syntax is recommended note that no commas separate the expressions in the for clause but that value and group by expressions must be comma-separated examples this example uses a single value expression a single column expression and a single row expression from cities pivot sum population for year in 2000 2010 2020 group by country country 2000 2010 2020 --------- ------ ------ ------ nl 1005 1065 1158 us 8579 8783 9510 this example is somewhat contrived but serves as an example of using multiple value expressions and multiple columns in the for clause from cities pivot sum population as total count population as count for year in 2000 2010 2020 country in nl us name 2000_nl_total 2000_nl_count 2000_us_total 2000_us_count 2010_nl_total 2010_nl_count 2010_us_total 2010_us_count 2020_nl_total 2020_nl_count 2020_us_total 2020_us_count --------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- amsterdam 1005 1 null 0 1065 1 null 0 1158 1 null 0 seattle null 0 564 1 null 0 608 1 null 0 738 1 new york city null 0 8015 1 null 0 8175 1 null 0 8772 1 sql standard pivot full syntax diagram below is the full syntax diagram of the sql standard version of the pivot statement",
			"category": "Statements",
			"url": "/docs/sql/statements/pivot",
			"blurb": "The PIVOT statement allows values within a column to be separated into their own columns."
		},
		{
			"title": "Postgres Import",
			"text": "to run a query directly on a running postgres database the postgres extension is required this can be installed use the install sql command this only needs to be run once install postgres to load the postgres extension for usage use the load sql command load postgres after the postgres extension is installed tables can be queried from postgres using the postgres_scan function -- scan the table mytable from the schema public using the empty default connection string select from postgres_scan public mytable the first parameter to the postgres_scan function is the postgres connection string alternatively the entire file can be attached using the postgres_attach command this creates views over all of the tables in the postgres database that allow you to query the tables using regular sql syntax -- attach the postgres database using the given connection string call postgres_attach -- the table tbl_name can now be queried as if it is a regular table select from tbl_name for more information see the postgres scanner documentation",
			"category": "Import",
			"url": "/docs/guides/import/query_postgres",
			"blurb": "To run a query directly on a running Postgres database, the postgres extension is required. This can be installed use..."
		},
		{
			"title": "Postgres Scanner",
			"text": "the postgres extension allows duckdb to directly read data from a running postgres instance the data can be queried directly from the underlying postgres tables or read into duckdb tables loading the extension in order to use the postgres extension it must first be installed and loaded this can be done using the following commands install postgres load postgres usage to make a postgres database accessible to duckdb use the postgres_attach command -- load all data from public schema of the postgres instance running on localhost into the schema main call postgres_attach -- attach the database with the given schema loading tables from the source schema public into the target schema abc call postgres_attach dbname postgres user postgres host 127 0 0 1 source_schema public sink_schema abc postgres_attach takes a single required string parameter which is the libpq connection string for example you can pass dbname postgresscanner to select a different database name in the simplest case the parameter is just there are three additional named parameters source_schema the name of a non-standard schema name in postgres to get tables from default is public sink_schema the schema name in duckdb to create views default is main overwrite whether we should overwrite existing views in the target schema default is false filter_pushdown whether filter predicates that duckdb derives from the query should be forwarded to postgres defaults to false the tables in the database are registered as views in duckdb you can list them as follows pragma show_tables then you can query those views normally using sql querying individual tables if you prefer to not attach all tables but just query a single table that is possible using the postgres_scan function e g select from postgres_scan public mytable postgres_scan takes three string parameters the libpq connection string see above a postgres schema name and a table name the schema name is often public to use filter_pushdown use the postgres_scan_pushdown function extra information see the repo for the source code of the extension or the official announcement for implementation details and background",
			"category": "Extensions",
			"url": "/docs/extensions/postgres_scanner",
			"blurb": "The postgres extension allows DuckDB to directly read data from a running Postgres instance. The data can be queried..."
		},
		{
			"title": "Pragmas",
			"text": "the pragma statement is an sql extension adopted by duckdb from sqlite pragma statements can be issued in a similar manner to regular sql statements pragma commands may alter the internal state of the database engine and can influence the subsequent execution or behavior of the engine list of supported pragma statements below is a list of supported pragma statements database_list show_tables show_tables_expanded table_info show functions -- list all databases usually one pragma database_list -- list all tables pragma show_tables -- list all tables with extra information similar to describe pragma show_tables_expanded -- get info for a specific table pragma table_info table_name call pragma_table_info table_name -- also show table structure but slightly different format for compatibility pragma show table_name -- list all functions pragma functions table_info returns information about the columns of the table with name table_name the exact format of the table returned is given below cid integer -- cid of the column name varchar -- name of the column type varchar -- type of the column notnull boolean -- if the column is marked as not null dflt_value varchar -- default value of the column or null if not specified pk boolean -- part of the primary key or not memory_limit threads -- set the memory limit pragma memory_limit 1gb -- set the amount of threads for parallel query execution pragma threads 4 database_size -- get the file and memory size of each database pragma database_size call pragma_database_size database_size returns information about the file and memory size of each database the column types of the returned results are given below database_name varchar -- database name database_size varchar -- total block count times the block size block_size bigint -- database block size total_blocks bigint -- total blocks in the database used_blocks bigint -- used blocks in the database free_blocks bigint -- free blocks in the database wal_size varchar -- write ahead log size memory_usage varchar -- memory used by the database buffer manager memory_limit varchar -- maximum memory allowed for the database collations default_collation -- list all available collations pragma collations -- set the default collation to one of the available ones pragma default_collation nocase default_null_order default_order -- set the ordering for nulls to be either nulls first or nulls last pragma default_null_order nulls last -- set the default result set ordering direction to ascending or descending pragma default_order descending version -- show duckdb version pragma version call pragma_version platform platform returns an identifier for the platform the current duckdb executable has been compiled for this matches the platform_name as described on the extension loading explainer -- show platform of current duckdb executable pragma platform call pragma_platform enable_progress_bar disable_progress_bar enable_profiling disable_profiling profiling_output -- show progress bar when running queries pragma enable_progress_bar -- don t show a progress bar for running queries pragma disable_progress_bar -- enable profiling pragma enable_profiling -- enable profiling in a specified format pragma enable_profiling json query_tree query_tree_optimizer -- disable profiling pragma disable_profiling -- specify a file to save the profiling output to pragma profiling_output path to file json pragma profile_output path to file json enable the gathering and printing of profiling information after the execution of a query optionally the format of the resulting profiling information can be specified as either json query_tree or query_tree_optimizer the default format is query_tree which prints the physical operator tree together with the timings and cardinalities of each operator in the tree to the screen below is an example output of the profiling information for the simple query select 42 query profiling information select 42 total time 0 0001s projection 42 1 0 00s dummy_scan 1 0 00s the printing of profiling information can be disabled again using disable_profiling by default profiling information is printed to the console however if you prefer to write the profiling information to a file the pragma profiling_output can be used to write to a specified file note that the file contents will be overwritten for every new query that is issued hence the file will only contain the profiling information of the last query that is run disable_optimizer enable_optimizer -- disables the query optimizer pragma disable_optimizer -- enables the query optimizer pragma enable_optimizer log_query_path explain_output enable_verification disable_verification verify_parallelism disable_verify_parallelism -- set a path for query logging pragma log_query_path tmp duckdb_log -- disable query logging again pragma log_query_path -- either show all or only optimized plans in the explain output pragma explain_output optimized -- enable query verification for development pragma enable_verification -- disable query verification for development pragma disable_verification -- enable force parallel query processing for development pragma verify_parallelism -- disable force parallel query processing for development pragma disable_verify_parallelism -- force index joins where applicable pragma force_index_join these are pragma s mostly used for development and internal testing create_fts_index drop_fts_index only available when the fts extension is built documented here verify_external disable_verify_external -- enable verification of external operators pragma verify_external -- disable verification of external operators pragma disable_verify_external verify_serializer disable_verify_serializer -- enable verification of round-trip capabilities for supported logical plans pragma verify_serializer -- disable verification of round-trip capabilities pragma disable_verify_serializer enable_object_cache disable_object_cache -- enable caching of objects for e g parquet metadata pragma enable_object_cache -- disable caching of objects pragma disable_object_cache force_checkpoint -- when checkpoint is called when no changes are made force a checkpoint regardless pragma force_checkpoint enable_print_progress_bar disable_print_progress_bar -- enable printing of the progress bar if it s enabled pragma enable_print_progress_bar -- disable printing of the progress bar pragma disable_print_progress_bar enable_checkpoint_on_shutdown disable_checkpoint_on_shutdown -- run a checkpoint on successful shutdown and delete the wal to leave only a single database file behind pragma enable_checkpoint_on_shutdown -- don t run a checkpoint on shutdown pragma disable_checkpoint_on_shutdown temp directory for spilling data to disk -- defaults to tmp pragma temp_directory path to temp tmp storage_info pragma storage_info table_name call pragma_storage_info table_name returns the following per column in the given table name type description ---------------- ----------- ------------------------------------------------------- row_group_id bigint column_name varchar column_id bigint column_path varchar segment_id bigint segment_type varchar start bigint the start row id of this chunk count bigint the amount of entries in this storage chunk compression varchar compression type used for this column - see blog post stats varchar has_updates boolean persistent boolean false if temporary table block_id bigint empty unless persistent block_offset bigint empty unless persistent see storage for more information",
			"category": "SQL",
			"url": "/docs/sql/pragmas",
			"blurb": "The PRAGMA statement is an SQL extension adopted by DuckDB from SQLite. PRAGMA statements can be issued in a similar..."
		},
		{
			"title": "Profile Queries",
			"text": "in order to profile a query prepend explain analyze to a query explain analyze select from tbl the query plan will be pretty-printed to the screen using timings for every operator note that the cumulative wall-clock time that is spent on every operator is shown when multiple threads are processing the query in parallel the total processing time of the query may be lower than the sum of all the times spent on the individual operators below is an example of running explain analyze on q1 of the tpc-h benchmark total time 0 0496s explain_analyze 0 0 00s order_by lineitem l_returnflag asc lineitem l_linestatus asc 4 0 00s hash_group_by 0 1 sum 2 sum 3 sum 4 sum 5 avg 6 avg 7 avg 8 count_star 4 0 28s projection l_returnflag l_linestatus l_quantity l_extendedprice 4 4 1 00 l_tax l_quantity l_extendedprice l_discount 5916591 0 02s projection l_returnflag l_linestatus l_quantity l_extendedprice l_extendedprice 1 00 - l_discount l_tax l_discount 5916591 0 02s seq_scan lineitem l_shipdate l_returnflag l_linestatus l_quantity l_extendedprice l_discount l_tax filters l_shipdate 1998 -09-02 and l_shipdate null 5916591 0 08s",
			"category": "Meta",
			"url": "/docs/guides/meta/explain_analyze",
			"blurb": "In order to profile a query, prepend EXPLAIN ANALYZE to a query. EXPLAIN ANALYZE SELECT * FROM tbl; The query plan..."
		},
		{
			"title": "Python API",
			"text": "installation the duckdb python api can be installed using pip pip install duckdb please see the installation page for details it is also possible to install duckdb using conda conda install python-duckdb -c conda-forge basic api usage the most straight-forward manner of running sql queries using duckdb is using the duckdb sql command import duckdb duckdb sql select 42 show this will run queries using an in-memory database that is stored globally inside the python module the result of the query is returned as a relation a relation is a symbolic representation of the query the query is not executed until the result is fetched or requested to be printed to the screen relations can be referenced in subsequent queries by storing them inside variables and using them as tables this way queries can be constructed incrementally import duckdb r1 duckdb sql select 42 as i duckdb sql select i 2 as k from r1 show data input duckdb can ingest data from a wide variety of formats - both on-disk and in-memory see the data ingestion page for more information import duckdb duckdb read_csv example csv read a csv file into a relation duckdb read_parquet example parquet read a parquet file into a relation duckdb read_json example json read a json file into a relation duckdb sql select from example csv directly query a csv file duckdb sql select from example parquet directly query a parquet file duckdb sql select from example json directly query a json file dataframes duckdb can also directly query pandas dataframes polars dataframes and arrow tables import duckdb directly query a pandas dataframe import pandas as pd pandas_df pd dataframe a 42 duckdb sql select from pandas_df directly query a polars dataframe import polars as pl polars_df pl dataframe a 42 duckdb sql select from polars_df directly query a pyarrow table import pyarrow as pa arrow_table pa table from_pydict a 42 duckdb sql select from arrow_table result conversion duckdb supports converting query results efficiently to a variety of formats see the result conversion page for more information import duckdb duckdb sql select 42 fetchall python objects duckdb sql select 42 df pandas dataframe duckdb sql select 42 pl polars dataframe duckdb sql select 42 arrow arrow table duckdb sql select 42 fetchnumpy numpy arrays writing data to disk duckdb supports writing relation objects directly to disk in a variety of formats the copy statement can be used to write data to disk using sql as an alternative import duckdb duckdb sql select 42 write_parquet out parquet write to a parquet file duckdb sql select 42 write_csv out csv write to a csv file duckdb sql copy select 42 to out parquet copy to a parquet file persistent storage by default duckdb operates on an in-memory database that means that any tables that are created are not persisted to disk using the connect method a connection can be made to a persistent database any data written to that connection will be persisted and can be reloaded by re-connecting to the same file import duckdb create a connection to a file called file db con duckdb connect file db create a table and load data into it con sql create table test i integer con sql insert into test values 42 query the table con table test show explicitly close the connection con close note connections also closed implicitly when they go out of scope the connection object and the duckdb module can be used interchangeably - they support the same methods the only difference is that when using the duckdb module a global in-memory database is used note that if you are developing a package designed for others to use using duckdb it is recommend that you create connection objects instead of using the methods on the duckdb module that is because the duckdb module uses a shared global database - which can cause hard to debug issues if used from within multiple different packages",
			"category": "Python",
			"url": "/docs/api/python/overview",
			"blurb": "Installation The DuckDB Python API can be installed using pip : pip install duckdb . Please see the installation page..."
		},
		{
			"title": "Python Client API",
			"text": "span class target id module-duckdb span dl class py data",
			"category": "Reference",
			"url": "/docs/api/python/reference/index",
			"blurb": "<span class=target id=module-duckdb> </span> <dl class=py data>"
		},
		{
			"title": "Python DB API",
			"text": "the standard duckdb python api provides a sql interface compliant with the db-api 2 0 specification described by pep 249 similar to the sqlite python api connection to use the module you must first create a duckdbpyconnection object that represents the database the connection object takes as a parameter the database file to read and write from if the database file does not exist it will be created the file extension may be db duckdb or anything else the special value memory the default can be used to create an in-memory database note that for an in-memory database no data is persisted to disk i e all data is lost when you exit the python process if you would like to connect to an existing database in read-only mode you can set the read_only flag to true read-only mode is required if multiple python processes want to access the same database file at the same time by default we create an in-memory-database that lives inside the duckdb module every method of duckdbpyconnection is also available on the duckdb module this connection is what s used by these methods you can also get a reference to this connection by providing the special value default to connect import duckdb duckdb execute create table tbl as select 42 a con duckdb connect default con sql select from tbl a int32 42 import duckdb to start an in-memory database con duckdb connect database memory to use a database file not shared between processes con duckdb connect database my-db duckdb read_only false to use a database file shared between processes con duckdb connect database my-db duckdb read_only true to explicitly get the default connection con duckdb connect database default if you want to create a second connection to an existing database you can use the cursor method this might be useful for example to allow parallel threads running queries independently a single connection is thread-safe but is locked for the duration of the queries effectively serializing database access in this case connections are closed implicitly when they go out of scope or if they are explicitly closed using close once the last connection to a database instance is closed the database instance is closed as well querying sql queries can be sent to duckdb using the execute method of connections once a query has been executed results can be retrieved using the fetchone and fetchall methods on the connection fetchall will retrieve all results and complete the transaction fetchone will retrieve a single row of results each time that it is invoked until no more results are available the transaction will only close once fetchone is called and there are no more results remaining the return value will be none as an example in the case of a query only returning a single row fetchone should be called once to retrieve the results and a second time to close the transaction below are some short examples create a table con execute create table items item varchar value decimal 10 2 count integer insert two items into the table con execute insert into items values jeans 20 0 1 hammer 42 2 2 retrieve the items again con execute select from items print con fetchall jeans decimal 20 00 1 hammer decimal 42 20 2 retrieve the items one at a time con execute select from items print con fetchone jeans decimal 20 00 1 print con fetchone hammer decimal 42 20 2 print con fetchone this closes the transaction any subsequent calls to fetchone will return none none the description property of the connection object contains the column names as per the standard duckdb also supports prepared statements in the api with the execute and executemany methods the values may be passed as an additional parameter after a query that contains or 1 dollar symbol and a number placeholders using the notation adds the values in the same sequence as passed within the python parameter using the notation allows for values to be reused within the sql statement based on the number and index of the value found within the python parameter here are some examples insert a row using prepared statements con execute insert into items values laptop 2000 1 insert several rows using prepared statements con executemany insert into items values chainsaw 500 10 iphone 300 2 query the database using a prepared statement con execute select item from items where value 400 print con fetchall laptop chainsaw query using notation for prepared statement and reused values con execute select 1 1 2 duck goose print con fetchall duck duck goose named parameters besides the standard unnamed parameters like 1 2 etc it s also possible to supply named parameters like my_parameter when using named parameters you have to provide a dictionary mapping of str to value in the parameters argument an example use import duckdb duckdb execute select my_param other_param also_param my_param 5 other_param duckdb also_param 42 fetchall 5 duckdb 42 do not use executemany to insert large amounts of data into duckdb see the data ingestion page for better options",
			"category": "Python",
			"url": "/docs/api/python/dbapi",
			"blurb": "The standard DuckDB Python API provides a SQL interface compliant with the DB-API 2.0 specification described by PEP..."
		},
		{
			"title": "Python Function API",
			"text": "you can create a duckdb user defined function udf out of a python function so it can be used in sql queries just like regular functions they need to have a name a return type and parameter types example using a python function that calls a third party library import duckdb import duckdb from duckdb typing import from faker import faker def random_name fake faker return fake name duckdb create_function random_name random_name varchar res duckdb sql select random_name fetchall print res gerald ashley creating functions to register a python udf simply use the create_function method from a duckdb connection here is the syntax import duckdb con duckdb connect con create_function name function argument_type_list return_type type null_handling the create_function method requires the following parameters name a string representing the unique name of the udf within the connection catalog function the python function you wish to register as a udf return_type scalar functions return one element per row this parameter specifies the return type of the function parameters scalar functions can operate on one or more columns this parameter takes a list of column types used as input type optional duckdb supports both built-in python types and pyarrow tables by default built-in types are assumed but you can specify type arrow to use pyarrow tables null_handling optional by default null values are automatically handled as null-in null-out users can specify a desired behavior for null values by setting null_handling special exception_handling optional by default when an exception is thrown from the python function it will be re-thrown in python users can disable this behavior and instead return null by set this parameter to return_null side_effects optional by default functions are expected to produce the same result for the same input if the result of a function is impacted by any type of randomness side_effects must be set to true to unregister a udf you can call the remove_function method with the udf name con remove_function name type annotation when the function has type annotation it s often possible to leave out all of the optional parameters using duckdbpytype we can implicitly convert many known types to duckdbs type system for example import duckdb def my_function x int - str return x duckdb create_function my_func my_function duckdb sql select my_func 42 my_func 42 varchar 42 if only the parameter list types can be inferred you ll need to pass in none as argument_type_list null handling by default when functions receive a null value this instantly returns null as part of the default null handling when this is not desired you need to explicitly set this parameter to special import duckdb from duckdb typing import def dont_intercept_null x return 5 duckdb create_function dont_intercept dont_intercept_null bigint bigint res duckdb sql select dont_intercept null fetchall print res none duckdb remove_function dont_intercept duckdb create_function dont_intercept dont_intercept_null bigint bigint null_handling special res duckdb sql select dont_intercept null fetchall print res 5 exception handling by default when an exception is thrown from the python function we ll forward re-throw the exception if you want to disable this behavior and instead return null you ll need to set this parameter to return_null import duckdb from duckdb typing import def will_throw raise valueerror error duckdb create_function throws will_throw bigint try res duckdb sql select throws fetchall except duckdb invalidinputexception as e print e duckdb create_function doesnt_throw will_throw bigint exception_handling return_null res duckdb sql select doesnt_throw fetchall print res none side effects by default duckdb will assume the created function is a pure function meaning it will produce the same output when given the same input if your function does not follow that rule for example when your function makes use of randomness then you will need to mark this function as having side_effects for example this function will produce a new count for every invocation def count - int old count counter count counter 1 return old count counter 0 if we create this function without marking it as having side effects the result will be the following con duckdb connect con create_function my_counter count side_effects false res con sql select my_counter from range 10 fetchall 0 0 0 0 0 0 0 0 0 0 which is obviously not the desired result when we add side_effects true the result is as we would expect con create_function my_counter count side_effects true res con sql select my_counter from range 10 fetchall 0 1 2 3 4 5 6 7 8 9 python function types currently two function types are supported native default and arrow arrow if the function is expected to receive arrow arrays set the type parameter to arrow this will let the system know to provide arrow arrays of up to standard_vector_size tuples to the function and also expect an array of the same amount of tuples to be returned from the function native when the function type is set to native the function will be provided with a single tuple at a time and expect only a single value to be returned this can be useful to interact with python libraries that don t operate on arrow such as faker import duckdb from duckdb typing import from faker import faker def random_date fake faker return fake date_between duckdb create_function random_date random_date date res duckdb sql select random_date fetchall print res datetime date 2019 5 15",
			"category": "Python",
			"url": "/docs/api/python/function",
			"blurb": "You can create a DuckDB User Defined Function (UDF) out of a python function so it can be used in SQL queries. Just..."
		},
		{
			"title": "QUALIFY Clause",
			"text": "the qualify clause is used to filter the results of window functions this filtering of results is similar to how a having clause filters the results of aggregate functions applied based on the group by clause the qualify clause avoids the need for a subquery or with clause to perform this filtering much like having avoids a subquery an example using a with clause instead of qualify is included below the qualify examples note that this is filtering based on window functions not necessarily based on the window clause the window clause is optional and can be used to simplify the creation of multiple window function expressions the position of where to specify a qualify clause is following the window clause in a select statement window does not need to be specified and before the order by examples each of the following examples produce the same output located below -- filter based on a window function defined in the qualify clause select schema_name function_name -- in this example the function_rank column in the select clause is for reference row_number over partition by schema_name order by function_name as function_rank from duckdb_functions qualify row_number over partition by schema_name order by function_name 3 -- filter based on a window function defined in the select clause select schema_name function_name row_number over partition by schema_name order by function_name as function_rank from duckdb_functions qualify function_rank 3 -- filter based on a window function defined in the qualify clause but using the window clause select schema_name function_name -- in this example the function_rank column in the select clause is for reference row_number over my_window as function_rank from duckdb_functions window my_window as partition by schema_name order by function_name qualify row_number over my_window 3 -- filter based on a window function defined in the select clause but using the window clause select schema_name function_name row_number over my_window as function_rank from duckdb_functions window my_window as partition by schema_name order by function_name qualify function_rank 3 -- equivalent query based on a with clause without qualify clause with ranked_functions as select schema_name function_name row_number over partition by schema_name order by function_name as function_rank from duckdb_functions select from ranked_functions where function_rank 3 schema_name function_name function_rank --- --- --- main __postfix 1 main 2 pg_catalog col_description 1 pg_catalog format_pg_type 2 syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/qualify",
			"blurb": "The QUALIFY clause is used to filter the results of WINDOW functions."
		},
		{
			"title": "Query",
			"text": "duckdb-wasm provides functions for querying data queries are run sequentially first a connection need to be created by calling connect then queries can be run by calling query or send query execution create a new connection const conn await db connect either materialize the query result await conn query v arrow int select from generate_series 1 100 t v or fetch the result chunks lazily for await const batch of await conn send v arrow int select from generate_series 1 100 t v close the connection to release memory await conn close prepared statements create a new connection const conn await db connect prepare query const stmt await conn prepare select v from generate_series 0 10000 as t v and run the query with materialized results await stmt query 234 or result chunks for await const batch of await stmt send 234 close the statement to release memory await stmt close closing the connection will release statements as well await conn close arrow table to json create a new connection const conn await db connect query const arrowresult await conn query v arrow int select from generate_series 1 100 t v convert arrow table to json const result arrowresult toarray map row row tojson close the connection to release memory await conn close export parquet create a new connection const conn await db connect export parquet conn send copy select from tbl to result-snappy parquet format parquet const parquet_buffer await this _db copyfiletobuffer result-snappy parquet generate a download link const link url createobjecturl new blob parquet_buffer close the connection to release memory await conn close",
			"category": "Wasm",
			"url": "/docs/api/wasm/query",
			"blurb": "DuckDB-Wasm provides functions for querying data. Queries are run sequentially. First, a connection need to be..."
		},
		{
			"title": "Querying Parquet Metadata",
			"text": "parquet metadata the parquet_metadata function can be used to query the metadata contained within a parquet file which reveals various internal details of the parquet file such as the statistics of the different columns this can be useful for figuring out what kind of skipping is possible in parquet files or even to obtain a quick overview of what the different columns contain select from parquet_metadata test parquet below is a table of the columns returned by parquet_metadata field type ------------------------- --------- file_name varchar row_group_id bigint row_group_num_rows bigint row_group_num_columns bigint row_group_bytes bigint column_id bigint file_offset bigint num_values bigint path_in_schema varchar type varchar stats_min varchar stats_max varchar stats_null_count bigint stats_distinct_count bigint stats_min_value varchar stats_max_value varchar compression varchar encodings varchar index_page_offset bigint dictionary_page_offset bigint data_page_offset bigint total_compressed_size bigint total_uncompressed_size bigint parquet schema the parquet_schema function can be used to query the internal schema contained within a parquet file note that this is the schema as it is contained within the metadata of the parquet file if you want to figure out the column names and types contained within a parquet file it is easier to use describe -- fetch the column names and column types describe select from test parquet -- fetch the internal schema of a parquet file select from parquet_schema test parquet below is a table of the columns returned by parquet_schema field type ----------------- --------- file_name varchar name varchar type varchar type_length varchar repetition_type varchar num_children bigint converted_type varchar scale bigint precision bigint field_id bigint logical_type varchar",
			"category": "Parquet",
			"url": "/docs/data/parquet/metadata",
			"blurb": "Parquet Metadata The parquet_metadata function can be used to query the metadata contained within a Parquet file,..."
		},
		{
			"title": "R API",
			"text": "installation the duckdb r api can be installed using install packages please see the installation page for details basic api usage the standard duckdb r api implements the dbi interface for r if you are not familiar with dbi yet see here for an introduction startup shutdown to use duckdb you must first create a connection object that represents the database the connection object takes as parameter the database file to read and write from if the database file does not exist it will be created the file extension may be db duckdb or anything else the special value memory the default can be used to create an in-memory database note that for an in-memory database no data is persisted to disk i e all data is lost when you exit the r process if you would like to connect to an existing database in read-only mode set the read_only flag to true read-only mode is required if multiple r processes want to access the same database file at the same time library dbi to start an in-memory database con - dbconnect duckdb duckdb dbdir memory to use a database file not shared between processes con - dbconnect duckdb duckdb dbdir my-db duckdb read_only false to use a database file shared between processes con - dbconnect duckdb duckdb dbdir my-db duckdb read_only true connections are closed implicitly when they go out of scope or if they are explicitly closed using dbdisconnect to shut down the database instance associated with the connection use dbdisconnect con shutdown true querying duckdb supports the standard dbi methods to send queries and retrieve result sets dbexecute is meant for queries where no results are expected like create table or update etc and dbgetquery is meant to be used for queries that produce results e g select below an example create a table dbexecute con create table items item varchar value decimal 10 2 count integer insert two items into the table dbexecute con insert into items values jeans 20 0 1 hammer 42 2 2 retrieve the items again res - dbgetquery con select from items print res item value count 1 jeans 20 0 1 2 hammer 42 2 2 duckdb also supports prepared statements in the r api with the dbexecute and dbgetquery methods here is an example prepared statement parameters are given as a list dbexecute con insert into items values list laptop 2000 1 if you want to reuse a prepared statement multiple times use dbsendstatement and dbbind stmt - dbsendstatement con insert into items values dbbind stmt list iphone 300 2 dbbind stmt list android 3 5 1 dbclearresult stmt query the database using a prepared statement res - dbgetquery con select item from items where value list 400 print res item 1 laptop do not use prepared statements to insert large amounts of data into duckdb see below for better options efficient transfer to write a r data frame into duckdb use the standard dbi function dbwritetable this creates a table in duckdb and populates it with the data frame contents for example dbwritetable con iris_table iris res - dbgetquery con select from iris_table limit 1 print res sepal length sepal width petal length petal width species 1 5 1 3 5 1 4 0 2 setosa it is also possible to register a r data frame as a virtual table comparable to a sql view this does not actually transfer data into duckdb yet below is an example duckdb duckdb_register con iris_view iris res - dbgetquery con select from iris_view limit 1 print res sepal length sepal width petal length petal width species 1 5 1 3 5 1 4 0 2 setosa duckdb keeps a reference to the r data frame after registration this prevents the data frame from being garbage-collected the reference is cleared when the connection is closed but can also be cleared manually using the duckdb duckdb_unregister method also refer to the data import documentation for more options of efficiently importing data dbplyr duckdb also plays well with the dbplyr dplyr packages for programmatic query construction from r here is an example library dbi library dplyr con - dbconnect duckdb duckdb duckdb duckdb_register con flights nycflights13 flights tbl con flights group_by dest summarise delay mean dep_time na rm true collect when using dbplyr csv and parquet files can be read using the dplyr tbl function write csv mtcars mtcars csv tbl con mtcars csv group_by cyl summarise across disp wt fns mean collect dbexecute con copy flights to dataset format parquet partition_by year month tbl con read_parquet dataset parquet hive_partitioning 1 filter month 3 summarise delay mean dep_time na rm true collect",
			"category": "Api",
			"url": "/docs/api/r",
			"blurb": "Installation The DuckDB R API can be installed using install.packages . Please see the installation page for details...."
		},
		{
			"title": "Reading Multiple Files",
			"text": "duckdb can read multiple files of different types csv parquet json files at the same time using either the glob syntax or by providing a list of files to read see the combining schemas page for tips on reading files with different schemas csv -- read all files with a name ending in csv in the folder dir select from dir csv -- read all files with a name ending in csv two directories deep select from csv -- read all files with a name ending in csv at any depth in the folder dir select from dir csv -- read the csv files flights1 csv and flights2 csv select from read_csv_auto flights1 csv flights2 csv -- read the csv files flights1 csv and flights2 csv unifying schemas by name and outputting a filename column select from read_csv_auto flights1 csv flights2 csv union_by_name true filename true parquet -- read all files that match the glob pattern select from test parquet -- read 3 parquet files and treat them as a single table select from read_parquet file1 parquet file2 parquet file3 parquet -- read all parquet files from 2 specific folders select from read_parquet folder1 parquet folder2 parquet -- read all parquet files that match the glob pattern at any depth select from read_parquet dir parquet multi-file reads and globs duckdb can also read a series of parquet files and treat them as if they were a single table note that this only works if the parquet files have the same schema you can specify which parquet files you want to read using a list parameter glob pattern matching syntax or a combination of both list parameter the read_parquet function can accept a list of filenames as the input parameter -- read 3 parquet files and treat them as a single table select from read_parquet file1 parquet file2 parquet file3 parquet glob syntax any file name input to the read_parquet function can either be an exact filename or use a glob syntax to read multiple files that match a pattern wildcard description ------------ ----------------------------------------------------------- matches any number of any characters including none matches any number of subdirectories including none matches any single character abc matches one character given in the bracket a-z matches one character from the range given in the bracket note that the wildcard in globs is not supported for reads over s3 due to http encoding issues here is an example that reads all the files that end with parquet located in the test folder -- read all files that match the glob pattern select from read_parquet test parquet list of globs the glob syntax and the list input parameter can be combined to scan files that meet one of multiple patterns -- read all parquet files from 2 specific folders select from read_parquet folder1 parquet folder2 parquet duckdb can read multiple csv files at the same time using either the glob syntax or by providing a list of files to read filename the filename argument can be used to add an extra filename column to the result that indicates which row came from which file for example select from read_csv_auto flights1 csv flights2 csv union_by_name true filename true flightdate origincityname destcityname uniquecarrier filename ------------ ---------------- ----------------- --------------- -------------- 1988-01-01 new york ny los angeles ca null flights1 csv 1988-01-02 new york ny los angeles ca null flights1 csv 1988-01-03 new york ny los angeles ca aa flights2 csv glob function to find filenames the glob pattern matching syntax can also be used to search for filenames using the glob table function it accepts one parameter the path to search which may include glob patterns -- search the current directory for all files select from glob file --------------- duckdb exe test csv test json test parquet test2 csv test2 parquet todos json",
			"category": "Multiple Files",
			"url": "/docs/data/multiple_files/overview",
			"blurb": "DuckDB can read multiple files of different types (CSV, Parquet, JSON files) at the same time using either the glob..."
		},
		{
			"title": "Relational API",
			"text": "the relational api is an alternative api that can be used to incrementally construct queries the api is centered around duckdbpyrelation nodes the relations can be seen as symbolic representations of sql queries they do not hold any data - and nothing is executed - until a method that triggers execution is called constructing relations relations can be created from sql queries using the duckdb sql method alternatively they can be created from the various data ingestion methods read_parquet read_csv read_json for example here we create a relation from a sql query import duckdb rel duckdb sql select from range 10000000000 tbl id rel show id int64 0 1 2 3 4 5 6 7 8 9 9990 9991 9992 9993 9994 9995 9996 9997 9998 9999 rows 9999 rows 20 shown note how we are constructing a relation that computes an immense amount of data 10b rows or 74gb of data the relation is constructed instantly - and we can even print the relation instantly when printing a relation using show or displaying it in the terminal the first 10k rows are fetched if there are more than 10k rows the output window will show 9999 rows as the amount of rows in the relation is unknown data ingestion outside of sql queries the following methods are provided to construct relation objects from external data from_arrow from_df read_csv read_json read_parquet sql queries relation objects can be queried through sql through so-called replacement scans if you have a relation object stored in a variable you can refer to that variable as if it was a sql table in the from clause this allows you to incrementally build queries using relation objects import duckdb rel duckdb sql select from range 1000000 tbl id duckdb sql select sum id from rel show sum id int128 499999500000 operations there are a number of operations that can be performed on relations these are all short-hand for running the sql queries - and will return relations again themselves aggregate expr groups apply an optionally grouped aggregate over the relation the system will automatically group by any columns that are not aggregates import duckdb rel duckdb sql select from range 1000000 tbl id rel aggregate id 2 as g sum id min id max id g sum id min id max id int64 int128 int64 int64 0 249999500000 0 999998 1 250000000000 1 999999 except_ rel select all rows in the first relation that do not occur in the second relation the relations must have the same number of columns import duckdb r1 duckdb sql select from range 10 tbl id r2 duckdb sql select from range 5 tbl id r1 except_ r2 show id int64 5 6 7 8 9 filter condition apply the given condition to the relation filtering any rows that do not satisfy the condition import duckdb rel duckdb sql select from range 1000000 tbl id rel filter id 5 limit 3 show id int64 6 7 8 intersect rel select the intersection of two relations - returning all rows that occur in both relations the relations must have the same number of columns import duckdb r1 duckdb sql select from range 10 tbl id r2 duckdb sql select from range 5 tbl id r1 intersect r2 show id int64 0 1 2 3 4 join rel condition type inner combine two relations joining them based on the provided condition import duckdb r1 duckdb sql select from range 5 tbl id set_alias r1 r2 duckdb sql select from range 10 15 tbl id set_alias r2 r1 join r2 r1 id 10 r2 id show id id int64 int64 0 10 1 11 2 12 3 13 4 14 limit n offset 0 select the first n rows optionally offset by offset import duckdb rel duckdb sql select from range 1000000 tbl id rel limit 3 show id int64 0 1 2 order expr sort the relation by the given set of expressions import duckdb rel duckdb sql select from range 1000000 tbl id rel order id desc limit 3 show id int64 999999 999998 999997 project expr apply the given expression to each row in the relation import duckdb rel duckdb sql select from range 1000000 tbl id rel project id 10 as id_plus_ten limit 3 show id_plus_ten int64 10 11 12 union rel combine two relations returning all rows in r1 followed by all rows in r2 the relations must have the same number of columns import duckdb r1 duckdb sql select from range 5 tbl id r2 duckdb sql select from range 10 15 tbl id r1 union r2 show id int64 0 1 2 3 4 10 11 12 13 14 result output the result of relations can be converted to various types of python structures see the result conversion page for more information the result of relations can also be directly written to files using the below methods write_csv write_parquet",
			"category": "Python",
			"url": "/docs/api/python/relational_api",
			"blurb": "The Relational API is an alternative API that can be used to incrementally construct queries. The API is centered..."
		},
		{
			"title": "Relational API and Pandas",
			"text": "duckdb offers a relational api that can be used to chain together query operations these are lazily evaluated so that duckdb can optimize their execution these operators can act on pandas dataframes duckdb tables or views which can point to any underlying storage format that duckdb can read such as csv or parquet files etc here we show a simple example of reading from a pandas dataframe and returning a dataframe import duckdb import pandas connect to an in-memory database con duckdb connect input_df pandas dataframe from_dict i 1 2 3 4 j one two three four create a duckdb relation from a dataframe rel con from_df input_df chain together relational operators this is a lazy operation so the operations are not yet executed equivalent to select i j i 2 as two_i from input_df order by i desc limit 2 transformed_rel rel filter i 2 project i j i 2 as two_i order i desc limit 2 trigger execution by requesting df of the relation df could have been added to the end of the chain above - it was separated for clarity output_df transformed_rel df relational operators can also be used to group rows aggregate find distinct combinations of values join union and more they are also able to directly insert results into a duckdb table or write to a csv please see these additional examples and the available relational methods on the duckdbpyrelation class",
			"category": "Python",
			"url": "/docs/guides/python/relational_api_pandas",
			"blurb": "DuckDB offers a relational API that can be used to chain together query operations. These are lazily evaluated so..."
		},
		{
			"title": "Result Conversion",
			"text": "duckdb s python client provides multiple additional methods that can be used to efficiently retrieve data numpy fetchnumpy fetches the data as a dictionary of numpy arrays pandas df fetches the data as a pandas dataframe fetchdf is an alias of df fetch_df is an alias of df fetch_df_chunk vector_multiple fetches a portion of the results into a dataframe the number of rows returned in each chunk is the vector size 2048 by default vector_multiple 1 by default apache arrow arrow fetches the data as an arrow table fetch_arrow_table is an alias of arrow fetch_record_batch chunk_size returns an arrow record batch reader with chunk_size rows per batch polars pl fetches the data as a polars dataframe below are some examples using this functionality see the python guides for more examples fetch as pandas dataframe df con execute select from items fetchdf print df item value count 0 jeans 20 0 1 1 hammer 42 2 2 2 laptop 2000 0 1 3 chainsaw 500 0 10 4 iphone 300 0 2 fetch as dictionary of numpy arrays arr con execute select from items fetchnumpy print arr item masked_array data jeans hammer laptop chainsaw iphone mask false false false false false fill_value dtype object value masked_array data 20 0 42 2 2000 0 500 0 300 0 mask false false false false false fill_value 1e 20 count masked_array data 1 2 1 10 2 mask false false false false false fill_value 999999 dtype int32 fetch as an arrow table converting to pandas afterwards just for pretty printing tbl con execute select from items fetch_arrow_table print tbl to_pandas item value count 0 jeans 20 00 1 1 hammer 42 20 2 2 laptop 2000 00 1 3 chainsaw 500 00 10 4 iphone 300 00 2",
			"category": "Python",
			"url": "/docs/api/python/result_conversion",
			"blurb": "DuckDB's Python client provides multiple additional methods that can be used to efficiently retrieve data. NumPy..."
		},
		{
			"title": "Rust API",
			"text": "installation the duckdb rust api can be installed from crates io please see the docs rs for details basic api usage duckdb-rs is an ergonomic wrapper based on the duckdb c api please refer to the readme for details startup shutdown to use duckdb you must first initialize a connection handle using connection open connection open takes as parameter the database file to read and write from if the database file does not exist it will be created the file extension may be db duckdb or anything else you can also use connection open_in_memory to create an in-memory database note that for an in-memory database no data is persisted to disk i e all data is lost when you exit the process use duckdb params connection result let conn connection open_in_memory you can conn close the connection manually or just leave it out of scope we had implement the drop trait which will automatically close the underlining db connection for you querying sql queries can be sent to duckdb using the execute method of connections or we can also prepare the statement and then query on that conn execute insert into person name data values params me name me data let mut stmt conn prepare select id name data from person let person_iter stmt query_map row ok person id row get 0 name row get 1 data row get 2 for person in person_iter println found person person unwrap",
			"category": "Api",
			"url": "/docs/api/rust",
			"blurb": "Installation The DuckDB Rust API can be installed from crates.io . Please see the docs.rs for details. Basic API..."
		},
		{
			"title": "S3 Parquet Export",
			"text": "to write a parquet file to s3 the httpfs extension is required this can be installed use the install sql command this only needs to be run once install httpfs to load the httpfs extension for usage use the load sql command load httpfs after loading the httpfs extension set up the credentials and s3 region to write data you may either use an access key and secret or a token set s3_region us-east-1 set s3_access_key_id aws access key id set s3_secret_access_key aws secret access key the alternative is to use a token set s3_region us-east-1 set s3_session_token aws session token after the httpfs extension is set up and the s3 credentials are correctly configured parquet files can be written to s3 using the following command copy table_name to s3 bucket file parquet similarly google cloud storage gcs is supported through the interoperability api you need to create hmac keys and declare them set s3_endpoint storage googleapis com set s3_access_key_id key_id set s3_secret_access_key access_key please note you will need to use the s3 url to write your files copy table_name to s3 gcs_bucket file parquet",
			"category": "Import",
			"url": "/docs/guides/import/s3_export",
			"blurb": "To write a Parquet file to S3, the HTTPFS extension is required. This can be installed use the INSTALL SQL command...."
		},
		{
			"title": "S3, GCS, or R2 Parquet Import",
			"text": "to load a parquet file from s3 the httpfs extension is required this can be installed use the install sql command this only needs to be run once install httpfs to load the httpfs extension for usage use the load sql command load httpfs after loading the httpfs extension set up the credentials and s3 region to read data firstly the region where the data resides needs to be configured set s3_region us-east-1 with the only the region set public s3 data can be queried to query private s3 data you need to either use an access key and secret set s3_access_key_id aws access key id set s3_secret_access_key aws secret access key or a session token set s3_session_token aws session token after the httpfs extension is set up and the s3 configuration is set correctly parquet files can be read from s3 using the following command select from read_parquet s3 bucket file for google cloud storage gcs the interoperability api enables you to have access to it like an s3 connection you need to create hmac keys and declare them set s3_endpoint storage googleapis com set s3_access_key_id key_id set s3_secret_access_key access_key please note you will need to use the s3 url to read your data select from read_parquet s3 gcs_bucket file for cloudflare r2 the s3 compatibility api allows you to use duckdb s s3 support to read and write from r2 buckets you will need to generate an s3 auth token and update the s3_endpoint used set s3_region auto set s3_endpoint your-account-id r2 cloudflarestorage com set s3_access_key_id key_id set s3_secret_access_key access_key note that you will need to use the s3 url to read your data from r2 select from read_parquet s3 r2_bucket_name file",
			"category": "Import",
			"url": "/docs/guides/import/s3_import",
			"blurb": "To load a Parquet file from S3, the HTTPFS extension is required. This can be installed use the INSTALL SQL command...."
		},
		{
			"title": "SAMPLE Clause",
			"text": "the sample clause allows you to run the query on a sample from the base table this can significantly speed up processing of queries at the expense of accuracy in the result samples can also be used to quickly see a snapshot of the data when exploring a data set the sample clause is applied right after anything in the from clause i e after any joins but before the where clause or any aggregates see the sample page for more information examples -- select a sample of 1 of the addresses table using default system sampling select from addresses using sample 1 -- select a sample of 1 of the addresses table using bernoulli sampling select from addresses using sample 1 bernoulli -- select a sample of 10 rows from the subquery select from select from addresses using sample 10 rows syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/sample",
			"blurb": "The SAMPLE clause allows you to run the query on a sample from the base table. This can significantly speed up..."
		},
		{
			"title": "SELECT Clause",
			"text": "the select clause specifies the list of columns that will be returned by the query while it appears first in the clause logically the expressions here are executed only at the end the select clause can contain arbitrary expressions that transform the output as well as aggregates and window functions examples -- select all columns from the table called table_name select from table_name -- perform arithmetic on columns in a table and provide an alias select col1 col2 as res sqrt col1 as root from table_name -- select all unique cities from the addresses table select distinct city from addresses -- return the total number of rows in the addresses table select count from addresses -- select all columns except the city column from the addresses table select exclude city from addresses -- select all columns from the addresses table but replace city with lower city select replace lower city as city from addresses -- select all columns matching the given regex from the table select columns number d from addresses -- compute a function on all given columns of a table select min columns from addresses syntax select list the select clause contains a list of expressions that specify the result of a query the select list can refer to any columns in the from clause and combine them using expressions as the output of a sql query is a table - every expression in the select clause also has a name the expressions can be explicitly named using the as clause e g expr as name if a name is not provided by the user the expressions are named automatically by the system star expressions -- select all columns from the table called table_name select from table_name -- select all columns matching the given regex from the table select columns number d from addresses the star expression is a special expression that expands to multiple expressions based on the contents of the from clause in the simplest case expands to all expressions in the from clause columns can also be selected using regular expressions or lambda functions see the star expression page for more details distinct clause -- select all unique cities from the addresses table select distinct city from addresses the distinct clause can be used to return only the unique rows in the result - so that any duplicate rows are filtered out distinct on clause -- select only the highest population city for each country select distinct on country city population from cities order by population desc the distinct on clause returns only one row per unique value in the set of expressions as defined in the on clause if an order by clause is present the row that is returned is the first row that is encountered as per the order by criteria if an order by clause is not present the first row that is encountered is not defined and can be any row in the table aggregates -- return the total number of rows in the addresses table select count from addresses -- return the total number of rows in the addresses table grouped by city select city count from addresses group by city aggregate functions are special functions that combine multiple rows into a single value when aggregate functions are present in the select clause the query is turned into an aggregate query in an aggregate query all expressions must either be part of an aggregate function or part of a group as specified by the group by clause window functions -- generate a row_number column containing incremental identifiers for each row select row_number over from sales -- compute the difference between the current amount and the previous amount by order of time select amount - lag amount over order by time from sales window functions are special functions that allow the computation of values relative to other rows in a result window functions are marked by the over clause which contains the window specification the window specification defines the frame or context in which the window function is computed see the window functions page for more information unnest -- unnest an array by one level select unnest 1 2 3 -- unnest a struct by one level select unnest a 42 b 84 unnest is a special function that can be used together with arrays or structs the unnest function strips one level of nesting from the type for example int is transformed into int struct a int b int is transformed into a int b int the unnest function can be used to transform nested types into regular scalar types which makes them easier to operate on",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/select",
			"blurb": "The SELECT clause specifies the list of columns that will be returned by the query."
		},
		{
			"title": "SQL Introduction",
			"text": "here we provide an overview of how to perform simple operations in sql this tutorial is only intended to give you an introduction and is in no way a complete tutorial on sql this tutorial is adapted from the postgresql tutorial in the examples that follow we assume that you have installed the duckdb command line interface cli shell see here for information on how to install the cli if you build from the source tree you can launch the cli from the build directory build release duckdb launching the shell should give you the following prompt duckdb 5fb6fe57ab enter help for usage hints connected to a transient in-memory database use open filename to reopen on a persistent database d by launching the database like this an in-memory database is launched that means that no data is persisted on disk to persist data on disk you should also pass a database path to the shell the database will then be stored at that path and can be reloaded from disk later concepts duckdb is a relational database management system rdbms that means it is a system for managing data stored in relations a relation is essentially a mathematical term for a table each table is a named collection of rows each row of a given table has the same set of named columns and each column is of a specific data type tables themselves are stored inside schemas and a collection of schemas constitutes the entire database that you can access creating a new table you can create a new table by specifying the table name along with all column names and their types create table weather city varchar temp_lo integer -- minimum temperature on a day temp_hi integer -- maximum temperature on a day prcp real date date you can enter this into the shell with the line breaks the command is not terminated until the semicolon white space i e spaces tabs and newlines can be used freely in sql commands that means you can type the command aligned differently than above or even all on one line two dash characters -- introduce comments whatever follows them is ignored up to the end of the line sql is case insensitive about key words and identifiers in the sql command we first specify the type of command that we want to perform create table after that follows the parameters for the command first the table name weather is given then the column names and column types follow city varchar specifies that the table has a column called city that is of type varchar varchar specifies a data type that can store text of arbitrary length the temperature fields are stored in an integer type a type that stores integer numbers i e whole numbers without a decimal point real columns store single precision floating-point numbers i e numbers with a decimal point date stores a date i e year month day combination date only stores the specific day not a time associated with that day duckdb supports the standard sql types integer smallint real double decimal char n varchar n date time and timestamp the second example will store cities and their associated geographical location create table cities name varchar lat decimal lon decimal finally it should be mentioned that if you don t need a table any longer or want to recreate it differently you can remove it using the following command drop table tablename populating a table with rows the insert statement is used to populate a table with rows insert into weather values san francisco 46 50 0 25 1994-11-27 constants that are not numeric values e g text and dates must be surrounded by single quotes as in the example input dates for the date type must be formatted as yyyy-mm-dd we can insert into the cities table in the same manner insert into cities values san francisco -194 0 53 0 the syntax used so far requires you to remember the order of the columns an alternative syntax allows you to list the columns explicitly insert into weather city temp_lo temp_hi prcp date values san francisco 43 57 0 0 1994-11-29 you can list the columns in a different order if you wish or even omit some columns e g if the prcp is unknown insert into weather date city temp_hi temp_lo values 1994-11-29 hayward 54 37 many developers consider explicitly listing the columns better style than relying on the order implicitly please enter all the commands shown above so you have some data to work with in the following sections you could also have used copy to load large amounts of data from csv files this is usually faster because the copy command is optimized for this application while allowing less flexibility than insert an example would be copy weather from home user weather csv where the file name for the source file must be available on the machine running the process there are many other ways of loading data into duckdb see the corresponding documentation section for more information querying a table to retrieve data from a table the table is queried a sql select statement is used to do this the statement is divided into a select list the part that lists the columns to be returned a table list the part that lists the tables from which to retrieve the data and an optional qualification the part that specifies any restrictions for example to retrieve all the rows of table weather type select from weather here is a shorthand for all columns so the same result would be had with select city temp_lo temp_hi prcp date from weather the output should be city temp_lo temp_hi prcp date --------------- --------- ---------- ------ ------------ san francisco 46 50 0 25 1994-11-27 san francisco 43 57 0 0 1994-11-29 hayward 37 54 1994-11-29 3 rows you can write expressions not just simple column references in the select list for example you can do select city temp_hi temp_lo 2 as temp_avg date from weather this should give city temp_avg date --------------- ---------- ------------ san francisco 48 1994-11-27 san francisco 50 1994-11-29 hayward 45 1994-11-29 3 rows notice how the as clause is used to relabel the output column the as clause is optional a query can be qualified by adding a where clause that specifies which rows are wanted the where clause contains a boolean truth value expression and only rows for which the boolean expression is true are returned the usual boolean operators and or and not are allowed in the qualification for example the following retrieves the weather of san francisco on rainy days select from weather where city san francisco and prcp 0 0 result city temp_lo temp_hi prcp date --------------- --------- --------- ------ ------------ san francisco 46 50 0 25 1994-11-27 you can request that the results of a query be returned in sorted order select from weather order by city city temp_lo temp_hi prcp date --------------- --------- --------- ------ ------------ hayward 37 54 1994-11-29 san francisco 43 57 0 0 1994-11-29 san francisco 46 50 0 25 1994-11-27 in this example the sort order isn t fully specified and so you might get the san francisco rows in either order but you d always get the results shown above if you do select from weather order by city temp_lo you can request that duplicate rows be removed from the result of a query select distinct city from weather city --------------- hayward san francisco 2 rows here again the result row ordering might vary you can ensure consistent results by using distinct and order by together select distinct city from weather order by city joins between tables thus far our queries have only accessed one table at a time queries can access multiple tables at once or access the same table in such a way that multiple rows of the table are being processed at the same time a query that accesses multiple rows of the same or different tables at one time is called a join query as an example say you wish to list all the weather records together with the location of the associated city to do that we need to compare the city column of each row of the weather table with the name column of all rows in the cities table and select the pairs of rows where these values match this would be accomplished by the following query select from weather cities where city name city temp_lo temp_hi prcp date name lon lat --------------- --------- --------- ------ ------------ --------------- ----- ---- san francisco 46 50 0 25 1994-11-27 san francisco -194 53 san francisco 43 57 0 1994-11-29 san francisco -194 53 2 rows observe two things about the result set there is no result row for the city of hayward this is because there is no matching entry in the cities table for hayward so the join ignores the unmatched rows in the weather table we will see shortly how this can be fixed there are two columns containing the city name this is correct because the lists of columns from the weather and cities tables are concatenated in practice this is undesirable though so you will probably want to list the output columns explicitly rather than using select city temp_lo temp_hi prcp date lon lat from weather cities where city name since the columns all had different names the parser automatically found which table they belong to if there were duplicate column names in the two tables you d need to qualify the column names to show which one you meant as in select weather city weather temp_lo weather temp_hi weather prcp weather date cities lon cities lat from weather cities where cities name weather city it is widely considered good style to qualify all column names in a join query so that the query won t fail if a duplicate column name is later added to one of the tables join queries of the kind seen thus far can also be written in this alternative form select from weather inner join cities on weather city cities name this syntax is not as commonly used as the one above but we show it here to help you understand the following topics now we will figure out how we can get the hayward records back in what we want the query to do is to scan the weather table and for each row to find the matching cities row s if no matching row is found we want some empty values to be substituted for the cities table s columns this kind of query is called an outer join the joins we have seen so far are inner joins the command looks like this select from weather left outer join cities on weather city cities name city temp_lo temp_hi prcp date name lon lat --------------- --------- --------- ------ ------------ --------------- ----- ---- san francisco 46 50 0 25 1994-11-27 san francisco -194 53 san francisco 43 57 0 1994-11-29 san francisco -194 53 hayward 37 54 1994-11-29 3 rows this query is called a left outer join because the table mentioned on the left of the join operator will have each of its rows in the output at least once whereas the table on the right will only have those rows output that match some row of the left table when outputting a left-table row for which there is no right-table match empty null values are substituted for the right-table columns aggregate functions like most other relational database products duckdb supports aggregate functions an aggregate function computes a single result from multiple input rows for example there are aggregates to compute the count sum avg average max maximum and min minimum over a set of rows as an example we can find the highest low-temperature reading anywhere with select max temp_lo from weather max ----- 46 1 row if we wanted to know what city or cities that reading occurred in we might try select city from weather where temp_lo max temp_lo -- wrong but this will not work since the aggregate max cannot be used in the where clause this restriction exists because the where clause determines which rows will be included in the aggregate calculation so obviously it has to be evaluated before aggregate functions are computed however as is often the case the query can be restated to accomplish the desired result here by using a subquery select city from weather where temp_lo select max temp_lo from weather city --------------- san francisco 1 row this is ok because the subquery is an independent computation that computes its own aggregate separately from what is happening in the outer query aggregates are also very useful in combination with group by clauses for example we can get the maximum low temperature observed in each city with select city max temp_lo from weather group by city city max --------------- ----- hayward 37 san francisco 46 2 rows which gives us one output row per city each aggregate result is computed over the table rows matching that city we can filter these grouped rows using having select city max temp_lo from weather group by city having max temp_lo 40 city max --------- ----- hayward 37 1 row which gives us the same results for only the cities that have all temp_lo values below 40 finally if we only care about cities whose names begin with s we can use the like operator select city max temp_lo from weather where city like s -- 1 group by city having max temp_lo 40 more information about the like operator can be found here it is important to understand the interaction between aggregates and sql s where and having clauses the fundamental difference between where and having is this where selects input rows before groups and aggregates are computed thus it controls which rows go into the aggregate computation whereas having selects group rows after groups and aggregates are computed thus the where clause must not contain aggregate functions it makes no sense to try to use an aggregate to determine which rows will be inputs to the aggregates on the other hand the having clause always contains aggregate functions in the previous example we can apply the city name restriction in where since it needs no aggregate this is more efficient than adding the restriction to having because we avoid doing the grouping and aggregate calculations for all rows that fail the where check updates you can update existing rows using the update command suppose you discover the temperature readings are all off by 2 degrees after november 28 you can correct the data as follows update weather set temp_hi temp_hi - 2 temp_lo temp_lo - 2 where date 1994-11-28 look at the new state of the data select from weather city temp_lo temp_hi prcp date --------------- --------- --------- ------ ------------ san francisco 46 50 0 25 1994-11-27 san francisco 41 55 0 1994-11-29 hayward 35 52 1994-11-29 3 rows deletions rows can be removed from a table using the delete command suppose you are no longer interested in the weather of hayward then you can do the following to delete those rows from the table delete from weather where city hayward all weather records belonging to hayward are removed select from weather city temp_lo temp_hi prcp date --------------- --------- --------- ------ ------------ san francisco 46 50 0 25 1994-11-27 san francisco 41 55 0 1994-11-29 2 rows one should be wary of statements of the form delete from tablename without a qualification delete will remove all rows from the given table leaving it empty the system will not request confirmation before doing this",
			"category": "SQL",
			"url": "/docs/sql/introduction",
			"blurb": "Here we provide an overview of how to perform simple operations in SQL. This tutorial is only intended to give you an..."
		},
		{
			"title": "SQL on Apache Arrow",
			"text": "duckdb can query multiple different types of apache arrow objects apache arrow tables arrow tables stored in local variables can be queried as if they are regular tables within duckdb import duckdb import pyarrow as pa connect to an in-memory database con duckdb connect my_arrow_table pa table from_pydict i 1 2 3 4 j one two three four query the apache arrow table my_arrow_table and return as an arrow table results con execute select from my_arrow_table where i 2 arrow apache arrow datasets arrow datasets stored as variables can also be queried as if they were regular tables datasets are useful to point towards directories of parquet files to analyze large datasets duckdb will push column selections and row filters down into the dataset scan operation so that only the necessary data is pulled into memory import duckdb import pyarrow as pa import tempfile import pathlib import pyarrow parquet as pq import pyarrow dataset as ds connect to an in-memory database con duckdb connect my_arrow_table pa table from_pydict i 1 2 3 4 j one two three four create example parquet files and save in a folder base_path pathlib path tempfile gettempdir base_path parquet_folder mkdir exist_ok true pq write_to_dataset my_arrow_table str base_path parquet_folder link to parquet files using an arrow dataset my_arrow_dataset ds dataset str base_path parquet_folder query the apache arrow dataset my_arrow_dataset and return as an arrow table results con execute select from my_arrow_dataset where i 2 arrow apache arrow scanners arrow scanners stored as variables can also be queried as if they were regular tables scanners read over a dataset and select specific columns or apply row-wise filtering this is similar to how duckdb pushes column selections and filters down into an arrow dataset but using arrow compute operations instead arrow can use asynchronous io to quickly access files import duckdb import pyarrow as pa import tempfile import pathlib import pyarrow parquet as pq import pyarrow dataset as ds import pyarrow compute as pc connect to an in-memory database con duckdb connect my_arrow_table pa table from_pydict i 1 2 3 4 j one two three four create example parquet files and save in a folder base_path pathlib path tempfile gettempdir base_path parquet_folder mkdir exist_ok true pq write_to_dataset my_arrow_table str base_path parquet_folder link to parquet files using an arrow dataset my_arrow_dataset ds dataset str base_path parquet_folder define the filter to be applied while scanning equivalent to where i 2 scanner_filter pc field i pc scalar 2 arrow_scanner ds scanner from_dataset my_arrow_dataset filter scanner_filter query the apache arrow scanner arrow_scanner and return as an arrow table results con execute select from arrow_scanner arrow apache arrow recordbatchreaders arrow recordbatchreaders are a reader for arrow s streaming binary format and can also be queried directly as if they were tables this streaming format is useful when sending arrow data for tasks like interprocess communication or communicating between language runtimes import duckdb import pyarrow as pa connect to an in-memory database con duckdb connect my_recordbatch pa recordbatch from_pydict i 1 2 3 4 j one two three four my_recordbatchreader pa ipc recordbatchreader from_batches my_recordbatch schema my_recordbatch query the apache arrow recordbatchreader my_recordbatchreader and return as an arrow table results con execute select from my_recordbatchreader where i 2 arrow",
			"category": "Python",
			"url": "/docs/guides/python/sql_on_arrow",
			"blurb": "DuckDB can query multiple different types of Apache Arrow objects. Apache Arrow Tables Arrow Tables stored in local..."
		},
		{
			"title": "SQL on Pandas",
			"text": "pandas dataframes stored in local variables can be queried as if they are regular tables within duckdb import duckdb import pandas create a pandas dataframe my_df pandas dataframe from_dict a 42 query the pandas dataframe my_df note duckdb sql connects to the default in-memory database connection results duckdb sql select from my_df df",
			"category": "Python",
			"url": "/docs/guides/python/sql_on_pandas",
			"blurb": "Pandas DataFrames stored in local variables can be queried as if they are regular tables within DuckDB. import duckdb..."
		},
		{
			"title": "SQLite Import",
			"text": "to run a query directly on a sqlite file the sqlite extension is required this can be installed use the install sql command this only needs to be run once install sqlite to load the sqlite extension for usage use the load sql command load sqlite after the sqlite extension is installed tables can be queried from sqlite using the sqlite_scan function -- scan the table tbl_name from the sqlite file test db select from sqlite_scan test db tbl_name alternatively the entire file can be attached using the sqlite_attach command this creates views over all of the tables in the file that allow you to query the tables using regular sql syntax -- attach the sqlite file test db call sqlite_attach test db -- the table tbl_name can now be queried as if it is a regular table select from tbl_name for more information see the sqlite scanner documentation",
			"category": "Import",
			"url": "/docs/guides/import/query_sqlite",
			"blurb": "To run a query directly on a SQLite file, the sqlite extension is required. This can be installed use the INSTALL SQL..."
		},
		{
			"title": "SQLite Scanner",
			"text": "the sqlite extension allows duckdb to directly read data from a sqlite database file the data can be queried directly from the underlying sqlite tables or read into duckdb tables loading the extension in order to use the sqlite extension it must first be installed and loaded this can be done using the following commands install sqlite load sqlite usage to make a sqlite file accessible to duckdb use the attach statement which supports read write or the older sqlite_attach function for example with the bundled sakila db file attach sakila db type sqlite -- or call sqlite_attach sakila db the tables in the file are registered as views in duckdb you can list them as follows pragma show_tables name actor address category city country customer customer_list film film_actor film_category film_list film_text inventory language payment rental sales_by_film_category sales_by_store staff staff_list store then you can query those views normally using sql e g using the example queries from sakila-examples sql select cat name category_name sum ifnull pay amount 0 revenue from category cat left join film_category flm_cat on cat category_id flm_cat category_id left join film fil on flm_cat film_id fil film_id left join inventory inv on fil film_id inv film_id left join rental ren on inv inventory_id ren inventory_id left join payment pay on ren rental_id pay rental_id group by cat name order by revenue desc limit 5 querying individual tables instead of attaching you can also query individual tables using the sqlite_scan function select from sqlite_scan sakila db film data types sqlite is a weakly typed database system as such when storing data in a sqlite table types are not enforced the following is valid sql in sqlite create table numbers i integer insert into numbers values hello duckdb is a strongly typed database system as such it requires all columns to have defined types and the system rigorously checks data for correctness when querying sqlite duckdb must deduce a specific column type mapping duckdb follows sqlite s type affinity rules with a few extensions if the declared type contains the string int then it is translated into the type bigint if the declared type of the column contains any of the strings char clob or text then it is translated into varchar if the declared type for a column contains the string blob or if no type is specified then it is translated into blob if the declared type for a column contains any of the strings real floa doub dec or num then it is translated into double if the declared type is date then it is translated into date if the declared type contains the string time then it is translated into timestamp if none of the above apply then it is translated into varchar as duckdb enforces the corresponding columns to contain only correctly typed values we cannot load the string hello into a column of type bigint as such an error is thrown when reading from the numbers table above error mismatch type error invalid type in column i column was declared as integer found hello of type text instead this error can be avoided by setting the sqlite_all_varchar option set global sqlite_all_varchar true when set this option overrides the type conversion rules described above and instead always converts the sqlite columns into a varchar column note that this setting must be set before sqlite_attach is called running more than once if you want to run the sqlite_scan procedure more than once in the same duckdb session you ll need to pass in the overwrite flag as shown below call sqlite_attach sakila db overwrite true extra information see the repo for the source code of the extension",
			"category": "Extensions",
			"url": "/docs/extensions/sqlite_scanner",
			"blurb": "The sqlite extension allows DuckDB to directly read data from a SQLite database file. The data can be queried..."
		},
		{
			"title": "Samples",
			"text": "samples are used to randomly select a subset of a dataset examples -- select a sample of 5 rows from tbl using reservoir sampling select from tbl using sample 5 -- select a sample of 10 of the table using system sampling cluster sampling select from tbl using sample 10 -- select a sample of 10 of the table using bernoulli sampling select from tbl using sample 10 percent bernoulli -- select a sample of 50 rows of the table using reservoir sampling with a fixed seed 100 select from tbl using sample reservoir 50 rows repeatable 100 -- select a sample of 20 of the table using system sampling with a fixed seed 377 select from tbl using sample 10 system 377 -- select a sample of 10 of tbl before the join with tbl2 select from tbl tablesample reservoir 20 tbl2 where tbl i tbl2 i -- select a sample of 10 of tbl after the join with tbl2 select from tbl tbl2 where tbl i tbl2 i using sample reservoir 20 syntax samples allow you to randomly extract a subset of a dataset samples are useful for exploring a dataset faster as often you might not be interested in the exact answers to queries but only in rough indications of what the data looks like and what is in the data samples allow you to get approximate answers to queries faster as they reduce the amount of data that needs to pass through the query engine duckdb supports three different types of sampling methods reservoir bernoulli and system by default duckdb uses reservoir sampling when an exact number of rows is sampled and system sampling when a percentage is specified the sampling methods are described in detail below samples require a sample size which is an indication of how many elements will be sampled from the total population samples can either be given as a percentage 10 or as a fixed number of rows 10 rows all three sampling methods support sampling over a percentage but only reservoir sampling supports sampling a fixed number of rows samples are probablistic that is to say samples can be different between runs unless the seed is specifically specified specifying the seed only guarantees that the sample is the same if multi-threading is not enabled i e pragma threads 1 in the case of multiple threads running over a sample samples are not necessarily consistent even with a fixed seed reservoir reservoir sampling is a stream sampling technique that selects a random sample by keeping a reservoir of size equal to the sample size and randomly replacing elements as more elements come in reservoir sampling allows us to specify exactly how many elements we want in the resulting sample by selecting the size of the reservoir as a result reservoir sampling always outputs the same amount of elements unlike system and bernoulli sampling reservoir sampling is only recommended for small sample sizes and is not recommended for use with percentages that is because reservoir sampling needs to materialize the entire sample and randomly replace tuples within the materialized sample the larger the sample size the higher the performance hit incurred by this process reservoir sampling also incurs an additional performance penalty when multi-processing is used since the reservoir is to be shared amongst the different threads to ensure unbiased sampling this is not a big problem when the reservoir is very small but becomes costly when the sample is large avoid using reservoir sample with large sample sizes if possible reservoir sampling requires the entire sample to be materialized in memory bernoulli bernoulli sampling can only be used when a sampling percentage is specified it is rather straightforward every tuple in the underlying table is included with a chance equal to the specified percentage as a result bernoulli sampling can return a different number of tuples even if the same percentage is specified the amount of rows will generally be more or less equal to the specified percentage of the table but there will be some variance because bernoulli sampling is completely independent there is no shared state there is no penalty for using bernoulli sampling together with multiple threads system system sampling is a variant of bernoulli sampling with one crucial difference every vector is included with a chance equal to the sampling percentage this is a form of cluster sampling system sampling is more efficient than bernoulli sampling as no per-tuple selections have to be performed there is almost no extra overhead for using system sampling whereas bernoulli sampling can add additional cost as it has to perform random number generation for every single tuple system sampling is not suitable for smaller data sets as the granularity of the sampling is on the order of 1000 tuples that means that if system sampling is used for small data sets e g 100 rows either all the data will be filtered out or all the data will be included table samples the tablesample and using sample clauses are identical in terms of syntax and effect with one important difference tablesamples sample directly from the table for which they are specified whereas the sample clause samples after the entire from clause has been resolved this is relevant when there are joins present in the query plan the tablesample clause is essentially equivalent to creating a subquery with the using sample clause i e the following two queries are identical -- sample 20 of tbl before the join select from tbl tablesample reservoir 20 tbl2 where tbl i tbl2 i -- sample 20 of tbl before the join select from select from tbl using sample reservoir 20 tbl tbl2 where tbl i tbl2 i -- sample 20 after the join i e sample 20 of the join result select from tbl tbl2 where tbl i tbl2 i using sample reservoir 20",
			"category": "SQL",
			"url": "/docs/sql/samples",
			"blurb": "Samples are used to randomly select a subset of a dataset. Examples -- select a sample of 5 rows from tbl using..."
		},
		{
			"title": "Scala JDBC API",
			"text": "installation the duckdb java jdbc api can be used in scala and can be installed from maven central please see the installation page for details basic api usage scala uses duckdb s jdbc api implements the main parts of the standard java database connectivity jdbc api version 4 0 describing jdbc is beyond the scope of this page see the official documentation for details below we focus on the duckdb-specific parts startup shutdown in scala database connections are created through the standard java sql drivermanager class the driver should auto-register in the drivermanager if that does not work for some reason you can enforce registration like so class forname org duckdb duckdbdriver to create a duckdb connection call drivermanager with the jdbc duckdb jdbc url prefix like so val conn drivermanager getconnection jdbc duckdb when using the jdbc duckdb url alone an in-memory database is created note that for an in-memory database no data is persisted to disk i e all data is lost when you exit the java program if you would like to access or create a persistent database append its file name after the path for example if your database is stored in tmp my_database use the jdbc url jdbc duckdb tmp my_database to create a connection to it it is possible to open a duckdb database file in read-only mode this is for example useful if multiple java processes want to read the same database file at the same time to open an existing database file in read-only mode set the connection property duckdb read_only like so val ro_prop new properties ro_prop setproperty duckdb read_only true val conn_ro drivermanager getconnection jdbc duckdb tmp my_database ro_prop the jdbc drivermanager api is a relatively poor fit for embedded database management systems such as duckdb if you would like to create multiple connections to the same database it would be somewhat logical to just create additional connections with the same url this is however only supported for read-only connections if you would like to create multiple read-write connections to the same database file or the same in-memory database instance you can use the custom duplicate method like so val conn2 duckdbconnection conn duplicate querying duckdb supports the standard jdbc methods to send queries and retrieve result sets first a statement object has to be created from the connection this object can then be used to send queries using execute and executequery execute is meant for queries where no results are expected like create table or update etc and executequery is meant to be used for queries that produce results e g select below two examples see also the jdbc statement and resultset documentations create a table val stmt conn createstatement stmt execute create table items item varchar value decimal 10 2 count integer insert two items into the table stmt execute insert into items values jeans 20 0 1 hammer 42 2 2 val rs stmt executequery select from items while rs next system out println rs getstring 1 system out println rs getint 3 rs close jeans 1 hammer 2 duckdb also supports prepared statements as per the jdbc api val p_stmt conn preparestatement insert into test values p_stmt setstring 1 chainsaw p_stmt setdouble 2 500 0 p_stmt setint 3 42 p_stmt execute more calls to execute possible p_stmt close do not use prepared statements to insert large amounts of data into duckdb see the data import documentation for better options",
			"category": "Api",
			"url": "/docs/api/scala",
			"blurb": "Installation The DuckDB Java JDBC API can be used in Scala and can be installed from Maven Central . Please see the..."
		},
		{
			"title": "Select Statement",
			"text": "the select statement retrieves rows from the database examples -- select all columns from the table tbl select from tbl -- select the rows from tbl select j from tbl where i 3 -- perform an aggregate grouped by the column i select i sum j from tbl group by i -- select only the top 3 rows from the tbl select from tbl order by i desc limit 3 -- join two tables together using the using clause select from t1 join t2 using a b -- use column indexes to select the first and third column from the table tbl select 1 3 from tbl syntax the select statement retrieves rows from the database the canonical order of a select statement is as follows with less common clauses being indented select select_list from tables using sample sample_expr where condition group by groups having group_filter window window_expr qualify qualify_filter order by order_expr limit n optionally the select statement can be prefixed with a with clause as the select statement is so complex we have split up the syntax diagrams into several parts the full syntax diagram can be found at the bottom of the page select clause the select clause specifies the list of columns that will be returned by the query while it appears first in the clause logically the expressions here are executed only at the end the select clause can contain arbitrary expressions that transform the output as well as aggregates and window functions from clause the from clause specifies the source of the data on which the remainder of the query should operate logically the from clause is where the query starts execution the from clause can contain a single table a combination of multiple tables that are joined together or another select query inside a subquery node sample clause the sample clause allows you to run the query on a sample from the base table this can significantly speed up processing of queries at the expense of accuracy in the result samples can also be used to quickly see a snapshot of the data when exploring a data set the sample clause is applied right after anything in the from clause i e after any joins but before the where clause or any aggregates see the sample page for more information where clause the where clause specifies any filters to apply to the data this allows you to select only a subset of the data in which you are interested logically the where clause is applied immediately after the from clause group by having clause the group by clause specifies which grouping columns should be used to perform any aggregations in the select clause if the group by clause is specified the query is always an aggregate query even if no aggregations are present in the select clause window clause the window clause allows you to specify named windows that can be used within window functions these are useful when you have multiple window functions as they allow you to avoid repeating the same window clause qualify clause the qualify clause is used to filter the result of window functions order by limit clause order by and limit are output modifiers logically they are applied at the very end of the query the limit clause restricts the amount of rows fetched and the order by clause sorts the rows on the sorting criteria in either ascending or descending order values list a values list is a set of values that is supplied instead of a select statement row ids for each table the rowid pseudocolumn returns the row identifiers based on the physical storage d create table t id int content string d insert into t values 42 hello 43 world d select rowid id content from t rowid id content 0 42 hello 1 43 world in the current storage these identifiers are contiguous unsigned integers 0 1 if no rows were deleted deletions introduce gaps in the rowids which may be reclaimed later therefore it is strongly recommended not to use rowids as identifiers the rowid values are stable within a transaction if there is a user-defined column named rowid it shadows the rowid pseudocolumn common table expressions full syntax diagram below is the full syntax diagram of the select statement",
			"category": "Statements",
			"url": "/docs/sql/statements/select",
			"blurb": "The SELECT statement retrieves rows from the database."
		},
		{
			"title": "Set Operations",
			"text": "set operations allow queries to be combined according to set operation semantics set operations refer to the union all intersect and except clauses traditional set operations unify queries by column position and require the to-be-combined queries to have the same number of input columns if the columns are not of the same type casts may be added the result will use the column names from the first query duckdb also supports union by name which joins columns by name instead of by position union by name does not require the inputs to have the same number of columns null values will be added in case of missing columns examples -- the values 0 10 and 0 5 select from range 10 t1 union all select from range 5 t2 -- the values 0 10 union eliminates duplicates select from range 10 t1 union select from range 5 t2 -- the values 0 5 all values that are both in t1 and t2 select from range 10 t1 intersect select from range 6 t2 -- the values 5 10 select from range 10 t1 except select from range 5 t2 -- two rows 24 null and null amsterdam select 24 as id union all by name select amsterdam as city syntax example table create table capitals city varchar country varchar insert into capitals values amsterdam nl berlin germany create table weather city varchar degrees integer date date insert into weather values amsterdam 10 2022-10-14 seattle 8 2022-10-12 union all the union clause can be used to combine rows from multiple queries the queries are required to have the same number of columns and the same column types the union clause performs duplicate elimination by default - only unique rows will be included in the result union all returns all rows of both queries without duplicate elimination select city from capitals union select city from weather -- amsterdam berlin seattle select city from capitals union all select city from weather -- amsterdam amsterdam berlin seattle intersect the intersect clause can be used to select all rows that occur in the result of both queries note that intersect performs duplicate elimination so only unique rows are returned select city from capitals intersect select city from weather -- amsterdam except the except clause can be used to select all rows that only occur in the left query note that except performs duplicate elimination so only unique rows are returned select city from capitals except select city from weather -- berlin union all by name the union all by name clause can be used to combine rows from different tables by name instead of by position union by name does not require both queries to have the same number of columns any columns that are only found in one of the queries are filled with null values for the other query select from capitals union by name select from weather city country degrees date varchar varchar int32 date amsterdam null 10 2022-10-14 seattle null 8 2022-10-12 amsterdam nl null null berlin germany null null union by name performs duplicate elimination whereas union all by name does not",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/setops",
			"blurb": "Set operations allow queries to be combined according to set operation semantics . Set operations refer to the UNION..."
		},
		{
			"title": "Set/Reset",
			"text": "the set statement modifies the provided duckdb configuration option at the specified scope examples -- update the memory_limit configuration value set memory_limit 10gb -- configure the system to use 1 thread set threads to 1 -- change configuration option to default value reset threads syntax set updates a duckdb configuration option to the provided value reset the reset statement changes the given duckdb configuration option to the default value scopes local - not yet implemented session - configuration value is used or reset only for the current session attached to a duckdb instance global - configuration value is used or reset across the entire duckdb instance when not specified the default scope for the configuration option is used for most options this is global configuration see the configuration page for the full list of configuration options",
			"category": "Statements",
			"url": "/docs/sql/statements/set",
			"blurb": "The SET statement modifies the provided DuckDB configuration option at the specified scope. Examples -- Update the..."
		},
		{
			"title": "Sitemap",
			"text": "",
			"category": "Docs",
			"url": "/docs/index",
			"blurb": ""
		},
		{
			"title": "Spatial",
			"text": "the spatial extension provides support for geospatial data processing in duckdb geometry type the core of the spatial extension is the geometry type if you re unfamiliar with geospatial data and gis tooling this type probably works very different from what you d expect in short while the geometry type is a binary representation of geometry data made up out of sets of vertices pairs of x and y double precision floats it actually stores one of several geometry subtypes these are point linestring polygon as well as their collection equivalents multipoint multilinestring and multipolygon lastly there is geometrycollection which can contain any of the other subtypes as well as other geometrycollection s recursively this may seem strange at first since duckdb already have types like list struct and union which could be used in a similar way but the design and behaviour of the geometry type is actually based on the simple features geometry model which is a standard used by many other databases and gis software that said the spatial extension also includes a couple of experimental non-standard explicit geometry types such as point_2d linestring_2d polygon_2d and box_2d that are based on duckdbs native nested types such as structs and lists in theory it should be possible to optimize a lot of operations for these types much better than for the geometry type which is just a binary blob but only a couple functions are implemented so far all of these are implicitly castable to geometry but with a conversion cost so the geometry type is still the recommended type to use for now if you are planning to work with a lot of different spatial functions geometry is not currently capable of storing additional geometry types z m coordinates or srid information these features may be added in the future spatial scalar functions the spatial extension implements a large number of scalar functions and overloads most of these are implemented using the geos library but we d like to implement more of them natively in this extension to better utilize duckdb s vectorized execution and memory management the following symbols are used to indicate which implementation is used - geos - functions that are implemented using the geos library - duckdb - functions that are implemented natively in this extension that are capable of operating directly on the duckdb types - cast geometry - functions that are supported by implicitly casting to geometry and then using the geometry implementation the currently implemented spatial functions can roughly be categorized into the following groups geometry conversion convert between geometries and other formats scalar functions geometry point_2d linestring_2d polygon_2d box_2d ----------------------------------- ---------- ---------- --------------- ------------ ----------------- varchar st_asgeojson geometry as polygon varchar st_ashexwkb geometry varchar st_astext geometry as polygon wkb_blob st_aswkb geometry geometry st_geomfromtext varchar as polygon geometry st_geomfromwkb blob as polygon geometry construction construct new geometries from other geometries or other data scalar functions geometry point_2d linestring_2d polygon_2d box_2d ------------------------------------------------------ -------- -------- ------------- ---------- -------------- geometry st_point double double geometry st_convexhull geometry as polygon geometry st_boundary geometry as polygon geometry st_buffer geometry as polygon geometry st_centroid geometry geometry st_collect geometry geometry st_normalize geometry as polygon geometry st_simplifypreservetopology geometry double as polygon geometry st_simplify geometry double as polygon geometry st_union geometry geometry as polygon geometry st_intersection geometry geometry as polygon geometry st_makeline geometry geometry st_envelope geometry as polygon geometry st_flipcoordinates geometry geometry st_transform geometry varchar varchar spatial properties calculate and access spatial properties of geometries scalar functions geometry point_2d linestring_2d polygon_2d box_2d -------------------------------------- -------- -------- ------------- ---------- -------------- double st_area geometry boolean st_isclosed geometry as polygon boolean st_isempty geometry as polygon boolean st_isring geometry as polygon boolean st_issimple geometry as polygon boolean st_isvalid geometry as polygon double st_x geometry as polygon double st_y geometry as polygon geometrytype st_geometrytype geometry as polygon double st_length geometry as polygon spatial relationships compute relationships and spatial predicates between geometries scalar functions geometry point_2d linestring_2d polygon_2d box_2d ---------------------------------------------- -------- -------- ------------- ---------- -------------- boolean st_within geometry geometry or as polygon boolean st_touches geometry geometry as polygon boolean st_overlaps geometry geometry as polygon boolean st_contains geometry geometry or as polygon boolean st_coveredby geometry geometry as polygon boolean st_covers geometry geometry as polygon boolean st_crosses geometry geometry as polygon boolean st_difference geometry geometry as polygon boolean st_disjoint geometry geometry as polygon boolean st_intersects geometry geometry as polygon boolean st_equals geometry geometry as polygon double st_distance geometry geometry or or as polygon boolean st_dwithin geometry geometry double as polygon spatial table functions the spatial extension provides a st_read table function based on the gdal translator library to read spatial data from a variety of geospatial vector file formats as if they were duckdb tables for example to create a new table from a geojson file you can use the following query create table table as select from st_read some file path filename json st_read can take a number of optional arguments the full signature is st_read varchar sequential_layer_scan boolean spatial_filter wkb_blob open_options varchar layer varchar allowed_drivers varchar sibling_files varchar spatial_filter_box box_2d sequential_layer_scan default false if set to true the table function will scan through all layers sequentially and return the first layer that matches the given layer name this is required for some drivers to work properly e g the osm driver spatial_filter default null if set to a wkb blob the table function will only return rows that intersect with the given wkb geometry some drivers may support efficient spatial filtering natively in which case it will be pushed down otherwise the filtering is done by gdal which may be much slower open_options default a list of key-value pairs that are passed to the gdal driver to control the opening of the file e g the geojson driver supports a flatten_nested_attributes yes option to flatten nested attributes layer default null the name of the layer to read from the file if null the first layer is returned can also be a layer index starting at 0 allowed_drivers default a list of gdal driver names that are allowed to be used to open the file if empty all drivers are allowed sibling_files default a list of sibling files that are required to open the file e g the esri shapefile driver requires a shx file to be present although most of the time these can be discovered automatically spatial_filter_box default null if set to a box_2d the table function will only return rows that intersect with the given bounding box similar to spatial_filter note that gdal is single-threaded so this table function will not be able to make full use of parallelism we re planning to implement support for the most common vector formats natively in this extension with additional table functions in the future we currently support over 50 different formats you can generate the following table of supported gdal drivers yourself by executing select from st_drivers short_name long_name can_create can_copy can_open help_url -------------- --------------------------------------------------- ---------- -------- -------- -------------------------------------------------- esri shapefile esri shapefile true false true https gdal org drivers vector shapefile html mapinfo file mapinfo file true false true https gdal org drivers vector mitab html uk ntf uk ntf false false true https gdal org drivers vector ntf html lvbag kadaster lv bag extract 2 0 false false true https gdal org drivers vector lvbag html s57 iho s-57 enc true false true https gdal org drivers vector s57 html dgn microstation dgn true false true https gdal org drivers vector dgn html ogr_vrt vrt - virtual datasource false false true https gdal org drivers vector vrt html memory memory true false true csv comma separated value csv true false true https gdal org drivers vector csv html gml geography markup language gml true false true https gdal org drivers vector gml html gpx gpx true false true https gdal org drivers vector gpx html kml keyhole markup language kml true false true https gdal org drivers vector kml html geojson geojson true false true https gdal org drivers vector geojson html geojsonseq geojson sequence true false true https gdal org drivers vector geojsonseq html esrijson esrijson false false true https gdal org drivers vector esrijson html topojson topojson false false true https gdal org drivers vector topojson html ogr_gmt gmt ascii vectors gmt true false true https gdal org drivers vector gmt html gpkg geopackage true true true https gdal org drivers vector gpkg html sqlite sqlite spatialite true false true https gdal org drivers vector sqlite html wasp wasp map format true false true https gdal org drivers vector wasp html openfilegdb esri filegdb true false true https gdal org drivers vector openfilegdb html dxf autocad dxf true false true https gdal org drivers vector dxf html cad autocad driver false false true https gdal org drivers vector cad html flatgeobuf flatgeobuf true false true https gdal org drivers vector flatgeobuf html geoconcept geoconcept true false true georss georss true false true https gdal org drivers vector georss html vfk czech cadastral exchange data format false false true https gdal org drivers vector vfk html pgdump postgresql sql dump true false false https gdal org drivers vector pgdump html osm openstreetmap xml and pbf false false true https gdal org drivers vector osm html gpsbabel gpsbabel true false true https gdal org drivers vector gpsbabel html wfs ogc wfs web feature service false false true https gdal org drivers vector wfs html oapif ogc api - features false false true https gdal org drivers vector oapif html edigeo french edigeo exchange format false false true https gdal org drivers vector edigeo html svg scalable vector graphics false false true https gdal org drivers vector svg html ods open document libreoffice openoffice spreadsheet true false true https gdal org drivers vector ods html xlsx ms office open xml spreadsheet true false true https gdal org drivers vector xlsx html elasticsearch elastic search true false true https gdal org drivers vector elasticsearch html carto carto true false true https gdal org drivers vector carto html amigocloud amigocloud true false true https gdal org drivers vector amigocloud html sxf storage and exchange format false false true https gdal org drivers vector sxf html selafin selafin true false true https gdal org drivers vector selafin html jml openjump jml true false true https gdal org drivers vector jml html plscenes planet labs scenes api false false true https gdal org drivers vector plscenes html csw ogc csw catalog service for the web false false true https gdal org drivers vector csw html vdv vdv-451 vdv-452 intrest data format true false true https gdal org drivers vector vdv html mvt mapbox vector tiles true false true https gdal org drivers vector mvt html ngw nextgis web true true true https gdal org drivers vector ngw html mapml mapml true false true https gdal org drivers vector mapml html tiger u s census tiger line false false true https gdal org drivers vector tiger html avcbin arc info binary coverage false false true https gdal org drivers vector avcbin html avce00 arc info e00 ascii coverage false false true https gdal org drivers vector avce00 html note that far from all of these drivers have been tested properly and some may require additional options to be passed to work as expected if you run into any issues please first consult the gdal docs spatial copy functions much like the st_read table function the spatial extension provides a gdal based copy function to export duckdb tables to different geospatial vector formats for example to export a table to a geojson file with generated bounding boxes you can use the following query copy table to some file path filename geojson with format gdal driver geojson layer_creation_options write_bbox yes format is the only required option and must be set to gdal to use the gdal based copy function driver is the gdal driver to use for the export see the table above for a list of available drivers layer_creation_options list of options to pass to the gdal driver see the gdal docs for the driver you are using for a list of available options extra information see the repo for the source code of the extension or the blog post",
			"category": "Extensions",
			"url": "/docs/extensions/spatial",
			"blurb": "The spatial extension provides support for geospatial data processing in DuckDB. GEOMETRY type The core of the..."
		},
		{
			"title": "Specifier",
			"text": "specifier - description",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "Specifier - Description"
		},
		{
			"title": "Star Expression",
			"text": "examples -- select all columns present in the from clause select from table_name -- select all columns from the table called table_name select table_name from table_name join other_table_name using id -- select all columns except the city column from the addresses table select exclude city from addresses -- select all columns from the addresses table but replace city with lower city select replace lower city as city from addresses -- select all columns matching the given expression select columns c - c like num from addresses -- select all columns matching the given regex from the table select columns number d from addresses syntax star expression the expression can be used in a select statement to select all columns that are projected in the from clause select from tbl the expression can be modified using the exclude and replace exclude clause exclude allows us to exclude specific columns from the expression select exclude col from tbl replace clause replace allows us to replace specific columns with different expressions select replace col 1000 as col from tbl columns the columns expression can be used to execute the same expression on multiple columns like the expression it can only be used in the select clause create table numbers id int number int insert into numbers values 1 10 2 20 3 null select min columns count columns from numbers min numbers id min numbers number count numbers id count numbers number ----------------- --------------------- ------------------- ----------------------- 1 10 3 2 the expression in the columns statement can also contain exclude or replace similar to regular star expressions select min columns replace number id as number count columns exclude number from numbers min numbers id min number number id count numbers id ----------------- ------------------------------ ------------------- 1 11 3 columns expressions can also be combined as long as the columns contains the same star expression select columns columns from numbers numbers id numbers id numbers number numbers number --------------------------- ----------------------------------- 2 20 4 40 6 null columns regular expression columns supports passing a regex in as a string constant select columns id numbers from numbers id number ---- -------- 1 10 2 20 3 null columns lambda function columns also supports passing in a lambda function the lambda function will be evaluated for all columns present in the from clause and only columns that match the lambda function will be returned this allows the execution of arbitrary expressions in order to select columns select columns c - c like num from numbers number -------- 10 20 null struct the expression can also be used to retrieve all keys from a struct as separate columns this is particularly useful when a prior operation creates a struct of unknown shape or if a query must handle any potential struct keys see the struct and nested function pages for more details on working with structs -- all keys within a struct can be returned as separate columns using select a from select x 1 y 2 z 3 as a x y z --- --- --- 1 2 3",
			"category": "Expressions",
			"url": "/docs/sql/expressions/star",
			"blurb": "Examples -- select all columns present in the FROM clause SELECT * FROM table_name; -- select all columns from the..."
		},
		{
			"title": "Statements Overview",
			"text": "more",
			"category": "Statements",
			"url": "/docs/sql/statements/overview",
			"blurb": ""
		},
		{
			"title": "Struct",
			"text": "struct data type conceptually a struct column contains an ordered list of columns called entries the entries are referenced by name using strings this document refers to those entry names as keys each row in the struct column must have the same keys the names of the struct entries are part of the schema each row in a struct column must have the same layout the names of the struct entries are case-insensitive struct s are typically used to nest multiple columns into a single column and the nested column can be of any type including other struct s and list s struct s are similar to postgres s row type the key difference is that duckdb struct s require the same keys in each row of a struct column this allows duckdb to provide significantly improved performance by fully utilizing its vectorized execution engine and also enforces type consistency for improved correctness duckdb includes a row function as a special way to produce a struct but does not have a row data type see an example below and the nested functions docs for details see the data types overview for a comparison between nested data types structs can be created using the struct_pack name expr function or the equivalent array notation name expr notation the expressions can be constants or arbitrary expressions creating structs -- struct of integers select x 1 y 2 z 3 -- struct of strings with a null value select yes duck maybe goose huh null no heron -- struct with a different type for each key select key1 string key2 1 key3 12 345 -- struct using the struct_pack function -- note the lack of single quotes around the keys and the use of the operator select struct_pack key1 value1 key2 42 -- struct of structs with null values select birds yes duck maybe goose huh null no heron aliens null amphibians yes frog maybe salamander huh dragon no toad -- create a struct from columns and or expressions using the row function -- this returns x 1 v2 2 y a select row x x 1 y from select 1 as x a as y -- if using multiple expressions when creating a struct the row function is optional -- this also returns x 1 v2 2 y a select x x 1 y from select 1 as x a as y adding field s value s to structs -- add to a struct of integers select struct_insert a 1 b 2 c 3 d 4 retrieving from structs retrieving a value from a struct can be accomplished using dot notation bracket notation or through struct functions like struct_extract -- use dot notation to retrieve the value at a key s location this returns 1 -- the subquery generates a struct column a which we then query with a x select a x from select x 1 y 2 z 3 as a -- if key contains a space simply wrap it in double quotes this returns 1 -- note use double quotes not single quotes -- this is because this action is most similar to selecting a column from within the struct select a x space from select x space 1 y 2 z 3 as a -- bracket notation may also be used this returns 1 -- note use single quotes since the goal is to specify a certain string key -- only constant expressions may be used inside the brackets no columns select a x space from select x space 1 y 2 z 3 as a -- the struct_extract function is also equivalent this returns 1 select struct_extract x space 1 y 2 z 3 x space struct rather than retrieving a single key from a struct star notation can be used to retrieve all keys from a struct as separate columns this is particularly useful when a prior operation creates a struct of unknown shape or if a query must handle any potential struct keys -- all keys within a struct can be returned as separate columns using select a from select x 1 y 2 z 3 as a x y z --- --- --- 1 2 3 dot notation order of operations referring to structs with dot notation can be ambiguous with referring to schemas and tables in general duckdb looks for columns first then for struct keys within columns duckdb resolves references in these orders using the first match to occur no dots select part1 from tbl part1 is a column one dot select part1 part2 from tbl part1 is a table part2 is a column part1 is a column part2 is a property of that column two or more dots select part1 part2 part3 from tbl part1 is a schema part2 is a table part3 is a column part1 is a table part2 is a column part3 is a property of that column part1 is a column part2 is a property of that column part3 is a property of that column any extra parts e g part4 part5 etc are always treated as properties creating structs with the row function the row function can be used to automatically convert multiple columns to a single struct column the name of each input column is used as a key and the value of each column becomes the struct s value at that key when converting multiple expressions into a struct the row function name is optional - a set of parenthesis is all that is needed example data table named t1 my_column another_column --- --- 1 a 2 b row function example select row my_column another_column as my_struct_column my_column another_column as identical_struct_column from t1 example output my_struct_column identical_struct_column --- --- my_column 1 another_column a my_column 1 another_column a my_column 2 another_column b my_column 2 another_column b the row function or simplified parenthesis syntax may also be used with arbitrary expressions as input rather than column names in the case of an expression a key will be automatically generated in the format of vn where n is a number that refers to its parameter location in the row function ex v1 v2 etc this can be combined with column names as an input in the same call to the row function this example uses the same input table as above row function example with a column name a constant and an expression as input select row my_column 42 my_column 1 as my_struct_column my_column 42 my_column 1 as identical_struct_column from t1 example output my_struct_column identical_struct_column --- --- my_column 1 v2 42 v3 2 my_column 1 v2 42 v3 2 my_column 2 v2 42 v3 3 my_column 2 v2 42 v3 3 comparison operators nested types can be compared using all the comparison operators these comparisons can be used in logical expressions for both where and having clauses as well as for creating boolean values the ordering is defined positionally in the same way that words can be ordered in a dictionary null values compare greater than all other values and are considered equal to each other at the top level null nested values obey standard sql null comparison rules comparing a null nested value to a non- null nested value produces a null result comparing nested value members however uses the internal nested value rules for null s and a null nested value member will compare above a non- null nested value member functions see nested functions",
			"category": "Data Types",
			"url": "/docs/sql/data_types/struct",
			"blurb": "Struct Data Type Conceptually, a STRUCT column contains an ordered list of columns called entries. The entries are..."
		},
		{
			"title": "Subqueries",
			"text": "scalar subquery scalar subqueries are subqueries that return a single value they can be used anywhere where a regular expression can be used if a scalar subquery returns more than a single value the first value returned will be used consider the following table grades grade course --- --- 7 math 9 math 8 cs create table grades grade integer course varchar insert into grades values 7 math 9 math 8 cs we can run the following query to obtain the minimum grade select min grade from grades -- 7 by using a scalar subquery in the where clause we can figure out for which course this grade was obtained select course from grades where grade select min grade from grades -- math exists the exists operator is used to test for the existence of any row inside the subquery it returns either true when the subquery returns one or more records or false otherwise the exists operator is generally the most useful as a correlated subquery however it can be used as an uncorrelated subquery as well for example we can use it to figure out if there are any grades present for a given course select exists select from grades where course math -- true select exists select from grades where course history -- false in operator the in operator checks containment of the left expression inside the result defined by the subquery or the set of expressions on the right hand side rhs the in operator returns true if the expression is present in the rhs false if the expression is not in the rhs and the rhs has no null values or null if the expression is not in the rhs and the rhs has null values we can use the in operator in a similar manner as we used the exists operator select math in select course from grades -- true correlated subqueries all the subqueries presented here so far have been uncorrelated subqueries where the subqueries themselves are entirely self-contained and can be run without the parent query there exists a second type of subqueries called correlated subqueries for correlated subqueries the subquery uses values from the parent subquery conceptually the subqueries are run once for every single row in the parent query perhaps a simple way of envisioning this is that the correlated subquery is a function that is applied to every row in the source data set for example suppose that we want to find the minimum grade for every course we could do that as follows select from grades grades_parent where grade select min grade from grades where grades course grades_parent course -- 7 math 8 cs the subquery uses a column from the parent query grades_parent course conceptually we can see the subquery as a function where the correlated column is a parameter to that function select min grade from grades where course now when we execute this function for each of the rows we can see that for math this will return 7 and for cs it will return 8 we then compare it against the grade for that actual row as a result the row math 9 will be filtered out as 9 7",
			"category": "Expressions",
			"url": "/docs/sql/expressions/subqueries",
			"blurb": "Scalar Subquery Scalar subqueries are subqueries that return a single value. They can be used anywhere where a..."
		},
		{
			"title": "Substrait",
			"text": "the main goal of this extension is to support both production and consumption of substrait query plans in duckdb this extension is mainly exposed via 3 different apis - the sql api the python api and the r api here we depict how to consume and produce substrait query plans in each api sql in the sql api users can generate substrait plans into a blob or a json and consume substrait plans before using the extension you must always properly install and load it to install and load the released version of the substrait library you must execute the following sql commands install substrait load substrait blob generation to generate a substrait blob the get_substrait sql function must be called with a valid sql select query create table crossfit exercise text dificulty_level int insert into crossfit values push ups 3 pull ups 5 push jerk 7 bar muscle up 10 call get_substrait select count exercise as exercise from crossfit where dificulty_level 5 ---- x12 x09 x1a x07 x10 x01 x1a x03lte x12 x11 x1a x0f x10 x02 x1a x0bis_not_null x12 x09 x1a x07 x10 x03 x1a x03and x12 x10 x1a x0e x10 x04 x1a x0acount_star x1a xcb x01 x12 xc8 x01 x0a xbb x01 xb8 x01 x12 xab x01 xa8 x01 x12 x97 x01 x0a x94 x01 x12 x0a x08exercise x0a x0fdificulty_level x12 x11 x0a x07 xb2 x01 x04 x08 x0d x18 x01 x0a x04 x02 x10 x01 x18 x02 x1aj x1ah x08 x03 x1a x04 x0a x02 x10 x01 x1a x1a x1e x08 x01 x1a x04 x02 x10 x01 x0c x1a x0a x12 x08 x0a x04 x12 x02 x08 x01 x00 x06 x1a x04 x0a x02 x05 x1a x1a x18 x1a x16 x08 x02 x1a x04 x02 x10 x01 x0c x1a x0a x12 x08 x0a x04 x12 x02 x08 x01 x00 x0a x0a x06 x0a x02 x08 x01 x0a x00 x10 x01 x0a x0a x08crossfit x1a x00 x0a x0a x08 x08 x04 x04 x02 x10 x01 x1a x08 x12 x06 x0a x02 x12 x00 x00 x12 x08exercise json generation to generate a json representing the substrait plan the get_substrait_json sql function must be called with a valid sql select query call get_substrait_json select count exercise as exercise from crossfit where dificulty_level 5 ---- extensions extensionfunction functionanchor 1 name lte extensionfunction functionanchor 2 name is_not_null extensionfunction functionanchor 3 name and extensionfunction functionanchor 4 name count_star relations root input project input aggregate input read baseschema names exercise dificulty_level struct types varchar length 13 nullability nullability_nullable i32 nullability nullability_nullable nullability nullability_required filter scalarfunction functionreference 3 outputtype bool nullability nullability_nullable arguments value scalarfunction functionreference 1 outputtype i32 nullability nullability_nullable arguments value selection directreference structfield field 1 rootreference value literal i32 5 value scalarfunction functionreference 2 outputtype i32 nullability nullability_nullable arguments value selection directreference structfield field 1 rootreference projection select structitems field 1 maintainsingularstruct true namedtable names crossfit groupings measures measure functionreference 4 outputtype i64 nullability nullability_nullable expressions selection directreference structfield rootreference names exercise blob consumption to consume a substrait blob the from_substrait blob function must be called with a valid substrait blob plan call from_substrait x12 x07 x1a x05 x1a x03lte x12 x11 x1a x0f x10 x01 x1a x0bis_not_null x12 x09 x1a x07 x10 x02 x1a x03and x12 x10 x1a x0e x10 x03 x1a x0acount_star x1a xa4 x01 x12 xa1 x01 x0a x94 x01 x91 x01 x12 x86 x01 x83 x01 x12y w x12c x12a x12 x0a x12 x1b x0a x08exercise x0a x0fdificulty_level x0a x0a x08crossfit x1a2 x1a0 x08 x02 x18 x1a x16 x1a x14 x0a x1a x08 x12 x06 x0a x04 x12 x02 x08 x01 x06 x1a x04 x0a x02 x05 x12 x1a x10 x1a x0e x08 x01 x0a x1a x08 x12 x06 x0a x04 x12 x02 x08 x01 x1a x08 x12 x06 x0a x04 x12 x02 x08 x01 x1a x06 x12 x04 x0a x02 x12 x00 x1a x00 x04 x0a x02 x08 x03 x1a x06 x12 x04 x0a x02 x12 x00 x12 x08exercise blob ---- 2 python before using the extension you must remember to properly load it to load an extension in python you must execute the sql commands within a connection import duckdb con duckdb connect con install_extension substrait con load_extension substrait blob generation to generate a substrait blob the get_substrait sql function must be called from a connection with a valid sql select query con execute query create table crossfit exercise text dificulty_level int con execute query insert into crossfit values push ups 3 pull ups 5 push jerk 7 bar muscle up 10 proto_bytes con get_substrait query select count exercise as exercise from crossfit where dificulty_level 5 fetchone 0 json generation to generate a json representing the substrait plan the get_substrait_json sql function from a connection must be called with a valid sql select query json con get_substrait_json select count exercise as exercise from crossfit where dificulty_level 5 fetchone 0 blob consumption to consume a substrait blob the from_substrait blob function must be called from the connection with a valid substrait blob plan query_result con from_substrait proto proto_bytes r before using the extension you must remember to properly load it to load an extension in r you must execute the sql commands within a connection con - dbconnect duckdb duckdb dbexecute con install substrait dbexecute con load substrait blob generation to generate a substrait blob the duckdb_get_substrait con sql function must be called with a connection and a valid sql select query dbexecute con create table crossfit exercise text dificulty_level int dbexecute con insert into crossfit values push ups 3 pull ups 5 push jerk 7 bar muscle up 10 proto_bytes - duckdb duckdb_get_substrait con select from integers limit 5 json generation to generate a json representing the substrait plan duckdb_get_substrait_json con sql function with a connection and a valid sql select query json - duckdb duckdb_get_substrait_json con select count exercise as exercise from crossfit where dificulty_level 5 blob consumption to consume a substrait blob the duckdb_prepare_substrait con blob function must be called with a connection and a valid substrait blob plan result - duckdb duckdb_prepare_substrait con proto_bytes df - dbfetch result",
			"category": "Extensions",
			"url": "/docs/extensions/substrait",
			"blurb": "The main goal of this extension is to support both production and consumption of substrait query plans in DuckDB...."
		},
		{
			"title": "Summarize",
			"text": "the summarize command can be used to easily compute a number of aggregates over a table or a query the summarize command launches a query that computes a number of aggregates over all columns including min max avg std and approx_unique in order to summarize the contents of a table use summarize followed by the table name summarize tbl in order to summarize a query prepend summarize to a query summarize select from tbl below is an example of summarize on the lineitem table of tpc-h sf1 column_name column_type min max approx_unique avg std q25 q50 q75 count null_percentage l_orderkey integer 1 6000000 1486805 3000279 604204982 1732187 8734803426 1497471 3022276 4523225 6001215 0 0 l_partkey integer 1 200000 196125 100017 98932999402 57735 69082650517 50056 99973 150007 6001215 0 0 l_suppkey integer 1 10000 10010 5000 602606138924 2886 9619987306205 2499 5001 7498 6001215 0 0 l_linenumber integer 1 7 7 3 0005757167506912 1 7324314036519335 1 3 4 6001215 0 0 l_quantity integer 1 50 50 25 507967136654827 14 426262537016953 12 25 37 6001215 0 0 l_extendedprice decimal 15 2 901 00 104949 50 939196 38255 138484656854 23300 438710962204 18747 36719 55141 6001215 0 0 l_discount decimal 15 2 0 00 0 10 11 0 04999943011540163 0 031619855108125976 0 0 0 6001215 0 0 l_tax decimal 15 2 0 00 0 08 9 0 04001350893110812 0 02581655179884275 0 0 0 6001215 0 0 l_returnflag varchar a r 3 null null null null null 6001215 0 0 l_linestatus varchar f o 2 null null null null null 6001215 0 0 l_shipdate date 1992-01-02 1998-12-01 2554 null null null null null 6001215 0 0 l_commitdate date 1992-01-31 1998-10-31 2491 null null null null null 6001215 0 0 l_receiptdate date 1992-01-04 1998-12-31 2585 null null null null null 6001215 0 0 l_shipinstruct varchar collect cod take back return 4 null null null null null 6001215 0 0 l_shipmode varchar air truck 7 null null null null null 6001215 0 0 l_comment varchar tiresias zzle slyly final platelets sleep quickly 4587836 null null null null null 6001215 0 0",
			"category": "Meta",
			"url": "/docs/guides/meta/summarize",
			"blurb": "The SUMMARIZE command can be used to easily compute a number of aggregates over a table or a query. The SUMMARIZE..."
		},
		{
			"title": "Swift API",
			"text": "tl dr see the announcement post for more information the swift api is available in a standalone repository https github com duckdb duckdb-swift",
			"category": "Api",
			"url": "/docs/api/swift",
			"blurb": "TL;DR: See the announcement post for more information! The Swift API is available in a standalone repository:..."
		},
		{
			"title": "Tableau - A Data Visualisation Tool",
			"text": "tableau is a popular commercial data visualisation tool in addition to a large number of built in connectors it also provides generic database connectivity via odbc and jdbc connectors tableau has two main versions desktop and online server for desktop connecting to a duckdb database is similar to working in an embedded environment like python for online since duckdb is in-process the data needs to be either on the server itself or in a remote data bucket that is accessible from the server database creation when using a duckdb database file the data sets do not actually need to be imported into duckdb tables it suffices to create views of the data for example this will create a view of the h2oai parquet test file in the current duckdb code base create view h2oai as from read_parquet users username duckdb data parquet-testing h2oai h2oai_group_small parquet note that you should use full path names to local files so that they can be found from inside tableau also note that you will need to use a version of the driver that is compatible i e from the same release as the database format used by the duckdb tool e g python module command line that was used to create the file installing the jdbc driver tableau provides documentation on how to install a jdbc driver for tableau to use for now we recommend using the latest bleeding edge jdbc driver 0 8 2 as a number of fixes have been made for time compatibility note that tableau both desktop and server versions need to be restarted any time you add or modify drivers driver links the link here is for a recent version of the jdbc driver that is compatible with tableau if you wish to connect to a database file you will need to make sure the file was created with a file-compatible version of duckdb also check that there is only one version of the driver installed as there are multiple filenames in use download the snapshot jar macos copy it to library tableau drivers windows copy it to c program files tableau drivers linux copy it to opt tableau tableau_driver jdbc using the postgres dialect if you just want to do something simple you can try connecting directly to the jdbc driver and using tableau-provided postgres dialect create a duckdb file containing your views and or data launch tableau under connect to a server more click on other databases jdbc this will bring up the connection dialogue box for the url enter jdbc duckdb user username path to database db for the dialect choose postgresql the rest of the fields can be ignored tableau postgres however functionality will be missing such as median and percentile aggregate functions to make the data source connection more compatible with the postgresql dialect please use the duckdb taco connector as described below installing the tableau duckdb connector while it is possible to use the tableau-provided postgres dialect to communicate with the duckdb jdbc driver we strongly recommend using the duckdb taco connector this connector has been fully tested against the tableau dialect generator and is more compatible than the provided postgres dialect the documentation on how to install and use the connector is in its repository but essentially you will need the duckdb_jdbc taco file the current version of the taco is not signed so you will need to launch tableau with signature validation disabled despite what the tableau documentation says the real security risk is in the jdbc driver code not the small amount of javascript in the taco server online on linux copy the taco file to opt tableau connectors on windows copy the taco file to c program files tableau connectors then issue these commands to disable signature validation tsm configuration set -k native_api disable_verify_connector_plugin_signature -v true tsm pending-changes apply the last command will restart the server with the new settings macos desktop copy the taco file to the users macos user documents my tableau repository connectors folder then launch tableau desktop from the terminal with the the command line argument to disable signature validation applications tableau desktop year quarter app contents macos tableau -ddisableverifyconnectorpluginsignature true you can also package this up with applescript by using the following script do shell script applications tableau desktop 2023 2 app contents macos tableau -ddisableverifyconnectorpluginsignature true quit create this file with the script editor located in applications utilities and save it as a packaged application tableau-applescript you can then double-click it to launch tableau you will need to change the application name in the script when you get upgrades windows desktop copy the taco file to the c users windows user documents my tableau repository connectors directory then launch tableau desktop from a shell with the the -ddisableverifyconnectorpluginsignature true argument to disable signature validation output once loaded you can run queries against your data here is the result of the first h2oai benchmark query from the parquet test file tableau-parquet",
			"category": "Data Viewers",
			"url": "/docs/guides/data_viewers/tableau",
			"blurb": "Tableau is a popular commercial data visualisation tool. In addition to a large number of built in connectors, it..."
		},
		{
			"title": "Tad - A Tabular Data Viewer",
			"text": "tad is a fast free cross-platform tabular data viewer application powered by duckdb there are pre-built binary installers available for mac windows and linux and full source code is available on github tad is a low friction way to explore tabular data files csv and parquet as well as sqlite and duckdb database files a dirty little secret of the data world is that workflows often involve using commercial spreadsheet apps like excel to take a quick peek at the contents of a tabular data file like a csv before analysis work begins tad provides a pleasant and free alternative to this and thanks to duckdb is much faster than excel at opening csv files how much faster on a 2019 macbook pro opening metobjects csv a 230 mb csv with 450k rows takes around 33 seconds to open with the latest version of excel this same file opens in under 5 seconds in tad unlike commercial spreadsheet applications tad also natively supports parquet files since duckdb operates on parquet files in place without an extra import step the speed for opening parquet files is pretty mind-blowing here is tad browsing a directory of multi-million-row parquet files from the tpc-h-sf10 benchmark data set tad-parquet data exploration with tad for data exploration the core of tad is a hierarchical pivot table that allows you to specify a combination of pivot filter aggregate sort column selection column ordering and basic column formatting operations directly in the ui tad delegates to a sql database duckdb for storage and analytics and generates sql queries to perform all analytic operations specified in the ui here is a quick view of using tad to pivot and sort a tabular data file tad-metobjects-pivoted launching tad there are three ways to launch tad application icon you can double click on the tad application icon to open tad use strong file open strong and the standard file open dialog to open a tabular data file in any of the supported file formats context menu or drag and drop the installation process registers tad as a viewer for the supported file format extensions on any code csv code or code parquet code file in the finder macos or explorer windows use strong open with strong and select strong tad strong to open the file in tad command line once tad is installed and available on your path simply type tad somefile csv to launch tad to explore the file you can also open code tad code files previously saved tad view configurations and all the other supported file types via the command line browsing database files of particular interest to duckdb users tad can serve as a lightweight browser for duckdb and sqlite database files just run tad mydatabase duckdb from the command line to get a browsable view of the tables in a duckdb database file this also works for sqlite databases files with a sqlite extension peeking under the bonnet if for some reason you want to see the gnarly details of the sql queries that tad is constructing and executing you can use the code -f code option on the command line to keep tad in the foreground and code --show-queries code to get tad to print the generated sql queries like so tad -f --show-queries somefile csv not for the faint-hearted downloading and installing tad you can download pre-built installers for mac linux and windows from the tad github releases page help bug reports feedback please send any questions feedback bug reports and feature requests to tad-feedback tadviewer com",
			"category": "Data Viewers",
			"url": "/docs/guides/data_viewers/tad",
			"blurb": "Tad is a fast, free cross-platform tabular data viewer application powered by DuckDB. There are pre-built binary..."
		},
		{
			"title": "Text Functions",
			"text": "this section describes functions and operators for examining and manipulating string values denotes a space character function description example result --- --- --- --- string search_string alias for starts_with abc a true string string string concatenation duck db duckdb string index alias for array_extract duckdb 4 k string begin end alias for array_slice missing arguments are interpreted as null s duckdb 4 duck array_extract list index extract a single character using a 1-based index array_extract duckdb 2 u array_slice list begin end extract a string using slice conventions null s are interpreted as the bounds of the string negative values are accepted array_slice duckdb 5 null db ascii string returns an integer that represents the unicode code point of the first character of the string ascii \u03c9 937 bar x min max width draw a band whose width is proportional to x - min and equal to width characters when x max width defaults to 80 bar 5 0 20 10 base64 blob convert a blob to a base64 encoded string alias of to_base64 base64 a blob qq bit_length string number of bits in a string bit_length abc 24 chr x returns a character which is corresponding the ascii code value or unicode code point chr 65 a concat string concatenate many strings together concat hello world hello world concat_ws separator string concatenate strings together separated by the specified separator concat_ws banana apple melon banana apple melon contains string search_string return true if search_string is found within string contains abc a true format format parameters formats a string using fmt syntax format benchmark took seconds csv 42 benchmark csv took 42 seconds from_base64 string convert a base64 encoded string to a character string from_base64 qq a hash value returns an integer with the hash of the value hash 2595805878642663834 instr string search_string return location of first occurrence of search_string in string counting from 1 returns 0 if no match found instr test test es 2 lcase string alias of lower convert string to lower case lcase hello hello left string count extract the left-most count characters left hello 2 he left_grapheme string count extract the left-most grapheme clusters left_grapheme 1 length string number of characters in string length hello 6 length_grapheme string number of grapheme clusters in string length_grapheme 2 string like target returns true if the string matches the like specifier see pattern matching hello like lo true like_escape string like_specifier escape_character returns true if the string matches the like_specifier see pattern matching escape_character is used to search for wildcard characters in the string like_escape a c a c true list_element string index an alias for array_extract list_element duckdb 2 u list_extract string index an alias for array_extract list_extract duckdb 2 u lower string convert string to lower case lower hello hello lpad string count character pads the string with the character from the left until it has count characters lpad hello 10 hello ltrim string removes any spaces from the left side of the string ltrim test test ltrim string characters removes any occurrences of any of the characters from the left side of the string ltrim test test md5 value returns the md5 hash of the value md5 123 202cb962ac59075b964b07152d234b70 nfc_normalize string convert string to unicode nfc normalized string useful for comparisons and ordering if text data is mixed between nfc normalized and not nfc_normalize arde ch arde ch not_like_escape string like_specifier escape_character returns false if the string matches the like_specifier see pattern matching escape_character is used to search for wildcard characters in the string like_escape a c a c true ord string return ascii character code of the leftmost character in a string ord \u00fc 252 position search_string in string return location of first occurrence of search_string in string counting from 1 returns 0 if no match found position b in abc 2 prefix string search_string return true if string starts with search_string prefix abc ab true printf format parameters formats a string using printf syntax printf benchmark s took d seconds csv 42 benchmark csv took 42 seconds regexp_full_match string regex returns true if the entire string matches the regex see pattern matching regexp_full_match anabanana an false regexp_matches string regex returns true if a part of string matches the regex see pattern matching regexp_matches anabanana an true regexp_replace string regex replacement modifiers replaces the first occurrence of regex with the replacement use g modifier to replace all occurrences instead see pattern matching select regexp_replace hello lo - he-lo regexp_split_to_array string regex alias of string_split_regex splits the string along the regex regexp_split_to_array hello world 42 hello world 42 regexp_extract string regex group 0 split the string along the regex and extract first occurrence of group regexp_extract hello_world a-z _ 1 hello regexp_extract_all string regex group 0 split the string along the regex and extract all occurrences of group regexp_extract_all hello_world a-z _ 1 hello world repeat string count repeats the string count number of times repeat a 5 aaaaa replace string source target replaces any occurrences of the source with target in string replace hello l - he--o reverse string reverses the string reverse hello olleh right string count extract the right-most count characters right hello 3 lo right_grapheme string count extract the right-most count grapheme clusters right_grapheme 1 rpad string count character pads the string with the character from the right until it has count characters rpad hello 10 hello rtrim string removes any spaces from the right side of the string rtrim test test rtrim string characters removes any occurrences of any of the characters from the right side of the string rtrim test test split_part string separator index split the string along the separator and return the data at the 1-based index of the list if the index is outside the bounds of the list return an empty string to match postgres behavior split_part a b c 2 b starts_with string search_string return true if string begins with search_string starts_with abc a true string similar to regex returns true if the string matches the regex identical to regexp_full_match see pattern matching hello similar to l false strlen string number of bytes in string strlen 4 strpos string search_string alias of instr return location of first occurrence of search_string in string counting from 1 returns 0 if no match found strpos test test es 2 strip_accents string strips accents from string strip_accents m\u00fchleisen muhleisen str_split string separator alias of string_split splits the string along the separator str_split hello world hello world str_split_regex string regex alias of string_split_regex splits the string along the regex str_split_regex hello world 42 hello world 42 string_split string separator splits the string along the separator string_split hello world hello world string_split_regex string regex splits the string along the regex string_split_regex hello world 42 hello world 42 string_to_array string separator alias of string_split splits the string along the separator string_to_array hello world hello world substr string start length alias of substring extract substring of length characters starting from character start note that a start value of 1 refers to the first character of the string substr hello 2 2 el substring string start length extract substring of length characters starting from character start note that a start value of 1 refers to the first character of the string substring hello 2 2 el substring_grapheme string start length extract substring of length grapheme clusters starting from character start note that a start value of 1 refers to the first character of the string substring_grapheme 3 2 suffix string search_string return true if string ends with search_string suffix abc bc true strpos string characters alias of instr return location of first occurrence of characters in string counting from 1 returns 0 if no match found strpos test test es 2 to_base64 blob convert a blob to a base64 encoded string alias of base64 to_base64 a blob qq trim string removes any spaces from either side of the string trim test test trim string characters removes any occurrences of any of the characters from either side of the string trim test test ucase string alias of upper convert string to upper case ucase hello hello unicode string returns the unicode code of the first character of the string unicode \u00fc 252 upper string convert string to upper case upper hello hello text similarity functions these functions are used to measure the similarity of two strings using various metrics function description example result --- --- --- --- editdist3 string string alias of levenshtein for sqlite compatibility the minimum number of single-character edits insertions deletions or substitutions required to change one string to the other different case is considered different editdist3 duck db 3 hamming string string the number of positions with different characters for 2 strings of equal length different case is considered different hamming duck luck 1 jaccard string string the jaccard similarity between two strings different case is considered different returns a number between 0 and 1 jaccard duck luck 0 6 jaro_similarity string string the jaro similarity between two strings different case is considered different returns a number between 0 and 1 jaro_similarity duck duckdb 0 88 jaro_winkler_similarity string string the jaro-winkler similarity between two strings different case is considered different returns a number between 0 and 1 jaro_winkler_similarity duck duckdb 0 93 levenshtein string string the minimum number of single-character edits insertions deletions or substitutions required to change one string to the other different case is considered different levenshtein duck db 3 damerau_levenshtein string string extension of levenshtein distance to also include transposition of adjacent characters as an allowed edit operation in other words the minimum number of edit operations insertions deletions substitutions or transpositions required to change one string to another different case is considered different damerau_levenshtein duckdb udckbd 2 mismatches string string the number of positions with different characters for 2 strings of equal length different case is considered different mismatches duck luck 1",
			"category": "Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "This section describes functions and operators for examining and manipulating string values. \u2423 denotes a space..."
		},
		{
			"title": "Text Types",
			"text": "in duckdb strings can be stored in the varchar field name aliases description --- --- --- varchar char bpchar text string variable-length character string varchar n variable-length character string with maximum length n it is possible to supply a number along with the type by initializing a type as varchar n where n is a positive integer note that specifying this length is not required and has no effect on the system specifying this length will not improve performance or reduce storage space of the strings in the database this variant is supported for compatibility reasons with other systems that do require a length to be specified for strings if you wish to restrict the number of characters in a varchar column for data integrity reasons the check constraint should be used for example create table strings val varchar check length val 10 -- val has a maximum length of 10 characters the varchar field allows storage of unicode characters internally the data is encoded as utf-8 functions see character functions and pattern matching",
			"category": "Data Types",
			"url": "/docs/sql/data_types/text",
			"blurb": "In DuckDB, strings can be stored in the VARCHAR field."
		},
		{
			"title": "Time Functions",
			"text": "this section describes functions and operators for examining and manipulating time values time operators the table below shows the available mathematical operators for time types operator description example result --- --- --- --- addition of an interval time 01 02 03 interval 5 hour 06 02 03 - subtraction of an interval time 06 02 03 - interval 5 hour 01 02 03 time functions the table below shows the available scalar functions for time types function description example result --- --- --- --- current_time get_current_time current time start of current transaction date_diff part starttime endtime the number of partition boundaries between the times date_diff hour time 01 02 03 time 06 01 03 5 datediff part starttime endtime alias of date_diff the number of partition boundaries between the times datediff hour time 01 02 03 time 06 01 03 5 date_part part time get subfield equivalent to extract date_part minute time 14 21 13 21 datepart part time alias of date_part get subfield equivalent to extract datepart minute time 14 21 13 21 date_sub part starttime endtime the number of complete partitions between the times date_sub hour time 01 02 03 time 06 01 03 4 datesub part starttime endtime alias of date_sub the number of complete partitions between the times datesub hour time 01 02 03 time 06 01 03 4 extract part from time get subfield from a time extract hour from time 14 21 13 14 make_time bigint bigint double the time for the given parts make_time 13 34 27 123456 13 34 27 123456 the only date parts that are defined for times are epoch hours minutes seconds milliseconds and microseconds",
			"category": "Functions",
			"url": "/docs/sql/functions/time",
			"blurb": "This section describes functions and operators for examining and manipulating TIME values. Time Operators The table..."
		},
		{
			"title": "Time Zones",
			"text": "time zone reference list an up-to-date version of this list can be pulled from the pg_timezone_names table function select name abbrev utc_offset from pg_timezone_names order by name name abbrev utc_offset ---------------------------------- ---------------------------------- ----------- act act 09 30 00 aet aet 10 00 00 agt agt -03 00 00 art art 02 00 00 ast ast -09 00 00 africa abidjan iceland 00 00 00 africa accra iceland 00 00 00 africa addis_ababa eat 03 00 00 africa algiers africa algiers 01 00 00 africa asmara eat 03 00 00 africa asmera eat 03 00 00 africa bamako iceland 00 00 00 africa bangui africa bangui 01 00 00 africa banjul iceland 00 00 00 africa bissau africa bissau 00 00 00 africa blantyre cat 02 00 00 africa brazzaville africa brazzaville 01 00 00 africa bujumbura cat 02 00 00 africa cairo art 02 00 00 africa casablanca africa casablanca 00 00 00 africa ceuta africa ceuta 01 00 00 africa conakry iceland 00 00 00 africa dakar iceland 00 00 00 africa dar_es_salaam eat 03 00 00 africa djibouti eat 03 00 00 africa douala africa douala 01 00 00 africa el_aaiun africa el_aaiun 00 00 00 africa freetown iceland 00 00 00 africa gaborone cat 02 00 00 africa harare cat 02 00 00 africa johannesburg africa johannesburg 02 00 00 africa juba africa juba 02 00 00 africa kampala eat 03 00 00 africa khartoum africa khartoum 02 00 00 africa kigali cat 02 00 00 africa kinshasa africa kinshasa 01 00 00 africa lagos africa lagos 01 00 00 africa libreville africa libreville 01 00 00 africa lome iceland 00 00 00 africa luanda africa luanda 01 00 00 africa lubumbashi cat 02 00 00 africa lusaka cat 02 00 00 africa malabo africa malabo 01 00 00 africa maputo cat 02 00 00 africa maseru africa maseru 02 00 00 africa mbabane africa mbabane 02 00 00 africa mogadishu eat 03 00 00 africa monrovia africa monrovia 00 00 00 africa nairobi eat 03 00 00 africa ndjamena africa ndjamena 01 00 00 africa niamey africa niamey 01 00 00 africa nouakchott iceland 00 00 00 africa ouagadougou iceland 00 00 00 africa porto-novo africa porto-novo 01 00 00 africa sao_tome africa sao_tome 00 00 00 africa timbuktu iceland 00 00 00 africa tripoli libya 02 00 00 africa tunis africa tunis 01 00 00 africa windhoek africa windhoek 02 00 00 america adak america adak -10 00 00 america anchorage ast -09 00 00 america anguilla prt -04 00 00 america antigua prt -04 00 00 america araguaina america araguaina -03 00 00 america argentina buenos_aires agt -03 00 00 america argentina catamarca america argentina catamarca -03 00 00 america argentina comodrivadavia america argentina comodrivadavia -03 00 00 america argentina cordoba america argentina cordoba -03 00 00 america argentina jujuy america argentina jujuy -03 00 00 america argentina la_rioja america argentina la_rioja -03 00 00 america argentina mendoza america argentina mendoza -03 00 00 america argentina rio_gallegos america argentina rio_gallegos -03 00 00 america argentina salta america argentina salta -03 00 00 america argentina san_juan america argentina san_juan -03 00 00 america argentina san_luis america argentina san_luis -03 00 00 america argentina tucuman america argentina tucuman -03 00 00 america argentina ushuaia america argentina ushuaia -03 00 00 america aruba prt -04 00 00 america asuncion america asuncion -04 00 00 america atikokan america atikokan -05 00 00 america atka america atka -10 00 00 america bahia america bahia -03 00 00 america bahia_banderas america bahia_banderas -06 00 00 america barbados america barbados -04 00 00 america belem america belem -03 00 00 america belize america belize -06 00 00 america blanc-sablon prt -04 00 00 america boa_vista america boa_vista -04 00 00 america bogota america bogota -05 00 00 america boise america boise -07 00 00 america buenos_aires agt -03 00 00 america cambridge_bay america cambridge_bay -07 00 00 america campo_grande america campo_grande -04 00 00 america cancun america cancun -05 00 00 america caracas america caracas -04 00 00 america catamarca america catamarca -03 00 00 america cayenne america cayenne -03 00 00 america cayman america cayman -05 00 00 america chicago cst -06 00 00 america chihuahua america chihuahua -06 00 00 america coral_harbour america coral_harbour -05 00 00 america cordoba america cordoba -03 00 00 america costa_rica america costa_rica -06 00 00 america creston pnt -07 00 00 america cuiaba america cuiaba -04 00 00 america curacao prt -04 00 00 america danmarkshavn america danmarkshavn 00 00 00 america dawson america dawson -07 00 00 america dawson_creek america dawson_creek -07 00 00 america denver navajo -07 00 00 america detroit america detroit -05 00 00 america dominica prt -04 00 00 america edmonton america edmonton -07 00 00 america eirunepe america eirunepe -05 00 00 america el_salvador america el_salvador -06 00 00 america ensenada america ensenada -08 00 00 america fort_nelson america fort_nelson -07 00 00 america fort_wayne iet -05 00 00 america fortaleza america fortaleza -03 00 00 america glace_bay america glace_bay -04 00 00 america godthab america godthab -03 00 00 america goose_bay america goose_bay -04 00 00 america grand_turk america grand_turk -05 00 00 america grenada prt -04 00 00 america guadeloupe prt -04 00 00 america guatemala america guatemala -06 00 00 america guayaquil america guayaquil -05 00 00 america guyana america guyana -04 00 00 america halifax america halifax -04 00 00 america havana cuba -05 00 00 america hermosillo america hermosillo -07 00 00 america indiana indianapolis iet -05 00 00 america indiana knox america indiana knox -06 00 00 america indiana marengo america indiana marengo -05 00 00 america indiana petersburg america indiana petersburg -05 00 00 america indiana tell_city america indiana tell_city -06 00 00 america indiana vevay america indiana vevay -05 00 00 america indiana vincennes america indiana vincennes -05 00 00 america indiana winamac america indiana winamac -05 00 00 america indianapolis iet -05 00 00 america inuvik america inuvik -07 00 00 america iqaluit america iqaluit -05 00 00 america jamaica jamaica -05 00 00 america jujuy america jujuy -03 00 00 america juneau america juneau -09 00 00 america kentucky louisville america kentucky louisville -05 00 00 america kentucky monticello america kentucky monticello -05 00 00 america knox_in america knox_in -06 00 00 america kralendijk prt -04 00 00 america la_paz america la_paz -04 00 00 america lima america lima -05 00 00 america los_angeles pst -08 00 00 america louisville america louisville -05 00 00 america lower_princes prt -04 00 00 america maceio america maceio -03 00 00 america managua america managua -06 00 00 america manaus america manaus -04 00 00 america marigot prt -04 00 00 america martinique america martinique -04 00 00 america matamoros america matamoros -06 00 00 america mazatlan america mazatlan -07 00 00 america mendoza america mendoza -03 00 00 america menominee america menominee -06 00 00 america merida america merida -06 00 00 america metlakatla america metlakatla -09 00 00 america mexico_city america mexico_city -06 00 00 america miquelon america miquelon -03 00 00 america moncton america moncton -04 00 00 america monterrey america monterrey -06 00 00 america montevideo america montevideo -03 00 00 america montreal america montreal -05 00 00 america montserrat prt -04 00 00 america nassau america nassau -05 00 00 america new_york america new_york -05 00 00 america nipigon america nipigon -05 00 00 america nome america nome -09 00 00 america noronha america noronha -02 00 00 america north_dakota beulah america north_dakota beulah -06 00 00 america north_dakota center america north_dakota center -06 00 00 america north_dakota new_salem america north_dakota new_salem -06 00 00 america nuuk america nuuk -03 00 00 america ojinaga america ojinaga -06 00 00 america panama america panama -05 00 00 america pangnirtung america pangnirtung -05 00 00 america paramaribo america paramaribo -03 00 00 america phoenix pnt -07 00 00 america port-au-prince america port-au-prince -05 00 00 america port_of_spain prt -04 00 00 america porto_acre america porto_acre -05 00 00 america porto_velho america porto_velho -04 00 00 america puerto_rico prt -04 00 00 america punta_arenas america punta_arenas -03 00 00 america rainy_river america rainy_river -06 00 00 america rankin_inlet america rankin_inlet -06 00 00 america recife america recife -03 00 00 america regina america regina -06 00 00 america resolute america resolute -06 00 00 america rio_branco america rio_branco -05 00 00 america rosario america rosario -03 00 00 america santa_isabel america santa_isabel -08 00 00 america santarem america santarem -03 00 00 america santiago america santiago -04 00 00 america santo_domingo america santo_domingo -04 00 00 america sao_paulo bet -03 00 00 america scoresbysund america scoresbysund -01 00 00 america shiprock navajo -07 00 00 america sitka america sitka -09 00 00 america st_barthelemy prt -04 00 00 america st_johns cnt -03 30 00 america st_kitts prt -04 00 00 america st_lucia prt -04 00 00 america st_thomas prt -04 00 00 america st_vincent prt -04 00 00 america swift_current america swift_current -06 00 00 america tegucigalpa america tegucigalpa -06 00 00 america thule america thule -04 00 00 america thunder_bay america thunder_bay -05 00 00 america tijuana america tijuana -08 00 00 america toronto america toronto -05 00 00 america tortola prt -04 00 00 america vancouver america vancouver -08 00 00 america virgin prt -04 00 00 america whitehorse america whitehorse -07 00 00 america winnipeg america winnipeg -06 00 00 america yakutat america yakutat -09 00 00 america yellowknife america yellowknife -07 00 00 antarctica casey antarctica casey 11 00 00 antarctica davis antarctica davis 07 00 00 antarctica dumontdurville antarctica dumontdurville 10 00 00 antarctica macquarie antarctica macquarie 10 00 00 antarctica mawson antarctica mawson 05 00 00 antarctica mcmurdo nz 12 00 00 antarctica palmer antarctica palmer -03 00 00 antarctica rothera antarctica rothera -03 00 00 antarctica south_pole nz 12 00 00 antarctica syowa antarctica syowa 03 00 00 antarctica troll antarctica troll 00 00 00 antarctica vostok antarctica vostok 06 00 00 arctic longyearbyen arctic longyearbyen 01 00 00 asia aden asia aden 03 00 00 asia almaty asia almaty 06 00 00 asia amman asia amman 03 00 00 asia anadyr asia anadyr 12 00 00 asia aqtau asia aqtau 05 00 00 asia aqtobe asia aqtobe 05 00 00 asia ashgabat asia ashgabat 05 00 00 asia ashkhabad asia ashkhabad 05 00 00 asia atyrau asia atyrau 05 00 00 asia baghdad asia baghdad 03 00 00 asia bahrain asia bahrain 03 00 00 asia baku asia baku 04 00 00 asia bangkok asia bangkok 07 00 00 asia barnaul asia barnaul 07 00 00 asia beirut asia beirut 02 00 00 asia bishkek asia bishkek 06 00 00 asia brunei asia brunei 08 00 00 asia calcutta ist 05 30 00 asia chita asia chita 09 00 00 asia choibalsan asia choibalsan 08 00 00 asia chongqing ctt 08 00 00 asia chungking ctt 08 00 00 asia colombo asia colombo 05 30 00 asia dacca bst 06 00 00 asia damascus asia damascus 03 00 00 asia dhaka bst 06 00 00 asia dili asia dili 09 00 00 asia dubai asia dubai 04 00 00 asia dushanbe asia dushanbe 05 00 00 asia famagusta asia famagusta 02 00 00 asia gaza asia gaza 02 00 00 asia harbin ctt 08 00 00 asia hebron asia hebron 02 00 00 asia ho_chi_minh vst 07 00 00 asia hong_kong hongkong 08 00 00 asia hovd asia hovd 07 00 00 asia irkutsk asia irkutsk 08 00 00 asia istanbul turkey 03 00 00 asia jakarta asia jakarta 07 00 00 asia jayapura asia jayapura 09 00 00 asia jerusalem israel 02 00 00 asia kabul asia kabul 04 30 00 asia kamchatka asia kamchatka 12 00 00 asia karachi plt 05 00 00 asia kashgar asia kashgar 06 00 00 asia kathmandu asia kathmandu 05 45 00 asia katmandu asia katmandu 05 45 00 asia khandyga asia khandyga 09 00 00 asia kolkata ist 05 30 00 asia krasnoyarsk asia krasnoyarsk 07 00 00 asia kuala_lumpur singapore 08 00 00 asia kuching asia kuching 08 00 00 asia kuwait asia kuwait 03 00 00 asia macao asia macao 08 00 00 asia macau asia macau 08 00 00 asia magadan asia magadan 11 00 00 asia makassar asia makassar 08 00 00 asia manila asia manila 08 00 00 asia muscat asia muscat 04 00 00 asia nicosia asia nicosia 02 00 00 asia novokuznetsk asia novokuznetsk 07 00 00 asia novosibirsk asia novosibirsk 07 00 00 asia omsk asia omsk 06 00 00 asia oral asia oral 05 00 00 asia phnom_penh asia phnom_penh 07 00 00 asia pontianak asia pontianak 07 00 00 asia pyongyang asia pyongyang 09 00 00 asia qatar asia qatar 03 00 00 asia qostanay asia qostanay 06 00 00 asia qyzylorda asia qyzylorda 05 00 00 asia rangoon asia rangoon 06 30 00 asia riyadh asia riyadh 03 00 00 asia saigon vst 07 00 00 asia sakhalin asia sakhalin 11 00 00 asia samarkand asia samarkand 05 00 00 asia seoul rok 09 00 00 asia shanghai ctt 08 00 00 asia singapore singapore 08 00 00 asia srednekolymsk asia srednekolymsk 11 00 00 asia taipei roc 08 00 00 asia tashkent asia tashkent 05 00 00 asia tbilisi asia tbilisi 04 00 00 asia tehran iran 03 30 00 asia tel_aviv israel 02 00 00 asia thimbu asia thimbu 06 00 00 asia thimphu asia thimphu 06 00 00 asia tokyo jst 09 00 00 asia tomsk asia tomsk 07 00 00 asia ujung_pandang asia ujung_pandang 08 00 00 asia ulaanbaatar asia ulaanbaatar 08 00 00 asia ulan_bator asia ulan_bator 08 00 00 asia urumqi asia urumqi 06 00 00 asia ust-nera asia ust-nera 10 00 00 asia vientiane asia vientiane 07 00 00 asia vladivostok asia vladivostok 10 00 00 asia yakutsk asia yakutsk 09 00 00 asia yangon asia yangon 06 30 00 asia yekaterinburg asia yekaterinburg 05 00 00 asia yerevan net 04 00 00 atlantic azores atlantic azores -01 00 00 atlantic bermuda atlantic bermuda -04 00 00 atlantic canary atlantic canary 00 00 00 atlantic cape_verde atlantic cape_verde -01 00 00 atlantic faeroe atlantic faeroe 00 00 00 atlantic faroe atlantic faroe 00 00 00 atlantic jan_mayen atlantic jan_mayen 01 00 00 atlantic madeira atlantic madeira 00 00 00 atlantic reykjavik iceland 00 00 00 atlantic south_georgia atlantic south_georgia -02 00 00 atlantic st_helena iceland 00 00 00 atlantic stanley atlantic stanley -03 00 00 australia act aet 10 00 00 australia adelaide australia adelaide 09 30 00 australia brisbane australia brisbane 10 00 00 australia broken_hill australia broken_hill 09 30 00 australia canberra aet 10 00 00 australia currie australia currie 10 00 00 australia darwin act 09 30 00 australia eucla australia eucla 08 45 00 australia hobart australia hobart 10 00 00 australia lhi australia lhi 10 30 00 australia lindeman australia lindeman 10 00 00 australia lord_howe australia lord_howe 10 30 00 australia melbourne australia melbourne 10 00 00 australia nsw aet 10 00 00 australia north act 09 30 00 australia perth australia perth 08 00 00 australia queensland australia queensland 10 00 00 australia south australia south 09 30 00 australia sydney aet 10 00 00 australia tasmania australia tasmania 10 00 00 australia victoria australia victoria 10 00 00 australia west australia west 08 00 00 australia yancowinna australia yancowinna 09 30 00 bet bet -03 00 00 bst bst 06 00 00 brazil acre brazil acre -05 00 00 brazil denoronha brazil denoronha -02 00 00 brazil east bet -03 00 00 brazil west brazil west -04 00 00 cat cat 02 00 00 cet cet 01 00 00 cnt cnt -03 30 00 cst cst -06 00 00 cst6cdt cst6cdt -06 00 00 ctt ctt 08 00 00 canada atlantic canada atlantic -04 00 00 canada central canada central -06 00 00 canada east-saskatchewan canada east-saskatchewan -06 00 00 canada eastern canada eastern -05 00 00 canada mountain canada mountain -07 00 00 canada newfoundland cnt -03 30 00 canada pacific canada pacific -08 00 00 canada saskatchewan canada saskatchewan -06 00 00 canada yukon canada yukon -07 00 00 chile continental chile continental -04 00 00 chile easterisland chile easterisland -06 00 00 cuba cuba -05 00 00 eat eat 03 00 00 ect ect 01 00 00 eet eet 02 00 00 est est -05 00 00 est5edt est5edt -05 00 00 egypt art 02 00 00 eire eire 00 00 00 etc gmt gmt 00 00 00 etc gmt 0 gmt 00 00 00 etc gmt 1 etc gmt 1 -01 00 00 etc gmt 10 etc gmt 10 -10 00 00 etc gmt 11 etc gmt 11 -11 00 00 etc gmt 12 etc gmt 12 -12 00 00 etc gmt 2 etc gmt 2 -02 00 00 etc gmt 3 etc gmt 3 -03 00 00 etc gmt 4 etc gmt 4 -04 00 00 etc gmt 5 etc gmt 5 -05 00 00 etc gmt 6 etc gmt 6 -06 00 00 etc gmt 7 etc gmt 7 -07 00 00 etc gmt 8 etc gmt 8 -08 00 00 etc gmt 9 etc gmt 9 -09 00 00 etc gmt-0 gmt 00 00 00 etc gmt-1 etc gmt-1 01 00 00 etc gmt-10 etc gmt-10 10 00 00 etc gmt-11 etc gmt-11 11 00 00 etc gmt-12 etc gmt-12 12 00 00 etc gmt-13 etc gmt-13 13 00 00 etc gmt-14 etc gmt-14 14 00 00 etc gmt-2 etc gmt-2 02 00 00 etc gmt-3 etc gmt-3 03 00 00 etc gmt-4 etc gmt-4 04 00 00 etc gmt-5 etc gmt-5 05 00 00 etc gmt-6 etc gmt-6 06 00 00 etc gmt-7 etc gmt-7 07 00 00 etc gmt-8 etc gmt-8 08 00 00 etc gmt-9 etc gmt-9 09 00 00 etc gmt0 gmt 00 00 00 etc greenwich gmt 00 00 00 etc uct uct 00 00 00 etc utc uct 00 00 00 etc universal uct 00 00 00 etc zulu uct 00 00 00 europe amsterdam europe amsterdam 01 00 00 europe andorra europe andorra 01 00 00 europe astrakhan europe astrakhan 04 00 00 europe athens europe athens 02 00 00 europe belfast gb 00 00 00 europe belgrade europe belgrade 01 00 00 europe berlin europe berlin 01 00 00 europe bratislava europe bratislava 01 00 00 europe brussels europe brussels 01 00 00 europe bucharest europe bucharest 02 00 00 europe budapest europe budapest 01 00 00 europe busingen europe busingen 01 00 00 europe chisinau europe chisinau 02 00 00 europe copenhagen europe copenhagen 01 00 00 europe dublin eire 00 00 00 europe gibraltar europe gibraltar 01 00 00 europe guernsey gb 00 00 00 europe helsinki europe helsinki 02 00 00 europe isle_of_man gb 00 00 00 europe istanbul turkey 03 00 00 europe jersey gb 00 00 00 europe kaliningrad europe kaliningrad 02 00 00 europe kiev europe kiev 02 00 00 europe kirov europe kirov 03 00 00 europe kyiv europe kyiv 02 00 00 europe lisbon portugal 00 00 00 europe ljubljana europe ljubljana 01 00 00 europe london gb 00 00 00 europe luxembourg europe luxembourg 01 00 00 europe madrid europe madrid 01 00 00 europe malta europe malta 01 00 00 europe mariehamn europe mariehamn 02 00 00 europe minsk europe minsk 03 00 00 europe monaco ect 01 00 00 europe moscow w-su 03 00 00 europe nicosia europe nicosia 02 00 00 europe oslo europe oslo 01 00 00 europe paris ect 01 00 00 europe podgorica europe podgorica 01 00 00 europe prague europe prague 01 00 00 europe riga europe riga 02 00 00 europe rome europe rome 01 00 00 europe samara europe samara 04 00 00 europe san_marino europe san_marino 01 00 00 europe sarajevo europe sarajevo 01 00 00 europe saratov europe saratov 04 00 00 europe simferopol europe simferopol 03 00 00 europe skopje europe skopje 01 00 00 europe sofia europe sofia 02 00 00 europe stockholm europe stockholm 01 00 00 europe tallinn europe tallinn 02 00 00 europe tirane europe tirane 01 00 00 europe tiraspol europe tiraspol 02 00 00 europe ulyanovsk europe ulyanovsk 04 00 00 europe uzhgorod europe uzhgorod 02 00 00 europe vaduz europe vaduz 01 00 00 europe vatican europe vatican 01 00 00 europe vienna europe vienna 01 00 00 europe vilnius europe vilnius 02 00 00 europe volgograd europe volgograd 03 00 00 europe warsaw poland 01 00 00 europe zagreb europe zagreb 01 00 00 europe zaporozhye europe zaporozhye 02 00 00 europe zurich europe zurich 01 00 00 factory factory 00 00 00 gb gb 00 00 00 gb-eire gb 00 00 00 gmt gmt 00 00 00 gmt 0 gmt 00 00 00 gmt-0 gmt 00 00 00 gmt0 gmt 00 00 00 greenwich gmt 00 00 00 hst hst -10 00 00 hongkong hongkong 08 00 00 iet iet -05 00 00 ist ist 05 30 00 iceland iceland 00 00 00 indian antananarivo eat 03 00 00 indian chagos indian chagos 06 00 00 indian christmas indian christmas 07 00 00 indian cocos indian cocos 06 30 00 indian comoro eat 03 00 00 indian kerguelen indian kerguelen 05 00 00 indian mahe indian mahe 04 00 00 indian maldives indian maldives 05 00 00 indian mauritius indian mauritius 04 00 00 indian mayotte eat 03 00 00 indian reunion indian reunion 04 00 00 iran iran 03 30 00 israel israel 02 00 00 jst jst 09 00 00 jamaica jamaica -05 00 00 japan jst 09 00 00 kwajalein kwajalein 12 00 00 libya libya 02 00 00 met met 01 00 00 mit mit 13 00 00 mst mst -07 00 00 mst7mdt mst7mdt -07 00 00 mexico bajanorte mexico bajanorte -08 00 00 mexico bajasur mexico bajasur -07 00 00 mexico general mexico general -06 00 00 net net 04 00 00 nst nz 12 00 00 nz nz 12 00 00 nz-chat nz-chat 12 45 00 navajo navajo -07 00 00 plt plt 05 00 00 pnt pnt -07 00 00 prc ctt 08 00 00 prt prt -04 00 00 pst pst -08 00 00 pst8pdt pst8pdt -08 00 00 pacific apia mit 13 00 00 pacific auckland nz 12 00 00 pacific bougainville pacific bougainville 11 00 00 pacific chatham nz-chat 12 45 00 pacific chuuk pacific chuuk 10 00 00 pacific easter pacific easter -06 00 00 pacific efate pacific efate 11 00 00 pacific enderbury pacific enderbury 13 00 00 pacific fakaofo pacific fakaofo 13 00 00 pacific fiji pacific fiji 12 00 00 pacific funafuti pacific funafuti 12 00 00 pacific galapagos pacific galapagos -06 00 00 pacific gambier pacific gambier -09 00 00 pacific guadalcanal sst 11 00 00 pacific guam pacific guam 10 00 00 pacific honolulu pacific honolulu -10 00 00 pacific johnston pacific johnston -10 00 00 pacific kanton pacific kanton 13 00 00 pacific kiritimati pacific kiritimati 14 00 00 pacific kosrae pacific kosrae 11 00 00 pacific kwajalein kwajalein 12 00 00 pacific majuro pacific majuro 12 00 00 pacific marquesas pacific marquesas -09 30 00 pacific midway pacific midway -11 00 00 pacific nauru pacific nauru 12 00 00 pacific niue pacific niue -11 00 00 pacific norfolk pacific norfolk 11 00 00 pacific noumea pacific noumea 11 00 00 pacific pago_pago pacific pago_pago -11 00 00 pacific palau pacific palau 09 00 00 pacific pitcairn pacific pitcairn -08 00 00 pacific pohnpei sst 11 00 00 pacific ponape sst 11 00 00 pacific port_moresby pacific port_moresby 10 00 00 pacific rarotonga pacific rarotonga -10 00 00 pacific saipan pacific saipan 10 00 00 pacific samoa pacific samoa -11 00 00 pacific tahiti pacific tahiti -10 00 00 pacific tarawa pacific tarawa 12 00 00 pacific tongatapu pacific tongatapu 13 00 00 pacific truk pacific truk 10 00 00 pacific wake pacific wake 12 00 00 pacific wallis pacific wallis 12 00 00 pacific yap pacific yap 10 00 00 poland poland 01 00 00 portugal portugal 00 00 00 roc roc 08 00 00 rok rok 09 00 00 sst sst 11 00 00 singapore singapore 08 00 00 systemv ast4 systemv ast4 -04 00 00 systemv ast4adt systemv ast4adt -04 00 00 systemv cst6 systemv cst6 -06 00 00 systemv cst6cdt systemv cst6cdt -06 00 00 systemv est5 systemv est5 -05 00 00 systemv est5edt systemv est5edt -05 00 00 systemv hst10 systemv hst10 -10 00 00 systemv mst7 systemv mst7 -07 00 00 systemv mst7mdt systemv mst7mdt -07 00 00 systemv pst8 systemv pst8 -08 00 00 systemv pst8pdt systemv pst8pdt -08 00 00 systemv yst9 systemv yst9 -09 00 00 systemv yst9ydt systemv yst9ydt -09 00 00 turkey turkey 03 00 00 uct uct 00 00 00 us alaska ast -09 00 00 us aleutian us aleutian -10 00 00 us arizona pnt -07 00 00 us central cst -06 00 00 us east-indiana iet -05 00 00 us eastern us eastern -05 00 00 us hawaii us hawaii -10 00 00 us indiana-starke us indiana-starke -06 00 00 us michigan us michigan -05 00 00 us mountain navajo -07 00 00 us pacific pst -08 00 00 us pacific-new pst -08 00 00 us samoa us samoa -11 00 00 utc uct 00 00 00 universal uct 00 00 00 vst vst 07 00 00 w-su w-su 03 00 00 wet wet 00 00 00 zulu uct 00 00 00",
			"category": "Data Types",
			"url": "/docs/sql/data_types/timezones",
			"blurb": "A reference list for Time Zones."
		},
		{
			"title": "Timestamp Functions",
			"text": "this section describes functions and operators for examining and manipulating timestamp values timestamp operators the table below shows the available mathematical operators for timestamp types operator description example result --- --- --- --- addition of an interval timestamp 1992-03-22 01 02 03 interval 5 day 1992-03-27 01 02 03 - subtraction of timestamp s timestamp 1992-03-27 - timestamp 1992-03-22 5 days - subtraction of an interval timestamp 1992-03-27 01 02 03 - interval 5 day 1992-03-22 01 02 03 adding to or subtracting from infinite values produces the same infinite value timestamp functions the table below shows the available scalar functions for timestamp values function description example result --- --- --- --- age timestamp timestamp subtract arguments resulting in the time difference between the two timestamps age timestamp 2001-04-10 timestamp 1992-09-20 8 years 6 months 20 days age timestamp subtract from current_date age timestamp 1992-09-20 29 years 1 month 27 days 12 39 00 844 century timestamp extracts the century of a timestamp century timestamp 1992-03-22 20 date_diff part startdate enddate the number of partition boundaries between the timestamps date_diff hour timestamp 1992-09-30 23 59 59 timestamp 1992-10-01 01 58 00 2 datediff part startdate enddate alias of date_diff the number of partition boundaries between the timestamps datediff hour timestamp 1992-09-30 23 59 59 timestamp 1992-10-01 01 58 00 2 date_part part timestamp get subfield equivalent to extract date_part minute timestamp 1992-09-20 20 38 40 38 datepart part timestamp alias of date_part get subfield equivalent to extract datepart minute timestamp 1992-09-20 20 38 40 38 date_part part timestamp get the listed subfields as a struct the list must be constant date_part year month day timestamp 1992-09-20 20 38 40 year 1992 month 9 day 20 datepart part timestamp alias of date_part get the listed subfields as a struct the list must be constant datepart year month day timestamp 1992-09-20 20 38 40 year 1992 month 9 day 20 date_sub part startdate enddate the number of complete partitions between the timestamps date_sub hour timestamp 1992-09-30 23 59 59 timestamp 1992-10-01 01 58 00 1 datesub part startdate enddate alias of date_sub the number of complete partitions between the timestamps datesub hour timestamp 1992-09-30 23 59 59 timestamp 1992-10-01 01 58 00 1 date_trunc part timestamp truncate to specified precision date_trunc hour timestamp 1992-09-20 20 38 40 1992-09-20 20 00 00 datetrunc part timestamp alias of date_trunc truncate to specified precision datetrunc hour timestamp 1992-09-20 20 38 40 1992-09-20 20 00 00 dayname timestamp the english name of the weekday dayname timestamp 1992-03-22 sunday epoch timestamp converts a timestamp to seconds since the epoch epoch 2022-11-07 08 43 04 timestamp 1667810584 epoch_ms timestamp converts a timestamp to milliseconds since the epoch epoch_ms 2022-11-07 08 43 04 123456 timestamp 1667810584123 epoch_ms ms converts ms since epoch to a timestamp epoch_ms 701222400000 1992-03-22 00 00 00 epoch_ms timestamp return the total number of milliseconds since the epoch epoch_ms timestamp 2021-08-03 11 59 44 123456 1627991984123 epoch_us timestamp return the total number of microseconds since the epoch epoch_ms timestamp 2021-08-03 11 59 44 123456 1627991984123456 epoch_ns timestamp return the total number of nanoseconds since the epoch epoch_ns timestamp 2021-08-03 11 59 44 123456 1627991984123456000 extract field from timestamp get subfield from a timestamp extract hour from timestamp 1992-09-20 20 38 48 20 greatest timestamp timestamp the later of two timestamps greatest timestamp 1992-09-20 20 38 48 timestamp 1992-03-22 01 02 03 1234 1992-09-20 20 38 48 isfinite timestamp returns true if the timestamp is finite false otherwise isfinite timestamp 1992-03-07 true isinf timestamp returns true if the timestamp is infinite false otherwise isinf timestamp -infinity true last_day timestamp the last day of the month last_day timestamp 1992-03-22 01 02 03 1234 1992-03-31 least timestamp timestamp the earlier of two timestamps least timestamp 1992-09-20 20 38 48 timestamp 1992-03-22 01 02 03 1234 1992-03-22 01 02 03 1234 make_timestamp bigint bigint bigint bigint bigint double the timestamp for the given parts make_timestamp 1992 9 20 13 34 27 123456 1992-09-20 13 34 27 123456 make_timestamp microseconds the timestamp for the given number of \u00b5s since the epoch make_timestamp 1667810584123456 2022-11-07 08 43 04 123456 monthname timestamp the english name of the month monthname timestamp 1992-09-20 september strftime timestamp format converts timestamp to string according to the format string strftime timestamp 1992-01-01 20 38 40 a -d b y - i m s p wed 1 january 1992 - 08 38 40 pm strptime text format converts string to timestamp according to the format string throws on failure strptime wed 1 january 1992 - 08 38 40 pm a -d b y - i m s p 1992-01-01 20 38 40 strptime text format-list converts string to timestamp applying the format strings in the list until one succeeds throws on failure strptime 4 15 2023 10 56 00 d m y h m s m d y h m s 2023-04-15 10 56 00 time_bucket bucket_width timestamp origin truncate timestamp by the specified interval bucket_width buckets are aligned relative to origin timestamp origin defaults to 2000-01-03 00 00 00 for buckets that don t include a month or year interval and to 2000-01-01 00 00 00 for month and year buckets time_bucket interval 2 weeks timestamp 1992-04-20 15 26 00 timestamp 1992-04-01 00 00 00 1992-04-15 00 00 00 time_bucket bucket_width timestamp offset truncate timestamp by the specified interval bucket_width buckets are offset by offset interval time_bucket interval 10 minutes timestamp 1992-04-20 15 26 00-07 interval 5 minutes 1992-04-20 15 25 00 to_timestamp sec converts sec since epoch to a timestamp with time zone to_timestamp 1284352323 5 2010-09-13 04 32 03 5 00 try_strptime text format converts string to timestamp according to the format string returns null on failure try_strptime wed 1 january 1992 - 08 38 40 pm a -d b y - i m s p 1992-01-01 20 38 40 try_strptime text format-list converts string to timestamp applying the format strings in the list until one succeeds returns null on failure try_strptime 4 15 2023 10 56 00 d m y h m s m d y h m s 2023-04-15 10 56 00 there are also dedicated extraction functions to get the subfields functions applied to infinite dates will either return the same infinite dates e g greatest or null e g date_part depending on what makes sense in general if the function needs to examine the parts of the infinite date the result will be null timestamp table functions the table below shows the available table functions for timestamp types function description example --- --- --- generate_series timestamp timestamp interval generate a table of timestamps in the closed range stepping by the interval generate_series timestamp 2001-04-10 timestamp 2001-04-11 interval 30 minute range timestamp timestamp interval generate a table of timestamps in the half open range stepping by the interval range timestamp 2001-04-10 timestamp 2001-04-11 interval 30 minute infinite values are not allowed as table function bounds",
			"category": "Functions",
			"url": "/docs/sql/functions/timestamp",
			"blurb": "This section describes functions and operators for examining and manipulating TIMESTAMP values. Timestamp Operators..."
		},
		{
			"title": "Timestamp Type",
			"text": "timestamps represent points in absolute time usually called instants duckdb represents instants as the number of microseconds \u00b5s since 1970-01-01 00 00 00 00 name aliases description --- --- --- timestamp datetime time of day ignores time zone timestamp with time zone timestamptz time of day uses time zone a timestamp specifies a combination of date year month day and a time hour minute second millisecond timestamps can be created using the timestamp keyword where the data must be formatted according to the iso 8601 format yyyy-mm-dd hh mm ss zzzzzz -tt tt -- 11 30 am at 20 september 1992 gmt select timestamp 1992-09-20 11 30 00 -- 2 30 pm at 20 september 1992 gmt select timestamp 1992-09-20 14 30 00 special values there are also three special date values that can be used on input input string valid types description ------------- ---------------------------------- ----------------------------------------------- epoch timestamp timestamptz 1970-01-01 00 00 00 00 unix system time zero infinity timestamp timestamptz later than all other time stamps -infinity timestamp timestamptz earlier than all other time stamps the values infinity and -infinity are specially represented inside the system and will be displayed unchanged but epoch is simply a notational shorthand that will be converted to the time stamp value when read select -infinity timestamp epoch timestamp infinity timestamp negative epoch positive ---------- ------------------- --------- -infinity 1970-01-01 00 00 00 infinity functions see timestamp functions time zones the timestamptz type can be binned into calendar and clock bins using a suitable extension the built in icu extension implements all the binning and arithmetic functions using the international components for unicode time zone and calendar functions to set the time zone to use first load the icu extension the icu extension comes pre-bundled with several duckdb clients including python r jdbc and odbc so this step can be skipped in those cases in other cases you might first need to install and load the icu extension install icu load icu next use the set timezone command set timezone america los_angeles time binning operations for timestamptz will then be implemented using the given time zone a list of available time zones can be pulled from the pg_timezone_names table function select name abbrev utc_offset from pg_timezone_names order by name you can also find a reference table of available time zones here calendars the icu extension also supports non-gregorian calendars using the set calendar command note that the require icu step is only required if the duckdb client does not bundle the icu extension load icu set calendar japanese time binning operations for timestamptz will then be implemented using the given calendar in this example the era part will now report the japanese imperial era number a list of available calendars can be pulled from the icu_calendar_names table function select name from icu_calendar_names order by 1 settings the current value of the timezone and calendar settings are determined by icu when it starts up they can be looked from in the duckdb_settings table function select from duckdb_settings where name timezone -- america los_angeles select from duckdb_settings where name calendar -- gregorian",
			"category": "Data Types",
			"url": "/docs/sql/data_types/timestamp",
			"blurb": "A timestamp specifies a combination of a date (year, month, day) and a time (hour, minute, second, millisecond)."
		},
		{
			"title": "Timestamp With Time Zone Functions",
			"text": "this section describes functions and operators for examining and manipulating timestamp with time zone values despite the name these values do not store a time zone - just an instant like timestamp instead they request that the instant be binned and formatted using the current time zone time zone support is not built in but can be provided by an extension such as the icu extension that ships with duckdb in the examples below the current time zone is presumed to be america los_angeles using the gregorian calendar built-in timestamp with time zone functions the table below shows the available scalar functions for timestamp with time zone values since these functions do not involve binning or display they are always available function description example result --- --- --- --- current_timestamp current date and time start of current transaction current_timestamp 2022-10-08 12 44 46 122-07 get_current_timestamp current date and time start of current transaction get_current_timestamp 2022-10-08 12 44 46 122-07 greatest timestamptz timestamptz the later of two timestamps greatest timestamptz 1992-09-20 20 38 48 timestamptz 1992-03-22 01 02 03 1234 1992-09-20 20 38 48-07 isfinite timestamptz returns true if the timestamp with time zone is finite false otherwise isfinite timestamptz 1992-03-07 true isinf timestamptz returns true if the timestamp with time zone is infinite false otherwise isinf timestamptz -infinity true least timestamptz timestamptz the earlier of two timestamps least timestamptz 1992-09-20 20 38 48 timestamptz 1992-03-22 01 02 03 1234 1992-03-22 01 02 03 1234-08 now current date and time start of current transaction now 2022-10-08 12 44 46 122-07 transaction_timestamp current date and time start of current transaction transaction_timestamp 2022-10-08 12 44 46 122-07 timestamp with time zone strings with no time zone extension loaded timestamptz values will be cast to and from strings using offset notation this will let you specify an instant correctly without access to time zone information for portability timestamptz values will always be displayed using gmt offsets select 2022-10-08 13 13 34-07 timestamptz -- 2022-10-08 20 13 34 00 if a time zone extension such as icu is loaded then a time zone can be parsed from a string and cast to a representation in the local time zone select 2022-10-08 13 13 34 europe amsterdam timestamptz varchar -- 2022-10-08 04 13 34-07 icu timestamp with time zone operators the table below shows the available mathematical operators for timestamp with time zone values provided by the icu extension operator description example result --- --- --- --- addition of an interval timestamptz 1992-03-22 01 02 03 interval 5 day 1992-03-27 01 02 03 - subtraction of timestamptz s timestamptz 1992-03-27 - timestamptz 1992-03-22 5 days - subtraction of an interval timestamptz 1992-03-27 01 02 03 - interval 5 day 1992-03-22 01 02 03 adding to or subtracting from infinite values produces the same infinite value icu timestamp with time zone functions the table below shows the icu provided scalar functions for timestamp with time zone values function description example result --- --- --- --- age timestamptz timestamptz subtract arguments resulting in the time difference between the two timestamps age timestamptz 2001-04-10 timestamptz 1992-09-20 8 years 6 months 20 days age timestamptz subtract from current_date age timestamp 1992-09-20 29 years 1 month 27 days 12 39 00 844 date_diff part startdate enddate the number of partition boundaries between the timestamps date_diff hour timestamptz 1992-09-30 23 59 59 timestamptz 1992-10-01 01 58 00 2 datediff part startdate enddate alias of date_diff the number of partition boundaries between the timestamps datediff hour timestamptz 1992-09-30 23 59 59 timestamptz 1992-10-01 01 58 00 2 date_part part timestamptz get subfield equivalent to extract date_part minute timestamptz 1992-09-20 20 38 40 38 datepart part timestamptz alias of date_part get subfield equivalent to extract datepart minute timestamptz 1992-09-20 20 38 40 38 date_part part timestamptz get the listed subfields as a struct the list must be constant date_part year month day timestamptz 1992-09-20 20 38 40-07 year 1992 month 9 day 20 datepart part timestamptz alias of date_part get the listed subfields as a struct the list must be constant datepart year month day timestamptz 1992-09-20 20 38 40-07 year 1992 month 9 day 20 date_sub part startdate enddate the number of complete partitions between the timestamps date_sub hour timestamptz 1992-09-30 23 59 59 timestamptz 1992-10-01 01 58 00 1 datesub part startdate enddate alias of date_sub the number of complete partitions between the timestamps datesub hour timestamptz 1992-09-30 23 59 59 timestamptz 1992-10-01 01 58 00 1 date_trunc part timestamptz truncate to specified precision date_trunc hour timestamptz 1992-09-20 20 38 40 1992-09-20 20 00 00 datetrunc part timestamptz alias of date_trunc truncate to specified precision datetrunc hour timestamptz 1992-09-20 20 38 40 1992-09-20 20 00 00 extract field from timestamptz get subfield from a timestamp with time zone extract hour from timestamptz 1992-09-20 20 38 48 20 epoch_ms timestamptz converts a timestamptz to milliseconds since the epoch epoch_ms 2022-11-07 08 43 04 123456 00 timestamptz 1667810584123 epoch_us timestamptz converts a timestamptz to microseconds since the epoch epoch_us 2022-11-07 08 43 04 123456 00 timestamptz 1667810584123456 epoch_ns timestamptz converts a timestamptz to nanoseconds since the epoch epoch_ns 2022-11-07 08 43 04 123456 00 timestamptz 1667810584123456000 last_day timestamptz the last day of the month last_day timestamptz 1992-03-22 01 02 03 1234 1992-03-31 make_timestamptz bigint bigint bigint bigint bigint double the timestamp with time zone for the given parts in the current time zone make_timestamptz 1992 9 20 13 34 27 123456 1992-09-20 13 34 27 123456-07 make_timestamptz microseconds the timestamp with time zone for the given \u00b5s since the epoch make_timestamptz 1667810584123456 2022-11-07 16 43 04 123456-08 make_timestamptz bigint bigint bigint bigint bigint double string the timestamp with time zone for the given parts and time zone make_timestamptz 1992 9 20 15 34 27 123456 cet 1992-09-20 06 34 27 123456-07 strftime timestamptz format converts timestamp with time zone to string according to the format string strftime timestamptz 1992-01-01 20 38 40 a -d b y - i m s p wed 1 january 1992 - 08 38 40 pm strptime text format converts string to timestamp with time zone according to the format string if z is specified strptime wed 1 january 1992 - 08 38 40 pst a -d b y - h m s z 1992-01-01 08 38 40-08 time_bucket bucket_width timestamptz origin truncate timestamptz by the specified interval bucket_width buckets are aligned relative to origin timestamptz origin defaults to 2000-01-03 00 00 00 00 for buckets that don t include a month or year interval and to 2000-01-01 00 00 00 00 for month and year buckets time_bucket interval 2 weeks timestamptz 1992-04-20 15 26 00-07 timestamptz 1992-04-01 00 00 00-07 1992-04-15 00 00 00-07 time_bucket bucket_width timestamptz offset truncate timestamptz by the specified interval bucket_width buckets are offset by offset interval time_bucket interval 10 minutes timestamptz 1992-04-20 15 26 00-07 interval 5 minutes 1992-04-20 15 25 00-07 time_bucket bucket_width timestamptz timezone truncate timestamptz by the specified interval bucket_width bucket starts and ends are calculated using timezone timezone is a varchar and defaults to utc time_bucket interval 2 days timestamptz 1992-04-20 15 26 00-07 europe berlin 1992-04-19 15 00 00-07 there are also dedicated extraction functions to get the subfields icu timestamp table functions the table below shows the available table functions for timestamp with time zone types function description example --- --- --- generate_series timestamptz timestamptz interval generate a table of timestamps in the closed range including both the starting timestamp and the ending timestamp stepping by the interval generate_series timestamptz 2001-04-10 timestamptz 2001-04-11 interval 30 minute range timestamptz timestamptz interval generate a table of timestamps in the half open range including the starting timestamp but stopping before the ending timestamp stepping by the interval range timestamptz 2001-04-10 timestamptz 2001-04-11 interval 30 minute infinite values are not allowed as table function bounds icu timestamp without time zone functions the table below shows the icu provided scalar functions that operate on plain timestamp values these functions assume that the timestamp is a local timestamp a local timestamp is effectively a way of encoding the part values from a time zone into a single value they should be used with caution because the produced values can contain gaps and ambiguities thanks to daylight savings time often the same functionality can be implemented more reliably using the struct variant of the date_part function function description example result --- --- --- --- current_time returns a time whose gmt bin values correspond to local time in the current time zone current_time 08 47 56 497 current_localtimestamp returns a timestamp whose gmt bin values correspond to local date and time in the current time zone current_localtimestamp 2022-12-17 08 47 56 497 localtime synonym for the current_time function call localtime 2022-12-17 08 47 56 497 localtimestamp synonym for the current_localtimestamp function call localtimestamp 2022-12-17 08 47 56 497 timezone text timestamp use the date parts of the timestamp in gmt to construct a timestamp in the given time zone effectively the argument is a local time timezone america denver timestamp 2001-02-16 20 38 40 2001-02-16 19 38 40-08 timezone text timestamptz use the date parts of the timestamp in the given time zone to construct a timestamp effectively the result is a local time timezone america denver timestamptz 2001-02-16 20 38 40-05 2001-02-16 18 38 40 at time zone the at time zone syntax is syntactic sugar for the two argument timezone function listed above timestamp 2001-02-16 20 38 40 at time zone america denver -- 2001-02-16 19 38 40-08 timestamp with time zone 2001-02-16 20 38 40-05 at time zone america denver -- 2001-02-16 18 38 40 infinities functions applied to infinite dates will either return the same infinite dates e g greatest or null e g date_part depending on what makes sense in general if the function needs to examine the parts of the infinite temporal value the result will be null calendars the icu extension also supports non-gregorian calendars if such a calendar is current then the display and binning operations will use that calendar",
			"category": "Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "This section describes functions and operators for examining and manipulating TIMESTAMP WITH TIME ZONE values...."
		},
		{
			"title": "Types API",
			"text": "the duckdbpytype class represents a type instance of our data types converting from other types to make the api as easy to use as possible we have added implicit conversions from existing type objects to a duckdbpytype instance this means that wherever a duckdbpytype object is expected it is also possible to provide any of the options listed below python builtins the table below shows the mapping of python builtin type to duckdb type type duckdb type ---------------------------------------------- ------------------- str varchar int bigint bytearray blob bytes blob float double bool boolean numpy dtypes the table below shows the mapping of numpy dtype to duckdb type type duckdb type ---------------------------------------------- ------------------- bool boolean int8 tinyint int16 smallint int32 integer int64 bigint uint8 utinyint uint16 usmallint uint32 uinteger uint64 ubigint float32 float float64 double nested types list child_type list type objects map to a list type of the child type which can also be arbitrarily nested import duckdb from typing import union duckdb typing duckdbpytype list dict union str int str map union u1 varchar u2 bigint varchar dict key_type value_type dict type objects map to a map type of the key type and the value type import duckdb duckdb typing duckdbpytype dict str int map varchar bigint a field_one b field_two n field_n dict objects map to a struct composed of the keys and values of the dict import duckdb duckdb typing duckdbpytype a str b int struct a varchar b bigint union type_one type_n typing union objects map to a union type of the provided types import duckdb from typing import union duckdb typing duckdbpytype union int str bool bytearray union u1 bigint u2 varchar u3 boolean u4 blob creation functions for the builtin types you can use the constants defined in duckdb typing duckdb type ------------------- sqlnull boolean tinyint utinyint smallint usmallint integer uinteger bigint ubigint hugeint uuid float double date timestamp timestamp_ms timestamp_ns timestamp_s time time_tz timestamp_tz varchar blob bit interval for the complex types there are methods available on the duckdbpyconnection object or the duckdb module anywhere a duckdbpytype is accepted we will also accept one of the type objects that can implicitly convert to a duckdbpytype list_type array_type parameters child_type duckdbpytype struct_type row_type parameters fields union list duckdbpytype dict str duckdbpytype map_type parameters key_type duckdbpytype value_type duckdbpytype decimal_type parameters width int scale int union_type parameters members union list duckdbpytype dict str duckdbpytype string_type parameters collation optional str",
			"category": "Python",
			"url": "/docs/api/python/types",
			"blurb": "The DuckDBPyType class represents a type instance of our data types . Converting from other types To make the API as..."
		},
		{
			"title": "UNNEST",
			"text": "examples -- unnest a list generating 3 rows 1 2 3 select unnest 1 2 3 -- unnesting a struct generating two columns a b select unnest a 42 b 84 -- recursive unnest of a list of structs select unnest a 42 b 84 a 100 b null recursive true the unnest function is used to unnest lists or structs by one level the function can be used as a regular scalar function but only in the select clause unnest with the recursive parameter will unnest lists and structs of multiple levels unnesting lists -- unnest a list generating 3 rows 1 2 3 select unnest 1 2 3 -- unnest a scalar list generating 3 rows 1 10 2 11 3 null select unnest 1 2 3 unnest 10 11 -- unnest a scalar list generating 3 rows 1 10 2 10 3 10 select unnest 1 2 3 10 -- unnest a list column generated from a subquery select unnest l 10 from values 1 2 3 4 5 tbl l -- empty result select unnest -- empty result select unnest null unnest on a list will emit one tuple per entry in the list when unnest is combined with regular scalar expressions those expressions are repeated for every entry in the list when multiple lists are unnested in the same select clause the lists are unnested side-by-side if one list is longer than the other the shorter list will be padded with null values an empty list and a null list will both unnest to zero elements unnesting structs -- unnesting a struct generating two columns a b select unnest a 42 b 84 -- unnesting a struct generating two columns a b select unnest a 42 b x 84 unnest on a struct will emit one column per entry in the struct recursive unnest -- unnesting a list of lists recursively generating 5 rows 1 2 3 4 5 select unnest 1 2 3 4 5 recursive true -- unnesting a list of structs recursively generating two rows of two columns a b select unnest a 42 b 84 a 100 b null recursive true -- unnesting a struct generating two columns a b select unnest a 1 2 3 b 88 recursive true calling unnest with the recursive setting will fully unnest lists followed by fully unnesting structs this can be useful to fully flatten columns that contain lists within lists or lists of structs note that lists within structs are not unnested",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/unnest",
			"blurb": "Examples -- unnest a list, generating 3 rows (1, 2, 3) SELECT UNNEST([1, 2, 3]); -- unnesting a struct, generating..."
		},
		{
			"title": "Union",
			"text": "union data type a union type not to be confused with the sql union operator is a nested type capable of holding one of multiple alternative values much like the union in c the main difference being that these union types are tagged unions and thus always carry a discriminator tag which signals which alternative it is currently holding even if the inner value itself is null union types are thus more similar to c 17 s std variant rust s enum or the sum type present in most functional languages union types must always have at least one member and while they can contain multiple members of the same type the tag names must be unique union types can have at most 256 members under the hood union types are implemented on top of struct types and simply keep the tag as the first entry union values can be created with the union_value tag expr function or by casting from a member type example -- create a table with a union column create table tbl1 u union num int str varchar -- any type can be implicitly cast to a union containing the type -- any union can also be implicitly cast to a another union if -- the source union members are a subset of the targets -- note only if the cast is unambiguous -- more details in the union casts section below insert into tbl1 values 1 two union_value str three -- union use the member types varchar cast functions when casting to varchar select u from tbl1 -- returns -- 1 -- two -- three -- select all the str members select union_extract u str from tbl1 -- alternatively you can use dot syntax like with structs select u str from tbl1 -- returns -- null -- two -- three -- select the currently active tag from the union as an enum select union_tag u from tbl1 -- returns -- num -- str -- str union casts compared to other nested types union s allow a set of implicit casts to facilitate unintrusive and natural usage when working with their members as subtypes however these casts have been designed with two principles in mind to avoid ambiguity and to avoid casts that could lead to loss of information this prevents union s from being completely transparent while still allowing union types to have a supertype relationship with their members thus union types can t be implicitly cast to any of their member types in general since the information in the other members not matching the target type would be lost if you want to coerce a union into one of its members you should use the union_extract function explicitly instead the only exception to this is when casting a union to varchar in which case the members will all use their corresponding varchar casts since everything can be cast to varchar this is safe in a sense casting to unions a type can always be implicitly cast to a union if it can be implicitly cast to one of the union member types if there are multiple candidates the built in implicit casting priority rules determine the target type for example a float - union i int v varchar cast will always cast the float to the int member before varchar if the cast still is ambiguous i e there are multiple candidates with the same implicit casting priority an error is raised this usually happens when the union contains multiple members of the same type e g a float - union i int num int is always ambiguous so how do we disambiguate if we want to create a union with multiple members of the same type by using the union_value function which takes a keyword argument specifying the tag for example union_value num 2 int will create a union with a single member of type int with the tag num this can then be used to disambiguate in an explicit or implicit read on below union to union cast like cast union_value b 2 as union a int b int casting between unions union types can be cast between each other if the source type is a subset of the target type in other words all the tags in the source union must be present in the target union and all the types of the matching tags must be implicitly castable between source and target in essence this means that union types are covariant with respect to their members ok source target comments ---- --------------------- ----------------------- ------------------------------------ union a a b b union a a b b c c union a a b b union a a b c if b can be implicitly cast to c union a a b b c c union a a b b union a a b b union a a b c if b can t be implicitly cast to c union a b d union a b c comparison and sorting since union types are implemented on top of struct types internally they can be used with all the comparison operators as well as in both where and having clauses with the same semantics as struct s the tag is always stored as the first struct entry which ensures that the union types are compared and ordered by tag first functions see nested functions",
			"category": "Data Types",
			"url": "/docs/sql/data_types/union",
			"blurb": "Union Data Type A UNION type (not to be confused with the SQL UNION operator ) is a nested type capable of holding..."
		},
		{
			"title": "Unpivot Statement",
			"text": "the unpivot statement allows multiple columns to be stacked into fewer columns in the basic case multiple columns are stacked into two columns a name column which contains the name of the source column and a value column which contains the value from the source column duckdb implements both the sql standard unpivot syntax and a simplified unpivot syntax both can utilize a columns expression to automatically detect the columns to unpivot pivot_longer may also be used in place of the unpivot keyword simplified unpivot syntax the full syntax diagram is below but the simplified unpivot syntax can be summarized using spreadsheet pivot table naming conventions as unpivot dataset on column s into name name-column-name value value-column-name s example data all examples use the dataset produced by the queries below create or replace table monthly_sales empid int dept text jan int feb int mar int apr int may int jun int insert into monthly_sales values 1 electronics 1 2 3 4 5 6 2 clothes 10 20 30 40 50 60 3 cars 100 200 300 400 500 600 from monthly_sales empid dept jan feb mar apr may jun ------- ------------- ----- ----- ----- ----- ----- ----- 1 electronics 1 2 3 4 5 6 2 clothes 10 20 30 40 50 60 3 cars 100 200 300 400 500 600 unpivot manually the most typical unpivot transformation is to take already pivoted data and re-stack it into a column each for the name and value in this case all months will be stacked into a month column and a sales column unpivot monthly_sales on jan feb mar apr may jun into name month value sales empid dept month sales ------- ------------- ------- ------- 1 electronics jan 1 1 electronics feb 2 1 electronics mar 3 1 electronics apr 4 1 electronics may 5 1 electronics jun 6 2 clothes jan 10 2 clothes feb 20 2 clothes mar 30 2 clothes apr 40 2 clothes may 50 2 clothes jun 60 3 cars jan 100 3 cars feb 200 3 cars mar 300 3 cars apr 400 3 cars may 500 3 cars jun 600 unpivot dynamically using columns expression in many cases the number of columns to unpivot is not easy to predetermine ahead of time in the case of this dataset the query above would have to change each time a new month is added the columns expression can be used to select all columns that are not empid or dept this enables dynamic unpivoting that will work regardless of how many months are added the query below returns identical results to the one above unpivot monthly_sales on columns exclude empid dept into name month value sales empid dept month sales ------- ------------- ------- ------- 1 electronics jan 1 1 electronics feb 2 1 electronics mar 3 1 electronics apr 4 1 electronics may 5 1 electronics jun 6 2 clothes jan 10 2 clothes feb 20 2 clothes mar 30 2 clothes apr 40 2 clothes may 50 2 clothes jun 60 3 cars jan 100 3 cars feb 200 3 cars mar 300 3 cars apr 400 3 cars may 500 3 cars jun 600 unpivot into multiple value columns the unpivot statement has additional flexibility more than 2 destination columns are supported this can be useful when the goal is to reduce the extent to which a dataset is pivoted but not completely stack all pivoted columns to demonstrate this the query below will generate a dataset with a separate column for the number of each month within the quarter month 1 2 or 3 and a separate row for each quarter since there are fewer quarters than months this does make the dataset longer but not as long as the above to accomplish this multiple sets of columns are included in the on clause the q1 and q2 aliases are optional the number of columns in each set of columns in the on clause must match the number of columns in the value clause unpivot monthly_sales on jan feb mar as q1 apr may jun as q2 into name quarter value month_1_sales month_2_sales month_3_sales empid dept quarter month_1_sales month_2_sales month_3_sales ------- ------------- --------- --------------- --------------- --------------- 1 electronics q1 1 2 3 1 electronics q2 4 5 6 2 clothes q1 10 20 30 2 clothes q2 40 50 60 3 cars q1 100 200 300 3 cars q2 400 500 600 using unpivot within a select statement the unpivot statement may be included within a select statement as a cte a common table expression or with clause or a subquery this allows for an unpivot to be used alongside other sql logic as well as for multiple unpivot s to be used in one query no select is needed within the cte the unpivot keyword can be thought of as taking its place with unpivot_alias as unpivot monthly_sales on columns exclude empid dept into name month value sales select from unpivot_alias an unpivot may be used in a subquery and must be wrapped in parentheses note that this behavior is different than the sql standard unpivot as illustrated in subsequent examples select from unpivot monthly_sales on columns exclude empid dept into name month value sales unpivot_alias internals unpivoting is implemented entirely as rewrites into sql queries each unpivot is implemented as set of unnest functions operating on a list of the column names and a list of the column values if dynamically unpivoting the columns expression is evaluated first to calculate the column list for example unpivot monthly_sales on jan feb mar apr may jun into name month value sales is translated into select empid dept unnest jan feb mar apr may jun as month unnest jan feb mar apr may jun as sales from monthly_sales note the single quotes to build a list of text strings to populate month and the double quotes to pull the column values for use in sales this produces the same result as the initial example empid dept month sales ------- ------------- ------- ------- 1 electronics jan 1 1 electronics feb 2 1 electronics mar 3 1 electronics apr 4 1 electronics may 5 1 electronics jun 6 2 clothes jan 10 2 clothes feb 20 2 clothes mar 30 2 clothes apr 40 2 clothes may 50 2 clothes jun 60 3 cars jan 100 3 cars feb 200 3 cars mar 300 3 cars apr 400 3 cars may 500 3 cars jun 600 simplified unpivot full syntax diagram below is the full syntax diagram of the unpivot statement sql standard unpivot syntax the full syntax diagram is below but the sql standard unpivot syntax can be summarized as from dataset unpivot include nulls value-column-name s for name-column-name in column s note that only one column can be included in the name-column-name expression sql standard unpivot manually to complete the basic unpivot operation using the sql standard syntax only a few additions are needed from monthly_sales unpivot sales for month in jan feb mar apr may jun empid dept month sales ------- ------------- ------- ------- 1 electronics jan 1 1 electronics feb 2 1 electronics mar 3 1 electronics apr 4 1 electronics may 5 1 electronics jun 6 2 clothes jan 10 2 clothes feb 20 2 clothes mar 30 2 clothes apr 40 2 clothes may 50 2 clothes jun 60 3 cars jan 100 3 cars feb 200 3 cars mar 300 3 cars apr 400 3 cars may 500 3 cars jun 600 sql standard unpivot dynamically using columns expression the columns expression can be used to determine the in list of columns dynamically this will continue to work even if additional month columns are added to the dataset it produces the same result as the query above from monthly_sales unpivot sales for month in columns exclude empid dept sql standard unpivot into multiple value columns the unpivot statement has additional flexibility more than 2 destination columns are supported this can be useful when the goal is to reduce the extent to which a dataset is pivoted but not completely stack all pivoted columns to demonstrate this the query below will generate a dataset with a separate column for the number of each month within the quarter month 1 2 or 3 and a separate row for each quarter since there are fewer quarters than months this does make the dataset longer but not as long as the above to accomplish this multiple columns are included in the value-column-name portion of the unpivot statement multiple sets of columns are included in the in clause the q1 and q2 aliases are optional the number of columns in each set of columns in the in clause must match the number of columns in the value-column-name portion from monthly_sales unpivot month_1_sales month_2_sales month_3_sales for quarter in jan feb mar as q1 apr may jun as q2 empid dept quarter month_1_sales month_2_sales month_3_sales ------- ------------- --------- --------------- --------------- --------------- 1 electronics q1 1 2 3 1 electronics q2 4 5 6 2 clothes q1 10 20 30 2 clothes q2 40 50 60 3 cars q1 100 200 300 3 cars q2 400 500 600 sql standard unpivot full syntax diagram below is the full syntax diagram of the sql standard version of the unpivot statement",
			"category": "Statements",
			"url": "/docs/sql/statements/unpivot",
			"blurb": "The UNPIVOT statement allows columns to be stacked into rows that indicate the prior column name and value."
		},
		{
			"title": "Update Statement",
			"text": "the update statement modifies the values of rows in a table examples -- for every row where i is null set the value to 0 instead update tbl set i 0 where i is null -- set all values of i to 1 and all values of j to 2 update tbl set i 1 j 2 syntax update changes the values of the specified columns in all rows that satisfy the condition only the columns to be modified need be mentioned in the set clause columns not explicitly modified retain their previous values update from other table a table can be updated based upon values from another table this can be done by specifying a table in a from clause or using a sub-select statement both approaches have the benefit of completing the update operation in bulk for increased performance create or replace table original as select 1 as key original value as value union all select 2 as key original value 2 as value create or replace table new as select 1 as key new value as value union all select 2 as key new value 2 as value select from original key value ----- ------------------ 1 original value 2 original value 2 update original set value new value from new where original key new key -- or update original set value select new value from new where original key new key select from original key value ----- ------------- 1 new value 2 new value 2 update from same table the only difference between this case and the above is that a different table alias must be specified on both the target table and the source table in this example as true_original and as new are both required update original as true_original set value select new value a change as value from original as new where true_original key new key upsert insert or update see the insert documentation for details",
			"category": "Statements",
			"url": "/docs/sql/statements/update",
			"blurb": "The UPDATE statement modifies the values of rows in a table. Examples -- for every row where i is NULL, set the value..."
		},
		{
			"title": "Use",
			"text": "the use statement selects a database and optional schema to use as the default examples --- sets the memory database as the default use memory --- sets the duck main database and schema as the default use duck main syntax the use statement sets a default database or database schema combination to use for future operations for instance tables created without providing a fully qualified table name will be created in the default database",
			"category": "Statements",
			"url": "/docs/sql/statements/use",
			"blurb": "The USE statement selects a database and optional schema to use as the default. Examples --- Sets the 'memory'..."
		},
		{
			"title": "Utility Functions",
			"text": "utility functions the functions below are difficult to categorize into specific function types and are broadly useful function description example result ---------------------------------- --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- --------------------------------------- -------------------------------------- alias column return the name of the column alias column1 column1 checkpoint database synchronize wal with file for optional database without interrupting transactions checkpoint my_db success boolean coalesce expr return the first expression that evaluates to a non- null value accepts 1 or more parameters each expression can be a column literal value function result or many others coalesce null null default_string default_string ifnull expr other a two-argument version of coalesce ifnull null default_string default_string nullif a b return null if a b else return a equivalent to case when a b then null else a end nullif 1 1 2 null current_schema return the name of the currently active schema default is main current_schema main current_schemas boolean return list of schemas pass a parameter of true to include implicit schemas current_schemas true temp main pg_catalog current_setting setting_name return the current value of the configuration setting current_setting access_mode automatic currval sequence_name return the current value of the sequence note that nextval must be called at least once prior to calling currval currval my_sequence_name 1 force_checkpoint database synchronize wal with file for optional database interrupting transactions force_checkpoint my_db success boolean gen_random_uuid alias of uuid return a random uuid similar to this eeccb8c5-9943-b2bb-bb5e-222f4e14b687 gen_random_uuid various hash value returns an integer with the hash of the value hash 2595805878642663834 icu_sort_key string collator surrogate key used to sort special characters according to the specific locale collator parameter is optional valid only when icu extension is installed icu_sort_key \u00f6 de 460145960106 md5 string return an md5 one-way hash of the string md5 123 202cb962ac59075b964b07152d234b70 nextval sequence_name return the following value of the sequence nextval my_sequence_name 2 pg_typeof expression returns the lower case name of the data type of the result of the expression for postgres compatibility pg_typeof abc varchar stats expression returns a string with statistics about the expression expression can be a column constant or sql expression stats 5 min 5 max 5 has null false txid_current returns the current transaction s id a bigint it will assign a new one if the current transaction does not have one already txid_current various typeof expression returns the name of the data type of the result of the expression typeof abc varchar uuid return a random uuid similar to this eeccb8c5-9943-b2bb-bb5e-222f4e14b687 uuid various version return the currently active version of duckdb in this format v0 3 2 version various utility table functions a table function is used in place of a table in a from clause function description example --- --- --- glob search_path return filenames found at the location indicated by the search_path in a single column named file the search_path may contain glob pattern matching syntax glob",
			"category": "Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "Utility Functions The functions below are difficult to categorize into specific function types and are broadly..."
		},
		{
			"title": "VALUES Clause",
			"text": "the values clause is used to specify a fixed number of rows the values clause can be used as a stand-alone statement as part of the from clause or as input to an insert into statement examples -- generate two rows and directly return them values amsterdam 1 london 2 -- generate two rows as part of a from clause and rename the columns select from values amsterdam 1 london 2 cities name id -- generate two rows and insert them into a table insert into cities values amsterdam 1 london 2 -- create a table directly from a values clause create table cities as select from values amsterdam 1 london 2 cities name id syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/values",
			"blurb": "The VALUES clause is used to specify a fixed number of rows. The VALUES clause can be used as a stand-alone..."
		},
		{
			"title": "Vacuum",
			"text": "the vacuum statement is primarily in place for postgresql compatibility examples -- no-op vacuum -- rebuild database statistics vacuum analyze -- rebuild statistics for the table column vacuum analyze memory main my_table my_column syntax the vacuum statement alone does nothing vacuum analyze will recompute table statistics if they have become stale due to table updates or deletions",
			"category": "Statements",
			"url": "/docs/sql/statements/vacuum",
			"blurb": "The VACUUM statement is primarily in place for PostgreSQL compatibility. Examples -- No-op. VACUUM; -- Rebuild..."
		},
		{
			"title": "WHERE Clause",
			"text": "the where clause specifies any filters to apply to the data this allows you to select only a subset of the data in which you are interested logically the where clause is applied immediately after the from clause examples -- select all rows that have id equal to 3 select from table_name where id 3 -- select all rows that match the given case-insensitive like expression select from table_name where name ilike mark -- select all rows that match the given composite expression select from table_name where id 3 or id 7 syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/where",
			"blurb": "The WHERE clause specifies any filters to apply to the data. This allows you to select only a subset of the data in..."
		},
		{
			"title": "WINDOW Clause",
			"text": "the window clause allows you to specify named windows that can be used within window functions these are useful when you have multiple window functions as they allow you to avoid repeating the same window clause syntax",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/window",
			"blurb": "The WINDOW clause allows you to specify named windows that can be used within window functions. These are useful when..."
		},
		{
			"title": "WITH Clause",
			"text": "the with clause allows you to specify common table expressions ctes regular non-recursive common-table-expressions are essentially views that are limited in scope to a particular query ctes can reference each-other and can be nested basic cte examples -- create a cte called cte and use it in the main query with cte as select 42 as x select from cte x 42 -- create two ctes where the second cte references the first cte with cte as select 42 as i cte2 as select i 100 as x from cte select from cte2 x 4200 recursive cte examples tree traversal with recursive can be used to traverse trees for example take a hierarchy of tags create table tag id int name varchar subclassof int insert into tag values 1 u2 5 2 blur 5 3 oasis 5 4 2pac 6 5 rock 7 6 rap 7 7 music 9 8 movies 9 9 art null the following query returns the path from the node oasis to the root of the tree art with recursive tag_hierarchy id source path as select id name name as path from tag where subclassof is null union all select tag id tag name list_prepend tag name tag_hierarchy path from tag tag_hierarchy where tag subclassof tag_hierarchy id select path from tag_hierarchy where source oasis path oasis rock music art graph traversal the with recursive clause can be used to express graph traversal on arbitrary graphs however if the graph has cycles the query must perform cycle detection to prevent infinite loops one way to achieve this is to store the path of a traversal in a list and before extending the path with a new edge check whether its endpoint has been visited before see the example later take the following directed graph from the ldbc graphalytics benchmark create table edge node1id int node2id int insert into edge values 1 3 1 5 2 4 2 5 2 10 3 1 3 5 3 8 3 10 5 3 5 4 5 8 6 3 6 4 7 4 8 1 9 4 note that the graph contains directed cycles e g between nodes 1 2 and 5 enumerate all paths from a node the following query returns all paths starting in node 1 with recursive paths startnode endnode path as select -- define the path as the first edge of the traversal node1id as startnode node2id as endnode node1id node2id as path from edge where startnode 1 union all select -- concatenate new edge to the path paths startnode as startnode node2id as endnode array_append path node2id as path from paths join edge on paths endnode node1id -- prevent adding a repeated node to the path -- this ensures that no cycles occur where node2id all paths path select startnode endnode path from paths order by length path path startnode endnode path 1 3 1 3 1 5 1 5 1 5 1 3 5 1 8 1 3 8 1 10 1 3 10 1 3 1 5 3 1 4 1 5 4 1 8 1 5 8 1 4 1 3 5 4 1 8 1 3 5 8 1 8 1 5 3 8 1 10 1 5 3 10 note that the result of this query is not restricted to shortest paths e g for node 5 the results include paths 1 5 and 1 3 5 enumerate unweighted shortest paths from a node in most cases enumerating all paths is not practical or feasible instead only the unweighted shortest paths are of interest to find these the second half of the with recursive query should be adjusted such that it only includes a node if it has not yet been visited this is implemented by using a subquery that checks if any of the previous paths includes the node with recursive paths startnode endnode path as select -- define the path as the first edge of the traversal node1id as startnode node2id as endnode node1id node2id as path from edge where startnode 1 union all select -- concatenate new edge to the path paths startnode as startnode node2id as endnode array_append path node2id as path from paths join edge on paths endnode node1id -- prevent adding a node that was visited previously by any path -- this ensures that 1 no cycles occur and 2 only nodes that -- were not visited by previous shorter paths are added to a path where not exists select 1 from paths previous_paths where list_contains previous_paths path node2id select startnode endnode path from paths order by length path path startnode endnode path 1 3 1 3 1 5 1 5 1 8 1 3 8 1 10 1 3 10 1 4 1 5 4 1 8 1 5 8 enumerate unweighted shortest paths between two nodes with recursive can also be used to find all unweighted shortest paths between two nodes to ensure that the recursive query is stopped as soon as we reach the end node we use a window function which checks whether the end node is among the newly added nodes the following query returns all unweighted shortest paths between nodes 1 start node and 8 end node with recursive paths startnode endnode path endreached as select -- define the path as the first edge of the traversal node1id as startnode node2id as endnode node1id node2id as path node2id 8 as endreached from edge where startnode 1 union all select -- concatenate new edge to the path paths startnode as startnode node2id as endnode array_append path node2id as path max case when node2id 8 then 1 else 0 end over rows between unbounded preceding and unbounded following as endreached from paths join edge on paths endnode node1id where not exists select 1 from paths previous_paths where list_contains previous_paths path node2id and paths endreached 0 select startnode endnode path from paths where endnode 8 order by length path path startnode endnode path 1 8 1 3 8 1 8 1 5 8 common table expressions",
			"category": "Query Syntax",
			"url": "/docs/sql/query_syntax/with",
			"blurb": "The WITH clause allows you to specify common table expressions (CTEs). Regular (non-recursive) common-table-..."
		},
		{
			"title": "Window Functions",
			"text": "examples -- generate a row_number column containing incremental identifiers for each row select row_number over from sales -- generate a row_number column by order of time select row_number over order by time from sales -- generate a row_number column by order of time partitioned by region select row_number over partition by region order by time from sales -- compute the difference between the current amount and the previous amount by order of time select amount - lag amount over order by time from sales -- compute the percentage of the total amount of sales per region for each row select amount sum amount over partition by region from sales syntax window functions can only be used in the select clause to share over specifications between functions use the statement s window clause and use the over window-name syntax general-purpose window functions the table below shows the available general window functions function return type description example --- --- --- --- row_number bigint the number of the current row within the partition counting from 1 row_number rank bigint the rank of the current row with gaps same as row_number of its first peer rank dense_rank bigint the rank of the current row without gaps this function counts peer groups dense_rank rank_dense bigint alias for dense_rank rank_dense percent_rank double the relative rank of the current row rank - 1 total partition rows - 1 percent_rank cume_dist double the cumulative distribution number of partition rows preceding or peer with current row total partition rows cume_dist ntile num_buckets integer bigint an integer ranging from 1 to the argument value dividing the partition as equally as possible ntile 4 lag expr any offset integer default any same type as expr returns expr evaluated at the row that is offset rows before the current row within the partition if there is no such row instead return default which must be of the same type as expr both offset and default are evaluated with respect to the current row if omitted offset defaults to 1 and default to null lag column 3 0 lead expr any offset integer default any same type as expr returns expr evaluated at the row that is offset rows after the current row within the partition if there is no such row instead return default which must be of the same type as expr both offset and default are evaluated with respect to the current row if omitted offset defaults to 1 and default to null lead column 3 0 first_value expr any same type as expr returns expr evaluated at the row that is the first row of the window frame first_value column last_value expr any same type as expr returns expr evaluated at the row that is the last row of the window frame last_value column nth_value expr any nth integer same type as expr returns expr evaluated at the nth row of the window frame counting from 1 null if no such row nth_value column 2 first expr any same type as expr alias for first_value first column last expr any same type as expr alias for last_value last column aggregate window functions all aggregate functions can be used in a windowing context ignoring nulls the following functions support the ignore nulls specification function description example --- --- --- lag expr any offset integer default any skips null values when counting lag column 3 ignore nulls lead expr any offset integer default any skips null values when counting lead column 3 ignore nulls first_value expr any skips leading null s first_value column ignore nulls last_value expr any skips trailing null s last_value column ignore nulls nth_value expr any nth integer skips null values when counting nth_value column 2 ignore nulls note that there is no comma separating the arguments from the ignore nulls specification the inverse of ignore nulls is respect nulls which is the default for all functions evaluation windowing works by breaking a relation up into independent partitions ordering those partitions and then computing a new column for each row as a function of the nearby values some window functions depend only on the partition boundary and the ordering but a few including all the aggregates also use a frame frames are specified as a number of rows on either side preceding or following of the current row the distance can either be specified as a number of rows or a range of values using the partition s ordering value and a distance the full syntax is shown in the diagram at the top of the page and this diagram visually illustrates computation environment partition and ordering partitioning breaks the relation up into independent unrelated pieces partitioning is optional and if none is specified then the entire relation is treated as a single partition window functions cannot access values outside of the partition containing the row they are being evaluated at ordering is also optional but without it the results are not well-defined each partition is ordered using the same ordering clause here is a table of power generation data after partitioning by plant and ordering by date it will have this layout plant date mwh --- --- --- boston 2019-01-02 564337 boston 2019-01-03 507405 boston 2019-01-04 528523 boston 2019-01-05 469538 boston 2019-01-06 474163 boston 2019-01-07 507213 boston 2019-01-08 613040 boston 2019-01-09 582588 boston 2019-01-10 499506 boston 2019-01-11 482014 boston 2019-01-12 486134 boston 2019-01-13 531518 worcester 2019-01-02 118860 worcester 2019-01-03 101977 worcester 2019-01-04 106054 worcester 2019-01-05 92182 worcester 2019-01-06 94492 worcester 2019-01-07 99932 worcester 2019-01-08 118854 worcester 2019-01-09 113506 worcester 2019-01-10 96644 worcester 2019-01-11 93806 worcester 2019-01-12 98963 worcester 2019-01-13 107170 in what follows we shall use this table or small sections of it to illustrate various pieces of window function evaluation the simplest window function is row_number this function just computes the 1-based row number within the partition using the query select plant date row_number over partition by plant order by date as row from history order by 1 2 the result will be plant date row --- --- --- boston 2019-01-02 1 boston 2019-01-03 2 boston 2019-01-04 3 worcester 2019-01-02 1 worcester 2019-01-03 2 worcester 2019-01-04 3 note that even though the function is computed with an order by clause the result does not have to be sorted so the select also needs to be explicitly sorted if that is desired framing framing specifies a set of rows relative to each row where the function is evaluated the distance from the current row is given as an expression either preceding or following the current row this distance can either be specified as an integral number of rows or as a range delta expression from the value of the ordering expression for a range specification there must be only one ordering expression and it has to support addition and subtraction i e numbers or interval s the default values for frames are from unbounded preceding to current row it is invalid for a frame to start after it ends row framing here is a simple row frame query using an aggregate function select points sum points over rows between 1 preceding and 1 following we from results this query computes the sum of each point and the points on either side of it notice that at the edge of the partition there are only two values added together this is because frames are cropped to the edge of the partition range framing returning to the power data suppose the data is noisy we might want to compute a 7 day moving average for each plant to smooth out the noise to do this we can use this window query select plant date avg mwh over partition by plant order by date asc range between interval 3 days preceding and interval 3 days following as mwh 7-day moving average from generation history order by 1 2 this query partitions the data by plant to keep the different power plants data separate orders each plant s partition by date to put the energy measurements next to each other and uses a range frame of three days on either side of each day for the avg to handle any missing days this is the result plant date mwh 7-day br moving average --- --- --- boston 2019-01-02 517450 75 boston 2019-01-03 508793 20 boston 2019-01-04 508529 83 boston 2019-01-13 499793 00 worcester 2019-01-02 104768 25 worcester 2019-01-03 102713 00 worcester 2019-01-04 102249 50 window clauses multiple different over clauses can be specified in the same select and each will be computed separately often however we want to use the same layout for multiple window functions the window clause can be used to define a named window that can be shared between multiple window functions select plant date min mwh over seven as mwh 7-day moving minimum avg mwh over seven as mwh 7-day moving average max mwh over seven as mwh 7-day moving maximum from generation history window seven as partition by plant order by date asc range between interval 3 days preceding and interval 3 days following order by 1 2 the three window functions will also share the data layout which will improve performance multiple windows can be defined in the same window clause by comma-separating them select plant date min mwh over seven as mwh 7-day moving minimum avg mwh over seven as mwh 7-day moving average max mwh over seven as mwh 7-day moving maximum min mwh over three as mwh 3-day moving minimum avg mwh over three as mwh 3-day moving average max mwh over three as mwh 3-day moving maximum from generation history window seven as partition by plant order by date asc range between interval 3 days preceding and interval 3 days following three as partition by plant order by date asc range between interval 1 days preceding and interval 1 days following order by 1 2 the queries above do not use a number of clauses commonly found in select statements like where group by etc for more complex queries you can find where window clauses fall in the canonical order of a select statement here box and whisker queries all aggregates can be used as windowing functions including the complex statistical functions these function implementations have been optimised for windowing and we can use the window syntax to write queries that generate the data for moving box-and-whisker plots select plant date min mwh over seven as mwh 7-day moving minimum quantile_cont mwh 0 25 0 5 0 75 over seven as mwh 7-day moving iqr max mwh over seven as mwh 7-day moving maximum from generation history window seven as partition by plant order by date asc range between interval 3 days preceding and interval 3 days following order by 1 2",
			"category": "SQL",
			"url": "/docs/sql/window_functions",
			"blurb": "Examples -- generate a row_number column containing incremental identifiers for each row SELECT row_number() OVER ()..."
		},
		{
			"title": "`",
			"text": "` - `",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "` - `"
		},
		{
			"title": "`\\",
			"text": "`\\ - \\",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "`\\ - \\"
		},
		{
			"title": "abs",
			"text": "abs(x) - absolute value",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "abs(x) - absolute value"
		},
		{
			"title": "acos",
			"text": "acos(x) - computes the arccosine of x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "acos(x) - computes the arccosine of x"
		},
		{
			"title": "age",
			"text": "age(timestamptz) - subtract from current_date",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "age(timestamptz) - Subtract from current_date"
		},
		{
			"title": "array_extract",
			"text": "array_extract(list, index) - extract a single character using a (1-based) index.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "array_extract(list, index) - Extract a single character using a (1-based) index."
		},
		{
			"title": "array_pop_back",
			"text": "array_pop_back(list) - returns the list without the last element.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "array_pop_back(list) - Returns the list without the last element."
		},
		{
			"title": "array_pop_front",
			"text": "array_pop_front(list) - returns the list without the first element.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "array_pop_front(list) - Returns the list without the first element."
		},
		{
			"title": "array_slice",
			"text": "array_slice(list, begin, end) - extract a string using slice conventions. null s are interpreted as the bounds of the string. negative values are accepted.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "array_slice(list, begin, end) - Extract a string using slice conventions. NULL s are interpreted as the bounds of the..."
		},
		{
			"title": "ascii",
			"text": "ascii(string) - returns an integer that represents the unicode code point of the first character of the string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "ascii(string) - Returns an integer that represents the Unicode code point of the first character of the string"
		},
		{
			"title": "asin",
			"text": "asin(x) - computes the arcsine of x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "asin(x) - computes the arcsine of x"
		},
		{
			"title": "atan",
			"text": "atan(x) - computes the arctangent of x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "atan(x) - computes the arctangent of x"
		},
		{
			"title": "atan2",
			"text": "atan2(y, x) - computes the arctangent (y, x)",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "atan2(y, x) - computes the arctangent (y, x)"
		},
		{
			"title": "bar",
			"text": "bar(x, min, max [, width ]) - draw a band whose width is proportional to ( x - min ) and equal to width characters when x = max. width defaults to 80.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "bar(x, min, max [, width ]) - Draw a band whose width is proportional to ( x - min ) and equal to width characters..."
		},
		{
			"title": "bit_count",
			"text": "bit_count(x) - returns the number of bits that are set",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "bit_count(x) - returns the number of bits that are set"
		},
		{
			"title": "bit_length",
			"text": "bit_length(string) - number of bits in a string.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "bit_length(string) - Number of bits in a string."
		},
		{
			"title": "bit_position",
			"text": "bit_position(substring, bitstring) - returns first starting index of the specified substring within bits, or zero if it's not present. the first (leftmost) bit is indexed 1",
			"category": "Bitstring Functions",
			"url": "/docs/sql/functions/bitstring",
			"blurb": "bit_position(substring, bitstring) - Returns first starting index of the specified substring within bits, or zero if..."
		},
		{
			"title": "bitstring",
			"text": "bitstring(bitstring, length) - returns a bitstring of determined length.",
			"category": "Bitstring Functions",
			"url": "/docs/sql/functions/bitstring",
			"blurb": "bitstring(bitstring, length) - Returns a bitstring of determined length."
		},
		{
			"title": "cardinality",
			"text": "cardinality(map) - return the size of the map (or the number of entries in the map).",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "cardinality(map) - Return the size of the map (or the number of entries in the map)."
		},
		{
			"title": "cbrt",
			"text": "cbrt(x) - returns the cube root of the number",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "cbrt(x) - returns the cube root of the number"
		},
		{
			"title": "ceil",
			"text": "ceil(x) - rounds the number up",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "ceil(x) - rounds the number up"
		},
		{
			"title": "century",
			"text": "century(timestamp) - extracts the century of a timestamp",
			"category": "Timestamp Functions",
			"url": "/docs/sql/functions/timestamp",
			"blurb": "century(timestamp) - Extracts the century of a timestamp"
		},
		{
			"title": "checkpoint",
			"text": "checkpoint(database) - synchronize wal with file for (optional) database without interrupting transactions.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "checkpoint(database) - Synchronize WAL with file for (optional) database without interrupting transactions."
		},
		{
			"title": "chr",
			"text": "chr(x) - returns a character which is corresponding the ascii code value or unicode code point",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "chr(x) - returns a character which is corresponding the ASCII code value or Unicode code point"
		},
		{
			"title": "coalesce",
			"text": "coalesce(expr, ...) - return the first expression that evaluates to a non- null value. accepts 1 or more parameters. each expression can be a column, literal value, function result, or many others.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "coalesce(expr, ...) - Return the first expression that evaluates to a non- NULL value. Accepts 1 or more parameters...."
		},
		{
			"title": "concat",
			"text": "concat(string, ...) - concatenate many strings together",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "concat(string, ...) - Concatenate many strings together"
		},
		{
			"title": "concat_ws",
			"text": "concat_ws(separator, string, ...) - concatenate strings together separated by the specified separator",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "concat_ws(separator, string, ...) - Concatenate strings together separated by the specified separator"
		},
		{
			"title": "contains",
			"text": "contains(string, search_string) - return true if search_string is found within string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "contains(string, search_string) - Return true if search_string is found within string"
		},
		{
			"title": "cos",
			"text": "cos(x) - computes the cosine of x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "cos(x) - computes the cosine of x"
		},
		{
			"title": "cot",
			"text": "cot(x) - computes the cotangent of x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "cot(x) - computes the cotangent of x"
		},
		{
			"title": "current_date",
			"text": "current_date - current date (at start of current transaction)",
			"category": "Date Functions",
			"url": "/docs/sql/functions/date",
			"blurb": "current_date - Current date (at start of current transaction)"
		},
		{
			"title": "current_localtimestamp",
			"text": "current_localtimestamp() - returns a timestamp whose gmt bin values correspond to local date and time in the current time zone.",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "current_localtimestamp() - Returns a TIMESTAMP whose GMT bin values correspond to local date and time in the current..."
		},
		{
			"title": "current_schema",
			"text": "current_schema() - return the name of the currently active schema. default is main.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "current_schema() - Return the name of the currently active schema. Default is main."
		},
		{
			"title": "current_schemas",
			"text": "current_schemas(boolean) - return list of schemas. pass a parameter of true to include implicit schemas.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "current_schemas(boolean) - Return list of schemas. Pass a parameter of True to include implicit schemas."
		},
		{
			"title": "current_setting",
			"text": "current_setting(setting_name) - return the current value of the configuration setting",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "current_setting(setting_name) - Return the current value of the configuration setting"
		},
		{
			"title": "current_time",
			"text": "current_time() - returns a time whose gmt bin values correspond to local time in the current time zone.",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "current_time() - Returns a TIME whose GMT bin values correspond to local time in the current time zone."
		},
		{
			"title": "current_time / get_current_time",
			"text": "current_time / get_current_time() - current time (start of current transaction)",
			"category": "Time Functions",
			"url": "/docs/sql/functions/time",
			"blurb": "current_time / get_current_time() - Current time (start of current transaction)"
		},
		{
			"title": "current_timestamp",
			"text": "current_timestamp - current date and time (start of current transaction)",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "current_timestamp - Current date and time (start of current transaction)"
		},
		{
			"title": "currval",
			"text": "currval(sequence_name) - return the current value of the sequence. note that nextval must be called at least once prior to calling currval.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "currval(sequence_name) - Return the current value of the sequence. Note that nextval must be called at least once..."
		},
		{
			"title": "damerau_levenshtein",
			"text": "damerau_levenshtein(string, string) - extension of levenshtein distance to also include transposition of adjacent characters as an allowed edit operation. in other words, the minimum number of edit operations (insertions, deletions, substitutions or transpositions) required to change one string to another. different case is considered different.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "damerau_levenshtein(string, string) - Extension of Levenshtein distance to also include transposition of adjacent..."
		},
		{
			"title": "date_diff",
			"text": "date_diff(part, startdate, enddate) - the number of partition boundaries between the timestamps",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "date_diff(part, startdate, enddate) - The number of partition boundaries between the timestamps"
		},
		{
			"title": "date_part",
			"text": "date_part([ part, ...], timestamptz) - get the listed subfields as a struct. the list must be constant.",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "date_part([ part, ...], timestamptz) - Get the listed subfields as a struct. The list must be constant."
		},
		{
			"title": "date_sub",
			"text": "date_sub(part, startdate, enddate) - the number of complete partitions between the timestamps",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "date_sub(part, startdate, enddate) - The number of complete partitions between the timestamps"
		},
		{
			"title": "date_trunc",
			"text": "date_trunc(part, timestamptz) - truncate to specified precision",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "date_trunc(part, timestamptz) - Truncate to specified precision"
		},
		{
			"title": "day",
			"text": "day(date) - day",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "day(date) - Day"
		},
		{
			"title": "dayname",
			"text": "dayname(timestamp) - the (english) name of the weekday",
			"category": "Timestamp Functions",
			"url": "/docs/sql/functions/timestamp",
			"blurb": "dayname(timestamp) - The (English) name of the weekday"
		},
		{
			"title": "dayofmonth",
			"text": "dayofmonth(date) - day (synonym)",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "dayofmonth(date) - Day (synonym)"
		},
		{
			"title": "dayofweek",
			"text": "dayofweek(date) - numeric weekday (sunday = 0, saturday = 6)",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "dayofweek(date) - Numeric weekday (Sunday = 0, Saturday = 6)"
		},
		{
			"title": "dayofyear",
			"text": "dayofyear(date) - numeric iso weekday (monday = 1, sunday = 7)",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "dayofyear(date) - Numeric ISO weekday (Monday = 1, Sunday = 7)"
		},
		{
			"title": "decade",
			"text": "decade(date) - decade (year / 10)",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "decade(date) - Decade (year / 10)"
		},
		{
			"title": "decode",
			"text": "decode(blob) - convert blob to varchar. fails if blob is not valid utf-8.",
			"category": "Blob Functions",
			"url": "/docs/sql/functions/blob",
			"blurb": "decode(blob) - Convert blob to varchar. Fails if blob is not valid utf-8."
		},
		{
			"title": "degrees",
			"text": "degrees(x) - converts radians to degrees",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "degrees(x) - converts radians to degrees"
		},
		{
			"title": "element_at",
			"text": "element_at(map, key) - return a list containing the value for a given key or an empty list if the key is not contained in the map. the type of the key provided in the second parameter must match the type of the map's keys else an error is returned.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "element_at(map, key) - Return a list containing the value for a given key or an empty list if the key is not..."
		},
		{
			"title": "encode",
			"text": "encode(string) - convert varchar to blob. converts utf-8 characters into literal encoding.",
			"category": "Blob Functions",
			"url": "/docs/sql/functions/blob",
			"blurb": "encode(string) - Convert varchar to blob. Converts utf-8 characters into literal encoding."
		},
		{
			"title": "enum_code",
			"text": "enum_code(enum_value) - returns the numeric value backing the given enum value",
			"category": "Enum Functions",
			"url": "/docs/sql/functions/enum",
			"blurb": "enum_code(enum_value) - Returns the numeric value backing the given enum value"
		},
		{
			"title": "enum_first",
			"text": "enum_first(enum) - returns the first value of the input enum type.",
			"category": "Enum Functions",
			"url": "/docs/sql/functions/enum",
			"blurb": "enum_first(enum) - Returns the first value of the input enum type."
		},
		{
			"title": "enum_last",
			"text": "enum_last(enum) - returns the last value of the input enum type.",
			"category": "Enum Functions",
			"url": "/docs/sql/functions/enum",
			"blurb": "enum_last(enum) - Returns the last value of the input enum type."
		},
		{
			"title": "enum_range",
			"text": "enum_range(enum) - returns all values of the input enum type as an array.",
			"category": "Enum Functions",
			"url": "/docs/sql/functions/enum",
			"blurb": "enum_range(enum) - Returns all values of the input enum type as an array."
		},
		{
			"title": "enum_range_boundary",
			"text": "enum_range_boundary(enum, enum) - returns the range between the two given enum values as an array. the values must be of the same enum type. when the first parameter is null , the result starts with the first value of the enum type. when the second parameter is null , the result ends with the last value of the enum type.",
			"category": "Enum Functions",
			"url": "/docs/sql/functions/enum",
			"blurb": "enum_range_boundary(enum, enum) - Returns the range between the two given enum values as an array. The values must be..."
		},
		{
			"title": "epoch",
			"text": "epoch(timestamp) - converts a timestamp to seconds since the epoch",
			"category": "Timestamp Functions",
			"url": "/docs/sql/functions/timestamp",
			"blurb": "epoch(timestamp) - Converts a timestamp to seconds since the epoch"
		},
		{
			"title": "epoch_ms",
			"text": "epoch_ms(timestamptz) - converts a timestamptz to milliseconds since the epoch",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "epoch_ms(timestamptz) - Converts a timestamptz to milliseconds since the epoch"
		},
		{
			"title": "epoch_ns",
			"text": "epoch_ns(timestamptz) - converts a timestamptz to nanoseconds since the epoch",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "epoch_ns(timestamptz) - Converts a timestamptz to nanoseconds since the epoch"
		},
		{
			"title": "epoch_us",
			"text": "epoch_us(timestamptz) - converts a timestamptz to microseconds since the epoch",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "epoch_us(timestamptz) - Converts a timestamptz to microseconds since the epoch"
		},
		{
			"title": "era",
			"text": "era(date) - calendar era",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "era(date) - Calendar era"
		},
		{
			"title": "even",
			"text": "even(x) - round to next even number by rounding away from zero.",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "even(x) - round to next even number by rounding away from zero."
		},
		{
			"title": "exp",
			"text": "exp(x) - computes e ** x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "exp(x) - computes e ** x"
		},
		{
			"title": "extract",
			"text": "extract(field from timestamptz) - get subfield from a timestamp with time zone",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "extract(field from timestamptz) - Get subfield from a timestamp with time zone"
		},
		{
			"title": "factorial",
			"text": "factorial(x) - see ! operator. computes the product of the current integer and all integers below it",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "factorial(x) - See ! operator. Computes the product of the current integer and all integers below it"
		},
		{
			"title": "flatten",
			"text": "flatten(list_of_lists) - concatenate a list of lists into a single list. this only flattens one level of the list (see examples ).",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "flatten(list_of_lists) - Concatenate a list of lists into a single list. This only flattens one level of the list..."
		},
		{
			"title": "floor",
			"text": "floor(x) - rounds the number down",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "floor(x) - rounds the number down"
		},
		{
			"title": "force_checkpoint",
			"text": "force_checkpoint(database) - synchronize wal with file for (optional) database interrupting transactions.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "force_checkpoint(database) - Synchronize WAL with file for (optional) database interrupting transactions."
		},
		{
			"title": "format",
			"text": "format(format, parameters ...) - formats a string using fmt syntax",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "format(format, parameters ...) - Formats a string using fmt syntax"
		},
		{
			"title": "from_base64",
			"text": "from_base64(string) - convert a base64 encoded string to a character string.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "from_base64(string) - Convert a base64 encoded string to a character string."
		},
		{
			"title": "gamma",
			"text": "gamma(x) - interpolation of (x-1) factorial (so decimal inputs are allowed)",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "gamma(x) - interpolation of (x-1) factorial (so decimal inputs are allowed)"
		},
		{
			"title": "gcd",
			"text": "gcd(x, y) - computes the greatest common divisor of x and y",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "gcd(x, y) - computes the greatest common divisor of x and y"
		},
		{
			"title": "get_bit",
			"text": "get_bit(bitstring, index) - extracts the nth bit from bitstring; the first (leftmost) bit is indexed 0.",
			"category": "Bitstring Functions",
			"url": "/docs/sql/functions/bitstring",
			"blurb": "get_bit(bitstring, index) - Extracts the nth bit from bitstring; the first (leftmost) bit is indexed 0."
		},
		{
			"title": "get_current_timestamp",
			"text": "get_current_timestamp() - current date and time (start of current transaction)",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "get_current_timestamp() - Current date and time (start of current transaction)"
		},
		{
			"title": "greatest",
			"text": "greatest(timestamptz, timestamptz) - the later of two timestamps",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "greatest(timestamptz, timestamptz) - The later of two timestamps"
		},
		{
			"title": "greatest_common_divisor",
			"text": "greatest_common_divisor(x, y) - computes the greatest common divisor of x and y",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "greatest_common_divisor(x, y) - computes the greatest common divisor of x and y"
		},
		{
			"title": "hamming",
			"text": "hamming(string, string) - the number of positions with different characters for 2 strings of equal length. different case is considered different.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "hamming(string, string) - The number of positions with different characters for 2 strings of equal length. Different..."
		},
		{
			"title": "hash",
			"text": "hash(value) - returns an integer with the hash of the value",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "hash(value) - Returns an integer with the hash of the value"
		},
		{
			"title": "hour",
			"text": "hour(date) - hours",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "hour(date) - Hours"
		},
		{
			"title": "icu_sort_key",
			"text": "icu_sort_key(string, collator) - surrogate key used to sort special characters according to the specific locale. collator parameter is optional. valid only when icu extension is installed.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "icu_sort_key(string, collator) - Surrogate key used to sort special characters according to the specific locale...."
		},
		{
			"title": "ifnull",
			"text": "ifnull(expr, other) - a two-argument version of coalesce",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "ifnull(expr, other) - A two-argument version of coalesce"
		},
		{
			"title": "instr",
			"text": "instr(string, search_string) - return location of first occurrence of search_string in string , counting from 1. returns 0 if no match found.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "instr(string, search_string) - Return location of first occurrence of search_string in string , counting from 1...."
		},
		{
			"title": "isfinite",
			"text": "isfinite(timestamptz) - returns true if the timestamp with time zone is finite, false otherwise",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "isfinite(timestamptz) - Returns true if the timestamp with time zone is finite, false otherwise"
		},
		{
			"title": "isinf",
			"text": "isinf(timestamptz) - returns true if the timestamp with time zone is infinite, false otherwise",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "isinf(timestamptz) - Returns true if the timestamp with time zone is infinite, false otherwise"
		},
		{
			"title": "isnan",
			"text": "isnan(x) - returns true if the floating point value is not a number, false otherwise",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "isnan(x) - Returns true if the floating point value is not a number, false otherwise"
		},
		{
			"title": "isodow",
			"text": "isodow(date) - numeric iso weekday (monday = 1, sunday = 7)",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "isodow(date) - Numeric ISO weekday (Monday = 1, Sunday = 7)"
		},
		{
			"title": "isoyear",
			"text": "isoyear(date) - iso year number (starts on monday of week containing jan 4th)",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "isoyear(date) - ISO Year number (Starts on Monday of week containing Jan 4th)"
		},
		{
			"title": "jaccard",
			"text": "jaccard(string, string) - the jaccard similarity between two strings. different case is considered different. returns a number between 0 and 1.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "jaccard(string, string) - The Jaccard similarity between two strings. Different case is considered different. Returns..."
		},
		{
			"title": "jaro_similarity",
			"text": "jaro_similarity(string, string) - the jaro similarity between two strings. different case is considered different. returns a number between 0 and 1.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "jaro_similarity(string, string) - The Jaro similarity between two strings. Different case is considered different...."
		},
		{
			"title": "jaro_winkler_similarity",
			"text": "jaro_winkler_similarity(string, string) - the jaro-winkler similarity between two strings. different case is considered different. returns a number between 0 and 1.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "jaro_winkler_similarity(string, string) - The Jaro-Winkler similarity between two strings. Different case is..."
		},
		{
			"title": "last_day",
			"text": "last_day(timestamptz) - the last day of the month.",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "last_day(timestamptz) - The last day of the month."
		},
		{
			"title": "lcm",
			"text": "lcm(x, y) - computes the least common multiple of x and y",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "lcm(x, y) - computes the least common multiple of x and y"
		},
		{
			"title": "least",
			"text": "least(timestamptz, timestamptz) - the earlier of two timestamps",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "least(timestamptz, timestamptz) - The earlier of two timestamps"
		},
		{
			"title": "least_common_multiple",
			"text": "least_common_multiple(x, y) - computes the least common multiple of x and y",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "least_common_multiple(x, y) - computes the least common multiple of x and y"
		},
		{
			"title": "left",
			"text": "left(string, count) - extract the left-most count characters",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "left(string, count) - Extract the left-most count characters"
		},
		{
			"title": "left_grapheme",
			"text": "left_grapheme(string, count) - extract the left-most grapheme clusters",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "left_grapheme(string, count) - Extract the left-most grapheme clusters"
		},
		{
			"title": "len",
			"text": "len(list) - return the length of the list.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "len(list) - Return the length of the list."
		},
		{
			"title": "length",
			"text": "length(string) - number of characters in string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "length(string) - Number of characters in string"
		},
		{
			"title": "length_grapheme",
			"text": "length_grapheme(string) - number of grapheme clusters in string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "length_grapheme(string) - Number of grapheme clusters in string"
		},
		{
			"title": "levenshtein",
			"text": "levenshtein(string, string) - the minimum number of single-character edits (insertions, deletions or substitutions) required to change one string to the other. different case is considered different.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "levenshtein(string, string) - The minimum number of single-character edits (insertions, deletions or substitutions)..."
		},
		{
			"title": "lgamma",
			"text": "lgamma(x) - computes the log of the gamma function.",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "lgamma(x) - computes the log of the gamma function."
		},
		{
			"title": "like_escape",
			"text": "like_escape(string, like_specifier, escape_character) - returns true if the string matches the like_specifier (see pattern matching ). escape_character is used to search for wildcard characters in the string.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "like_escape(string, like_specifier, escape_character) - Returns true if the string matches the like_specifier (see..."
		},
		{
			"title": "list_aggregate",
			"text": "list_aggregate(list, name) - executes the aggregate function name on the elements of list. see the list aggregates section for more details.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_aggregate(list, name) - Executes the aggregate function name on the elements of list. See the List Aggregates..."
		},
		{
			"title": "list_any_value",
			"text": "list_any_value(list) - returns the first non-null value in the list",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_any_value(list) - Returns the first non-null value in the list"
		},
		{
			"title": "list_append",
			"text": "list_append(list, element) - appends element to list.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_append(list, element) - Appends element to list."
		},
		{
			"title": "list_concat",
			"text": "list_concat(list1, list2) - concatenates two lists.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_concat(list1, list2) - Concatenates two lists."
		},
		{
			"title": "list_contains",
			"text": "list_contains(list, element) - returns true if the list contains the element.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_contains(list, element) - Returns true if the list contains the element."
		},
		{
			"title": "list_distinct",
			"text": "list_distinct(list) - removes all duplicates and nulls from a list. does not preserve the original order.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_distinct(list) - Removes all duplicates and NULLs from a list. Does not preserve the original order."
		},
		{
			"title": "list_extract",
			"text": "list_extract(list, index) - extract the index th (1-based) value from the list.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_extract(list, index) - Extract the index th (1-based) value from the list."
		},
		{
			"title": "list_filter",
			"text": "list_filter(list, lambda) - constructs a list from those elements of the input list for which the lambda function returns true. see the lambda functions section for more details.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_filter(list, lambda) - Constructs a list from those elements of the input list for which the lambda function..."
		},
		{
			"title": "list_position",
			"text": "list_position(list, element) - returns the index of the element if the list contains the element.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_position(list, element) - Returns the index of the element if the list contains the element."
		},
		{
			"title": "list_prepend",
			"text": "list_prepend(element, list) - prepends element to list.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_prepend(element, list) - Prepends element to list."
		},
		{
			"title": "list_resize",
			"text": "list_resize(list, size [, value ]) - resizes the list to contain size elements. initializes new elements with value or null if value is not set.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_resize(list, size [, value ]) - Resizes the list to contain size elements. Initializes new elements with value..."
		},
		{
			"title": "list_reverse_sort",
			"text": "list_reverse_sort(list) - sorts the elements of the list in reverse order. see the sorting lists section for more details about the null sorting order.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_reverse_sort(list) - Sorts the elements of the list in reverse order. See the Sorting Lists section for more..."
		},
		{
			"title": "list_slice",
			"text": "list_slice(list, begin, end) - extract a sublist using slice conventions. null s are interpreted as the bounds of the list. negative values are accepted.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_slice(list, begin, end) - Extract a sublist using slice conventions. NULL s are interpreted as the bounds of the..."
		},
		{
			"title": "list_sort",
			"text": "list_sort(list) - sorts the elements of the list. see the sorting lists section for more details about the sorting order and the null sorting order.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_sort(list) - Sorts the elements of the list. See the Sorting Lists section for more details about the sorting..."
		},
		{
			"title": "list_transform",
			"text": "list_transform(list, lambda) - returns a list that is the result of applying the lambda function to each element of the input list. see the lambda functions section for more details.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_transform(list, lambda) - Returns a list that is the result of applying the lambda function to each element of..."
		},
		{
			"title": "list_unique",
			"text": "list_unique(list) - counts the unique elements of a list.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_unique(list) - Counts the unique elements of a list."
		},
		{
			"title": "list_value",
			"text": "list_value(any, ...) - create a list containing the argument values.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "list_value(any, ...) - Create a LIST containing the argument values."
		},
		{
			"title": "ln",
			"text": "ln(x) - computes the natural logarithm of x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "ln(x) - computes the natural logarithm of x"
		},
		{
			"title": "localtime",
			"text": "localtime - synonym for the current_time() function call.",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "localtime - Synonym for the current_time() function call."
		},
		{
			"title": "localtimestamp",
			"text": "localtimestamp - synonym for the current_localtimestamp() function call.",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "localtimestamp - Synonym for the current_localtimestamp() function call."
		},
		{
			"title": "log",
			"text": "log(x) - computes the 10-log of x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "log(x) - computes the 10-log of x"
		},
		{
			"title": "log2",
			"text": "log2(x) - computes the 2-log of x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "log2(x) - computes the 2-log of x"
		},
		{
			"title": "lower",
			"text": "lower(string) - convert string to lower case",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "lower(string) - Convert string to lower case"
		},
		{
			"title": "lpad",
			"text": "lpad(string, count, character) - pads the string with the character from the left until it has count characters",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "lpad(string, count, character) - Pads the string with the character from the left until it has count characters"
		},
		{
			"title": "ltrim",
			"text": "ltrim(string, characters) - removes any occurrences of any of the characters from the left side of the string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "ltrim(string, characters) - Removes any occurrences of any of the characters from the left side of the string"
		},
		{
			"title": "make_date",
			"text": "make_date(bigint, bigint, bigint) - the date for the given parts",
			"category": "Date Functions",
			"url": "/docs/sql/functions/date",
			"blurb": "make_date(bigint, bigint, bigint) - The date for the given parts"
		},
		{
			"title": "make_time",
			"text": "make_time(bigint, bigint, double) - the time for the given parts",
			"category": "Time Functions",
			"url": "/docs/sql/functions/time",
			"blurb": "make_time(bigint, bigint, double) - The time for the given parts"
		},
		{
			"title": "make_timestamp",
			"text": "make_timestamp(microseconds) - the timestamp for the given number of \u00b5s since the epoch",
			"category": "Timestamp Functions",
			"url": "/docs/sql/functions/timestamp",
			"blurb": "make_timestamp(microseconds) - The timestamp for the given number of \u00b5s since the epoch"
		},
		{
			"title": "make_timestamptz",
			"text": "make_timestamptz(bigint, bigint, bigint, bigint, bigint, double, string) - the timestamp with time zone for the given parts and time zone",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "make_timestamptz(bigint, bigint, bigint, bigint, bigint, double, string) - The timestamp with time zone for the given..."
		},
		{
			"title": "map",
			"text": "map() - returns an empty map.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "map() - Returns an empty map."
		},
		{
			"title": "map_entries",
			"text": "map_entries(map) - return a list of struct(k, v) for each key-value pair in the map.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "map_entries(map) - Return a list of struct(k, v) for each key-value pair in the map."
		},
		{
			"title": "map_from_entries[])",
			"text": "map_from_entries(struct(k, v)[]) - returns a map created from the entries of the array",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "map_from_entries(STRUCT(k, v)[]) - Returns a map created from the entries of the array"
		},
		{
			"title": "map_keys",
			"text": "map_keys(map) - return a list of all keys in the map.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "map_keys(map) - Return a list of all keys in the map."
		},
		{
			"title": "map_values",
			"text": "map_values(map) - return a list of all values in the map.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "map_values(map) - Return a list of all values in the map."
		},
		{
			"title": "md5",
			"text": "md5(string) - return an md5 one-way hash of the string.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "md5(string) - Return an md5 one-way hash of the string."
		},
		{
			"title": "microsecond",
			"text": "microsecond(date) - sub-minute microseconds",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "microsecond(date) - Sub-minute microseconds"
		},
		{
			"title": "microseconds",
			"text": "microseconds - sub-minute microseconds",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "microseconds - Sub-minute microseconds"
		},
		{
			"title": "millennium",
			"text": "millennium(date) - millennium",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "millennium(date) - Millennium"
		},
		{
			"title": "millisecond",
			"text": "millisecond(date) - sub-minute milliseconds",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "millisecond(date) - Sub-minute milliseconds"
		},
		{
			"title": "milliseconds",
			"text": "milliseconds - sub-minute milliseconds",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "milliseconds - Sub-minute milliseconds"
		},
		{
			"title": "minute",
			"text": "minute(date) - minutes",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "minute(date) - Minutes"
		},
		{
			"title": "mismatches",
			"text": "mismatches(string, string) - the number of positions with different characters for 2 strings of equal length. different case is considered different.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "mismatches(string, string) - The number of positions with different characters for 2 strings of equal length...."
		},
		{
			"title": "month",
			"text": "month(date) - month",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "month(date) - Month"
		},
		{
			"title": "monthname",
			"text": "monthname(timestamp) - the (english) name of the month.",
			"category": "Timestamp Functions",
			"url": "/docs/sql/functions/timestamp",
			"blurb": "monthname(timestamp) - The (English) name of the month."
		},
		{
			"title": "nextafter",
			"text": "nextafter(x, y) - return the next floating point value after x in the direction of y",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "nextafter(x, y) - return the next floating point value after x in the direction of y"
		},
		{
			"title": "nextval",
			"text": "nextval(sequence_name) - return the following value of the sequence.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "nextval(sequence_name) - Return the following value of the sequence."
		},
		{
			"title": "nfc_normalize",
			"text": "nfc_normalize(string) - convert string to unicode nfc normalized string. useful for comparisons and ordering if text data is mixed between nfc normalized and not.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "nfc_normalize(string) - Convert string to Unicode NFC normalized string. Useful for comparisons and ordering if text..."
		},
		{
			"title": "not_like_escape",
			"text": "not_like_escape(string, like_specifier, escape_character) - returns false if the string matches the like_specifier (see pattern matching ). escape_character is used to search for wildcard characters in the string.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "not_like_escape(string, like_specifier, escape_character) - Returns false if the string matches the like_specifier..."
		},
		{
			"title": "now",
			"text": "now() - current date and time (start of current transaction)",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "now() - Current date and time (start of current transaction)"
		},
		{
			"title": "nullif",
			"text": "nullif(a, b) - return null if a = b, else return a. equivalent to case when a=b then null else a end.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "nullif(a, b) - Return null if a = b, else return a. Equivalent to CASE WHEN a=b THEN NULL ELSE a END."
		},
		{
			"title": "octet_length",
			"text": "octet_length(blob) - number of bytes in blob",
			"category": "Blob Functions",
			"url": "/docs/sql/functions/blob",
			"blurb": "octet_length(blob) - Number of bytes in blob"
		},
		{
			"title": "ord",
			"text": "ord(string) - return ascii character code of the leftmost character in a string.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "ord(string) - Return ASCII character code of the leftmost character in a string."
		},
		{
			"title": "pg_typeof",
			"text": "pg_typeof(expression) - returns the lower case name of the data type of the result of the expression. for postgres compatibility.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "pg_typeof(expression) - Returns the lower case name of the data type of the result of the expression. For Postgres..."
		},
		{
			"title": "pi",
			"text": "pi() - returns the value of pi",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "pi() - returns the value of pi"
		},
		{
			"title": "position",
			"text": "position(search_string in string) - return location of first occurrence of search_string in string , counting from 1. returns 0 if no match found.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "position(search_string in string) - Return location of first occurrence of search_string in string , counting from 1...."
		},
		{
			"title": "pow",
			"text": "pow(x, y) - computes x to the power of y",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "pow(x, y) - computes x to the power of y"
		},
		{
			"title": "prefix",
			"text": "prefix(string, search_string) - return true if string starts with search_string.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "prefix(string, search_string) - Return true if string starts with search_string."
		},
		{
			"title": "printf",
			"text": "printf(format, parameters ...) - formats a string using printf syntax",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "printf(format, parameters ...) - Formats a string using printf syntax"
		},
		{
			"title": "quarter",
			"text": "quarter(date) - quarter",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "quarter(date) - Quarter"
		},
		{
			"title": "radians",
			"text": "radians(x) - converts degrees to radians",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "radians(x) - converts degrees to radians"
		},
		{
			"title": "random",
			"text": "random() - returns a random number between 0 and 1",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "random() - returns a random number between 0 and 1"
		},
		{
			"title": "regexp_extract",
			"text": "regexp_extract(string, regex [, group = 0]) - split the string along the regex and extract first occurrence of group",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "regexp_extract(string, regex [, group = 0]) - Split the string along the regex and extract first occurrence of group"
		},
		{
			"title": "regexp_extract ;",
			"text": "regexp_extract(string, pattern, name_list) ; - if string contains the regexp pattern , returns the capturing groups as a struct with corresponding names from name_list",
			"category": "Patternmatching Functions",
			"url": "/docs/sql/functions/patternmatching",
			"blurb": "regexp_extract(string, pattern, name_list) ; - if string contains the regexp pattern , returns the capturing groups..."
		},
		{
			"title": "regexp_extract_all",
			"text": "regexp_extract_all(string, regex [, group = 0]) - split the string along the regex and extract all occurrences of group",
			"category": "Patternmatching Functions",
			"url": "/docs/sql/functions/patternmatching",
			"blurb": "regexp_extract_all(string, regex [, group = 0]) - Split the string along the regex and extract all occurrences of group"
		},
		{
			"title": "regexp_full_match",
			"text": "regexp_full_match(string, regex) - returns true if the entire string matches the regex",
			"category": "Patternmatching Functions",
			"url": "/docs/sql/functions/patternmatching",
			"blurb": "regexp_full_match(string, regex) - Returns true if the entire string matches the regex"
		},
		{
			"title": "regexp_matches",
			"text": "regexp_matches(string, pattern) - returns true if string contains the regexp pattern , false otherwise",
			"category": "Patternmatching Functions",
			"url": "/docs/sql/functions/patternmatching",
			"blurb": "regexp_matches(string, pattern) - returns TRUE if string contains the regexp pattern , FALSE otherwise"
		},
		{
			"title": "regexp_replace",
			"text": "regexp_replace(string, regex, replacement, modifiers) - replaces the first occurrence of regex with the replacement , use 'g' modifier to replace all occurrences instead (see pattern matching )",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "regexp_replace(string, regex, replacement, modifiers) - Replaces the first occurrence of regex with the replacement ,..."
		},
		{
			"title": "regexp_replace ;",
			"text": "regexp_replace(string, pattern, replacement) ; - if string contains the regexp pattern , replaces the matching part with replacement",
			"category": "Patternmatching Functions",
			"url": "/docs/sql/functions/patternmatching",
			"blurb": "regexp_replace(string, pattern, replacement) ; - if string contains the regexp pattern , replaces the matching part..."
		},
		{
			"title": "repeat",
			"text": "repeat(string, count) - repeats the string count number of times",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "repeat(string, count) - Repeats the string count number of times"
		},
		{
			"title": "replace",
			"text": "replace(string, source, target) - replaces any occurrences of the source with target in string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "replace(string, source, target) - Replaces any occurrences of the source with target in string"
		},
		{
			"title": "reverse",
			"text": "reverse(string) - reverses the string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "reverse(string) - Reverses the string"
		},
		{
			"title": "right",
			"text": "right(string, count) - extract the right-most count characters",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "right(string, count) - Extract the right-most count characters"
		},
		{
			"title": "right_grapheme",
			"text": "right_grapheme(string, count) - extract the right-most count grapheme clusters",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "right_grapheme(string, count) - Extract the right-most count grapheme clusters"
		},
		{
			"title": "round",
			"text": "round(v numeric, s int) - round to s decimal places, values s < 0 are allowed",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "round(v numeric, s int) - round to s decimal places, values s < 0 are allowed"
		},
		{
			"title": "row",
			"text": "row(any, ...) - create a struct containing the argument values. if the values are column references, the entry name will be the column name; otherwise it will be the string 'vn' where n is the (1-based) position of the argument.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "row(any, ...) - Create a STRUCT containing the argument values. If the values are column references, the entry name..."
		},
		{
			"title": "rpad",
			"text": "rpad(string, count, character) - pads the string with the character from the right until it has count characters",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "rpad(string, count, character) - Pads the string with the character from the right until it has count characters"
		},
		{
			"title": "rtrim",
			"text": "rtrim(string, characters) - removes any occurrences of any of the characters from the right side of the string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "rtrim(string, characters) - Removes any occurrences of any of the characters from the right side of the string"
		},
		{
			"title": "second",
			"text": "second(date) - seconds",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "second(date) - Seconds"
		},
		{
			"title": "set_bit",
			"text": "set_bit(bitstring, index, new_value) - sets the nth bit in bitstring to newvalue; the first (leftmost) bit is indexed 0. returns a new bitstring.",
			"category": "Bitstring Functions",
			"url": "/docs/sql/functions/bitstring",
			"blurb": "set_bit(bitstring, index, new_value) - Sets the nth bit in bitstring to newvalue; the first (leftmost) bit is indexed..."
		},
		{
			"title": "setseed",
			"text": "setseed(x) - sets the seed to be used for the random function",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "setseed(x) - sets the seed to be used for the random function"
		},
		{
			"title": "sign",
			"text": "sign(x) - returns the sign of x as -1, 0 or 1",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "sign(x) - returns the sign of x as -1, 0 or 1"
		},
		{
			"title": "signbit",
			"text": "signbit(x) - returns whether the signbit is set or not",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "signbit(x) - returns whether the signbit is set or not"
		},
		{
			"title": "sin",
			"text": "sin(x) - computes the sin of x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "sin(x) - computes the sin of x"
		},
		{
			"title": "split_part",
			"text": "split_part(string, separator, index) - split the string along the separator and return the data at the (1-based) index of the list. if the index is outside the bounds of the list, return an empty string (to match postgres behavior).",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "split_part(string, separator, index) - Split the string along the separator and return the data at the (1-based)..."
		},
		{
			"title": "sqrt",
			"text": "sqrt(x) - returns the square root of the number",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "sqrt(x) - returns the square root of the number"
		},
		{
			"title": "starts_with",
			"text": "starts_with(string, search_string) - return true if string begins with search_string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "starts_with(string, search_string) - Return true if string begins with search_string"
		},
		{
			"title": "stats",
			"text": "stats(expression) - returns a string with statistics about the expression. expression can be a column, constant, or sql expression.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "stats(expression) - Returns a string with statistics about the expression. Expression can be a column, constant, or..."
		},
		{
			"title": "strftime",
			"text": "strftime(timestamptz, format) - converts timestamp with time zone to string according to the format string",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "strftime(timestamptz, format) - Converts timestamp with time zone to string according to the format string"
		},
		{
			"title": "string LIKE target",
			"text": "string like target - returns true if the string matches the like specifier (see pattern matching )",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "string LIKE target - Returns true if the string matches the like specifier (see Pattern Matching )"
		},
		{
			"title": "string SIMILAR TO regex",
			"text": "string similar to regex - returns true if the string matches the regex ; identical to regexp_full_match (see pattern matching )",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "string SIMILAR TO regex - Returns true if the string matches the regex ; identical to regexp_full_match (see Pattern..."
		},
		{
			"title": "string_split",
			"text": "string_split(string, separator) - splits the string along the separator",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "string_split(string, separator) - Splits the string along the separator"
		},
		{
			"title": "string_split_regex",
			"text": "string_split_regex(string, regex) - splits the string along the regex",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "string_split_regex(string, regex) - Splits the string along the regex"
		},
		{
			"title": "strip_accents",
			"text": "strip_accents(string) - strips accents from string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "strip_accents(string) - Strips accents from string"
		},
		{
			"title": "strlen",
			"text": "strlen(string) - number of bytes in string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "strlen(string) - Number of bytes in string"
		},
		{
			"title": "strptime",
			"text": "strptime(text, format) - converts string to timestamp with time zone according to the format string if %z is specified.",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "strptime(text, format) - Converts string to timestamp with time zone according to the format string if %Z is specified."
		},
		{
			"title": "struct_extract",
			"text": "struct_extract(struct, entry) - extract the named entry from the struct.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "struct_extract(struct, entry) - Extract the named entry from the struct."
		},
		{
			"title": "struct_insert",
			"text": "struct_insert(struct, name := any, ...) - add field(s)/value(s) to an existing struct with the argument values. the entry name(s) will be the bound variable name(s).",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "struct_insert(struct, name := any, ...) - Add field(s)/value(s) to an existing STRUCT with the argument values. The..."
		},
		{
			"title": "struct_pack",
			"text": "struct_pack(name := any, ...) - create a struct containing the argument values. the entry name will be the bound variable name.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "struct_pack(name := any, ...) - Create a STRUCT containing the argument values. The entry name will be the bound..."
		},
		{
			"title": "substring",
			"text": "substring(string, start, length) - extract substring of length characters starting from character start. note that a start value of 1 refers to the first character of the string.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "substring(string, start, length) - Extract substring of length characters starting from character start. Note that a..."
		},
		{
			"title": "substring_grapheme",
			"text": "substring_grapheme(string, start, length) - extract substring of length grapheme clusters starting from character start. note that a start value of 1 refers to the first character of the string.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "substring_grapheme(string, start, length) - Extract substring of length grapheme clusters starting from character..."
		},
		{
			"title": "suffix",
			"text": "suffix(string, search_string) - return true if string ends with search_string.",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "suffix(string, search_string) - Return true if string ends with search_string."
		},
		{
			"title": "tan",
			"text": "tan(x) - computes the tangent of x",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "tan(x) - computes the tangent of x"
		},
		{
			"title": "time_bucket",
			"text": "time_bucket(bucket_width, timestamptz [, timezone ]) - truncate timestamptz by the specified interval bucket_width. bucket starts and ends are calculated using timezone. timezone is a varchar and defaults to utc.",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "time_bucket(bucket_width, timestamptz [, timezone ]) - Truncate timestamptz by the specified interval bucket_width...."
		},
		{
			"title": "timezone",
			"text": "timezone(text, timestamptz) - use the date parts of the timestamp in the given time zone to construct a timestamp. effectively, the result is a \"local\" time.",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "timezone(text, timestamptz) - Use the date parts of the timestamp in the given time zone to construct a timestamp...."
		},
		{
			"title": "timezone_hour",
			"text": "timezone_hour(date) - time zone offset hour portion",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "timezone_hour(date) - Time zone offset hour portion"
		},
		{
			"title": "timezone_minute",
			"text": "timezone_minute(date) - time zone offset minutes portion",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "timezone_minute(date) - Time zone offset minutes portion"
		},
		{
			"title": "to_days",
			"text": "to_days(integer) - construct a day interval",
			"category": "Interval Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "to_days(integer) - Construct a day interval"
		},
		{
			"title": "to_hours",
			"text": "to_hours(integer) - construct a hour interval",
			"category": "Interval Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "to_hours(integer) - Construct a hour interval"
		},
		{
			"title": "to_microseconds",
			"text": "to_microseconds(integer) - construct a microsecond interval",
			"category": "Interval Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "to_microseconds(integer) - Construct a microsecond interval"
		},
		{
			"title": "to_milliseconds",
			"text": "to_milliseconds(integer) - construct a millisecond interval",
			"category": "Interval Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "to_milliseconds(integer) - Construct a millisecond interval"
		},
		{
			"title": "to_minutes",
			"text": "to_minutes(integer) - construct a minute interval",
			"category": "Interval Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "to_minutes(integer) - Construct a minute interval"
		},
		{
			"title": "to_months",
			"text": "to_months(integer) - construct a month interval",
			"category": "Interval Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "to_months(integer) - Construct a month interval"
		},
		{
			"title": "to_seconds",
			"text": "to_seconds(integer) - construct a second interval",
			"category": "Interval Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "to_seconds(integer) - Construct a second interval"
		},
		{
			"title": "to_timestamp",
			"text": "to_timestamp(sec) - converts sec since epoch to a timestamp with time zone",
			"category": "Timestamp Functions",
			"url": "/docs/sql/functions/timestamp",
			"blurb": "to_timestamp(sec) - Converts sec since epoch to a timestamp with time zone"
		},
		{
			"title": "to_years",
			"text": "to_years(integer) - construct a year interval",
			"category": "Interval Functions",
			"url": "/docs/sql/functions/interval",
			"blurb": "to_years(integer) - Construct a year interval"
		},
		{
			"title": "today",
			"text": "today() - current date (start of current transaction)",
			"category": "Date Functions",
			"url": "/docs/sql/functions/date",
			"blurb": "today() - Current date (start of current transaction)"
		},
		{
			"title": "transaction_timestamp",
			"text": "transaction_timestamp() - current date and time (start of current transaction)",
			"category": "Timestamptz Functions",
			"url": "/docs/sql/functions/timestamptz",
			"blurb": "transaction_timestamp() - Current date and time (start of current transaction)"
		},
		{
			"title": "trim",
			"text": "trim(string, characters) - removes any occurrences of any of the characters from either side of the string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "trim(string, characters) - Removes any occurrences of any of the characters from either side of the string"
		},
		{
			"title": "try_strptime",
			"text": "try_strptime(text, format-list) - converts string to timestamp applying the format strings in the list until one succeeds. returns null on failure.",
			"category": "Timestamp Functions",
			"url": "/docs/sql/functions/timestamp",
			"blurb": "try_strptime(text, format-list) - Converts string to timestamp applying the format strings in the list until one..."
		},
		{
			"title": "txid_current",
			"text": "txid_current() - returns the current transaction's id (a bigint ). it will assign a new one if the current transaction does not have one already.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "txid_current() - Returns the current transaction's ID (a BIGINT ). It will assign a new one if the current..."
		},
		{
			"title": "typeof",
			"text": "typeof(expression) - returns the name of the data type of the result of the expression.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "typeof(expression) - Returns the name of the data type of the result of the expression."
		},
		{
			"title": "unicode",
			"text": "unicode(string) - returns the unicode code of the first character of the string",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "unicode(string) - Returns the unicode code of the first character of the string"
		},
		{
			"title": "union_extract",
			"text": "union_extract(union, tag) - extract the value with the named tags from the union. null if the tag is not currently selected",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "union_extract(union, tag) - Extract the value with the named tags from the union. NULL if the tag is not currently..."
		},
		{
			"title": "union_tag",
			"text": "union_tag(union) - retrieve the currently selected tag of the union as an enum.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "union_tag(union) - Retrieve the currently selected tag of the union as an Enum."
		},
		{
			"title": "union_value",
			"text": "union_value(tag := any) - create a single member union containing the argument value. the tag of the value will be the bound variable name.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "union_value(tag := any) - Create a single member UNION containing the argument value. The tag of the value will be..."
		},
		{
			"title": "unnest",
			"text": "unnest(list) - unnests a list by one level. note that this is a special function that alters the cardinality of the result. see the unnest page for more details.",
			"category": "Nested Functions",
			"url": "/docs/sql/functions/nested",
			"blurb": "unnest(list) - Unnests a list by one level. Note that this is a special function that alters the cardinality of the..."
		},
		{
			"title": "upper",
			"text": "upper(string) - convert string to upper case",
			"category": "Char Functions",
			"url": "/docs/sql/functions/char",
			"blurb": "upper(string) - Convert string to upper case"
		},
		{
			"title": "uuid",
			"text": "uuid() - return a random uuid similar to this: eeccb8c5-9943-b2bb-bb5e-222f4e14b687.",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "uuid() - Return a random uuid similar to this: eeccb8c5-9943-b2bb-bb5e-222f4e14b687."
		},
		{
			"title": "version",
			"text": "version() - return the currently active version of duckdb in this format: v0.3.2",
			"category": "Utility Functions",
			"url": "/docs/sql/functions/utility",
			"blurb": "version() - Return the currently active version of DuckDB in this format: v0.3.2"
		},
		{
			"title": "week",
			"text": "week(date) - iso week",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "week(date) - ISO Week"
		},
		{
			"title": "weekday",
			"text": "weekday(date) - numeric weekday synonym (sunday = 0, saturday = 6)",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "weekday(date) - Numeric weekday synonym (Sunday = 0, Saturday = 6)"
		},
		{
			"title": "weekofyear",
			"text": "weekofyear(date) - iso week (synonym)",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "weekofyear(date) - ISO Week (synonym)"
		},
		{
			"title": "xor",
			"text": "xor(x) - bitwise xor",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "xor(x) - bitwise XOR"
		},
		{
			"title": "year",
			"text": "year(date) - year",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "year(date) - Year"
		},
		{
			"title": "yearweek",
			"text": "yearweek(date) - bigint of combined iso year number and 2-digit version of iso week number",
			"category": "Datepart Functions",
			"url": "/docs/sql/functions/datepart",
			"blurb": "yearweek(date) - BIGINT of combined ISO Year number and 2-digit version of ISO Week number"
		},
		{
			"title": "~",
			"text": "~ - bitwise negation",
			"category": "Numeric Functions",
			"url": "/docs/sql/functions/numeric",
			"blurb": "~ - bitwise negation"
		}
	]
}